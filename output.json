{
  "team_id": "aline123",
  "items": [
    {
      "title": "Resumes suck. Here's the data.",
      "content": "*Note: This post is syndicated from [Aline Lerner’s personal blog](https://blog.alinelerner.com). Aline is the CEO and co-founder of interviewing.io, and results like these are what inspired her to start this company.*\n\nAbout a year ago, after looking at the resumes of engineers we had interviewed at TrialPay in 2012, I learned that the strongest signal for whether someone would get an offer was the [number of typos and grammatical errors on their resume](https://interviewing.io/blog/lessons-from-a-years-worth-of-hiring-data). On the other hand, where people went to school, their GPA, and highest degree earned didn’t matter at all. These results were pretty unexpected, ran counter to how resumes were normally filtered, and left me scratching my head about how good people are at making value judgments based on resumes, period. So, I decided to run an experiment.\n\nIn this experiment, I wanted to see how good engineers and recruiters were at resume-based candidate filtering. **Going into it, I was pretty sure that engineers would do a much better job than recruiters.** (They are technical! They don’t need to rely on proxies as much!) However, that’s not what happened at all. **As it turned out, people were pretty bad at filtering resumes across the board, and after running the numbers, it began to look like resumes might not be a particularly effective filtering tool in the first place.**\n\nSetup\n-----\n\nThe setup was simple. I would:\n\n1. Take resumes from my collection.\n2. Remove all personally identifying info (name, contact info, dates, etc.).\n3. Show them to a bunch of recruiters and engineers.\n4. For each resume, ask just one question: *Would you interview this candidate?*\n\n![Screenshot of a resume filtering form](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fsample_stimulus3_b5495fe25f.webp%3Fupdated_at%3D2022-11-21T20%3A18%3A21.787Z&w=1080&q=75 \"Resume filtering\")\n\nEssentially, each participant saw something like this:\n\nIf the participant didn’t want to interview the candidate, they’d have to write a few words about why. If they did want to interview, they also had the option of substantiating their decision, but, in the interest of not fatiguing participants, I didn’t require it.\n\nTo make judging easier, I told participants to pretend that they were hiring for a full-stack or back-end web dev role, as appropriate. I also told participants not to worry too much about the candidate’s seniority when making judgments and to assume that the seniority of the role matched the seniority of the candidate.\n\n**For each resume, I had a pretty good idea of how strong the engineer in question was, and I split resumes into two strength-based groups**. To make this judgment call, I drew on my personal experience — most of the resumes came from candidates I placed (or tried to place) at top-tier startups. In these cases, I knew exactly how the engineer had done in technical interviews, and, more often than not, I had visibility into how they performed on the job afterwards. The remainder of resumes came from engineers I had worked with directly. **The question was whether the participants in this experiment could figure out who was who just from the resume.**\n\nAt this juncture, a disclaimer is in order. Certainly, someone’s subjective hirability based on the experience of one recruiter is not an oracle of engineering ability — with the advent of more data and more rigorous analysis, perhaps these results will be proven untrue. But, you gotta start somewhere. That said, here’s the experiment by the numbers.\n\n* I used a total of **51 resumes** in this study. 64% belonged to strong candidates.\n* A total of **152 people** participated in the experiment.\n* **Each participant made judgments on 6 randomly selected resumes** from the original set of 51, for a total of **716 data points**.[1](#user-content-fn-1)\n\nIf you want to take the experiment for a whirl yourself, you can do so [here](https://qtrial2014.az1.qualtrics.com/SE/?SID=SV_25Gmmef9ktW34A5).\n\nParticipants were broken up into engineers (both engineers involved in hiring and hiring managers themselves) and recruiters (both in-house and agency). There were 46 recruiters (22 in-house and 24 agency) and 106 engineers (20 hiring managers and 86 non-manager engineers who were still involved in hiring).\n\nResults\n-------\n\nSo, what ended up happening? Below, you can see a comparison of resume scores for both groups of candidates. A resume score is the average of all the votes each resume got, where a ‘no’ counted as 0 and a ‘yes’ vote counted as 1. The dotted line in each box is the mean for each resume group — you can see they’re pretty much the same. The solid line is the median, and the boxes contain the 2nd and 3rd quartiles on either side of it. **As you can see, people weren’t very good at this task — what’s pretty alarming is that scores are all over the place, for both strong and less strong candidates.**\n\nAnother way to look at the data is to look at the distribution of accuracy scores. *Accuracy* in this context refers to how many resumes people were able to tag correctly out of the subset of 6 that they saw. As you can see, results were all over the board.\n\n**On average, participants guessed correctly 53% of the time.** This was pretty surprising, and at the risk of being glib, **according to these results, when a good chunk of people involved in hiring make resume judgments, they might as well be flipping a coin**.\n\n![Cartoon coin toss](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Facb35_tie_coin_toss_1_68afd9383e.webp&w=640&q=75 \"A coin toss\")\n\nSource: <https://what-if.xkcd.com/19/>\n\nWhat about performance broken down by participant group? Here’s the breakdown:\n\n* Agency recruiters – 56%\n* Engineers – 54%\n* In-house recruiters – 52%\n* Eng hiring managers – 48%\n\nNone of the differences between participant groups were statistically significant. In other words, all groups did equally poorly. For each group, you can see how well people did below.\n\n![Charts showing the accuracy of each group](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Faccuracy_charts_8792e468a0.webp%3Fupdated_at%3D2022-11-21T20%3A22%3A06.161Z&w=1920&q=75 \"Accuracy of various groups\")\n\nTo try to understand whether people really were this bad at the task or whether perhaps the task itself was flawed, I ran some more stats. One thing I wanted to understand, in particular, was whether *inter-rater agreement* was high. In other words, when rating resumes, were participants disagreeing with each other more often than you’d expect to happen by chance? **If so, then even if my criteria for whether each resume belonged to a strong candidate wasn’t perfect, the results would still be compelling** — no matter how you slice it, if people involved in hiring consistently can’t come to a consensus, then something about the task at hand is too ambiguous.\n\nThe test I used to gauge inter-rater agreement is called [Fleiss’ kappa](https://en.wikipedia.org/wiki/Fleiss%27_kappa). The result is on the following scale of -1 to 1:\n\n* *-1* perfect disagreement; no rater agrees with any other\n* 0 random; the raters might as well have been flipping a coin\n* *1* perfect agreement; the raters all agree with one another\n\n**Fleiss’ kappa for this data set was 0.13.** 0.13 is close to zero, implying just mildly better than coin flip. In other words, the task of making value judgments based on these resumes was likely too ambiguous for humans to do well on with the given information alone.\n\n**TL;DR Resumes might actually suck.**\n\nSome interesting patterns\n-------------------------\n\nIn addition to the finding out that people aren’t good at judging resumes, I was able to uncover a few interesting patterns.\n\n### **Times didn’t matter**\n\nWe’ve all heard of and were probably a bit incredulous about [the study](https://info.theladders.com/our-team/you-only-get-6-seconds-of-fame-make-it-count) that showed recruiters spend less than 10 seconds on a resume on average. In this experiment, people took a lot longer to make value judgments. People took a median of 1 minute and 40 seconds per resume. In-house recruiters were fastest, and agency recruiters were slowest. **However, how long someone spent looking at a resume appeared to have no bearing, overall, on whether they’d guess correctly.**\n\n### **Different things mattered to engineers and recruiters**\n\nWhenever a participant deemed a candidate not worth interviewing, they had to substantiate their decision. Though these criteria are clearly not the be-all and end-all of resume filtering — if they were, people would have done better — it was interesting to see that engineers and recruiters were looking for different things.[2](#user-content-fn-2)\n\n![Chart showing recruiter's top 5 rejection reasons](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Frecruiter_rejection_reasons_3fd67131ac.webp&w=1200&q=75 \"Recruiter's top 5 rejection reasons\")\n\nIncidentally, *lack of relevant experience* didn’t refer to lack of experience with a specific stack. Verbatim rejection reasons under this category tended to say stuff like “projects not extensive enough”, “lack of core computer science”, or “a lot of academic projects around EE, not a lot on the resume about programming or web development”. *Culture fit* in the engineering graph denotes concerns about engineering culture fit, rather than culture fit overall. This could be anything from concern that someone used to working with Microsoft technologies might not be at home in a RoR shop to worrying that the candidate is too much of a hacker to write clean, maintainable code.\n\n### **Different groups did better on different kinds of resumes**\n\nFirst of all, and not surprisingly, engineers tended to do slightly better on resumes that had projects. Engineers also tended to do better on resumes that included detailed and clear explanations of what the candidate worked on. To get an idea of what I mean by detailed and clear explanations, take a look at the two versions below (source: [Lessons from a year’s worth of hiring data](https://blog.alinelerner.com/lessons-from-a-years-worth-of-hiring-data/ \"Lessons from a year’s worth of hiring data\")). The first description can apply to pretty much any software engineering project, whereas after reading the second, you have a pretty good idea of what the candidate worked on.\n\n![A example of bad description](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fbad_description_5b87fee909.webp&w=1920&q=75 \"Bad description\")\n\nA bad description\n\n![An example of a good description](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fgood_description_67fd485b2f.webp&w=1920&q=75 \"Good description\")\n\nA good description\n\nRecruiters, on the other hand, tended to do better with candidates from top companies. This also makes sense. Agency recruiters deal with a huge, disparate candidate set while also dealing with a large number of companies in parallel. They’re going to have a lot of good breadth-first insight including which companies have the highest engineering bar, which companies recently had layoffs, which teams within a specific company are the strongest, and so on.\n\nResumes just aren’t that useful\n-------------------------------\n\nSo, why are people pretty bad at this task? As we saw above, **it may not be a matter of being good or bad at judging resumes but rather a matter of the task itself being flawed — at the end of the day, the resume is a low-signal document.**\n\nIf we’re honest, no one really knows how to write resumes particularly well. Many people get their first resume writing tips from their university’s career services department, which is staffed with people who’ve never held a job in the field they’re advising for. Shit, some of the most fervent resume advice I ever got was from a technical recruiter, who insisted that I list every technology I’d ever worked with on every single undergrad research project I’d ever done. I left his office in a cold sweaty panic, desperately trying to remember what version of Apache MIT had been running at the time.\n\nVery smart people, who are otherwise fantastic writers, seem to check every ounce of intuition and personality at the door and churn out soulless documents expounding their experience with the software development life cycle or whatever… because they’re scared that sounding like a human being on their resume or not peppering it with enough keywords will eliminate them from the applicant pool before an engineer even has the chance to look at it.\n\nWriting aside, reading resumes is a shitty and largely thankless task. If it’s not your job, it’s a distraction that you want to get over with so you can go back to writing code. And if it is your job, you probably have a huge stack to get through, so it’s going to be hard to do deep dives into people’s work and projects, even if you’re technical enough to understand them, provided they even include links to their work in the first place. On top of that, spending more time on a given resume may not even yield a more accurate result, at least according to what I observed in this study.\n\nHow to fix top-of-the-funnel filtering\n--------------------------------------\n\nAssuming that my results are reproducible and people, across the board, are really quite bad at filtering resumes, there are a few things we can do to make top-of-the-funnel filtering better. In the short term, improving collaboration across different teams involved in hiring is a good start. As we saw, engineers are better at judging certain kinds of resumes, and recruiters are better at others. If a resume has projects or a GitHub account with content listed, passing it over to an engineer to get a second opinion is probably a good idea. And if a candidate is coming from a company with a strong brand, but one that you’re not too familiar with, getting some insider info from a recruiter might not be the worst thing.\n\n**Longer-term, how engineers are filtered fundamentally needs to change.** In my [TrialPay study](https://blog.alinelerner.com/lessons-from-a-years-worth-of-hiring-data/ \"Lessons from a year’s worth of hiring data\"), I found that, in addition to grammatical errors, one of the things that mattered most was how clearly people described their work. In this study, I found that engineers were better at making judgments on resumes that included these kinds of descriptions. Given these findings, relying more heavily on a writing sample during the filtering process might be in order. For the writing sample, I am imagining something that isn’t a cover letter — people tend to make those pretty formulaic and don’t talk about anything too personal or interesting. Rather, it should be a concise description of something you worked on recently that you are excited to talk about, as explained to a non-technical audience. I think the non-technical audience aspect is critical because if you can break down complex concepts for a layman to understand, you’re probably a good communicator and actually understand what you worked on. Moreover, recruiters could actually read this description and make valuable judgments about whether the writing is good and whether they understand what the person did.\n\nHonestly, I really hope that the resume dies a grisly death. One of the coolest things about coding is that it doesn’t take much time/effort to determine if someone can perform above some minimum threshold — all you need is the internets and a code editor. Of course, figuring out if someone is great is tough and takes more time, but figuring out if someone meets a minimum standard, mind you the same kind of minimum standard we’re trying to meet when we go through a pile of resumes, is pretty damn fast. And in light of this, relying on low-signal proxies doesn’t make sense at all.\n\nAcknowledgements\n----------------\n\nA huge thank you to:\n\n* All the engineers who let me use their resumes for this experiment\n* Everyone who participated and took the time to judge resumes\n* The fine people at [Statwing](https://www.statwing.com) and [Plotly](https://plot.ly)\n* [Stan Le](https://www.linkedin.com/pub/stan-le/44/aa2/435) for doing all the behind-the-scenes work that made running this experiment possible\n* All the smart people who were kind enough to proofread this behemoth\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. This number is less than 152\\*6=912 because not everyone who participated evaluated all 6 resumes. [↩](#user-content-fnref-1)\n2. I created the categories below from participants’ full-text rejection reasons, after the fact. [↩](#user-content-fnref-2)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/resumes-suck-heres-the-data",
      "author": "",
      "user_id": ""
    },
    {
      "title": "How to get in the door at top companies: cold outreach to hiring managers. Part 2 of 2.",
      "content": "In [part 1 of this post](https://interviewing.io/blog/how-to-get-in-the-door-at-top-companies-part-1), we talked about which channels are most effective for getting in the door and did an analysis of those channels along two axes: effectiveness and how much control you actually have. Here’s a quick summary.\n\n![A diagram comparing the effectiveness and utility of all the recruitment/hiring channels to how much control you have over them.](https://strapi-iio.s3.us-west-2.amazonaws.com/effectiveness_and_utility_quadrant_for_channels_1c6bec37f1.png)\n\nIn the quadrant above, you can see that while getting contacted by an in-house recruiter is very effective, whether you get contacted or not is largely out of your hands. The channel that maximizes both effectiveness and control is cold outreach to hiring managers (not recruiters!) “done right”. What does “done right” mean? That’s what we’ll talk about in this post (part 2 of 2). Most people do this type of outreach incorrectly. Here, we’ll get very tactical and tell you exactly what to say and do to reach out to hiring managers at the companies you’re interested in and actually get responses.\n\nHere’s our recommended, hyper-practical approach.\n\nPrerequisites/tooling\n---------------------\n\n* Buy a month or two of LinkedIn Sales Navigator. This will run you a few hundred dollars, but it’s worth it.\n* Get an account with an email discovery tool like [RocketReach](https://rocketreach.co/) (an excellent email discovery tool).\n* Get [Streak](https://www.streak.com/mail-merge-gmail), which lets you do mail merges in Gmail. You create an email template, with variables for everything from recipient name to long snippets of personalized text, and then you upload a CSV with all the values. The resulting emails feel personalized but get sent to hundreds of people at once.\n\nTreat your job search like a sales funnel\n-----------------------------------------\n\nIf you’re an engineer, chances are you haven’t ever done sales (maybe you had a job in high school selling Cutco knives or magazines, in which case what we’re about to say will resonate). But if you do sales for any appreciable amount of time, you’ll start thinking about everything in life as a funnel.\n\nFunnels are wide at the top and narrow at the bottom. That’s why they’re such an apt metaphor for the sales process — you do a lot of outreach, and you don’t get many responses. Of the responses you do get, relatively few will do the thing you want them to do. And even fewer will ultimately “close” (aka, buying — or, in this case, hiring).\n\nIn your engineering career, you’ve intellectually mastered many abstract concepts that are much more complex than a funnel. Despite its simplicity, however, the funnel is one of the hardest concepts to internalize emotionally, especially for people who are used to having control over outcomes. When you write code for *n* hours, you can expect that you will build *m* features.\n\nIn sales though, you do a lot of work, very little of it will pan out, and when it does pan out, it can feel almost random; an impersonal, mediocre email gets a response while your beautifully targeted email is met with deafening silence.\n\nAnd then there’s rejection. When you apply to jobs online and don’t hear back, it stings, but the sting is softened by the possibility that a human never even saw your application. You’re not reaching out to people when you apply online; you’re dealing with a bureaucratic machine.\n\nOn the other hand, when you email a real human and they don’t respond, that hurts: you put yourself out there, someone made a value judgment about you, and you lost.\n\n**The good news is that, after a while, the pain lessens, and you build up some useful emotional calluses and acquire the thousand-yard stare of someone who’s been rejected a million times for a million reasons, ranging from soul-crushingly legitimate to incontrovertibly random.** Sadly, there’s no shortcut. You’ve got to do the reps, you’ve got to get the rejections, and you’ve got to pick yourself up again. You get used to it, and then it doesn’t hurt as much, because experience has taught you that if you keep going, you will eventually get to a yes.\n\nWhat to actually do\n-------------------\n\n**First, come up with a target list of companies.** How to do that is out of scope for this post, but we may write about it in the future. For now, we’ll assume you have a list.\n\nOnce you have your list of companies, use LinkedIn Sales Navigator to find hiring managers at those companies (or founders or directors or VPs, as above). Below is an example query where we look for Google hiring managers.\n\nYou might think that Google is so big that sifting through all their various hiring managers will be intractable. Fortunately, you can whittle down the list to a pretty manageable size by applying some filters.\n\n![linkedin_sales_navigator.png](https://strapi-iio.s3.us-west-2.amazonaws.com/linkedin_sales_navigator_1543713d2a.png)\n\nHere are our filters:\n\n* **Just targeting managers**, not directors or VPs. Google is a huge organization. You want the people who are most likely to help, and they’re the ones who are struggling to hire for their teams.\n* **In position for less than 2 years:** These are the people who are still trying to prove themselves and who are less likely to have a long-standing relationship with their recruiter to the point where they only rely on internal recruiting and overlook other sources of candidates.\n* **Geography:** Let's focus on the places we most want to work.\n* **1st- or 2nd-degree connection:** This way, when they look you up, they’ll see some social proof. You can expand this to 3rd-degree connections, if needed.\n\n**Once you have your list, put their LinkedIn URLs into a spreadsheet. Then, do a pass through your targets’ profiles and see if any of them link to personal websites, social media accounts, blogs, or anything else that will help you find common ground with them.** Add any useful links in your spreadsheet because we’ll be mining them when we actually write our emails.\n\n### Look up their email addresses\n\n**Once you have your list of LinkedIn URLs, use a tool like RocketReach to look up their emails.**\n\nWhy not reach out on LinkedIn? While recruiters live on LinkedIn, managers generally do not. Possibly, they don't even like or check LinkedIn much. They live in their emails, so that's where you want to target them.\n\nRocketReach is a nice tool for email discovery because 1) it takes LinkedIn URLs as inputs and 2) its email database is generally up-to-date and correct.[1](#user-content-fn-1)\n\nIf RocketReach fails or you don't wish to pay for it, you might just be able to guess their email address, as email addresses tend to follow common forms: [aline@interviewing.io](mailto:aline@interviewing.io) (my actual email address), [alerner@interviewing.io](mailto:alerner@interviewing.io), or [aline.lerner@interviewing.io](mailto:aline.lerner@interviewing.io).\n\nWhere possible, contact managers via their work email address.[2](#user-content-fn-2) In some cases, you won’t be able to find their work email, in which case it’s acceptable to fall back to their personal email.\n\n### Write succinct, highly personalized emails\n\n**Next, compose a fairly personalized, yet short, email.** All too often, candidates write a long, generic cover letter that’s obviously been sent to a ton of people. I get many emails that look like this:\n\n![Example 1 of a bad cold email](https://strapi-iio.s3.us-west-2.amazonaws.com/bad_cold_email_8ed5f88af2.png)\n\n\nDon’t do this!\n\n![Example 2 of a bad cold email](https://strapi-iio.s3.us-west-2.amazonaws.com/bad_cold_email_2_cd282fccc0.png)\n\n\nDon’t do this either! There is nothing here about why this candidate is a good fit for interviewing.io, and the bullets aren’t compelling enough on their own. Note that this particular email is from a marketer, not an engineer, but the anti-patterns are the same.\n\nEmails like the above are impersonal, but worst of all, they have a poor signal-to-noise ratio — I want to find a reason to say yes and to invest my valuable time into this person. But they’re not giving me one, and they’re making me work for it in the process.\n\n* **Don't open email with how they found you.** This is a big pet peeve of ours. I don’t care how you found me! I know I’m on LinkedIn. What I care about is why talking to you will add value for me or why you’re interesting. Use the most significant real estate in the email, the first sentence, to tell me that!\n* **Don't be overly formal in how you address the person. Use their first name.**\n* **Don't get their gender wrong** (e.g., referring to a woman as \"sir\" — you’d be surprised how often this happens).\n* **Don't paste in a generic cover letter.** These are sure to get ignored immediately — if you’re not going to put in the effort to write to me personally, why would I put in the effort to read your email?\n* **Don't forget to include a link to a LinkedIn or a personal website.** We don’t recommend attaching your resume, though. It can seem overly formal/somewhat presumptuous if you're trying to build rapport.\n\nMore broadly, if you want someone to go out on a limb for you, make it dead simple for them to justify expending their social/political capital on you. Hiring managers, as a rule, want to help. Make it a no-brainer for them.\n\nThere are three components to a great cold email:\n\n1. Common ground with your target\n2. Proof that you’re worthy of their time\n3. A strong call to action\n\nNot every cold email will have (1) because you won’t always be able to find common ground with everyone — there’s simply not enough information out there about some targets to be able to craft a compelling narrative that’s highly personalized to them.\n\nBut every cold email you write should have (2). It is your job to sell yourself quickly and succinctly. You want your target to feel like they’d be an idiot to pass up the chance to talk to you.\n\n#### Finding common ground\n\nThe email below is personal, succinct, and finds common ground. Not only that, but it conveniently finds common ground that *benefits the candidate* (a soft-spot for non-traditional candidates, like himself!).\n\n![Example 1 of a good cold email](https://strapi-iio.s3.us-west-2.amazonaws.com/good_cold_email_2_8ecdbfe927.png)\n\nTo find common ground, reference something your target cares about. Then either show them that you care about it too or that helping you would fit into their worldview and further that cause.\n\nAs we mentioned above, finding common ground may be tough because there might not be enough information available about your target, but it’s important to do the work before you give up on this route — finding common ground is the tactic that’s going to get you the highest response rates.\n\nHere are some examples of great ways to build common ground:\n\n* Reference a project they worked on (maybe they wrote a blog post about it, mentioned it in a comment on Hacker News, or are a contributor to some open source project). Then…\n  + If possible, talk about relevant work you’ve done. It’s important not to make this connection too tenuous. If you do, this approach might backfire because they’ll start to get excited about you, only to be let down and ultimately feel tricked.\n  + If you do not have relevant work to share, ask a thoughtful question or two about theirs.\n* Reference a controversial point of view that they hold, and affirm it in an authentic way.\n* In the absence of something technical, it’s okay to reference something non-technical you've seen on their public profiles. We've seen candidates connect with strangers based on a shared love of Star Wars or Hearthstone.\n\nWe understand that you won't always be able to find common ground. But if you can, it'll help you a lot, especially if you’re light on social proof or accomplishments.\n\n#### Selling yourself\n\nSelling yourself is usually about one of two things:\n\n* Accomplishments: What have you built or created?\n* Social proof: Have you worked at a top company or attended a top school?\n\nSome people are fortunate enough to have both, but many will have just one. That’s okay. We’ll work with what you have!\n\n#### Accomplishments\n\nWhat have you done that most other people haven’t? What have you done that, if you were to tell it to a stranger, would cause them to pause and think you're special or interesting?\n\nBelow are some examples:\n\n* You’ve had a blog post about a technical topic or a personal project do well[3](#user-content-fn-3) on Hacker News, Reddit, or social media.\n* Something you built at work got some great press when your company announced its last funding round.\n* You refactored a piece of code at work, and now it runs 100X faster.\n* You won a company hackathon.\n* You’re a core contributor to a notable open-source project.\n* Something you built is being used by a number of other people.\n\n#### Social proof\n\n**Social proof is more about your pedigree. If you attended a top school or worked at a company known for having a high engineering bar, you should absolutely mention it!** People won't click on links or open your resume until *after* they're interested, so you need to get them interested right away. That is: you should spoon feed them the most impressive-sounding things about you out of the gate. This may feel strange and uncomfortable, like you’re bragging. We assure you, however, that it’s necessary to get your target’s attention. They’re not thinking you’re bragging. They’re thinking, “Is this worth my time?” Your job is to convince them that it is.\n\nAlso, don’t forget to link to your LinkedIn or personal website. Attaching a resume may feel too heavy-handed for a first conversation, as we discussed above.\n\nHere's an example of a prospective intern, leveraging both social proof and accomplishments, to write a compelling email. His email isn't super personalized, but he did make some effort to say that what we do at interviewing.io is important.\n\n![Example 2 of a good cold email](https://strapi-iio.s3.us-west-2.amazonaws.com/good_cold_email_3_393cc556e3.png)\n  \n\n#### Formulating a strong call to action\n\n**A call to action is an invitation for the recipient to do something. You can go one of two ways with your call to action: ask for a job interview or start a conversation.** Which you do should be a function of how much firepower you have in the way of social proof and accomplishments. It’s not fair, but if you can get your target’s attention with one or both of those, being bold and asking for a job interview makes sense. This approach can be effective, but it won’t work for most people… because most people don’t have enough social proof or accomplishments to justify this type of request.\n\nIf you can’t leverage social proof or accomplishments, you’re going to have to work harder and bank entirely on building common ground, which will likely take some time and effort and involve a live conversation before they’re convinced to expend their social capital on you.\n\nIf you’re asking for an interview, just come right out and say it. You can use the intern candidate’s email from earlier as a guide. However, this isn’t our preferred way to do it, and we really recommend starting a conversation instead.\n\nTake a look at the email below.\n\n![Example 3 of a good cold email](https://strapi-iio.s3.us-west-2.amazonaws.com/good_cold_email_1_38095f2d40.png)\n\nIn this email, the candidate doesn’t ask me about jobs — he just asks to meet to discuss a topic. Indeed, he’s done his research. I write a *ton* about judging resumes, and it’s a topic I could go on about for hours if you’ll let me. His email read like he’s genuinely interested in the subject and that we’d have a good conversation, so of course I responded. You’d be surprised how rare emails like this are. If you can find the topic your target cares about and write something that shows earnest, genuine interest, they’ll respond.\n\n**With these emails, you’re asking for a conversation, not a job interview… because the conversation is what will hopefully prove to the hiring manager that you’re worth interviewing. *Then*, once you have a conversation, the hiring manager will walk away with the impression that you’re a competent, thoughtful human being who’s interested in this sort of work. From there, getting a job interview will feel like an afterthought.**\n\nAs such, don’t talk about jobs at all in this type of email, and in this particular case, don’t attach your resume — that will feel out of place and transactional. You can and should link to your LinkedIn so they know who you are and have some context. But spend the bulk of the email building common ground and coming up with an interesting reason for the two of you to talk.\n\n**This approach is much more effective than asking for an interview out of the gate!** You’re not going to land a job from one email, so, as with any seemingly insurmountable goal, it’s important to think of your outreach as a series of steps where you put one foot in front of the other. Like in sales, all you need is to get to a conversation.\n\nIf your call to action is to set up a time to talk (which it probably should be because it’s specific), we recommend providing them with a time window. \"Would you want to meet up sometime?\" puts the burden on the recipient to pose a time, while \"Can we talk next Monday at 3pm?\" is problematic because, most likely, they aren't free then. Instead, try something like the candidate above did: \"Would you be available sometime within the next two weeks for a thirty-minute call? I'm free most weekdays between X and Y and can pretty much do any time on weekends if those are better for you.\"\n\nTwo templates for you to use\n----------------------------\n\n**Below are two templates you can use for cold outreach. The first one is ideal but requires more effort and can't always be used. The second one is weaker but more generic.** You can choose what fits your needs best. We expect both of these templates to be far more effective than throwing your resume into the blackhole of online portals.\n\n### Template #1: Use this template if your target has an online presence\n\nThis template includes common ground, accomplishments/social proof, and a call to action. It will get you the highest response rates, possibly anywhere from 25-50%. However, it can be challenging to use because it requires you to 1) do a deep dive into their online presence and 2) tie what you find back to something you’re doing. Sometimes, that tie-in might be tenuous or non-existent (in which case, maybe skip it).\n\nHey {Their First Name},\n\nI’ve read your work on {insert some details about their writing}, and I {insert your thoughts on the work}.\n\n{If you can make the connection between their work and yours, talk about something similar you’ve been working on.}\n\n{If you cannot, ask them a specific, thoughtful question about your work. Don’t worry about making it “the perfect question” like you might when you attend a talk and want to sound smart. Any earnest question will do. You don’t have to use this as a chance to show off!}\n\n{Finally, close with a sentence or two about you, if you have some social proof or impressive accomplishments you can share.}\n\nWould you be up for a quick chat this week or next?\n\nBest,\n\n{Your name}  \n{Insert 1-2 useful links about you. If you have a personal site, that’s great. If not, a LinkedIn will do.}\n\nNote that in this template, we leave some places for you to insert some social proof and your accomplishments. Even though this email is primarily about them and their work, and your references to yourself are primarily through that lens, it never hurts to drop in a few pieces of evidence that you’re someone who’s accomplished things and/or someone who looks good on paper.\n\n### Template #2: Use this template if you don’t have anything except a LinkedIn profile for your target\n\nThe reality is that you won’t always have enough information about your target to find common ground. In this case, you’ll lead with accomplishments/social proof and a strong call to action. We expect this template will get you response rates anywhere from 5-25%, depending on the strength of your achievements and pedigree. That said, we recommend treating this template as a last resort. Using it means you’ve exhausted any possibility of writing something personal.\n\nHey {Their First Name},\n\n{List 2 things about you. They can be impressive accomplishments of yours or social proof, as above.}\n\nI’m really interested in the work you’re doing at {Company Name}. {If you know what team they’re on and are interested in that specific team or are familiar with that team’s accomplishments, great! If not, just write a few earnest sentences about why the company is interesting to you.}\n\nWould you be up for a quick chat this week or next?\n\nBest,\n\n{Your name}  \n{Insert 1-2 useful links about you. If you have a personal site, that’s great. If not, a LinkedIn will do.}\n\nKeep your note short. The intent here is to make your target believe you’re an entity worth paying attention to, rather than them doing the easy thing: deleting your email.\n\nRegardless of which template you use, just like you have to manage your psychology when you prepare for technical interviews, you have to manage your psychology when doing outreach like this. You have to:\n\n* Mentally prepare yourself for the slog of writing personalized emails and doing the requisite research.\n* Get used to rejection. If you do write good emails and target the right people, you’ll have a much better hit rate than when you apply online, but you will still get ghosted a lot, and it will sting much more because, this time, you actually tried. But you know what? If you stick with it and do this right, within a few months, you’ll have a connection to a top-tier company.\n\nNow that you’ve girded your proverbial loins, it’s time to do the work. If you follow our advice, you’ll get 1-2 orders of magnitude more responses than from applying online, and with this approach, you’ll have at least a hiring manager at that company rooting for you!\n\nFootnotes:\n\nFootnotes\n---------\n\n1. RocketReach also has a LinkedIn-like faceted search you can use to find engineering managers, but we’ve found that it’s not nearly as reliable or rich as LinkedIn, which is why we recommend using LinkedIn for search and then RocketReach for email discovery. [↩](#user-content-fnref-1)\n2. Recruiters should not contact candidates on their work email address, but that's because they're trying to make the candidate leave their job. You are trying to join the manager, which is why it's okay to use their work email address. [↩](#user-content-fnref-2)\n3. Many people think that for something to be worth mentioning, it has to have gone viral. That’s simply not correct — in our niche space, a few hundred likes or a few thousand upvotes is already really impressive. [↩](#user-content-fnref-3)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/how-to-get-in-the-door-at-top-companies-cold-out-reach-to-hiring-managers-part-2",
      "author": "",
      "user_id": ""
    },
    {
      "title": "The other half, or why interviewers aren't always great and how to make them better",
      "content": "*Hey, Aline (founder of interviewing.io) here. Time for another awesome guest post.*\n\nWe’ve written before [about what the data says makes a good interviewer](https://interviewing.io/blog/best-technical-interviews-common) and about how [companies do a poor job of incentivizing engineers to be good interviewers](https://interviewing.io/blog/we-have-the-best-technical-interviewers-heres-how-we-do-it). But we’ve never really described what it feels like to interact with a great interviewer and how being one matters now, more than ever, given how well-prepared candidates have become and how much harder it is to get signal from them as a result.\n\nWhich is why I’m so excited about this guest post from Jos Visser. Enjoy!\n\n*Btw, if you have strong opinions about interviewing or hiring that you’ve been itching to write about, we’d love to hear from you. Please email me at [aline@interviewing.io](mailto:aline@interviewing.io).*\n\n![Author avatar](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fjosv_mug_f6353fb1fd.jpg&w=384&q=75 \"Jos Visser\")\n\nJos Visser\n\nJos Visser is a consummate IT professional with over four decades of experience interviewing and being interviewed. He has over 500 interviews under his belt at companies like Google, Facebook, Amazon, and OpenAI. On top of that he was a member of hiring committees and created and delivered many different training programs aimed at making people better interviewers. Jos can be reached at [josvisser66@gmail.com](mailto:josvisser66@gmail.com) or through [LinkedIn](https://www.linkedin.com/in/josvisser1/).\n\nIt is high time we start talking about interviewing. I know it seems like we are talking about interviewing all the time, but we are usually talking about only one half of the equation: How to be a good candidate. What about [the other half](https://marillionofficial.bandcamp.com/track/the-other-half)? What about the interviewer?\n\nIn the last decade, there has been an explosion of attention for candidates and how to improve their interview performance. There are blogs, [sites](https://interviewing.io/), [companies](https://interviewing.io/), and [books](https://www.amazon.com/Cracking-Coding-Interview-Programming-Questions/dp/0984782850) that help you improve your interviewing skills. You can sharpen your coding skills, learn how to approach a [system design interview](https://www.amazon.com/System-Design-Interview-insiders-Second/dp/B08CMF2CQF/ref=sr_1_1?dib=eyJ2IjoiMSJ9.CZwZ7txhICEtME2JuLCqj9fgDsTqCNi6KP7xke4eHp__iSdKSNwJz56sicfx_gWfRM7WlOrimmrvhV3DbzxY-0yUKmZGzld1ApW5-nESf1MKb1R_X9QteAFNuLJICcdWXjZKH8r4AH_0iEBs23dbBT4rpp3Nq87gQuIohh2HraAtqEvgw34HTXdR1ZnKnRR4-d9ZIA3m3wxWE2K8j4ZgubgB5dXZ3_0KjYWyti-kwcw.ysz-FWrN1DJdhG6ecHx7ipt7R1Cl3dOEnxwKurNtL18&dib_tag=se&keywords=alex+yu+system+design&qid=1717331123&sr=8-1), read entire volumes of [standard answers to standard questions](https://www.themuse.com/advice/interview-questions-and-answers), do [mock interviews](https://interviewing.io/), and learn how to use the [STAR method](https://www.themuse.com/advice/star-interview-method) to answer behavioral questions. These days, candidates are often very well prepared for a standard interview and therefore, bar a brain fart, do well at that.\n\nThis stands in stark contrast to the preparation of the *interviewer*. If you are lucky, your interviewer might have gotten a two hour class on how to ask only bona fide work related questions and has sat through two shadow interviews. Maybe they have even done a few interviews! Consequently, there is a lot of bad interviewing being done. That needs to change.\n\nLet’s start by acknowledging that interviewing is very hard. The average interview is a 45 minute video conference in which you need to get the audio and video hardware working, engage in introductory chit chat, give the candidate some time to ask you any questions , and ascertain whether they got what it takes to be successful in the role. Given all the overhead involved, you *might* only get 35 minutes for the meat of the interview. This is not a lot of time, especially if the candidate is, as I indicated above, often very well prepared.\n\nDon’t get me wrong, it is *good* that candidates are prepared. It shows an interest in doing well and displays the kind of methodical approach to reach a goal that I *want* to see in the candidate. But it *does* make interviewing a lot harder.\n\nIn the olden days, you could surprise a candidate by asking how to [remove a file called -f](https://www.linkedin.com/posts/josvisser1_a-standard-linux-interview-question-is-how-activity-7199771506543841281-1Rrj?utm_source=share&utm_medium=member_desktop) or [how to sort one million eight-digit numbers with only 1MB of RAM](https://stackoverflow.com/questions/12748246/sorting-1-million-8-decimal-digit-numbers-with-1-mb-of-ram/13004000#13004000). It would unfailingly be the first time the candidate ever saw that question and so you were immediately seeing how the candidate dealt with solving a problem that they had never solved before and probably never even thought about. This often prompted interesting discussions and you could see the candidate’s brain at work in solving a novel problem.\n\nWe live in different times now. A lot of candidates have seen all the common questions and are exquisitely prepared for them. As a result, whenever they give the right answer, you have learned nothing about the candidate, other than that they were probably well prepared for this particular question. Take the [min-stack question](https://leetcode.com/problems/min-stack/description/). I used to like this question because it is not entirely trivial and has some half-way interesting trade offs between time and space complexity. Then some genius came up with a non-trivial solution that [solves](https://www.geeksforgeeks.org/design-a-stack-that-supports-getmin-in-o1-time-and-o1-extra-space/#) the question in O(1) time and O(1) space. It is a super cool trick and I would not expect *anyone* to come up with it from scratch in 35 minutes. If you ask the question today and people immediately knock out the optimal solution, you *know* that they memorized the answer and you have learnt nothing. Same for how to design a [URL shortener](https://systemdesignschool.io/problems/url-shortener/solution) or a [key/value store](https://www.educative.io/courses/grokking-modern-system-design-interview-for-engineers-managers/system-design-the-key-value-store).\n\nThis experience might give rise to the idea that all the current questions are known and that we just need a collection of new questions. That idea is wrong; it is not about the questions. Exaggerating a bit, I am of the opinion that all interview questions that can be asked within the scope of a 45-minute interview are known already. Coming up with a “new” question typically just means replacing the gift wrapping around the core algorithm of an old question with new gift wrapping, thereby making it look new. Even if, through a stroke of genius, you manage to come up with a brand new unique never-before-seen question, there is no way to keep it secret. As soon as you give it to a candidate, the question is out and the entire candidate prepping industry will cover it on their sites and blogs, and in their mock interviews and books. It’s like keeping the new Taylor Swift album secret, [it ain’t gonna happen](https://www.washingtonpost.com/entertainment/2024/04/19/taylor-swift-tortured-poets-department-album-leak/).\n\nSo interviewing is hard to begin with and even harder with well-prepared candidates. How to deal with that?\n\nThe non-answer to this question in my opinion is to replace (some) interviews with take-home assignments. [I wrote about this before](https://josvisser.substack.com/p/interviewing-has-become-a-mess) and by and large I consider it a mistake. Not only does it put the burden of getting a better signal solely on the candidates, it also is a method to ensure that great candidates, with lots of places to go, do not bother to enter your interviewing process. It’s just another example of penny wise and pound foolish. I am not saying it cannot be done right, but what I have seen so far does not inspire me with confidence.\n\nInstead, my answer is to match like with like: If the candidates are becoming better at being candidates, the interviewers need to become better at being interviewers.\n\nOne of the ways in which a lot of interviewers need to become better is through getting rid of the idea that good interviewing starts and ends with good questions. Nothing could be further from the truth. Surely, a good question is always better than a bad question, but a *good* interviewer can still get lots of signal from handling a bad question well, whereas a *bad* interviewer cannot get any signal from a great question. It really is not about the question, and that is one of the things I think good interviewer training needs to cover.\n\nBut if good interviewing is not about the question, then what is it about? The answer is: **Digging**.\n\nLet’s take the world famous “how to remove a file called -f” interview question. There is a canonical answer, which is to use rm -- -f. If the candidate knows this, that’s okay, but not more than that, because you haven’t learned anything about the candidate’s knowledge and thinking yet (other than they know this “trick”). When I get this answer, I would be wondering whether the candidate *understands* why this works. Do they understand if rm -f? Or rm '-f'? Or rm ./-f? And what about X=-f rm $X? And if so, or if not, why, or why not? And why exactly does a plain “rm -f” not work? Would rm -\\* work? And does it matter what Unix system you are on? Or which shell you are using? And does python -c \"import os; os.unlink('-f')\" work? And if so why? And if rm -- -f works, does that automatically mean that touch -- -f works to create that file?\n\nYou can use this very simple question as a starting point for an extensive discussion about shell argument expansion, command line parsing by Unix binaries, the getopt(3) library function, the unlink(2) system call, and the many interesting ways in which the GNU, BSD, and System V versions of common utilities differ in this respect. While you are at it, you might as well throw in follow up questions about $IFS, spaces in file names, hidden files, inode numbers, device files, hard links and symbolic links If -f is a symbolic link, does rm -- -f remove the link or the file linked to? And what if -f was a hard link?\n\nInterviewing is about *digging*, about never taking the candidate’s answer at face value, and about taking the answer and using it as a jumping board to ask follow-up questions. I really don’t care if the candidate knows the answer to my questions up-front. What I care about is whether they can reason towards the answer using the knowledge they have and the additional knowledge I might give them in the course of the interview. If the candidate comes in knowing next to nothing about the Unix shell, but they can deduce what is going on from the examples I give them and propose experiments to test the hypotheses they come up with, I am as [happy as a clam at high tide](https://idioms.thefreedictionary.com/happy+as+a+clam+at+high+tide).\n\nInterviewers can learn how to dig. They can learn how to write better feedback. They can learn how to evaluate a candidate correctly against the hiring bar. It might not be easy, but it is not not rocket science either. That said, learning how to interview well is real work; real work that you can *only* expect from people who are *interested* in being great interviewers and who are duly recognized.\n\nIt is probably a dirty secret that in many companies, a lot of engineers are seeing interviewing as a chore to be dispensed with quickly and with as little effort as possible: They often get little to no training, there is no quality control, no feedback on feedback, and there are no incentives to be great or to get better. No wonder that a lot of interviews are suboptimal.\n\nFortunately, in most companies, you don’t need all engineers to be interviewers, so you can restrict yourself to the ones who *want* to interview. You can then create a culture of interviewing; a culture where great interviewing is the norm, and where that great interviewing is rewarded.\n\nRewards are essential here. To quote [Charlie Munger](https://fs.blog/great-talks/psychology-human-misjudgment/): “Show me the incentives, and I’ll show you the outcome.” It doesn’t have to be money. Or at least, not a lot of money, as rewards come in many forms: Public recognition, swag, a “Great Interviewer” mug, a badge on the internal phone book pages, or a shout out at the all-hands. All of these conspire to give people the idea that they are doing important and meaningful work. Plus it gives the managers of the interviewers the idea that they are enabling an essential company function. And of course: Great interviewing needs to help you get promoted. If it doesn’t, then you might as well go write another design document rather than spending your time on attending your local interviewing club.\n\nThere is unfortunately no cheap and easy way for your interviewers to become better. It is not hard to improve interviewing quality and effectiveness, but it does require time, money, and effort. Plus it helps if you know what you are doing (as a company hiring engineers).",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/the-other-half-or-why-interviewers-aren-t-always-great-and-how-to-make-them-better",
      "author": "",
      "user_id": ""
    },
    {
      "title": "You can’t fix diversity in tech without fixing the technical interview.",
      "content": "In the last few months, several large players, including [Google](https://blog.google/outreach-initiatives/diversity/focusing-on-diversity30/) and [Facebook](https://newsroom.fb.com/news/2016/07/facebook-diversity-update-positive-hiring-trends-show-progress/), have released their latest and ultimately disappointing diversity numbers. Even with increased effort and resources poured into diversity hiring programs, Facebook’s headcount for women and people of color hasn’t really increased in the past 3 years. Google’s numbers have looked remarkably similar, and both players have yet to make significant impact in the space, despite a number of initiatives spanning everything from a [points system rewarding recruiters for bringing in candidates](https://www.wsj.com/articles/facebooks-point-system-fails-to-close-diversity-gap-1471387288) from diverse backgrounds, to increased funding for tech education, to efforts to hire more candidates from diverse backgrounds in key leadership positions.\n\nWhy have gains in diversity hiring been so lackluster across the board?\n\nFacebook justifies these disappointing numbers by citing the ubiquitous [pipeline problem](https://interviewing.io/blog/we-ran-the-numbers-and-there-really-is-a-pipeline-problem-in-eng-hiring), namely that not enough people from underrepresented groups have access to the education and resources they need to be set up for success. And Google’s take appears to be similar, judging from what portion of their diversity-themed, forward-looking investments are [focused on education](https://time.com/3849218/google-diversity-investment/).\n\nIn addition to blaming the pipeline, since Facebook’s and Google’s announcements, a growing flurry of conversations have loudly waxed causal about the *real* reason diversity hiring efforts haven’t worked. These have included everything from how diversity training isn’t sticky enough, to how work environments remain exclusionary and thereby unappealing to diverse candidates, to improper calibration of performance reviews to not accounting for how marginalized groups actually respond to diversity-themed messaging.\n\nWhile we are excited that more resources are being allocated to education and inclusive workplaces, at [interviewing.io](https://interviewing.io/), we posit another reason for why [diversity hiring initiatives aren’t working](https://interviewing.io/blog/diversity-hiring-initiatives-wrong). **After drawing on data from thousands of technical interviews, it’s become clear to us that technical interviewing is a process whose results are nondeterministic and often arbitrary. We believe that technical interviewing is a broken process for everyone but that the flaws within the system hit underrepresented groups the hardest… because they haven’t had the chance to internalize just how much of technical interviewing is a numbers game.** Getting a few interview invites here and there through increased diversity initiatives isn’t enough. It’s a beginning, but it’s not enough. It takes a lot of interviews to get used to the process and the format and to understand that the stuff you do in technical interviews isn’t actually the stuff you do at work every day. And it takes people in your social circle all going through the same experience, screwing up interviews here and there, and getting back on the horse to realize that poor performance in one interview isn’t predictive of whether you’ll be a good engineer.\n\nA brief history of technical interviewing\n-----------------------------------------\n\nA definitive work on the history of technical interviewing was surprisingly hard to find, but I was able to piece together a narrative by scouring books like [How Would You Move Mount Fuji](https://www.amazon.com/How-Would-Move-Mount-Fuji/dp/0316778494), [Programming Interviews Exposed](https://www.amazon.com/Programming-Interviews-Exposed-Secrets-Landing/dp/1118261364/), and the bounty of the internets. The story goes something like this.\n\nTechnical interviewing has its roots as far back as 1950s Palo Alto, at Shockley Semiconductor Laboratories. Shockley’s interviewing methodology came out of a need to separate the innovative, rapidly moving, Cold War-fueled tech space from hiring approaches taken in more traditionally established, skills-based assembly-line based industry. And so, he relied on questions that could gauge analytical ability, intellect, and potential quickly. One canonical question in this category has to do with coins. You have 8 identical-looking coins, except one is lighter than the rest. Figure out which one it is with just two weighings on a pan balance.\n\nThe techniques that Shockley developed were adapted by Microsoft during the 90s, as the first dot-com boom spurred an explosion in tech hiring. As with the constraints imposed by both the volume and the high analytical/adaptability bar imposed by Shockley, Microsoft, too, needed to vet people quickly for potential — as software engineering became increasingly complex over the course of the dot-com boom, it was no longer possible to have a few centralized “master programmers” manage the design and then delegate away the minutiae. Even rank and file developers needed to be able to produce under a variety of rapidly evolving conditions, where just mastery of specific skills wasn’t enough.\n\nThe puzzle format, in particular, was easy to standardize because individual hiring managers didn’t have to come up with their own interview questions, and a company could quickly build up its own interchangeable question repository.\n\nThis mentality also applied to the interview process itself — rather than having individual teams run their own processes and pipelines, it made much more sense to standardize things. This way, in addition to questions, you could effectively plug and play the interviewers themselves — any interviewer within your org could be quickly trained up and assigned to speak with any candidate, independent of prospective team.\n\nPuzzle questions were a good solution for this era for a different reason. Collaborative editing of documents didn’t become a thing until Google Docs’ launch in 2007. Without that capability, writing code in a phone interview was untenable — if you’ve ever tried to talk someone through how to code something up without at least a shared piece of paper in front of you, you know how painful it can be. In the absence of being able to write code in front of someone, the puzzle question was a decent proxy. Technology marched on, however, and its evolution made it possible to move from the proxy of puzzles to more concrete, coding-based interview questions. Around the same time, [Google itself publicly overturned the efficacy of puzzle questions](https://www.nytimes.com/2013/06/20/business/in-head-hunting-big-data-may-not-be-such-a-big-deal.html).\n\nSo where does this leave us? **Technical interviews are moving in the direction of more concreteness, but they are still very much a proxy for the day-to-day work that a software engineer actually does. The hope was that the proxy would be decent enough, but it was always understood that that’s what they were and that the cost-benefit of relying on a proxy worked out in cases where problem solving trumped specific skills and where the need for scale trumped everything else.**\n\nAs it happens, elevating problem-solving ability and the need for a scalable process are both eminently reasonable motivations. But here’s the unfortunate part: the second reason, namely the need for scalability, doesn’t apply in most cases. Very few companies are large enough to need plug and play interviewers. But coming up with interview questions and processes is really hard, so despite their differing needs, smaller companies often take their cues from the larger players, not realizing that companies like Google are successful at hiring because the work they do attracts an assembly line of smart, capable people… and that their success at hiring is often despite their hiring process and not because of it. **So you end up with a de facto interviewing cargo cult, where smaller players blindly mimic the actions of their large counterparts and blindly hope for the same results.**\n\n**The worst part is that these results may not even be repeatable…** for anyone. To show you what I mean, I’ll talk a bit about some data we collected at interviewing.io.\n\nTechnical interviewing is broken for everybody\n----------------------------------------------\n\n### Interview outcomes are kind of arbitrary\n\ninterviewing.io is a platform where people can practice technical interviewing anonymously and, in the process, find jobs. Interviewers and interviewees meet in a collaborative coding environment and jump right into a technical interview question. After each interview, both sides rate one another, and interviewers rate interviewees on their technical ability. And the same interviewee can do multiple interviews, each of which is with a different interviewer and/or different company, and this opens the door for some interesting and somewhat controlled comparative analysis.\n\nWe were curious to see how consistent the same interviewee’s performance was from interview to interview, so we dug into our data. After looking at thousands of interviews on the platform, we’ve discovered something alarming: **[interviewee performance from interview to interview varied quite a bit, even for people with a high average performance](https://interviewing.io/blog/after-a-lot-more-data-technical-interview-performance-really-is-kind-of-arbitrary)**. In the graph below, every represents the mean technical score for an individual interviewee who has done 2 or more interviews on interviewing.io. The y-axis is standard deviation of performance, so the higher up you go, the more volatile interview performance becomes.\n\n![Screen-Shot-2021-01-19-at-7.04.20-PM.png](https://strapi-iio.s3.us-west-2.amazonaws.com/Screen_Shot_2021_01_19_at_7_04_20_PM_f6010739c4.png)\n\nAs you can see, roughly 25% of interviewees are consistent in their performance, but the rest are all over the place. And over a third of people with a high mean (>=3) technical performance bombed at least one interview.\n\nDespite the noise, from the graph above, you can make some guesses about which people you’d want to interview. However, keep in mind that each person above represents a mean. Let’s pretend that, instead, you had to make a decision based on just one data point. That’s where things get dicey. **Looking at this data, it’s not hard to see why technical interviewing is often perceived as a game.** And, unfortunately, it’s a game where people often can’t tell how they’re doing.\n\n### No one can tell how they’re doing\n\nI mentioned above that on interviewing.io, we collect post-interview feedback. In addition to asking interviewers how their candidates did, we also ask interviewees how they think they did. Comparing those numbers for each interview showed us something really surprising: people are terrible at gauging their own interview performance, and impostor syndrome is particularly prevalent. In fact, **[people underestimate their performance over twice as often as they overestimate it](https://interviewing.io/blog/own-interview-performance)**. Take a look at the graph below to see what I mean:\n\n**Note that, in our data, impostor syndrome knows no gender or pedigree — it hits engineers on our platform across the board, regardless of who they are or where they come from.**\n\nNow here’s the messed up part. During the feedback step that happens after each interview, we ask interviewees if they’d want to work with their interviewer. As it turns out, there’s a very strong relationship between whether people think they did well and whether they would indeed want to work with the interviewer — **when people think they did poorly, even if they actually didn’t, they may be a lot less likely to want to work with you**. And, by extension, it means that in every interview cycle, some portion of interviewees are losing interest in joining your company just because they didn’t think they did well, despite the fact that they actually did.\n\n**As a result, companies are losing candidates from all walks of life because of a fundamental flaw in the process.**\n\n### Poor performances hit marginalized groups the hardest\n\nThough impostor syndrome appears to hit engineers from all walks of life, we’ve found that women get hit the hardest in the face of an actually poor performance. As we learned above, **poor performances in technical interviewing happen to most people, even people who are generally very strong. However, when we looked at our data, we discovered that after a poor performance, [women are 7 times more likely to stop practicing than men](https://interviewing.io/blog/voice-modulation-gender-technical-interviews)**:\n\nA [bevy of research appears to support confidence-based attrition as a very real cause for women departing from STEM fields](https://www.theatlantic.com/magazine/archive/2014/05/the-confidence-gap/359815/), but I would expect that the implications of the attrition we witnessed extend beyond women to underrepresented groups, across the board.\n\nWhat the real problem is\n------------------------\n\n**At the end of the day, because technical interviewing is indeed a game, like all games, it takes practice to improve. However, unless you’ve been socialized to expect and be prepared for the game-like aspect of the experience, it’s not something that you can necessarily intuit.** And if you go into your interviews expecting them to be indicative of your aptitude at the job, which is, at the outset, not an unreasonable assumption, you will be crushed the first time you crash and burn. But the process isn’t a great or predictable indicator of your aptitude. And on top of that, you likely can’t tell how you’re doing even when you do well.\n\n**These are issues that everyone who’s gone through the technical interviewing gauntlet has grappled with. But not everyone has the wherewithal or social support to realize that the process is imperfect and to stick with it. And the less people like you are involved, whether it’s because they’re not the same color as you or the same gender or because not a lot of people at your school study computer science or because you’re a dropout or for any number of other reasons, the less support or insider knowledge or 10,000 foot view of the situation you’ll have.** Full stop.\n\nInclusion and education isn’t enough\n------------------------------------\n\nTo help remedy the lack of diversity in its headcount, [Facebook has committed to three actionable steps](https://newsroom.fb.com/news/2016/07/facebook-diversity-update-positive-hiring-trends-show-progress/) on varying time frames. The first step revolves around creating a more inclusive interview/work environment for existing candidates. The other two are focused on addressing the perceived pipeline problem in tech:\n\n* Short Term: Building a Diverse Slate of Candidates and an Inclusive Working Environment\n* Medium Term: Supporting Students with an Interest in Tech\n* Long Term: Creating Opportunity and Access\n\nIndeed, efforts to promote inclusiveness and increased funding for education are extremely noble, especially in the face of potentially not being able to see results for years in the case of the latter. However, both take a narrow view of the problem and both continue to funnel candidates into a broken system.\n\nErica Baker really cuts to the heart of it in [her blog post about Twitter hiring a head of D&I](https://medium.com/this-is-hard/wrong-acacd043229b#.cf3iajdvn):\n\n> *“What irks me the most about this is that no company, Twitter or otherwise, should have a VP of Diversity and Inclusion. When the VP of Engineering… is thinking about hiring goals for the year, they are not going to concern themselves with the goals of the VP of Diversity and Inclusion. They are going to say ‘hiring more engineers is my job, worrying about the diversity of who I hire is the job of the VP of Diversity and Inclusion.’ When the VP of Diversity and Inclusion says ‘your org is looking a little homogenous, do something about it,’ the VP of Engineering won’t prioritize that because the VP of Engineering doesn’t report to the VP of Diversity and Inclusion, so knows there usually isn’t shit the VP of Diversity and Inclusion can do if the Eng org doesn’t see some improvement in diversity.”*\n\nIndeed, this is sad, but true. When faced with a high-visibility conundrum like diversity hiring, a pragmatic and even reasonable reaction on any company’s part is to make a few high-profile hires and throw money at the problem. Then, it looks like you’re doing something, and spinning up a task force or a department or new set of titles is a lot easier than attempting to uproot the entire status quo.\n\n**As such, we end up with a newly minted, well-funded department pumping a ton of resources into feeding people who’ve not yet learned about the interviewing being a game into a broken, nondeterministic machine of a process made further worse by the fact that said process favors confidence and persistence over bona fide ability… and where the link between success in navigating said process and subsequent on-the-job performance is tenuous at best.**\n\nHow to fix things\n-----------------\n\nIn the evolution of the technical interview, we saw a gradual reduction in the need for proxies as companies as the technology to write code together remotely emerged; with its advent, abstract, largely arbitrary puzzle questions could start to be phased out.\n\nWhat’s the next step? **Technology has the power to free us from relying on proxies**, so that we can look at each individual as an indicative, unique bundle of performance-based data points. At interviewing.io, we make it possible to move away from proxies by looking at each interviewee as a collection of data points that tell a story, rather than one arbitrary glimpse of something they did once.\n\nBut that’s not enough either. Interviews themselves need to continue to evolve. The process itself needs to be repeatable, predictive of aptitude at the actual job, and not a system to be gamed, where a huge benefit is incurred by knowing the rules. And the larger organizations whose processes act as a template for everyone else need to lead the charge. Only then can we really be welcoming to a truly diverse group of candidates.",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/you-cant-fix-diversity-in-tech-without-fixing-the-technical-interview",
      "author": "",
      "user_id": ""
    },
    {
      "title": "I’ve never written code, but I just passed a Google system design interview.",
      "content": "README and some disclaimers\n---------------------------\n\n*Before you read this post, I (Aline Lerner, founder of interviewing.io) wanted to introduce it properly. It’s about something that’s a little bit nuts: a non-engineer passed a Google system design interview! What??! Surely something is misrepresented here, and surely something is overblown.*\n\n*Here are the facts and some key context:*\n\n* *interviewing.io is an anonymous mock interview platform.*\n* *We have hundreds of interviewers who are (or have recently been) senior, staff, and principal engineers at FAANG.*\n* *We pay these interviewers to conduct anonymous mock interviews in the styles of their companies, and we instruct them to use at least the same bar that they would use in real life.*\n* *We instruct our interviewers to never use proprietary questions or do anything that would jeopardize their NDAs with these companies.*\n* *The Google interview in this post is a mock interview in the style of Google, conducted by a current or former Googler on our platform (we won’t say anything more about the interviewer to protect their identity, and we’ve also changed their voice). The interviewer didn’t have any context and thought this was a regular mock interview, and we stand behind its realism.*\n* *To corroborate the results, we had a few independent Google interviewers evaluate the interview replay without knowing the context. Most interviewers said the question was realistic. One said that the question was too hard for an L4 candidate. Several said that the interviewer was too lenient, and one said that the candidate squeaked by in part because he made the interviewer believe he was already in process for an L5 role. One said the feedback was on point. We didn’t get consensus… but that somewhat tracks real life, where interviews are messy and outcomes are not deterministic.*\n* *Even if the interviewer was a bit lenient, the fact that a non-engineer could pass or get close to passing a system design interview is nuts. It’s a testament to how teachable this material is, and it’s also a commentary on how these interviews are flawed and can be gamed, with enough effort. We are acutely aware of the shortcomings of the status quo and want to see a revolution in our industry as much as you do.*\n\n*At the bottom of this post, you can watch the full interview and judge for yourself.*\n\nMy name is Kevin. I am not and have never been a software engineer. I have never written or tested a single line of code, and I have never even worked as a PM. My tech career started in (don’t hate me!) recruiting. Then moved into career coaching and salary negotiation before I got where I am now: creating content for engineers.\n\nWith a fraction of the domain knowledge most engineers have, I passed a Google system design interview. I passed because I learned a LOT about system design interviews before taking this interview. I also learned a few tricks along the way. **If these tricks helped *me* pass, then imagine what you’ll be able to do with them**.\n\nHow a non-coder learned a LOT about system design interviews\n------------------------------------------------------------\n\nAt interviewing.io, part of my job is talking to candidates about the interview process. Many complain about system design interviews. They don’t know how to approach them or how to handle the ambiguity. Here’s what one of our users had to say:\n\n> *System design interviews are very intimidating. The de facto assumption is that if you have worked for 5+ years, you should have excellent scalable systems experience and if you fail to show that at interviews, you're penalized either by not getting the offer or by getting down leveled significantly. The analogy I can think of is like asking an elephant to fly when the job description is to lift and move heavy things.*\n\nAt the time, the most popular system design resources were eerily similar. None of them had good solutions for approaching, handling ambiguity in, or demystifying system design interviews. So, we decided to try to make the best system design guide in the world, and I got to lead the project.\n\nFirst, I talked to some of our highest-rated system design interviewers. Then, we started work on the guide. My role was to facilitate, listen, project manage, write, edit, and repeat.\n\nAfter months of work, we published [A Senior Engineer’s Guide to the System Design Interview](https://interviewing.io/guides/system-design-interview) on March 2nd, 2023. [It did well on Hacker News](https://news.ycombinator.com/item?id=34999464). If you Google “system design interview”, it’s the #1 result (as of when this post was published).\n\nWhy I decided to try doing a system design interview myself\n-----------------------------------------------------------\n\nAfter the guide was released, I met again with some of the contributors. Out of nowhere, one said, “I bet you could pass a system design interview.” The other two agreed. I accepted the challenge and decided I wouldn’t study; I’d just use what I learned from making the guide.\n\nI scheduled a mock system design interview with an interviewer from Google on interviewing.io for a Friday morning. My interviewer wouldn’t know who I was and would only know my years of experience (I said I had 4). The only other thing my interviewer would know about me was that I presumably was about to interview at Google and needed help preparing.\n\n![How to book Google system design interviews on interviewing.io](https://strapi-iio.s3.us-west-2.amazonaws.com/sd_g_67800b6ebb.png)\n\nHow to book Google system design interviews on interviewing.io\n\nSelected highlights from my system design interview\n---------------------------------------------------\n\nI’ll walk you through a few clips from the interview that cover three key aspects of my experience: an unconventional idea, something I failed spectacularly at, and a moment I’m proud of.\n\n### An unconventional idea\n\nLast year, I went to Harvard Law School’s Advanced Negotiation Workshop. They spend a lot of time (33% of the whole workshop!) drilling into the specifics of how negotiations are often won or lost in the first and last 3 minutes. I approached the first 180 seconds of this system design interview with that in mind, and I knew I had to quickly score some social proof with the interviewer.\n\n[The unwritten codes of FAANG interviews](https://interviewing.io/guides/hiring-process) taught me one unobvious way to score social proof. Win the competition about who cares less. **I’m naturally enthusiastic so I had to trick myself. To find that devil-may-care attitude, I’d repeat a lyric from Green Day in my head: *“I don’t care that you don’t care.”***\n\nIn this clip, I try to score some points in the first 180 seconds of the interview. Note that we swapped out both my voice and the interviewer’s voice, so it sounds a bit robotic, but the content is preserved.\n\n### Something I failed spectacularly at\n\nSimply put, I forgot you could cache data on a user’s device. More importantly, they grilled me about caching and I took it personally. I lost sight of the interview. Caught in a story of them as an adversary, I stopped listening. I ignored their follow-up questions and, ironically, their hints.\n\nAs Marcellus Wallace said in Pulp Fiction, \"Pride only hurts.\" I didn’t know the answers to their caching questions, and I was too proud to say “I don’t know.” Had I done that, they would’ve shifted gears. But I didn’t, so they assumed I actually did know, then the grilling continued. If I could go back in time, I’d do what our system design guide says to do when you’re not certain, but you have an idea.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/certainty_diagram_27234de7d3.webp)\n\nTell the interviewer:\n\n> *\"I don't know, I'm definitely going to look that up right after this interview, but if I had to give my best guess I'd say... [x] and here is why [explanation/thought process].\"*\n\nIn this clip, I get flustered, yell, and get a hint. Note that we swapped out both my voice and the interviewer’s voice, so it sounds a bit robotic, but the content is preserved.\n\n### A moment I’m proud of\n\nBreaking Google’s record for the number of times “Harry Styles” is repeated in one interview! Just kidding. My actual proud moment was staying calm when the interviewer said something scary. A candidate who gets scared, looks like an outsider. To look like an insider, stay calm.\n\n> *A common system design question is ‘Design Gmail.’ There are so many different dimensions, no one can design Gmail during an interview. That can’t happen. Whenever people ask you to ‘Design Gmail,’ that's to scare you.*\n>\n> -Former Staff Engineer at Google\n\nThe topic of scale came up. And the interviewer said this system should be able to support 800 million active users. That quote from that staff engineer popped into my head, so I–while feigning the most casual tone possible–responded with a neutral “okay.” The word choice and delivery suggest “this is another day at the office for me” which implies “I’m not scared of this.” In my head, what I told myself was “I don’t care that you don’t care.”\n\nTalking about scale like you’ve been there before 😎 Note that we swapped out both my voice and the interviewer’s voice, so it sounds a bit robotic, but the content is correct.\n\nThe interviewer’s feedback from the interview\n---------------------------------------------\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/feedback_93fb5e447f.png)\n\n**At 41:00, you can hear him give me a “Weak/Lean Hire” for L4.**\n\nWhat I learned about system design interviews\n---------------------------------------------\n\nWithout a firm foundation, tricks lose their magic. You can have all the tricks in the world, but if you don’t know [the material](https://interviewing.io/guides/system-design-interview), it won’t be enough. That said, just knowing the material isn’t enough either because doing well is also about managing your own psychology. The main mental obstacle is the discomfort of not knowing.\n\nSet yourself up for the least amount of discomfort. Book an anonymous, mock system design interview with engineers from top companies. Keep your skills sharp.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nUnlike engineering problems, design problems are more about not knowing. There are no optimal solutions. Progress is not quantifiable. There are no predetermined outcomes. **It’s not enough to sum them up as ambiguous; they test your ability to thrive in ambiguity. Luckily, this can be learned through practice.** It is a way to think, communicate, and solve problems. So, the skills you learn are transferable, even from practices that seem unrelated.\n\nThree ways to practice \"not knowing\"\n------------------------------------\n\nThey are: the \"not knowing\" game, improv class, and zen koans. Try one for 10 days. If your system design interview skills don’t improve, I’ll buy you a Coke. Share your experience, tell us what we should write about next, and add anything else in the Typeform at the end of this post.\n\n### First way: The \"not-knowing\" game\n\nThe way to play is to notice who knows stuff you don’t and then follow that thread. It’s easier with a topic you’re currently passionate about. For me, that’s art. I bring it up and listen for it everywhere. If someone mentions something I didn’t know, that’s a point. If they explain something I didn’t know, that’s double points. And if someone scores enough points: treat them like an old friend and see where it goes.\n\nThe other day, a stranger and I began talking at a café. A half hour later, we were in their house looking at their original copy of Dante’s Divine Trilogy which is about hell, purgatory, and heaven. It is hundreds of years old and surprisingly massive. Before this experience which was steeped in \"not knowing\"–an impromptu visit to a stranger’s house–I hadn’t even heard of this extremely rare art piece. After it, I had turned its pages (and not in a museum!!!).\n\n### Second way: Improv class\n\nIf unstructured conversation seems like hell, you might prefer an improv class such as clowning. There are two types of clowns: the high status (who “knows”) and the low status (who “doesn’t know”). Low status clowns do illogical things like jump when they’re told to sit. They are the rule breaking dopey sidekicks. When playing one, you tell yourself you have no idea what’s going on, and to be okay with that. This is similar to a meditative practice called “don’t know mind.”\n\n### Third way: Meditation\n\nIf strangers and clowns are scary, try meditation. Zen koans are a particularly baffling flavor of meditation. One is, “What is the sound of one hand clapping?” Another one is, “What is your original face, before even your parents were born?” Full of paradoxes and non-sequiturs which can’t be figured out with a normal rational everyday way of thinking, zen koans can teach you to thrive in ambiguity by yourself on an app. Shout out to all the introverts out there.\n\nConclusion\n----------\n\nTo do well in system design interviews, you need to do two things: first and foremost, you need to learn the material. Then, you need to practice “not knowing” to get better at handling ambiguity. With the ability to thrive in ambiguity, nothing in life is impossible. Without that ability, you might never crack that code, have that adventure, or save the world.\n\nWatch my full system design interview below:\n\nIn this clip, I try to score some points in the first 180 seconds of the interview. Note that we swapped out both my voice and the interviewer’s voice, so it sounds a bit robotic, but the content is preserved.\n\nIf you have other topics you want us to write about, or feedback on this post, please leave it [here](https://iiosurveys.typeform.com/to/DvByB0ao).",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/never-written-code-but-passed-google-system-design",
      "author": "",
      "user_id": ""
    },
    {
      "title": "How software engineering behavioral interviews are evaluated at Meta (from an ex-Meta manager)",
      "content": "*Hey, Aline (founder of interviewing.io) here. This is the 6th post in our Guest Author series.*\n\n*One of the things I’m most excited about with the Guest Author series is the diversity of opinions it’s bringing to our blog. Technical interviewing and hiring is fraught with controversy, and not everything these posts contain will be in line with my personal opinions or the official opinions of interviewing.io. But that’s what’s great about it. After over a decade in this business, I still don’t think there’s a right way to conduct interviews, and I think hiring is always going to be a bit of a mess because it’s a fundamentally human process. Even if we don’t always agree, I do promise that the content we put forth will be curated, high quality, and written by smart people who are passionate about this space.*\n\n*If you have strong opinions about interviewing or hiring that you’ve been itching to write about, we’d love to hear from you. Please email me at [aline@interviewing.io](mailto:aline@interviewing.io).*\n\n![Author avatar](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FLior_headshot_4afce72dce.jpeg&w=384&q=75 \"Lior Neu-ner\")\n\nLior Neu-ner\n\nLior Neu-ner is a former Software Engineer and Engineering Manager at Meta, where he worked on Workplace from Meta and Whatsapp. He is currently the founder of [*Remote Rocketship*](https://www.remoterocketship.com), a job board for remote tech job openings. You can reach him on *[*LinkedIn*](https://www.linkedin.com/in/liornn)* and *[*Twitter*](https://twitter.com/LiorNn)*.\n\nHi, I’m Lior. I spent close to five years at Meta as a software engineer and engineering manager. During my time there I conducted more than 150 behavioral interviews. In this post, I’ll be sharing what Meta looks for in a behavioral interview and how we evaluated candidates.\n\nPurpose of the interview\n------------------------\n\nThe interview has two purposes:\n\n1. Assess whether a candidate has a history of demonstrating the right behaviors that would make them successful at Meta\n2. Assess the seniority of the candidate i.e. junior (IC4), senior (IC5), or staff (IC6)\n\nTo do this, we assess eight main focus areas in the interview. In no particular order, they are:\n\n* **Motivation:** What drives them? Ideal candidates are self-motivated, passionate about technologies and products that have a real impact.\n* **Ability to be Proactive:** Are they able to take initiative? Given a difficult problem, are they able to figure out how to get it done and execute?\n* **Able to work in an unstructured environment:** How well are they able to take ownership in ambiguous situations? Or do they rely on others to be told what to do?\n* **Perseverance:** Are they able to push through difficult problems or blockers?\n* **Conflict Resolution:** How well are they able to handle and work through challenging relationships?\n* **Empathy:** How well are they able to see things from the perspective of others and understand their motivations?\n* **Growth:** How well do they understand their strengths, weaknesses and growth areas? Are they making a continued effort to grow?\n* **Communication:** Are they able to clearly communicate their stories during the interview?\n\nTo assess these focus areas, we ask questions on the candidates work history and dig into the details of how they handled various situations.\n\nTo assess the candidate’s seniority, for each focus area we determine if the scope of the situation is something we’d expect for a junior, senior, or staff engineer. We’ll go into this in more detail below.\n\nInterview Format\n----------------\n\nEach interview is 45 minutes long, broken down into the following segments:\n\n* Introduction - **5 minutes**\n* Interview questions on the candidate's past work experience - **35 minutes**\n* Candidate’s turns to ask questions - **5 minutes**\n\nExample Questions and Answers\n-----------------------------\n\nBelow are questions and answers illustrating how we collect signals on the candidate for each focus area. In a typical interview, we’ll ask the candidate five or six questions and dive deep into the details of each situation. Each question may provide signals on more than focus area.\n\n### Motivation\n\nExample Questions:\n\n* “What project are you most proud of and why?”\n* “Tell me about a recent day working that was really great and/or fun.”\n\nExample Responses:\n\n* Junior: A story about a project they are proud of that had an impact on their team.\n* Senior: A story about a project they are proud of that had a large impact on their team.\n* Staff: A story about a project they are proud of that had a large impact on their org.\n\n### Ability to be Proactive\n\nExample Questions:\n\n* “Tell me about a time when you wanted to change something that was outside of your regular scope of work.”\n* “Tell me about a time you had to make a fast decision and live with the results.”\n\nExample Responses:\n\n* Junior: A story about a change they proactively suggested and drove that had an impact on their team’s focus area. Usually only requiring the candidate themselves to work on.\n* Senior: A story about a change they proactively suggested and drove that had an impact on their entire team. Usually requiring three or more people to work on.\n* Staff: A story about a change they proactively suggested and drove that had an impact on their entire org. Usually requiring two or more teams to work on.\n\n### Able to work in an unstructured environment\n\nExample Questions:\n\n* “How do you decide what to work on next?”\n* “Tell me about a project or task that was ambiguous or underspecified.”\n\nExample Responses:\n\n* Junior: A story about an ambiguous task that the candidate took ownership of and was able to drive consensus from a few stakeholders in their team. Usually only requiring the candidate themselves to work on.\n* Senior: A story about an ambiguous project that the candidate took ownership of and was able to drive consensus from stakeholders in their team or org. Usually requiring three or more people to work on.\n* Staff: A story about an ambiguous project that the candidate took ownership of and was able to drive consensus from stakeholders in their org. Usually requiring two or more teams to work on.\n\n### Perseverance\n\nExample Questions:\n\n* “Tell me about a time when you needed to overcome external obstacles to complete” a task or project.\n* “Tell me about a time a project took longer as expected.”\n\nExample Responses:\n\n* Junior: A story about a task with many technical difficulties and how they overcame each blocker.\n* Senior: A story about a project with many technical difficulties that were blocking their team and how they overcame each blocker.\n* Staff: A story about a project with many technical difficulties that were blocking many teams and how they overcame each blocker.\n\n### Conflict Resolution and Empathy\n\nExample Questions:\n\n* “Tell me about a person or team who you found most challenging to work with.”\n* “Tell me about a time you disagreed with a coworker.”\n* “Tell me about a situation where two teams couldn’t agree on a path forward.”\n\nExample Responses:\n\n* Junior: A story about how they were able to work through a disagreement with a coworker on an implementation detail of a larger project.\n* Senior: A story about how they were able to work through a disagreement with a few coworkers or team leads on the direction of a larger project.\n* Staff: A story about how they were able to work through a disagreement with two or more teams on the direction of a large project.\n\n### Growth\n\nExample Questions:\n\n* “Describe a situation when you made a mistake, and what you learned from it.”\n* “Tell me about some constructive feedback you received from a manager or a peer”\n* “Tell me about a skill set that you observed in a peer or mentor that you want to develop in the next six months.”\n\nExample Responses:\n\n* Junior: A story about a new technology they want to learn and the progress they have made to learn it.\n* Senior: A story about a soft skill or technical skill they want to developer and the progress they have made to learn it. Usually a skill that will have the potential to affect the entire team.\n* Staff: A story about a soft skill or technical skill they want to developer and the progress they have made to learn it. Usually a skill that will have the potential to affect two or more teams.\n\n### Communication\n\nGenerally covered during the interview as to how clearly they are explaining the stories. There is also some overlap with Empathy and how they communicate with others.\n\nPutting it all together\n-----------------------\n\nAfter the interview, the interviewer will compile their feedback and give a hire/no-hire recommendation as well as the candidate’s seniority. The hire/no-hire decision is given as a spectrum, ranging from low confidence to high confidence. This is illustrated below:\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/unnamed_f12d624be3.jpg)\n\nThe recommendation is compiled from the signals the interviewer collected on the eight focus areas. Typically, this looks like the following:\n\n* For junior engineers, a hire recommendation is given if they displayed positive signals on nearly all of the eight focus areas.\n* For senior engineers, a hire recommendation is given if they displayed positive signals on nearly all of the eight focus areas AND at the level we would expect for senior engineers. Otherwise, a no-hire recommendation is given.\n* For staff engineers, a hire recommendation is given if they showed positive signals on nearly all of the eight focus areas AND at the level we would expect for staff engineers. Otherwise, a no-hire recommendation is given at a staff level, although they may receive a hire recommendation at a senior level.\n\nIt’s worth mentioning that the interviewer’s decision is not final. Once all interview feedback has been collected from all interviews, the interviewers will discuss any concerns or strengths of the candidate with a “debrief committee”.",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/how-software-engineering-behavioral-interviews-are-evaluated-meta",
      "author": "",
      "user_id": ""
    },
    {
      "title": "We built voice modulation to mask gender in technical interviews. Here’s what happened.",
      "content": "[interviewing.io](https://interviewing.io/) is a platform where people can practice technical interviewing anonymously and, in the process, find jobs based on their interview performance rather than their resumes. Since we started, we’ve amassed data from thousands of technical interviews, and in this blog, we routinely share some of the surprising stuff we’ve learned. In this post, I’ll talk about what happened when we built real-time voice masking to investigate the magnitude of bias against women in technical interviews. **In short, we made men sound like women and women sound like men and looked at how that affected their interview performance. We also looked at what happened when women did poorly in interviews, how drastically that differed from men’s behavior, and why that difference matters for the thorny issue of the gender gap in tech.**\n\nThe setup\n---------\n\nWhen an interviewer and an interviewee match on our platform, they meet in a collaborative coding environment with voice, text chat, and a whiteboard and jump right into a technical question. Interview questions on the platform tend to fall into the category of what you’d encounter at a phone screen for a back-end software engineering role, and interviewers typically come from a mix of large companies like Google, Facebook, Twitch, and Yelp, as well as engineering-focused startups like Asana, Mattermark, and others. For more context, some examples of interviews done on the platform can be found on our [mock interview](https://interviewing.io/mocks) page.\n\nAfter every interview, interviewers rate interviewees on a few different dimensions.\n\n![Feedback form for interviewers](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fpseudonym_survey_86e6399c78.png&w=1200&q=75 \"Interviewer feedback form\")\n\nAs you can see, we ask the interviewer if they would advance their interviewee to the next round. We also ask about a few different aspects of interview performance using a 1-4 scale. On our platform, a score of 3 or above is generally considered good.\n\nWomen historically haven’t performed as well as men…\n----------------------------------------------------\n\nOne of the big motivators to think about voice masking was the increasingly uncomfortable disparity in interview performance on the platform between men and women.[1](#user-content-fn-1) At that time, we had amassed over a thousand interviews with enough data to do some comparisons and were surprised to discover that women really were doing worse. Specifically, **men were getting advanced to the next round 1.4 times more often than women. Interviewee technical score wasn’t faring that well either — men on the platform had an average technical score of 3 out of 4, as compared to a 2.5 out of 4 for women**.\n\nDespite these numbers, it was really difficult for me to believe that women were just somehow worse at computers, so when some of our customers asked us to build voice masking to see if that would make a difference in the conversion rates of female candidates, we didn’t need much convincing.\n\n...so we built voice masking\n----------------------------\n\nSince we started working on interviewing.io, in order to achieve true interviewee anonymity, we knew that hiding gender would be something we’d have to deal with eventually but put it off for a while because it wasn’t technically trivial to build a real-time voice modulator. Some early ideas included sending female users a Bane mask.\n\n![Cartoon of the character Bane pretending to be Batman](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F1186d_bane_72ee94351f.webp&w=640&q=75 \"Early voice masking prototype\")\n\nWhen the Bane mask thing didn’t work out, we decided we ought to build something within the app, and if you play the videos below, you can get an idea of what voice masking on interviewing.io sounds like. In the first one, I’m talking in my normal voice.\n\nAnd in the second one, I’m modulated to sound like a man.[2](#user-content-fn-2)\n\nArmed with the ability to hide gender during technical interviews, we were eager to see what the hell was going on and get some insight into why women were consistently underperforming.\n\nThe experiment\n--------------\n\nThe setup for our experiment was simple. Every Tuesday evening at 7 PM Pacific, interviewing.io hosts what we call practice rounds. In these practice rounds, anyone with an account can show up, get matched with an interviewer, and go to town. And during a few of these rounds, **we decided to see what would happen to interviewees’ performance when we started messing with their perceived genders**.\n\nIn the spirit of not giving away what we were doing and potentially compromising the experiment, we told both interviewees and interviewers that we were slowly rolling out our new voice masking feature and that they could opt in or out of helping us test it out. Most people opted in, and we informed interviewees that their voice might be masked during a given round and asked them to refrain from sharing their gender with their interviewers. For interviewers, we simply told them that interviewee voices might sound a bit processed.\n\nWe ended up with 234 total interviews (roughly 2/3 male and 1/3 female interviewees), which fell into one of three categories:\n\n* Completely unmodulated (useful as a baseline)\n* Modulated without pitch change\n* Modulated with pitch change\n\nYou might ask why we included the second condition, i.e. modulated interviews that didn’t change the interviewee’s pitch. As you probably noticed, if you played the videos above, the modulated one sounds fairly processed. The last thing we wanted was for interviewers to assume that any processed-sounding interviewee must summarily have been the opposite gender of what they sounded like. So we threw that condition in as a further control.\n\nThe results\n-----------\n\nAfter running the experiment, we ended up with some rather surprising results. **Contrary to what we expected** (and probably contrary to what you expected as well!), **masking gender had no effect on interview performance** with respect to any of the scoring criteria (would advance to next round, technical ability, problem solving ability). If anything, we started to notice some trends in the opposite direction of what we expected: for technical ability, it appeared that men who were modulated to sound like women did a bit better than unmodulated men and that women who were modulated to sound like men did a bit worse than unmodulated women. Though these trends weren’t statistically significant, I am mentioning them because they were unexpected and definitely something to watch for as we collect more data.\n\nOn the subject of sample size, we have no delusions that this is the be-all and end-all of pronouncements on the subject of gender and interview performance. We’ll continue to monitor the data as we collect more of it, and it’s very possible that as we do, everything we’ve found will be overturned. I will say, though, that had there been any staggering gender bias on the platform, with a few hundred data points, we would have gotten some kind of result. So that, at least, was encouraging.\n\nSo if there’s no systemic bias, why are women performing worse?\n---------------------------------------------------------------\n\nAfter the experiment was over, I was left scratching my head. If the issue wasn’t interviewer bias, what could it be? I went back and looked at the seniority levels of men vs. women on the platform as well as the kind of work they were doing in their current jobs, and neither of those factors seemed to differ significantly between groups. But there was one nagging thing in the back of my mind. I spend a lot of my time poring over interview data, and I had noticed something peculiar when observing the behavior of female interviewees. Anecdotally, it seemed like women were leaving the platform a lot more often than men. So I ran the numbers.\n\nWhat I learned was pretty shocking. **As it happens, women leave interviewing.io roughly 7 times as often as men after they do badly in an interview.** And the numbers for two bad interviews aren’t much better. You can see the breakdown of attrition by gender below (the differences between men and women are indeed statistically significant with P < 0.00001).\n\nAlso note that as much as possible, I corrected for people leaving the platform because they found a job (practicing interviewing isn’t that fun after all, so you’re probably only going to do it if you’re still looking), were just trying out the platform out of curiosity, or they didn’t like something else about their interviewing.io experience.\n\nA totally speculative thought experiment\n----------------------------------------\n\nSo, if these are the kinds of behaviors that happen in the interviewing.io microcosm, how much is applicable to the broader world of software engineering? Please bear with me as I wax hypothetical and try to extrapolate what we’ve seen here to our industry at large. And also, please know that what follows is very speculative, based on not that much data, and could be totally wrong… but you gotta start somewhere.\n\nIf you consider the attrition data points above, you might want to do what any reasonable person would do in the face of an existential or moral quandary, i.e. fit the data to a curve. An exponential decay curve seemed reasonable for attrition behavior, and you can see what I came up with below. The x-axis is the number of what I like to call “attrition events”, namely things that might happen to you over the course of your computer science studies and subsequent career that might make you want to quit. The y-axis is what portion of people are left after each attrition event. The red curve denotes women, and the blue curve denotes men.\n\n![Chart showing the gender gap](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fe92a1_gender_gap_v2_d243a94167.webp%3Fupdated_at%3D2022-12-08T14%3A31%3A14.044Z&w=1920&q=75 \"Thar be gender gap\")\n\nSee interactive graph with Demos: <https://www.desmos.com/calculator/tugmyjkaj6>\n\nNow, as I said, this is pretty speculative, but it really got me thinking about what these curves might mean in the broader context of women in computer science. How many “attrition events” does one encounter between primary and secondary education and entering a collegiate program in CS and then starting to embark on a career? So, I don’t know, let’s say there are 8 of these events between getting into programming and looking around for a job. If that’s true, then we need 3 times as many women studying computer science than men to get to the same number in our pipelines. Note that that’s 3 times more than men, not 3 times more than there are now. If we think about how many there are now, which, depending on your source, is between 1/3 and a 1/4 of the number of men, **to get to pipeline parity, we actually have to increase the number of women studying computer science by an entire order of magnitude**.\n\nPrior art, or why maybe this isn’t so nuts after all\n----------------------------------------------------\n\nSince gathering these findings and starting to talk about them a bit in the community, I began to realize that there was some supremely interesting academic work being done on gender differences around self-perception, confidence, and performance. Some of the work below found slightly different trends than we did, but it’s clear that anyone attempting to answer the question of the gender gap in tech would be remiss in not considering the effects of confidence and self-perception in addition to the more salient matter of bias.\n\nIn a [study investigating the effects of perceived performance to likelihood of subsequent engagement](https://s3.wp.wsu.edu/uploads/sites/252/2014/10/EhrlingerDunning2003.pdf), Dunning (of Dunning-Kruger fame) and Ehrlinger administered a scientific reasoning test to male and female undergrads and then asked them how they did. Not surprisingly, though there was no difference in performance between genders, women underrated their own performance more often than men. Afterwards, participants were asked whether they’d like to enter a Science Jeopardy contest on campus in which they could win cash prizes. Again, women were significantly less likely to participate, with participation likelihood being directly correlated with self-perception rather than actual performance.[3](#user-content-fn-3)\n\nIn a different study, [sociologists followed a number of male and female STEM students over the course of their college careers](https://www.researchgate.net/publication/232564809_The_Validity_and_Utility_of_Selection_Methods_in_Personnel_Psychology) via diary entries authored by the students. One prevailing trend that emerged immediately was the difference between how men and women handled the “discovery of their [place in the] pecking order of talent, an initiation that is typical of socialization across the professions.” For women, realizing that they may no longer be at the top of the class and that there were others who were performing better, “the experience [triggered] a more fundamental doubt about their abilities to master the technical constructs of engineering expertise [than men].”\n\nAnd of course, what survey of gender difference research would be complete without an allusion to the wretched annals of dating? When I told the interviewing.io team about the disparity in attrition between genders, the resounding response was along the lines of, “Well, yeah. Just think about dating from a man’s perspective.” Indeed, [a study published in the *Archives of Sexual Behavior*](https://link.springer.com/article/10.1023/B:ASEB.0000028892.63150.be) confirms that men treat rejection in dating very differently than women, even going so far as to say that men “reported they would experience a more positive than negative affective response after… being sexually rejected.”\n\nMaybe tying coding to sex is a bit tenuous, but, as they say, programming is like sex — one mistake and you have to support it for the rest of your life.\n\nWhy I’m not depressed by our results and why you shouldn’t be either\n--------------------------------------------------------------------\n\nPrior art aside, I would like to leave off on a high note. I mentioned earlier that men are doing a lot better on the platform than women, but here’s the startling thing. **Once you factor out interview data from both men and women who quit after one or two bad interviews, the disparity goes away entirely.** So while the attrition numbers aren’t great, I’m massively encouraged by the fact that at least in these findings, it’s not about systemic bias against women or women being bad at computers or whatever. Rather, it’s about women being bad at dusting themselves off after failing, which, despite everything, is probably a lot easier to fix.\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. Roughly 15% of our users are female. We want way more, but it’s a start. [↩](#user-content-fnref-1)\n2. If you want to hear more examples of voice modulation or are just generously down to indulge me in some shameless bragging, we got to demo it on [NPR](https://www.npr.org/2016/04/12/473912220/blind-hiring-while-well-meaning-may-create-unintended-consequences) and in [Fast Company](https://www.fastcompany.com/3059522/this-interviewing-platform-changes-your-voice-to-eliminate-unconscious-bias). [↩](#user-content-fnref-2)\n3. In addition to asking interviewers how interviewees did, [we also ask interviewees to rate themselves](https://strapi-iio.s3.us-west-2.amazonaws.com/8ddad_interviewee_feedback_be549dadfb.png). After reading the Dunning and Ehrlinger study, we went back and checked to see what role self-perception played in attrition. In our case, the answer is, I’m afraid, TBD, as we’re going to need more self-ratings to say anything conclusive. [↩](#user-content-fnref-3)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/voice-modulation-gender-technical-interviews",
      "author": "",
      "user_id": ""
    },
    {
      "title": "We co-wrote the official sequel to Cracking the Coding Interview!",
      "content": "**EDIT 2:** [Read 9 chapters of the book for free!](https://bctci.co/free-chapters) These include:\n\n* The first seven chapters of the book, covering topics such as why technical interviews are broken, what recruiters won't tell you, why not to spend a lot of time on resumes, and how to get in the door at companies without a referral.\n* Two technical chapters: Sliding Windows and Binary Search. Our new take on Binary Search teaches one template that works for every binary search problem on LeetCode, with only a single-line change you need to remember. The Sliding Windows chapter features 6 unique sliding window templates that make off-by-one errors a thing of the past.\n\n**EDIT:** [*Beyond Cracking the Coding Interview*](https://www.amazon.com/dp/195570600X) is out now! Here's the table of contents:\n\n![Beyond CtCI table of contents](https://strapi-iio.s3.us-west-2.amazonaws.com/cover_toc_b77bec4593.png)\n\nI have some exciting news. Along with Gayle Laakmann McDowell, Mike Mroczka, and Nil Mamano, I’m writing the official sequel to *Cracking the Coding Interview* (often called the bible of technical interview prep). It's fittingly called [*Beyond Cracking the Coding Interview.*](https://www.amazon.com/dp/195570600X)\n\nI’ve always wanted to write a book about technical interviewing. And this is it. And of course it'll draw on all the hiring data we've collected over the past decade at interviewing.io.\n\nTechnical interviews are much harder today than they used to be. Engineers study for months and routinely get down-leveled despite that. [*Beyond Cracking the Coding Interview*](https://www.amazon.com/dp/195570600X), in addition to covering a bunch of new questions and topics, teaches you how to think instead of memorizing. Grinding and memorization isn’t the way in this market (though in fairness, it’s never really the way). With us, you’ll still have to do the work, of course, but we’ll teach you to work smarter.\n\n[![Beyond Cracking the Coding Interview book cover](https://strapi-iio.s3.us-west-2.amazonaws.com/book_cover_87f67665fd.png)](https://www.amazon.com/dp/195570600X)\n\nWe added at least thirteen new technical topics (I say “at least” because we’re still writing, and it might be more like twenty)—and over 150 new problems. Each problem includes step-by-step walkthroughs, and you can work each problem with our (actually good) AI Interviewer. And of course this book was written in partnership with interviewing.io. We’ve pulled in data from over 100k FAANG mock interviews on interviewing.io, and we include hundreds of curated interview replays from interviewing.io (shared with permission of course) – watch people make mistakes and learn so you’re not doomed to repeat them.\n\nBut it’s not *just* about interview prep. In today’s job market, the bar is higher but it’s also harder than ever to get noticed and run your job search end-to-end. My excellent co-authors killed it on the technical chapters. I focused on writing the job search stuff, including, but not limited to:\n\n* How to negotiate, exactly what to say, and how to not screw up your negotiations before they even start\n* How to manage your job search, end to end, and balance interview prep with applications and outreach\n* A worksheet to help you figure out what order you need to engage with the companies you’re targeting to ensure that all your offers come in at the same time\n* How to get in the door at top companies without relying on referrals, including email templates and examples of good and bad outreach\n* An internal look at FAANG (and other) company rubrics to help understand what interviewers really care about, no matter what company you're applying to\n* What you need to know about behavioral interviews, whether you want to or not, and how to avoid the mistakes that even great engineers make\n* A list of very specific questions to ask your interviewers (not just to look smart but to learn useful things)\n* How technical interviews got to be so broken and how to get over hating them so you can win\n\nI also spend some time on owning and sharing data on how flawed technical interviewing is and, most importantly, how to manage your psychology so you can get past that. I see so many engineers opting out of this interview style, arguably for good reason. But you’re leaving a lot of good opportunities on the table, and it doesn’t have to be like that.\n\nThis book is so much of what I’ve blogged about for the last 15 years, but it’s fleshed out with much more detail and actionable advice. If you read it, let me know what you think. Technical interviewing sucks (and so does looking for a job). But this book will help you do it well and get out alive.\n\n**Purchases of [*Beyond Cracking the Coding Interview*](https://www.amazon.com/dp/195570600X) get a $50 discount for interviewing.io. The book costs $45, so it’s not a bad deal. The book is out in January of 2025, and you can [get it on Amazon](https://www.amazon.com/dp/195570600X).**",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/we-co-wrote-the-official-sequel-to-cracking-the-coding-interview-introducing-beyond-ctci",
      "author": "",
      "user_id": ""
    },
    {
      "title": "People are still bad at gauging their own interview performance. Here’s the data.",
      "content": "[interviewing.io](https://interviewing.io/) is a platform where people can practice technical interviewing anonymously, and if things go well, get jobs at top companies in the process. We started it because [resumes suck](https://blog.alinelerner.com/resumes-suck-heres-the-data/) and because we believe that anyone, regardless of how they look on paper, should have the opportunity to prove their mettle.\n\nAt the end of 2015, we published a [post about how people are terrible at gauging their own interview performance](https://interviewing.io/blog/people-cant-gauge-their-own-interview-performance-and-that-makes-them-harder-to-hire). At the time, we just had a few hundred interviews to draw on, so as you can imagine, we were quite eager to rerun the numbers with the advent of more data. **After drawing on roughly one thousand interviews, we were surprised to find that the numbers have *really* held up, and that people continue to be terrible at gauging their own interview performance.**\n\nThe setup\n---------\n\nWhen an interviewer and an interviewee match on interviewing.io, they meet in a collaborative coding environment with voice, text chat, and a whiteboard and jump right into a technical question (feel free to watch this process in action on our [recordings](https://interviewing.io/mocks) page). After each interview, people leave one another feedback, and each party can see what the other person said about them once they both submit their reviews.\n\nIf you’re curious, you can see what the feedback forms look like below — in addition to one direct yes/no question, we also ask about a few different aspects of interview performance using a 1-4 scale. We also ask interviewees some extra questions that we don’t share with their interviewers, and one of those questions is about how well they think they did. For context, a technical score of 3 or above seems to be the rough cut-off for hirability.\n\n![Screenshot showing the interviewing.io interview feedback form for interviewers](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F49613_interviewer_feedback_ae7debf69a.webp%3Fupdated_at%3D2022-12-13T17%3A27%3A22.302Z&w=1920&q=75 \"Feedback form for interviewers\")\n\nFeedback form for interviewers\n\n\n\n![Screenshot showing the interviewing.io interview feedback form for interviewees](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F8ddad_interviewee_feedback_3f34ef7a0a.webp%3Fupdated_at%3D2022-12-13T17%3A30%3A40.759Z&w=1920&q=75 \"Feedback form for interviewees\")\n\nFeedback form for interviewees\n\nPerceived versus actual performance… revisited\n----------------------------------------------\n\nBelow are two heatmaps of perceived vs. actual performance per interview (for interviews where we had both pieces of data). In each heatmap, the darker areas represent higher interview concentration. For instance, the darkest square represents interviews where both perceived and actual performance was rated as a 3. You can hover over each square to see the exact interview count (denoted by “z”).\n\nThe first heatmap is our old data:\n\nAnd the second heatmap is our data as of August 2016:\n\n**As you can see, even with the advent of a lot more interviews, the heatmaps look remarkably similar.** The [R-squared](https://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit) for a linear regression on the first data set is 0.24. And for the more recent data set, it’s dropped to 0.18. In both cases, even though some small positive relationship between actual and perceived performance does exist, it is not a strong, predictable correspondence.\n\nYou can also see there’s a non-trivial amount of impostor syndrome going on in the graph above, which probably comes as no surprise to anyone who’s been an engineer. Take a look at the graph below to see what I mean.\n\nThe x-axis is the difference between actual and perceived performance, i.e. actual minus perceived. In other words, a negative value means that you overestimated your performance, and a positive one means that you underestimated it. Therefore, every bar above 0 is impostor syndrome country, and every bar below zero belongs to its foulsome, overconfident cousin, the [Dunning-Kruger](https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect) effect.[1](#user-content-fn-1)\n\nOn interviewing.io (though I wouldn’t be surprised if this finding extrapolated to the qualified engineering population at large), **impostor syndrome plagues interviewees roughly twice as often as Dunning-Kruger**. Which, I guess, is better than the alternative.\n\nWhy people underestimate their performance\n------------------------------------------\n\nWith all this data, I couldn’t resist digging into interviews where interviewees gave themselves 1’s and 2’s but where interviewers gave them 4’s to try to figure out if there were any common threads. And, indeed, a few trends emerged. **The interviews that tended to yield the most interviewee impostor syndrome were ones where question complexity was layered.** In other words, the interviewer would start with a fairly simple question and then, when the interviewee completed it successfully, they would change things up to make it harder. Lather, rinse, repeat. In some cases, an interviewer could get through up to 4 layered tiers in about an hour. Inevitably, even a good interviewee will hit a wall eventually, even if the place where it happens is way further out than the boundary for most people who attempt the same question.\n\n**Another trend I observed had to do with interviewees beating themselves up for issues that mattered a lot to them but fundamentally didn’t matter much to their interviewer**: off-by-one errors, small syntax errors that made it impossible to compile their code (even though everything was semantically correct), getting big-O wrong the first time and then correcting themselves, and so on.\n\n**Interestingly enough, how far off people were in gauging their own performance was independent of how highly rated (overall) their interviewer was or how strict their interviewer was.**\n\nWith that in mind, if I learned anything from watching these interviews, it was this. Interviewing is a flawed, human process. Both sides want to do a good job, but sometimes the things that matter to each side are vastly different. And sometimes the standards that both sides hold themselves to are vastly different as well.\n\nWhy this (still) matters for hiring, and what you can do to make it better\n--------------------------------------------------------------------------\n\nTechniques like layered questions are important to sussing out just how good a potential candidate is and can make for a really engaging positive experience, so removing them isn’t a good solution. And there probably isn’t that much you can do directly to stop an engineer from beating themselves up over a small syntax error (especially if it’s one the interviewer didn’t care about). However, all is not lost!\n\nAs you recall, during the feedback step that happens after each interview, we ask interviewees if they’d want to work with their interviewer. **As it turns out, there’s a very statistically significant relationship between whether people think they did well and whether they’d want to work with the interviewer. This means that when people think they did poorly, they may be a lot less likely to want to work with you**. And by extension, it means that in every interview cycle, some portion of interviewees are losing interest in joining your company just because they didn’t think they did well, *despite the fact that they actually did*.\n\nHow can one mitigate these losses? Give positive, actionable feedback immediately (or as soon as possible)! This way people don’t have time to go through the self-flagellation gauntlet that happens after a perceived poor performance, followed by the inevitable rationalization that they *totally* didn’t want to work there anyway.\n\nFootnotes\n---------\n\n1. I’m always terrified of misspelling “Dunning-Kruger” and not double-checking it because of overconfidence in my own spelling abilities. [↩](#user-content-fnref-1)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/own-interview-performance",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Technical phone screen superforecasters",
      "content": "*Hey, Aline (founder of interviewing.io) here. This is the third post in our Guest Author series.*\n\n*In this post, our latest Guest Author looks at interviews from the company’s perspective. So much engineering time goes into interviewing… [we know this firsthand](https://interviewing.io/blog/you-probably-dont-factor-in-engineering-time-when-calculating-cost-per-hire-heres-why-you-really-should), but what can be done about it? Some companies solve this problem by introducing homework. In this post, Alexey digs into some historical data to unearth a really clever, elegant way to save eng time that’s also better for candidate experience!*\n\n*If you have strong opinions about interviewing or hiring that you’ve been itching to write about, we’d love to hear from you. Please email me at [aline@interviewing.io](mailto:aline@interviewing.io) to get started.*\n\n![Author avatar](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ff3f50_alexey_scaled_e1606329022499_edef8e39a7.webp&w=384&q=75 \"Alexey Komissarouk\")\n\nAlexey Komissarouk\n\nAlexey Komissarouk is a growth engineering leader. He is currently Head of Growth Engineering at MasterClass. Before that, he spent 2016-2020 at Opendoor, first as an early engineer, then as an Engineering Manager. Between 2013 and 2016, he built out a product engineering consulting company, helping clients such as Dropbox, Pebble, Boomerang, and Binti grow and expand lines of business through a combination of product management and engineering. In his other lives, Alexey co-founded a boutique work+travel company, Hacker Paradise. Since the company’s inception in 2015, they’ve run trips to over a dozen locations and been joined by more than 800 alumni.\n\n“The new VP wants us to double engineering’s headcount in the next six months. If we have a chance in hell to hit the hiring target, you seriously need to reconsider how fussy you’ve become.”\n\nIt’s never good to have a recruiter ask engineers to lower their hiring bar, but he had a point. It can take upwards of [100 engineering hours](https://interviewing.io/blog/you-probably-dont-factor-in-engineering-time-when-calculating-cost-per-hire-heres-why-you-really-should) to hire a single candidate, and we had over 50 engineers to hire. Even with the majority of the team chipping in, engineers would often spend multiple hours a week in interviews. Folks began to complain about interview burnout. Also, fewer people were actually getting offers; the *onsite pass rate* had fallen by almost a third, from ~40% to under 30%. This meant we needed even more interviews for every hire.\n\n[Visnu](https://twitter.com/visnup) and I were early engineers bothered most by the state of our hiring process. We dug in. Within a few months, the **onsite pass rate** went back up, and interviewing burnout receded. **We didn’t lower the hiring bar, though. There was a better way.**\n\nIntroducing: the Phone Screen Team\n----------------------------------\n\nWe took the company’s best technical interviewers and organized them into a dedicated Phone Screen Team. No longer would engineers be assigned between onsite interviews and preliminary phone screens at recruiting coordinators’ whims. The Phone Screen Team specialized in phone screens; everybody else did onsites.\n\nWhy did you think this would be a good idea?\n--------------------------------------------\n\nHonestly, all I wanted at the start was to see if I was a higher-signal interviewer than my buddy Joe. So I graphed people’s phone screen pass rate against how those candidates performed in their onsite pass rate.\n\nJoe turned out to be the better interviewer. More importantly, I stumbled into the fact that a number of engineers doing phone screens performed consistently better across the board. They both had more candidates pass their phone screens and then those candidates would get offers at a higher rate.\n\n![Charts comparing expected vs actual pass rates](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F3e78e_alexeypost1_e6116fd5f4.webp&w=1920&q=75 \"Onsite Pass Rate vs Phone Screen Rate\")\n\nSample data, recreated for illustrative purposes\n\nThese numbers were consistent, quarter over quarter. As we compared the top quartile of phone screeners to everybody else, the difference was stark. Each group included a mix of strict and lenient phone screeners; on average, both groups had a phone screen pass rate of 40%.\n\nThe similarities ended there: the top quartile’s invitees were twice as likely to get an offer after the onsite (50% vs 25%). These results also were consistent across quarters.[1](#user-content-fn-1)\n\nArmed with newfound knowledge of phone screen [superforecasters](https://en.wikipedia.org/wiki/Superforecaster), the obvious move was to have them do all the interviews. In retrospect, [it made a ton of sense](https://medium.com/@alexallain/what-ive-learned-interviewing-500-people-the-interviewer-skills-ladder-for-high-growth-software-37778d2aae85) that some interviewers were “just better” than others.\n\nA quarter after implementing the new process, the “phone screen to onsite” rate stayed constant, but the “onsite pass rate” climbed from ~30% to ~40%, shaving more than 10 hours-per-hire [2](#user-content-fn-2). Opendoor was still running this process when I left several years later.\n\nYou should too. [3](#user-content-fn-3) [4](#user-content-fn-4)\n\nStarting your own Phone Screen Team\n-----------------------------------\n\n### 1. Identifying interviewers\n\nGet your Lever or Greenhouse (or [ATS](https://en.wikipedia.org/wiki/Applicant_tracking_system) of choice) into an analyzable place somewhere, and then quantify how well interviewers perform.[5](#user-content-fn-5) There’s lots of ways to analyze performance; here’s a simple approach which favors folks who generated lots of offers from as few as possible onsites and phone screens.\n\n![Phone screener quality equation](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fc21cb_screen_shot_2020_11_25_at_10_52_57_am_539a0e3740.webp&w=1920&q=75 \"Phone screener quality\")\n\nYou can adjust the constants to where zero would match a median interviewer. A score of zero, then, is good. Your query will look something like this:\n\n| **Interviewer** | **Phone Screens** | **Onsites** | **Offers** | **Score** |\n| --- | --- | --- | --- | --- |\n| Accurate Alice | 20 | 5 | 3 | (45 – 20 – 20) / 20 = 0.25 |\n| Friendly Fred | 20 | 9 | 4 | (60 – 36 – 20) / 20 = 0.2 |\n| Strict Sally | 20 | 4 | 2 | (30 – 16 – 20) / 20 = -0.3 |\n| Chaotic Chris | 20 | 10 | 3 | (45 – 40 – 20) / 20 = -0.75 |\n| No Good Nick | 20 | 12 | 2 | (30 – 48 – 20) / 30 = -1.9 |\n\nIdeally, hires would also be included in the funnel, since a great phone screen experience would make a candidate more likely to join. I tried including them; unfortunately, the numbers get too small and we start running out of statistical predictive power.\n\n### 2. Logistics & Scheduling\n\nPhone Screen interviewers no longer do onsite interviews (except as emergency backfills). The questions they ask are now retired from the onsite interview pool to avoid collisions.\n\nAsk the engineers to identify and block off 4 hour-long weekly slots to make available to recruiting (recruiting coordinators will love you). Use a tool like [youcanbook.me](https://youcanbook.me/) or [calendly](https://calendly.com/) to create a unified availability calendar. Aim to have no more than ~2.5 interviews per interviewer per week. To minimize burnout, one thing we tried was to take 2 weeks off interviewing every 6 weeks.\n\nTo avoid conflict, ensure that interviewers’ managers are bought in to the time commitment and incorporate their participation during performance reviews.\n\n### 3. Onboarding Interviewers\n\nWhen new engineers join the company and start interviewing, they will initially conduct on-site interviews only. If they perform well, consider inviting them into the phone screen team as slots open up. Encourage new members to keep the same question they were already calibrated on, but adapt it to the phone format as needed. In general, it helps to [make the question easier and shorter](https://triplebyte.com/blog/how-to-interview-engineers) than if you were conducting the interview in person.\n\nWhen onboarding a new engineer onto the team, have them shadow a current member twice, then be reverse-shadowed by that member twice. Discuss and offer feedback after each shadowing.\n\n### 4. Continuous Improvement\n\nInterviewing can get repetitive and lonely. Fight this head-on by having recruiting coordinators add a second interviewer (not necessarily from the team) to join 10% or so of interviews and discuss afterwards.\n\nHold a monthly retrospective with the team and recruiting, with three items on the agenda:\n\n* discuss potential process improvements to the interviewing process\n* review borderline interviews with the group to review together, if [your interviewing tool](https://coderpad.io) supports recording and playback\n* have interviewers read through feedback their candidates got from onsite interviewers and look for consistent patterns\n\n### 5. Retention\n\nEventually, interviewers may get burnt out and say things like “I’m interviewing way more people than others on my actual team – why? I could just go do onsite interviews.” This probably means it’s time to rotate them out. Six months feels about right for a typical “phone screen team” tour of duty, to give people a rest. Some folks may not mind and stay on the team for longer.\n\nBuy exclusive swag for team members. Swag are cheap and these people are doing incredibly valuable work. Leaderboards (“Sarah interviewed 10 of the new hires this year”) help raise awareness. Appreciation goes a long way.\n\nAlso, people want to be on teams with cool names. Come up with a cooler name than “Phone Screen Team.” My best idea so far is “Ambassadors.”\n\nConclusion\n----------\n\nThere’s something very Dunder Mifflin about companies that create Growth Engineering organizations to micro-optimize conversion, only to have those very growth engineers struggle to focus due to interview thrash from an inefficient hiring process. These companies invest millions into hiring, coaching and retaining the very best sales people. Then they leave recruiting – selling the idea of working at the company – in the hands of an engineer that hasn’t gotten a lick of feedback on their interviewing since joining two years ago, with a tight project deadline on the back of her mind.\n\nIf you accept the simple truth that not all interviewers are created equal, that the same rigorous quantitative process with which you improve the business should also be used to improve your internal operations, and if you’re trying to hire quickly, you should consider creating a Technical Phone Screen Team.\n\nFAQs, Caveats, and Preemptive Defensiveness\n-------------------------------------------\n\nFootnotes\n---------\n\n1. **Was this statistically significant, or are you conducting pseudoscience?** Definitely pseudoscience. Folks in the sample were conducting about 10 interviews a month, ~25 per quarter. Perhaps not yet ready to [publish in Nature](https://idp.nature.com/authorize?response_type=cookie&client_id=grover&redirect_uri=https%3A%2F%2Fwww.nature.com%2Farticles%2Fd41586-019-00857-9) but meaningful enough to infer from, especially considering the relatively low cost of being wrong. [↩](#user-content-fnref-1)\n2. **Why didn’t the on-site pass rate double, as predicted?** First, not all of the top folks ended up joining the team. Second, the best performers did well because of a combination of skill (great interviewers, friendly, high signal) and luck (got better candidates). Luck is fleeting, resulting in a [regression to the mean](https://en.wikipedia.org/wiki/Regression_toward_the_mean). [↩](#user-content-fnref-2)\n3. **What size does this start to make sense at**? Early on, you should just identify who you believe your [best interviewers](https://interviewing.io/blog/our-business-depends-on-having-the-best-interviewers-so-we-built-an-interviewer-rating-system-and-you-can-too) are and have them (or yourself) do all the phone screens. Then, once you start hiring rapidly enough that you are doing about 5-10 phone screens a week, run the numbers and invite your best 2-3 onsite interviewers to join and create the team. [↩](#user-content-fnref-3)\n4. **What did you do for specialized engineering roles?** They had their own dedicated processes. Data Science ran a take home, Front-End engineers had their own Phone Screen sub-team, and Data and ML Engineers went through the general full-stack engineer phone screen. [↩](#user-content-fnref-4)\n5. **Didn’t shrinking your Phone Screener pool hurt your diversity?** In fact, the opposite happened. First, the phone screener pool had a higher percentage of women than the engineering organization at the time; second, a common interviewing anti-pattern is “hazing” – asking difficult questions and then rejecting somebody for “not even remembering about Kahn’s algorithm, lolz.” The best phone screeners don’t haze, bringing a more diverse group onsite. [↩](#user-content-fnref-5)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/technical-phone-screen-superforecasters",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Why resume writing is snake oil",
      "content": "I just asked ChatGPT to size the global resume writing industry. Here’s what it had to say:\n\n*The global resume writing industry was valued at approximately $1.37 billion in 2024 and is expected to grow steadily, reaching around $1.44 billion by 2025, and about $1.59 billion by 2033... The growth is driven by increased demand for professional resume services due to heightened job market competition, coupled with advancements in technology and personalization through AI-driven resume writing tools.*\n\nI don’t know if these numbers are exactly true, and I don't know what portion of that is resume writing for engineers specifically, but it doesn’t really matter. I am certain that they are directionally correct. As the market has gotten worse, I’ve heard more and more job seekers ask for resume reviews and rewrites, and I’ve seen many companies in the interview prep space start offering resume reviews.\n\nThey’re all selling snake oil, and no one should spend a dime on it. I’ll explain why in a little bit, but first let’s talk about something else I found on the internet.\n\nA few days ago, I saw [this post on Reddit](https://www.reddit.com/r/codingbootcamp/comments/1jhitoc/recruiter_accidently_emailed_me_her_secret/?share_id=WXxsyzj2kiIrpK7t6Xj66). It was a leaked internal set of hiring requirements (sometimes called a “hiring spec”) that looked like this:\n\n![Image 1 from reddit post showing leaked internal set of hiring requirements](https://strapi-iio.s3.us-west-2.amazonaws.com/recruiter_accidently_emailed_me_her_secret_internal_v0_bbcbihi64bqe1_b9ed19715a.png)\n  \n![Image 2 from reddit post showing leaked internal set of hiring requirements](https://strapi-iio.s3.us-west-2.amazonaws.com/recruiter_accidently_emailed_me_her_secret_internal_v0_j2stzky64bqe1_89a24b8843.png)\n  \n\nOf course, there was the usual Reddit shock and awe and pearl clutching about whether this hiring spec could be real.\n\nYes, it’s real. As someone who’s been in hiring for over a decade, I’m certain of it. And not only is it real, but it’s routine. It’s business as usual.\n\nI’ve been a head of talent at top startups, and I used to run my own recruiting agency where I hired for a bunch of companies who have since become household names. When I worked as an agency recruiter 10 years ago, companies regularly shared documents like this one with me. The only difference between then and now is the idea of a “diversity bonus.” Everything else hasn’t changed in a decade.\n\nDocuments like this are why I quit recruiting to start interviewing.io.\n\nAnd documents like this are the reason that the entirety of the resume writing profession is a snake oil pit.\n\nThere is one notable exception to this rule, which I’ll talk about, but most people should not spend a dime on resume writers. Here’s why.\n\nRecruiters aren’t reading your resume. They’re skimming it for very specific things.\n------------------------------------------------------------------------------------\n\nIn 2024, we ran a [study](https://interviewing.io/blog/are-recruiters-better-than-a-coin-flip-at-judging-resumes) where we asked 76 recruiters to look at resumes and indicate which candidates they’d want to interview. Recruiters are most likely to contact you if:\n\n* You look good on paper (i.e., you have top-tier companies and/or schools on your resume… just like in the “hiring spec” above)\n* You belong to a group that’s been traditionally underrepresented in tech (i.e., you’re a woman or a person of color… just like in the “hiring spec” above)\n* To some extent, if you have niche skills (e.g., ML engineering)\n\nWhat's missing? Things like, for example, having a quantifiable impact or demonstrating teamwork. Essentially, everything recruiters look for is stuff that you either have or you don't.\n\nIn this same study, we also learned that when recruiters do look at resumes, they spend an average of 30 seconds reviewing them. That's not enough time to read every bullet. Instead, they are mainly skimming for recognizable companies and schools.\n\nHere is an excellent example, [also from Reddit](https://www.reddit.com/r/recruitinghell/comments/qhg5jo/this_resume_got_me_an_interview/), that makes this difference very clear.\n\n![this resume got me an interview 1.png](https://strapi-iio.s3.us-west-2.amazonaws.com/this_resume_got_me_an_interview_1_4beabb1543.png)\n\nThis resume certainly passes the skim-test: good companies, appropriate roles, and a good university too. It's only when you actually spend more than 30 seconds reading the resume that you learn that not only is this resume obviously fake, but it also celebrates accomplishments like \"Spread Herpes STD to 60% of intern team.” And yet, it got a 90% callback rate. Recruiters just aren't reading the details.\n\n**In other words, either you already have what recruiters are looking for (which often may be different than what’s explicitly listed in a job description… because they certainly aren’t sharing the real “hiring spec”) or you don’t. If you have it, then you don’t need a resume writer — though it’s always smart to make it easier for recruiters to find the things they’re looking for. If you don’t have what they’re looking for, no amount of agonizing over how you present yourself is going to move the needle.**\n\nSo, if recruiters aren’t reading and are just skimming for brands, why do people agonize over their resumes and give money to resume writers?\n\nWhy the resume myth persists\n----------------------------\n\nIn interviewing.io’s Discord server, I regularly see requests for resume reviews. I also see other interview prep companies charging money for resume reviews. Presumably they charge because the demand is there. But why are people willing to pay for something that is completely useless?\n\nI think it’s a mix of misinformation and the desire for control.\n\nRecruiters rarely admit that they’re skimming primarily for brands. If you read recruiters’ advice for job seekers, it almost always includes advice about quantifying your impact, including your side projects[1](#user-content-fn-1), and so on. These bits of advice are well-intentioned, I’m sure, but they perpetuate a harmful myth and an exploitative resume writing cottage industry.\n\nThe other reason is control. Job searches are intimidating, and putting yourself out there is hard. It’s much easier to retreat to the comfort of polishing up your bullet points because it’s something you can control. You get into a routine, rewrite your bullets, and upload your resume to a bunch of places. Then when you don’t hear back, you retreat to familiar ground, grind on your bullets some more, and rinse and repeat. Because it’s easier to believe that if you can just get your bullets right, you’ll finally hear back. That narrative sure beats out the idea that no one is reading your resume no matter how much you fine-tune it.\n\nThe notable exception: If you already look good on paper, polishing your resume CAN be useful.\n----------------------------------------------------------------------------------------------\n\nIf you’re fortunate enough to have top brands on your resume, cleaning it up can be a good use of your time. I still wouldn’t hire a resume writer because the details don’t matter very much. Just make sure that recruiters can easily spot the brands.\n\nHere’s an example. Take a look at the before and after screenshots of the resume below.\n\n**Before**\n\n![Before version of a resume from one of interviewing.io's users](https://strapi-iio.s3.us-west-2.amazonaws.com/resume_before_b8d5aa1c34.png)\n\nThis resume belongs to one of our users who was kind enough to let us share it. He actually has two of the three things that recruiters look for: FAANG experience and a niche title (ML engineer). But both are buried! And the section that gets the most attention is wasted on undergraduate awards.\n\nAs you can see, he spent almost 3 years at Apple, but a recruiter skimming his resume might not notice that because it was a while ago. Instead, he showcases an undergrad award and some technologies/languages that he knows. Neither of those is nearly as useful to recruiters as FAANG experience.\n\nHis current title is also ML engineer, and one at the Principal level at that. But it wasn’t always: He went from back-end to SRE to a little bit of everything to ML, and because of that, it’s possible a recruiter would miss it as well.\n\nAfter\n\n![After version of a resume from one of interviewing.io's users](https://strapi-iio.s3.us-west-2.amazonaws.com/resume_after_c148ae3b62.png)\n\nWe edited this candidate’s resume to put all the things recruiters look for at the very top of the resume and moved the buzzword soup to the bottom. This candidate is obviously well-positioned because he has FAANG experience, several top schools, and niche skills — but before, many recruiters didn’t spot them. After he made these changes, the number of interviews he got increased by 8X.\n\nNote that we didn’t really rewrite anything. We just moved stuff around. You can do this yourself without needing a professional writer.\n\nFor everyone else, stop working on your resume, and start doing outreach.\n-------------------------------------------------------------------------\n\nIf you’re like most people, you don’t have top brands on their resume, so no amount of rewriting is going to move the needle much. Instead of agonizing over it, stop applying and start doing outreach to hiring managers. It’s your best shot to get noticed and to get someone to look at you as a human being, instead of a collection of brands. [Here’s how to do it](https://interviewing.io/blog/how-to-get-in-the-door-at-top-companies-cold-out-reach-to-hiring-managers-part-2). For a deeper dive into both resume writing and how to get in the door, you can read [*Beyond Cracking the Coding Interview*](https://www.amazon.com/dp/195570600X) (both [chapters are also available for free](https://bctci.co/free-chapters)).\n\nFootnotes:\n\nFootnotes\n---------\n\n1. Will side projects help you get a job? Good question and one that should be teased apart a bit. Getting a job has two components: getting in the door and doing well in interviews. In general, side projects are useless for getting in the door. Yes, every once in a while, a side project goes viral. Or if you build something really cool with your target company’s API, it can get some attention. But that’s pretty rare. Most side projects that adorn resumes go completely unnoticed. When it comes to performing well in interviews, it depends. If the companies you’re interviewing at test you on practical skills, then they can be a great use of time. They can also be a great use of time to help you understand how APIs work, how the internet works, how clients and servers talk to each other, and so on. But if the companies you’re targeting primarily ask algorithmic questions, then side projects probably aren’t the best use of time. Finally, will side projects make you a better engineer? Absolutely. And that’s the best reason to do them. But that’s not quite the same as getting a job, is it? Once you're actively looking for a job, your time is better spent on interview prep and outreach. [↩](#user-content-fnref-1)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/why-resume-writing-is-snake-oil",
      "author": "",
      "user_id": ""
    },
    {
      "title": "My manager is not promoting me. What should I do? Advice from an eng manager at Amazon, Meta, and Microsoft.",
      "content": "![Author avatar](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fswift_kraken_2_0a2319b378.jpg&w=384&q=75 \"Swift Kraken\")\n\nSwift Kraken\n\nSwift Kraken (the author’s handle on interviewing.io; he asked to remain anonymous for this post) has held engineering management roles at Amazon, Meta, and Microsoft. His passion lies in the art of assembling exceptional teams dedicated to crafting outstanding products. The views expressed here are his own and do not represent those of his current or past employers.\n\nThroughout my engineering career, I have encountered the question: *How can I grow myself to get promoted?* I’ve been an engineering manager at Amazon, Meta, and Microsoft. I’ve been in many promotion reviews, and here is my advice on what it takes to get a promotion.\n\nThere are three core criteria required to get a promotion:\n\n1. **Readiness**: Are you ready for the promotion? Have you been consistently operating at the next level for a sufficient period of time?\n2. **Perception of readiness** in the organization: Perception matters equally to actual performance — it is important that your organization perceive you to be performing at the next level.\n3. **Business need**: Let’s assume that there is a clear business need for a higher-level person in the team; otherwise, a promotion won’t take place regardless of your readiness.\n\nThis post will focus on evaluating **Employee Readiness**, and, in a future post I will address the **Perception of Readiness**.\n\nUnderstanding your readiness for the next step in your career is vital. Not getting promoted or missing an opportunity can be frustrating when you’re actually prepared but aren’t actively pushing for advancement. Let me share two examples from my past experience, with names changed for privacy.\n\nMeet Sam, a skilled engineer who could solve complex problems with elegant code. He joined my team feeling frustrated that his previous manager failed to give him the promotion he believed he deserved. Over time, I noticed that Sam focused solely on technical aspects, neglecting how his solutions impacted end-users. In our infrastructure team, responsible for building APIs for internal customers, Sam’s proposals often led to prolonged discussions and missed deadlines. While he saw success in delivering APIs, the organization valued adoption as a key indicator. I coached him to take ownership and understand that just writing APIs wasn’t enough — he needed to create solutions that aligned with organizational goals.\n\nNext, let’s meet Sara, a friend and a seasoned backend engineer proficient in Java and cloud technologies such as AWS and Azure. She excelled in optimizing backend solutions, showcasing meticulous problem-solving skills. Sara’s expertise lay in crafting efficient and scalable server-side architectures. She led a small project team and mentored junior engineers. She always stayed at the forefront of backend development trends and brought innovative ideas for the team. She was already performing as Staff engineer, but despite her skills, she remained at the same level for a long time. She had self-doubt, and she never asked for the promotion she deserved. I encouraged her to evaluate herself, and we worked together to boost her confidence. It didn’t take long for her to secure the promotion.\n\nIt’s essential to note that not all managers proactively work on helping their employees get promotions; sometimes, you will need to clearly voice your aspirations because louder individuals might overshadow your achievements. In both cases, a clear understanding of readiness for the next level empowered these engineers to take the necessary steps and achieve their career goals.\n\nI’ve put together two brief quizzes to assess your preparedness for the next career level: one for aspiring Senior Engineers and one for aspiring Staff Engineers. These quizzes aim to pinpoint areas where you might need skill development. Disclaimer: Every project, team, and company is unique. These quizzes serve as a general evaluation and may not account for the specific nuance present in each individual situation.\n\n[Quiz: Are you ready to be promoted to Senior Software Engineer?](https://iiosurveys.typeform.com/to/xRlaefap)  \n[Quiz: Are you ready to be promoted to Staff Software Engineer?](https://iiosurveys.typeform.com/to/jjoMSTa1)\n\n*Thanks to [Vivek](https://www.linkedin.com/in/vkirub/), [Medha](https://www.linkedin.com/in/immedha/), [Aline](https://www.linkedin.com/in/alinelerner/) and [Liz](https://www.linkedin.com/in/liz-graves-95765b114/) for their meticulous proofreading and valuable suggestions in this post! In my next post, we will discuss how to evaluate the perception of your readiness in the organization and provide some tips to improve it.*\n\nWant to know if you’re ready to interview at the Senior or Staff level? Do anonymous mock interviews with Senior, Staff, or Principal engineers from FAANG and other top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/my-manager-is-not-promoting-me-advice-from-amazon-meta-microsoft-eng-mgr",
      "author": "",
      "user_id": ""
    },
    {
      "title": "It's OK to postpone your interviews if you're not ready",
      "content": "At interviewing.io, we’ve seen hundreds of thousands of engineers go through job searches, and the biggest mistakes we see people make are all variations on the same theme: not postponing their interview when they aren’t ready.\n\nI’ve found myself repeating that it’s OK to postpone interviews so often that I finally broke down and decided to make it a blog post. It’s very simple advice, so the bulk of this post will be spent trying to convince you that it’s fine to postpone. And then of course I’ll tell you what to say.\n\nDo any of these situations sound familiar?\n\n* A recruiter from a FAANG (or other top-tier) company contacts you out of the blue and invites you to interview. You do the recruiter call, and it goes well. The recruiter reaches out to schedule your technical phone screen. You haven’t practiced enough, and you know it, but you are scared to tell the recruiter that you want to postpone the interview by a few months (months?!) because the position may no longer be there. So, you plow ahead, do the phone screen, fail it, and then you’re frozen out for a year.\n* You were able to postpone your technical phone screen and take the time to study up on data structures & algorithms problems. You do well in the interview. But then your recruiter schedules your onsite the following week. Because you were so focused on DS&A prep, you haven’t had much time to study up on system design. You fumble the system design interview during the onsite, and you either get rejected or get down-leveled.\n\nBoth of these situations are extremely common, but they’re both preventable. You can just ask to postpone. There are a few edge cases where that’s not a good idea, but in most situations, it’s the right thing to do.\n\nWhen postponing is OK, and when you *shouldn’t* take our advice\n---------------------------------------------------------------\n\nIf you’re applying to a large company with a centralized process (in other words, a process where you interview first and get matched with a team later), postponing is almost always OK. These companies are perpetually hiring, and their open roles are evergreen.[1](#user-content-fn-1)\n\nEven if you’re applying to a large company with a decentralized process (where you interview for a specific team), we recommend postponing unless you’re extremely excited about the team you’re talking to. In that scenario, it’s possible that if you postpone, the slot will be filled. But if it’s not a perfect fit and you’d be OK with another team, we recommend postponing — in the worst case, you’ll simply get slotted into a different team.\n\nThe only time when postponing isn’t a good idea is when you’re applying to a very small company that has just one open headcount. In that scenario, it is possible that postponing will cost you the opportunity because they’ll choose another candidate. However, you can ask how likely that is to happen, up front.\n\nWith that edge case out of the way, here’s a little-known fact about how timing works at large companies: Recruiters don’t really care when you interview. Though they’d prefer that you interview sooner rather than later so they can hit their numbers, at the end of the day, they’d rather be responsible for successful candidates than unsuccessful ones.\n\nEvery recruiter, in every job search, will tell you that time is of the essence because of all the other candidates in the pipeline. Most of the time, that is irrelevant and just something they say to create an artificial sense of urgency. There are always other candidates in the pipeline because the roles are evergreen. But they have nothing to do with your prospects.\n\nExactly what to say to postpone your interviews\n-----------------------------------------------\n\nYou can use this text verbatim when postponing your interviews, and, with some small edits, you can even use it several times (e.g., before the phone screen and then again before the onsite).\n\n> *I’m really excited about interviewing at [company name]. Unfortunately, if I’m honest, I haven’t had a chance to practice as much as I’d like. I know how hard and competitive these interviews are, and I want to put my best foot forward. I think I’ll realistically need a couple of months to prepare. How about we schedule my interview for [date]?*\n\nOne important thing to remember is to be conservative about how long it will take. You’ve probably heard the adage about how, when you have to estimate the time an engineering task will take, you should think of a number and double it. Here, you may even want to triple it.\n\nI’ve seen many candidates ask for 2 weeks because that feels like a reasonable thing to ask for, only to have it blow up in their face when they realize they need to ask for another extension (which is still worth doing but harder because the company may think you’re taking them for a ride). If you need two months, ask for two months.\n\nPostponing can also be a good way to control the timing of your job search\n--------------------------------------------------------------------------\n\nThis section is extra credit, but once you get comfortable with postponing your interviews when you’re not prepared, you can use the same skills to batch your interviews and ultimately control the timing of your job search.\n\nWhat does it mean to control the timing of your job search? Ideally, you want all of your offers to come in at the same time, both because it maximizes optionality (one company that arbitrarily offers first doesn’t rush you into making a decision) and maximizes leverage (you can negotiate from a position of power).\n\nIf you want to dive deeper into this process, take a look at a book I recently co-wrote, *[Beyond Cracking the Coding Interview](https://www.amazon.com/dp/195570600X)*. It includes a full chapter about how to manage your job search, which covers everything from determining the order in which to approach companies to how to speed them up and slow them down once you’re in process. There’s a lot more detail than I can touch in this post, and much of it depends on your specific circumstances, but you can probably get 50% of the way there just by postponing your interviews in batches.\n\nThe big insight here is that, except for the edge cases we discussed above, a recruiting process can be paused at any point.\n\nIn other words, you can do a bunch of outreach to companies, then do a bunch of enthusiastic recruiter calls, and THEN pause all the processes until you’re prepared to do technical phone screens.\n\nThen, you batch the phone screens.\n\nFinally, if needed, you pause again to give yourself time to prepare for onsites. Onsite interviews require a different skill set than technical phone screens. The technical phone screen isn’t about depth or fit — it’s just a way to cut people who aren’t likely to pass the onsite.\n\nThe onsite, on the other hand, isn’t just meant to cut poor performers. It’s at once a deeper dive into your technical ability and a way to gauge fit. If you’ll be interviewing with your future team (typical at companies with a decentralized process), it’s also meant to assess your ability to work together, collaborate on hard things, complement the team's existing skillset, and so on. It usually has some coding (to verify that your technical phone screen wasn’t a fluke), but the focus is usually on system design and behavioral interviews[2](#user-content-fn-2), which are also the interviews commonly used for leveling decisions.\n\nSo, onsite prep is much more about system design and getting your stories right for your behavioral interviews. Some people can pull off prepping for both coding and sys design/behavioral at the same time. For many, depending on their existing familiarity with the material, it’s a tall order. So, it’s wise to take the time you need and prepare.\n\nThen, once you’re ready, you batch the onsites.\n\nWhen your offers come in, you should ask for extensions as needed, speed companies up, and start [negotiating](https://interviewing.io/blog/category/salary-negotiation) (which we’ve written about in the past and which, of course, is covered at great depth in the book).\n\nFootnotes:\n\nFootnotes\n---------\n\n1. Yes, it’s true that in 2022, we saw several FAANGs and many other companies freeze hiring, and if you had postponed your interviews, you’d have been left out in the cold. Despite how devastating these freezes were to affected candidates (and to the tech economy as whole), they are extremely rare, and in our humble opinion, not worth optimizing for or worrying about. You are very unlikely to have to deal with an impending freeze. [↩](#user-content-fnref-1)\n2. Some companies will also have technical deep dives, project presentations, assessments of niche skills, and so on. [↩](#user-content-fnref-2)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/its-ok-to-postpone-your-interviews-if-youre-not-ready",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Why you shouldn’t list certifications on LinkedIn",
      "content": "People often suggest that interviewing.io should create a certification that our users can post on their LinkedIn profile, e.g., something like “Top 10% performer on interviewing.io”. Presumably, these certifications would signal to recruiters that this person is a good engineer and worth reaching out to and should carry more signal than where said person went to school or worked previously.\n\nI think certifications are a terrible idea, and I’ve resisted building them. Simply put, the incentives they create for engineers and recruiters are all wrong. To explain what I mean, let’s split engineers into two distinct personas:\n\n* **The engineer who looks good on paper. This person will simply not list the certification on their profile – they have no reason to!** They’re already getting contacted by at least 10 recruiters a day. Chances are, they’re actively ignoring LinkedIn unless they’ve decided that they’re looking and want to respond to a handful of the hundreds of InMails they’ve gotten already to give themselves more optionality during their job search.\n* **The engineer who doesn’t look good on paper. This person will likely add a certification to their profile. It’s rational – anything that helps them stand out and legitimize their profile, in the absence of traditional pedigree, is a win. That sounds great, right? Unfortunately, here’s the rub. Unless your certification is respected by recruiters and well-established, they will not take it seriously… because it runs counter to everything they’ve been tuned to look for**. If your profile doesn’t have a reputable school or a top company on it, then an unknown certification won’t save you, and over time, listing it will do harm. Because the certification will almost always go hand in hand with lack of pedigree (as you saw above, people who look good on paper will have no reason to list it on their profiles), recruiters will start to develop negative associations with it. Because most recruiters will not engage with these candidates for long enough to find out if they’re good or not, this will happen even if the certification actually carries a positive signal. In other words, recruiters will learn, over time, to associate the presence of the certification with a “no-go” because it will only be on profiles that have previously been trained not to reach out to.\n\nThat’s the theory of why certifications are bad. They’re bad for the individuals listing them, and they’re bad for the industry as a whole because, ironically, they make it harder to find good candidates. But what happens when you look at the data?\n\nThe setup\n---------\n\nEngineers use interviewing.io for anonymous mock interviews. If things go well, they skip right to the technical interview at real companies (which is also fully anonymous). We started interviewing.io because resumes suck and because we believe that anyone, regardless of how they look on paper, should have the opportunity to prove their mettle.\n\nAt this point, we’ve hosted over 100k technical interviews, split between mocks and real ones.\n\nRegardless of the interview type, when an interviewer and an interviewee match on our platform, they join a collaborative coding environment with voice, text chat, and a whiteboard, and jump right into a technical interview. After each interview, both parties leave feedback, and once they’ve both submitted, each one can see what the other person said and how they were rated.\n\nHere’s what the feedback form that interviewers fill out looks like:\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/unnamed_1_d9f00a2248.png)\n**In this post, we aggregated scores from these interviews for each interviewee and then cross-referenced how many certifications they listed on their LinkedIn profiles.**\n\nYou might say that an engineer’s performance in interviews on our platform isn’t the canonical source of truth for their engineering ability, and you’d certainly be right. In the absence of holistic performance review data about our users, which is pretty much impossible to get, we decided running this study was still worthwhile. For what it’s worth, I have pretty high conviction that performance in interviews on our platform correlates very strongly with performance in interviews in the real world – our candidates tend to pass real interviews 3X better than candidates from other sources. Fully closing that loop with on-the-job performance data is the holy grail of any recruiting enterprise, and I hope we can do it one day.\n\nThe hypothesis\n--------------\n\nCaveats aside, before doing the analysis, our hypothesis was that having one or more certifications on your profile would have a strong negative correlation with your interview performance. Why?\n\n1. We’ve seen in the past that though [having attended a top school doesn’t correlate with interview performance](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt), having worked at a top employer does. We saw above that people who look good on paper aren’t going to be incentivized to put certifications on their profile, which means that we’ve just cut a lot of top performers from the pool.\n2. This leaves people who don’t look good on paper. Because top performers aren’t evenly distributed between pedigreed and unpedigreed candidates, there will be fewer top performers in this pool. But even if they were, by definition, only a small part of this population will be top performers. Therefore, most of the people in this pool will not be top performers, which means that most of the people who are incentivized to list certifications on their profile will not be top performers.\n\n**Basically, people who look good on paper are disincentivized to list certifications. People who don’t look good on paper are incentivized to list them, but most of them are not top performers. Therefore, most of the people listing certifications are not top performers.**\n\nIf we’re right, not only is creating certifications not useful, but doing so will also have the unfortunate side effect of making recruiters dig in their heels about the importance of pedigree, will, over time, reduce the marginal utility of any new certifications and will ultimately harm attempts at making technical recruiting more fair or meritocratic.\n\nWhat the data actually says\n---------------------------\n\nIn this analysis, we took a list of our users for whom we had interview data and, where possible, analyzed their LinkedIn profiles. **We ended up analyzing about 20K LinkedIn profiles, 28% of which had some kind of certification. We then pulled out the top 10 most frequent certification authorities, so we could break them out and do some more granular analysis.** These were (in order of frequency, i.e. Coursera had the most hits):\n\n* Coursera\n* LinkedIn\n* Triplebyte\n* Microsoft\n* Amazon Web Services (AWS)\n* Oracle\n* Udacity\n* Udemy\n* HackerRank\n* Cisco\n\nBecause people usually do multiple interviews on our platform, we ended up with about 40k observations (i.e., interviews) in each regression.\n\nOur first result is that people with certifications do worse in interviews, as shown in the bar chart below. **People with certifications on their LinkedIn profiles pass interviews on our platform about 53% of the time versus 57% of the time for people without certifications, a very statistically significant difference (p < 0.00001).** Remember that these interviews are completely anonymous. The interviewer isn’t basing their ratings on the person’s LinkedIn—just their interview performance.\n\nWant to make sure your interview performance is in tip-top shape, regardless of your LinkedIn? Sign up for anonymous mock interviews with engineers from top companies.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/certification_effect_eb27700235.png)\n\n**Notably, the certification “penalty” is equally large whether people had the certifications up in 2021 or 2023. So there’s no sign that today’s depressed labor market changed the nature of the signal.**\n\nHow much of this difference is due to the fact that people with certifications do different kinds of interviews? We next adjusted for the language (e.g., Python, Java) and focus (e.g., frontend, machine learning) of the interview and only compare Java coders to Java coders, Python to Python, etc. This control ensures that the results are not driven by broad patterns at the group level, asking whether certifications are predictive of performance compared only to candidates coding in the same language. If anything, the “Interview language & focus controls” bar shows that this makes certifications look slightly worse. When you compare certified people to non-certified people within the narrow types of interviews they typically choose, they lag just a bit further behind.\n\nNext, we wanted to know how much of this difference is explainable by the attributes of the person. For example, people who seek out certifications may have a non-quantitative background. Perhaps they majored in communications rather than computer science. Or they are a paralegal trying to switch career paths. **Indeed, we observe this pattern in the data: people with non-traditional backgrounds are about 30% more likely to have a certification.**\n\nTo account for this selection, we just compared people with similar backgrounds. For instance, does a Harvard graduate with a certification do better or worse than a Harvard graduate without one? This correction shrinks the gap by about 40 percent (see the “Pedigree controls” bar).\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/certification_effect_controls_592ab3c690.png)\n\nNote: All differences between the certified and uncertified users are statistically significant (p < .01 or smaller).\n\n**So, LinkedIn certifications are indeed a negative tag for candidates on our platform. This isn’t explained by the kinds of interviews they do. But we can show that part of it is due to the fact that certified people tend to have non-traditional backgrounds. The remainder of the gap is probably due to similar dynamics: you get certified if you have something to prove.**\n\n### Not all certifications are created equal\n\nThis analysis treats certifications as binary. Either you have it or you don’t. But there are a range of authorities out there that give certifications: are any of them a positive tag?\n\nWe did similar regression analysis for the top ten certifiers in the data. The results are below:\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/certification_effect_by_company_47d9163301.png)\n\nThe one standout is Triplebyte. Their graduates are 6 percentage points more likely to pass interviews —a serious boost, albeit not enough to dispel the negative signal that all the others carry. The worst is the Cisco badge, with a 10 percentage point drop in performance.\n\nConclusion\n----------\n\nWhen we dug into the data, we saw that people from non-traditional backgrounds do indeed list certifications on their LinkedIn profile more often than their well-pedigreed counterparts. People whose most recent schooling is a web development certificate or associate's degree score are about 30% more likely to display a certification.\n\nWe also saw that, generally speaking, certifications carry a negative signal and that these results hold up even in the increasingly employer-favorable 2023 job market (in other words, good candidates haven’t suddenly started listing certifications on their profiles to get noticed).\n\nAs we expected, these realities create an unfortunate feedback loop. Recruiters tend to value pedigree above all else, which means they’re less likely to talk to non-traditional candidates. When they see non-traditional candidate profiles with certifications, because they weren’t going to talk to them anyway, over time they’ll develop a negative association with those certifications.\n\nFurthermore, given that people who list certifications are more likely to perform worse in interviews, when they choose pedigreed candidates who have a certification and those candidates perform worse, that negative association will be strengthened.\n\nBecause of these mechanics, certifications get reinforced as bad in recruiters’ minds, and listing them on your profile ends up being a counterproductive strategy for diamonds in the rough, the very candidates whom certifications were supposed to help in the first place.",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/why-you-shouldnt-list-certifications-on-linkedIn",
      "author": "",
      "user_id": ""
    },
    {
      "title": "I’ve conducted over 600 technical interviews on interviewing.io. Here are 5 common problem areas I’ve seen.",
      "content": "*Hey, Aline (founder of interviewing.io) here. This is the second post in our Guest Author series The first post talked about* [red flags you might encounter while interviewing with companies](https://interviewing.io/blog/6-red-flags-i-saw-while-doing-60-technical-interviews-in-30-days)*. Complementarily, this post,* *authored by one of our prolific, long-time interviewers*, *explores common missteps that interviewees make.*\n\n*One of the things I’m most excited about with the Guest Author series is the diversity of opinions it’s bringing to our blog. Technical interviewing and hiring is fraught with controversy, and not everything these posts contain will be in line with my opinions or the official opinions of interviewing.io. But that’s what’s great about it. After over a decade in this business, I still don’t think there’s a right way to conduct interviews, and I think hiring is always going to be a bit of a mess because it’s a fundamentally human process. Even if we don’t always agree, I do promise that the content we put forth will be curated, high quality, and written by smart people who are passionate about this space.*\n\n*If you have strong opinions about interviewing or hiring that you’ve been itching to write about, we’d love to hear from you. Please email me at [aline@interviewing.io](mailto:aline@interviewing.io) to get started.*\n\n![Author avatar](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FIan_Douglas_44c37f40a1.jpg&w=384&q=75 \"Ian Douglas\")\n\nIan Douglas\n\nWilliam Ian Douglas goes by “Ian”, and uses he/him pronouns. He lives in the Denver, Colorado region and graduated from a Computer Engineering program in 1996. His career spans back-end systems, API architecture, DevOps/DBA duties and security, and has been a team lead managing small teams, and Director of Engineering. Ian branched out into professional technical interview coaching in 2014, and in 2017 pivoted his entire career to teaching software development for the Turing School of Software & Design in the Denver area. He joined interviewing.io as a contract interviewer in the summer of 2017 and is a big fan of the data analytics blog posts that IIO produces to help expose and eliminate bias in our tech industry interviews. Ian writes technical coaching information at [https://techinterview.guide](https://techinterview.guide/) and you can reach him on [Twitter](https://twitter.com/iandouglas736), [LinkedIn](https://www.linkedin.com/in/iandouglas736) and [GitHub](https://github.com/iandouglas).\n\n**I recently conducted my 600th interview on interviewing.io (IIO). I’d like to share lessons learned, why I approach interviews the way that I do, and shed some light on common problem areas I see happen in technical interviews.** Every interviewer on the platform is different, and so your results may vary. We have some excellent folks helping out on the platform, and have a wonderful community working to better ourselves.\n\nThe interviewing.io Mock Interview\n----------------------------------\n\nDuring our interviews on IIO, we rate people on three 4-point scales. A score of 1 means they did extremely poorly, and a 4 means they did extremely well in that category. I typically start my interview where everyone gets 3 out of 4 points right away, and then earn/lose points as the interview goes on.\n\nEvery interviewer on the platform will have some aspect that they favor over others. My own bias as an interviewer tends to be around communication and problem solving, which I’ll point out below.\n\n### Technical Proficiency\n\nIn this category, I grade a candidate on how proficient they seem in their language of choice, whether they had significant problems coding an algorithm of a particular style, if I needed to give a lot of hints during coding.\n\n### Problem Solving\n\nHere, I grade a candidate on how well they break the problem into smaller pieces, come up with a strategy for solving the smaller problems, and also debugging issues along the way. The ability to think through problems while debugging is just as important as writing the code in the first place. Are they stumped when a problem happens, or are they able to find the root cause on their own?\n\n### Communication\n\nInterviewers really want to hear your decision-making process. This is also very important when debugging code. I tended to hire folks who would fit in well on smaller teams or clusters of developers. With that in mind, collaboration and easy communication are a good way to win me over.\n\nCommon Problem Areas I See in Interviews\n----------------------------------------\n\nHere are the top problem areas I see in interviews, not just on IIO, but in general. I hope you find this advice helpful.\n\n### Common Problem Area 1: Jumping into code too soon\n\nI see this in developers of all types and levels, but mostly in the “intermediate” level of 2-5 years of experience. They hear a problem, talk about a high-level design for 30 seconds or less, and are eager to get coding. They feel like they’re on a timer. They want to rush to get things finished. It’s a race to the finish line. First one across the finish line is the winner.\n\nRight?\n\nNope.\n\nPlease, slow down. Plan your work. And share your thought process along the way.\n\nPeople who take time to think out a mid-level design, whether that’s pseudocode or just writing out notes of their approach, tend to spend less time debugging their code later. Folks who jump right into coding fall into what I call “design-as-you-go” problems, where you’re spending lots of time refactoring your code because you need to change a parameter passed or a return value, or wait, that loop is in the wrong place, etc.. This is very easy to spot as an interviewer.\n\nSpending some time on mid-level design doesn’t guarantee your success, but it might save you time in the long run by thinking through your plan a little deeper, and that extra time you bought could be used to fix problems later.\n\nAlso, as an interviewer, I want to see you succeed. Especially if you’re “on-site” (in person, or remote nowadays) because you’re costing our company a lot more money in an on-site interview process. While I need to be fair to all candidates in the amount of help I can give, if I can see your design ahead of time, and spot a flaw in that design, I can ask leading questions to guide you to the problem and correct your approach earlier.\n\n**If you jump straight into code, I have no idea if your implementation is even going to work, and that’s not a great place to put your interviewer. It’s much harder for me to correct a design when you have 100 lines of Java code written before I really understand what’s going on in your code.**\n\nI saw this lack of planning backfire in a horrible way in a real interview in 2012. The candidate was brought to my interview room by someone in HR, asked if they would like a bottle of water, and promised to return. We introduced ourselves and got down to the technical challenge. The candidate shared no details, no design, barely talked about a high-level approach, wrote nothing down, and started writing code on a whiteboard. (This would be my second-to-last whiteboard interview I ever conducted, I hate whiteboard interviews!) HR showed up a few minutes later, knocking loudly on the door, offering the bottle of water and leaving. The candidate, grateful for a drink, uncapped the bottle and started to take a sip when this awful, draining look came over their face. **The distraction of delivering a bottle of water made them completely lose their train of thought, and I couldn’t help them recover because they hadn’t shared any details with me about their approach.** They spent several minutes re-thinking the problem and starting over.\n\nOn the “other side” of this coin, however, you can spend “too long” on the design stage and run out of time to implement your genius plan. I’ve seen candidates talk through a mid-level design, then write notes, then manually walk through an example with those notes to really make sure their plan is a good one, and now they only have a few minutes left to actually implement the work. Extra points on communication, maybe, but we need to see some working code, too.\n\nSo what’s the best approach here?\n\nI typically recommend practicing until you spend about 5 minutes thinking through high-level design choices, 5 minutes to plan and prove the mid-level design, and then get to work on code. The good news here is that “practice makes better” — the more you practice this design break-down and problem solving, the better you’ll get. More on this later.\n\n### Common Problem Area 2: Communicating “Half-thoughts”\n\nThis is a term I’ve coined over the years, where you start to say a thought out loud, finish the thought in your head, and then change something about your code. It usually sounds something like this:\n\n*“Hmm, I wonder if I could … … … no, never mind, I’ll just do this instead.”*\n\nBack to my bias for communication.\n\nInterviewers want to know what’s going on in your thought process. It’s important that they know how you’re making decisions. How are you qualifying or disqualifying ideas? Why are you choosing to implement something in a particular way? Did you spot a potential problem in your code? What was it?\n\nThis missing information is a hidden treasure for your interviewer. It takes mere seconds to change your communication to something more like this:\n\n*“I wonder if … hmm … well, I was thinking about implementing this as a depth-first-search, but given a constraint around \\_\\_\\_ I think a better approach might be \\_\\_\\_, what do you think?”*\n\n**That took maybe 2 or 3 extra seconds, and you’ve asked for my opinion or buy-in, we can consider possibilities together, and now we’re collaborating on the process.** You already feel like my future coworker!\n\n### Common Problem Area 3: Not asking clarifying questions\n\nAn interview challenge I often ask as a warm-up question goes something like this:\n\n*You have a grouping of integer numbers. Write a method that finds two numbers that add up to a given target value, stop immediately, and report those numbers. Return two ‘null’ values if nothing is found.*\n\nThis is a great question that shows me how you think about algorithms and the kinds of assumptions you make when you hear a problem.\n\nI’ve been coding for a pretty long time. Since 1982, actually. There’s no data structure called “a grouping” in any language I’ve ever used. So what assumptions are you going to make about the problem?\n\nMost candidates immediately assume the “grouping” of numbers is in an array. You can successfully solve this problem by using an array to store your numbers. Your algorithm will likely be an O(n^2) (n-squared) algorithm because you’ll be iterating over the data in an exponential way: for each value, iterate through the rest of the values. There’s a more efficient way to solve this in O(n) time by choosing a different data structure.\n\nGo ahead and ask your interviewer questions about the problem. If they tell you to make your own assumptions that’s different, but ask if they’re good assumptions. Ask if there are alternate data sets that you’ll be using as test cases which could impact your algorithm.\n\n### Common Problem Area 4: Assuming your interviewer sets all the rules\n\nWait, what?\n\nYeah, you read me right.\n\nYes, you’re there for the interview, but you’re there to show them how you’ll work on the team, and teams work best when there is clear, open communication and a sense of collaboration. Spend the first few minutes of the interview setting expectations, especially around communication and work process.\n\n**There’s nothing wrong with having this kind of chat with your interviewer: “My typical work process in a technical challenge like this is to spend a minute or two thinking quietly about the problem and writing down notes, I’ll share those thoughts with you in a moment to get your input. Then, while I code, I tend to work quietly as well, but I’ll be sure to pause now and then to share my thought process as I go, and then walk you through the code more thoroughly before we run it the first time. Would that be okay with you, or do you have different expectations of how you’d like me to communicate or work through the problem?”**\n\nI promise you’ll blow their mind. Most interviewers won’t be ready for you to take their expectations into consideration like this. It shows that you’ll work well on a team. You’re setting the environment where you’re advocating for yourself, but also being considerate of others. You’re stating your intentions up front, and giving them the opportunity to collaborate on the process.\n\n### Common Problem Area 5: Not asking for help sooner\n\nAs your interviewer, I have a small amount of help that I’m likely able to provide during a technical challenge. I can’t coach you through everything, obviously, but I’d rather give you a hint, deduct a point on a rubric, and see you ultimately succeed at the problem, than to struggle silently and spin in circles and make us both feel like the interview is a waste of time.\n\nAs a professional interviewer and an instructor at a software school, I’ve become pretty good at asking leading questions to guide you to a realization or answer without me giving you the solution.\n\n**It’s okay to admit when you’re stuck. It doesn’t make you a failure, it makes you human. Let your interviewer know what you’re thinking and where you’re having problems.** Listen very carefully to their response, they might be offering a clue to the problem, or might give you more thorough advice on how to proceed.\n\nMy Favorite Resources to Share\n------------------------------\n\nWhen our interviews at IIO are over, I like to dive into a lot of feedback on their process and where I think they could use extra practice to improve. Generally, I spend 10 to 20 minutes, sometimes going way beyond my one-hour expected time slot, to answer questions for someone, and going into more detail on things. I **LOVE** to help people on IIO.\n\nHere are a few common areas of advice I offer to folks.\n\n### Communication\n\nThere’s nothing worse than listening to your own recorded voice. But all IIO interviews are recorded, and I often tell folks in my feedback and in the review notes I type up afterward to listen to the last few minutes of the interview recording to review the feedback I give them. You can also pause those recordings and grab a copy of your code at any time. (These recordings are of course private to your and your interviewer.)\n\n**During the playback, listen to your own thought process and how you communicate your ideas. As you work through other challenges, find a way to record yourself talking through the problem out loud if possible, and play that back for yourself. You’ll get better at articulating full and complete thoughts.**\n\n### Problem Solving and Mid-Level Design\n\nThe more common practice sites like HackerRank, CodeWars, LeetCode, etc, are great for writing a coded algorithm, but don’t give you any way to exercise your design process.\n\nI send my students to [Project Euler](https://projecteuler.net). **Euler was a mathematician, so the problems on the website will generally be pretty math-heavy, but you can change the problems to be whatever you’re comfortable building. If you don’t know how to calculate a prime number, that’s fine, swap that out for whether a number is equally divisible by 17 or something instead.**\n\nI like Project Euler because the challenges there are just word problems. You have to think of everything: the algorithm, which data structure(s) to use, and especially how to break the problem into smaller pieces.\n\nOne of my favorite problems is #19 in their archive: counting how many months between January 1901 and December 1999 began on a Sunday. They give you the number of days in each calendar month, tell you January 1st 1900 is a Monday, and how to calculate a leap year. The rest is up to you.\n\nThe more you expose yourself to different types of problems, the better you’ll get at spotting patterns.\n\n### Practice, Practice, Practice\n\nOne piece of advice we give our students is to practice each technical challenge several times. Our executive director, Jeff Casimir, tells students to practice something 10 times. That feels like a big effort. I aim more for 3 to 4 times, and here’s my reasoning:\n\nThe first time you solve a problem, all you’ve done is solve the problem. You might have struggled through certain parts, but your only real achievement here is finishing.\n\nIf you erase your work and start it a second time, you might think of a different approach to solving the problem, maybe a more efficient solution. Maybe not, but at least you’re getting practice with this kind of problem.\n\nNow erase your work and do it a third time. Then a fourth time. These are the times when you will start to actively build a memory of the strategy it takes to solve this particular problem. This “muscle memory” will help you when you see other technical challenges, where you’ll start to spot similarities. “Oh, this looks like the knapsack problem” and because you’ve solved that several times, the time you take on high level design and mid-level design just shortened quite a lot.\n\nOne of my favorite technical challenges can be solved using a handful of different algorithms (DFS, BFS, DP, etc). If you think you can solve a problem in a similar fashion, solve it 3 or 4 times with each of those algorithms as well. You’ll get REALLY good at spotting similarities, and have a great collection of strategies to approach other technical problems.\n\nShameless Self-Promotion\n------------------------\n\nI’ve been writing up notes for aspiring new developers at <https://techinterview.guide>. It’s not complete, but I have a lot of my own thoughts on preparing for technical interviews, networking and outreach, resumes and cover letters, and so on. I still have a few chapters to write about negotiation tactics and graceful resignations, but I’m happy to take feedback from others on the content.\n\nI also have a daily email series covering several kinds of interview questions, but not from a perspective of how to answer the questions perfectly, there are plenty of resources out there to do that. Instead, I examine questions from an interviewer’s perspective — what am I really asking, what do I hope you’ll tell me, what do I hope you won’t say, and so on. A small preview, for example: when you’re asked “Tell me about yourself” they’re not really asking for your life story. They’re really asking “Tell me a summary of things about you that will make you a valuable employee here”.",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/ive-conducted-over-600-technical-interviews-on-interviewing-io-here-are-5-common-problem-areas-ive-seen",
      "author": "",
      "user_id": ""
    },
    {
      "title": "When is hiring coming back? Our predictions for 2024.",
      "content": "**EDIT**: *If you don't like reading, here's me presenting the contents of this blog post in a video. Pick your poison.*\n\nPredictions are hard, and, inevitably, most of them turn out wrong. But we’d like to brave the scathing mockery of the internets and try anyway! Our courage is bolstered by some useful data we have (both proprietary and gathered from the internet), which we’ll use to guess what will happen in 2024 and to answer the question foremost in many of our minds: *“When is hiring coming back?*”\n\ninterviewing.io is an anonymous mock interview platform and recruiting marketplace for engineers. Engineers use us for mock interviews. Companies use us to hire top performers. In our lifetime, we’ve hosted over 100k technical interviews, split between mocks and real ones. **Data from these interviews helps us get an insider’s perspective on what’s actually going on in the engineering market. For instance, we know how many people are practicing for interviews at Google vs. Meta vs. other FAANGs. Because we offer salary negotiation, we know how often people are negotiating, how successful they are, and what their compensation looks like. We also know what the “bar” looks like in different kinds of technical interviews over time — how well you have to do to pass algorithmic interviews, system design interviews, and so on.**\n\nAnd, finally, we ran several surveys with our users recently (thank you, kind and patient users), to learn about their recent job searches and outcomes. We also asked our users to share some advice based on their experience with the current job market. We’ll include bits of advice as quotes throughout this piece.\n\nPutting all of this data together gives us the kind of clarity that wouldn’t be possible from looking at any one piece individually. For instance, once you realize that most FAANGs are only hiring for backfills, while Meta is on a hiring spree, you start to understand why they’ve been treating candidates poorly and why the market has allowed them to do it. We’ll get to all of that in a bit. First, here’s Nostradamus Corgi with our predictions for 2024.\n\nPredictions for 2024\n--------------------\n\n![Nostradamus Corgi](https://strapi-iio.s3.us-west-2.amazonaws.com/Nostradamus_Corgi_c5e6d122c7.png)\n\nThose who've owned a touch bar MacBook with butterfly keyboard will appreciate that a hologram-snow-globe provides a vastly superior input device.\n\n**Prediction #1: 2024 is the year when hiring comes back.**  \nWhether we like it or not, the FAANGs are driving overall eng hiring volume. As you’ll soon learn, Meta and Netflix are hiring aggressively while the other FAANGs are not, but I’m inclined to think that come next year, most of the FAANGs will follow Meta and Netflix’s lead and start actively hiring again.\n\n**Prediction #2: Mid-level & senior eng hiring will pick up significantly, come Jan 2024.**  \nWe’re less confident about this part, but we expect that hiring will be back to (or quite close to) H1 2022 levels in the spring of 2024.\n\n**Prediction #3: For at least the next 6 months, compensation at a given level will stay flat.**\n\n**Prediction #4: For at least the next 6 months, down-leveling will continue because of inertia.**\n\n**Prediction #5: For at least the next 6 months, recruiters are going to be increasingly stretched thin, which means that applying online is going to be an even less effective way to get into companies.**\n\n**Prediction #6: The hiring bar will NOT return to where it was for a long time.**\n\nOf course, anybody on the internet can make predictions about hiring, but ours are supported by actual data (below), which we want you to dive into to see if you agree with us. And at the end of this piece, we’ll include a final bonus prediction and actionable advice and resources you can use to capitalize on these predictions.\n\nFirst, let’s look at some benchmarks from the broader internet about the state of hiring and layoffs.\n\nEng jobs are recovering, and layoffs are way down\n-------------------------------------------------\n\nBelow is a graph of open tech jobs over time, from TrueUp. TrueUp indexes open jobs at “tech and tech-ish companies” that have one or both of the following traits: “product + services would not exist without the internet” and/or “raised money from a VC”.\n\n![Number of open tech jobs at tech startups, tech unicorns, and public tech companies ](https://strapi-iio.s3.us-west-2.amazonaws.com/Number_of_open_tech_jobs_be3f1062fe.png)\n\nSource: <https://www.trueup.io/job-trend>\n\nAs you can see, tech jobs started to drop in May 2022 (around the time the FAANGs froze hiring) and began to recover slowly in March 2023.\n\nAt the same time, layoffs are way down after peaking in Q1 2023. The number of people laid off is down 88%, and companies doing layoffs are down 68%. **Most importantly, layoffs have solidly returned to pre-downturn levels.**[1](#user-content-fn-1)\n\n![Number of tech employees laid off each quarter ](https://strapi-iio.s3.us-west-2.amazonaws.com/Tech_employees_laid_off_f5bea7bcdf.png)\n\nSource: layoffs.fyi; Q4 projections are ours\n\n**So, tech jobs are recovering and layoffs are way down. Woohoo!**\n\nBack to open tech jobs for a moment. When we saw fewer people booking mock interviews in 2022, we started to use the TrueUp graph as a sanity check to learn if it was just us. Turns out… it’s not just us. Here is the same graph from above, overlaid with mock interview purchases on interviewing.io. We’ve hidden the y-axis values because we’re not comfortable sharing our exact sales numbers with the world, but the shape of the graph is what matters most here.\n\n![interviewing.io purchases and number of open tech jobs are related](https://strapi-iio.s3.us-west-2.amazonaws.com/iio_purchases_and_number_of_open_tech_jobs_related_1cc780f368.png)\n\nSource: <https://www.trueup.io/job-trend> and proprietary interviewing.io data\n\nAlong with feeling some relief that we weren’t entirely to blame for fucking up our business and could, in good conscience, cast some blame on macroeconomic conditions, we also noticed another interesting thing — **purchases on our platform mirror what’s going on in the broader market and effectively give us insight into hiring trends.**\n\nIf you believe, like we do, that purchase activity is a good proxy, then we can conclude that **eng jobs appear to be recovering faster than tech jobs as a whole. In the graph above, hiring started turning around in January 2023 and has grown 58% since then, with most of the growth happening between Q3 and Q4 of this year.**\n\nWe’ll be using interviewing.io purchase activity as a proxy for open eng jobs for the rest of the post.\n\nWhat’s going on at specific FAANGs?\n-----------------------------------\n\nAs you’ll see in the next few sections, our mock interview purchase data is tightly coupled to events in the outside world. In addition to learning what’s going on with hiring as a whole, our data shows us what’s going on at specific companies, letting us see which ones are actively hiring and how much. We can clearly see when various companies freeze, go on hiring sprees, or just start hiring a little bit. Since many of our users are preparing specifically for FAANGs, we’ll start there.\n\nWe’ve done our best to corroborate the trends we’re seeing in our data with insiders at these companies, but that doesn’t mean we haven’t missed something or made mistakes. If something looks incorrect, we want to hear from you, and we’ll fix it. Just email [hello@interviewing.io](mailto:hello@interviewing.io).\n\n### Google\n\nBelow is a graph of Google-specific purchasing activity on interviewing.io over time. (As before, we’ve hidden y-axis values, but what matters most is the shape of the graph.) As you can see, it tracks with what was happening in the world — Google slowed their hiring in May 2022, along with other companies like Uber, and [officially froze hiring in July](https://www.theverge.com/2022/7/20/23271634/google-hiring-pause-two-weeks-review-headcount-needs).\n\n![Mock interview purchases over time for Google](https://strapi-iio.s3.us-west-2.amazonaws.com/Mock_interview_purchases_over_time_for_Google_e1498fc2d0.png)\n\nSource: Proprietary interviewing.io data\n\n**Since then, Google’s hiring continued to slow, falling 90% from peak and 50% since January of this year, until it finally leveled out around July. The latest news we’ve heard is that while Google is currently hiring, it’s primarily backfills for attrition rather than new headcount.**\n\nWant to know if you’re ready to interview at Google? Do anonymous mock interviews with real Google interviewers, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\n### Meta\n\n![Mock interview purchases over time for Google and Meta](https://strapi-iio.s3.us-west-2.amazonaws.com/Mock_interview_purchases_overtime_for_Google_and_Meta_b5d62232e5.png)\n\nSource: Proprietary interviewing.io data\n\nLike Google, Meta froze hiring in 2022 (they actually did it before Google – it was officially announced in [May](https://www.businessinsider.com/facebook-is-freezing-hiring-heres-why-and-who-it-impacts-2022-5)). **However, unlike Google, they’re currently hiring aggressively, and if you believe that mock interview purchases are a good proxy for hiring volume, they’re up about 800% since January of this year, and are now back to pre-freeze levels (though not yet back to peak).**\n\nSome anecdotal data we have backs this up. At interviewing.io, we offer salary negotiation help to our users, which means that we talk to a lot of people, in depth, about where they’re interviewing and how it’s going. Almost every negotiation candidate we’ve had recently has been interviewing at Meta. What’s even more interesting is the volume of possible teams that candidates talk to during team matching. Earlier this year, Meta’s story seemed to be:\n\n> *We’re primarily hiring for backfills. These are the 1-2 teams that might have open headcount. You gotta decide fast, or those slots will be filled.*\n\nNow, our users are regularly presented with more than 10 teams to choose from.\n\nMeta’s hiring boom, relative to most other FAANGs’ conservative hiring, has one notable dark side. Meta’s hiring volume is clearly outpacing all the other FAANGs (Netflix is also up, but Meta’s eng team is more than 10X the size of Netflix’s, so in the absolute, Netflix’s hiring volume isn’t enough to balance Meta out) — for all intents and purposes they’re the only FAANG that’s really hiring at scale — so they’re currently getting away with treating candidates really poorly.\n\nWe’ll talk more about this in the down-leveling section, but Meta has capitalized on the market imbalance and has been rampantly down-leveling candidates, presumably because they can — in the pre-downturn market, competition would have made that difficult, but now, given that they’re responsible for much of the FAANG hiring volume, they can rely on their candidates not having many comparable counteroffers and can present candidates with low-ball offers.\n\nOutside of down-leveling, which, in fairness, doesn’t happen to every candidate, Meta also has changed their stance on negotiation.\n\nBefore the downturn, it was basically enough to say that you were interviewing at Google to get a signing bonus (or a higher signing bonus). Now, Meta will not lead with signing bonuses unless you’ve been down-leveled, in which case they’ll use them as a consolation prize of sorts (still cheaper for them because it’s a conditional one-time payment). Moreover, Meta will not negotiate unless you can show higher counteroffers (they don’t actually make you show the paperwork, but they ask for details). Recruiters have been trained to say, “I’ll go to bat for you and take this to the compensation committee, but only if you can show me a compelling reason, i.e., other offers.” If you can’t show other offers, they will not budge a dime.\n\nFinally, Meta has gotten much more aggressive with offer deadlines. Recruiters will push you to sign within a day or two of completing team matching and getting your offer numbers.\n\nThese kinds of high-pressure, candidate-unfriendly hiring practices would have been unthinkable in a more competitive hiring market, but right now Meta can get away with it. I would argue that their strategy is shortsighted, however, because this kind of market instability can’t last long, and when the other FAANGs are back, Meta will start to see serious attrition from the engineers they’ve down-leveled and low-balled. I’m sure Meta talent execs have made this decision with their eyes open and are prepared to take the hit, but shame on them nevertheless, and I hope that the talent drain they experience next year is swift and brutal. Sorry, reader, I don’t usually insert my opinions in a primarily data-driven piece, but the kind of unnecessary stress and sleepless nights their shortsighted policies have created for our users, who are highly competent, thoughtful, and kind senior engineers, makes my blood boil.\n\n**TL;DR: [Meta is on a hiring spree](https://interviewing.io/guides/hiring-process/meta-facebook#meta-facebook), and it’s created an unstable equilibrium, which means that likely the other FAANGs will start hiring again soon. More on that later.**\n\nWant to know if you’re ready to interview at Meta? Do anonymous mock interviews with real Meta interviewers, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\n### Amazon\n\n![Mock interview purchases over time for Google, Meta and Amazon](https://strapi-iio.s3.us-west-2.amazonaws.com/Mock_interview_purchases_overtime_for_Google_Meta_and_Amazon_739953c328.png)\n\nSource: Proprietary interviewing.io data\n\nAmazon is an interesting beast. After both Google and Meta froze hiring in summer of 2022, Amazon went on an opportunistic hiring spree, and again, you can see in the graph above that mock interview purchase patterns do indeed reflect what’s going on in the outside world.\n\n**After a few months of basically a monopoly on eng hiring, Amazon pulled back and froze in [November 2022](https://techcrunch.com/2022/11/03/amazon-exec-confirms-corporate-hiring-freeze-through-end-of-year/). They’ve been flat since January of 2023 and, from what we’ve heard, are only hiring for backfills and very senior roles.**\n\nIt is rare to see one FAANG deviate drastically from the others when it comes to hiring, whether aggressively hiring when everyone else has paused or pausing when everyone else is hiring. It creates an unstable equilibrium, one that can’t last very long. We’ll talk a bit more about this later.\n\nWant to know if you’re ready to interview at Amazon? Do anonymous mock interviews with real Amazon interviewers, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\n### Apple and Microsoft\n\nMicrosoft officially froze hiring [sometime in Q4 of 2022](https://www.ciodive.com/news/aws-hiring-freeze-microsoft-google-cloud/635865/) (though [unofficially it seems to have been much earlier, around July](https://www.glassdoor.com/Community/jobs-in-tech/has-anyone-heard-if-theres-a-hiring-freeze-at-microsoft-right-now)). **Since then, [Microsoft hiring](https://interviewing.io/guides/hiring-process/microsoft#microsoft) has been flat and, from what we know, limited to backfills.**\n\nApple, on the other hand, [did not freeze hiring in 2022](https://9to5mac.com/2022/11/02/apple-hiring-freeze-budgets-report/) and continued to hire on a “deliberate basis” (headlines about it have been consistently misleading), and they’re the only FAANG that hasn’t done a large-scale layoff. **[Apple’s hiring](https://interviewing.io/guides/hiring-process/apple#apple) has been slow for the past year and a half but has fallen about 70% since January of 2023.**\n\n![Mock interview purchases over time for Apple and Microsoft](https://strapi-iio.s3.us-west-2.amazonaws.com/Mock_interview_purchases_over_time_for_Apple_and_Microsoft_66656291ce.png)\n\nSource: Proprietary interviewing.io data\n\n### Netflix\n\nWe couldn’t find an official hiring freeze announced by Netflix, but according to Blind and anecdotal reports, they slowed down hiring on a department-by-department basis.\n\nWe have a gap in our Netflix purchase data because we stopped offering Netflix-themed practice for some time, and this gap would make a multi-year graph misleading. That said, we have been offering **Netflix practice since the beginning of 2023, and since then, demand** (and [Netflix hiring](https://interviewing.io/guides/hiring-process/netflix#netflix)) **appears to have grown, up almost 300% since January.**\n\n![Mock interview purchases over time for Netflix](https://strapi-iio.s3.us-west-2.amazonaws.com/Mock_interview_purchases_over_time_for_Netflix_1cb05d59f9.png)\n\nSource: Proprietary interviewing.io data\n\nWant to know if you’re ready to interview at Netflix? Do anonymous mock interviews with real Netflix interviewers, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nWho’s actually hiring right now?\n--------------------------------\n\nHere’s a [list of all the (larger) companies](https://docs.google.com/spreadsheets/d/1icCMfo3NQk-UqL6GJrQxYkebz0UUIayiGfGL3YZGpck/edit#gid=0) where our users are interviewing right now (which means they’re actively hiring!), based on our onboarding flow, salary negotiation data, and a recent survey. In cases where we’ve written guides for these companies’ interview processes, I’ve linked to them in the doc as well.\n\n[![List of companies hiring in Q4 2023](https://strapi-iio.s3.us-west-2.amazonaws.com/List_of_companies_hiring_in_q4_2023_defad29b21.png)](https://docs.google.com/spreadsheets/d/1icCMfo3NQk-UqL6GJrQxYkebz0UUIayiGfGL3YZGpck/edit#gid=0)\n  \nIt appears that despite the slowdowns and primarily just hiring for backfills, FAANG is still dominating — over half of our users are interviewing at FAANG companies, with startups comprising only 17% of interview volume.\n![Where interviewing.io users are interviewing](https://strapi-iio.s3.us-west-2.amazonaws.com/Where_our_users_are_interviewing_d34e61755b.png)\n\nSalary negotiation: an important trailing indicator\n---------------------------------------------------\n\nBefore we analyze all these disparate FAANG data points and connect them to our 2024 predictions, we have a few more important pieces of data to share with you. One of them is around salary negotiation, a useful and important trailing indicator.\nAt interviewing.io, in addition to providing mock interviews, we provide help with salary negotiation. I do a lot of these sessions myself (it’s one of the most rewarding parts of my job), and leading these sessions gives me a direct line to our users and ultimately an invaluable ear to the ground. Users share with us where they’re interviewing, how it’s going, what their offers look like with respect to both compensation and level, and more.\n\nI’ll talk more about what we’ve learned in these conversations (in aggregate, of course) later in the post, but for now, I’d like to share how our salary negotiation business has changed over time and what insights these changes give us into the broader market.\n\nBelow is a graph of salary negotiation requests over time on interviewing.io (a request is someone booking salary negotiation help — they put down a credit card but don’t have to pay anything unless the negotiation is successful, so we don’t call them purchases like we did with mock interviews above).\n\n![Salary negotiation requests over time](https://strapi-iio.s3.us-west-2.amazonaws.com/Salary_Negotiation_Requests_Over_Time_18316fcf2f.png)\n\nSource: Proprietary interviewing.io data\n\nThere are a few important things that immediately jump out in this graph. First, you can see that salary negotiation requests were at a relative high in June 2022 (the data we have from before isn’t shown because we were logging activity differently). Then they took a nose-dive in November 2022 (dropping by 4X). Compare that to mock interview purchases over time above. You can see that salary negotiation activity has roughly a 3-month delay on mock interview activity, which makes sense because people tend to start thinking about negotiation around the time it looks like they’re going to get an offer, whereas they start practicing much earlier.\n\nThough negotiation requests are still not up to peak levels, they’ve been growing steadily and have 2X’ed since the start of 2023.\n\nOne other bit of anecdotal negotiation data confirms that hiring is on the rise — we have seen our candidates interviewing with roughly 2X more companies in Q3 and Q4 than in Q1.\n\nBecause of the delay between practice (and ultimately real interviews) and negotiation, salary negotiation requests are a trailing indicator. In other words, these changes in salary negotiations are only observable after hiring volume has already shifted. **Therefore, they don’t predict changes in hiring volume but rather confirm them after they have already occurred and ultimately help demonstrate that the growth we’ve seen in hiring volume is not a fluke.**\n\nWhat about compensation? And are people negotiating their salaries in this climate?\n-----------------------------------------------------------------------------------\n\nWe talked a bit already about the kinds of insights we get from our salary negotiation program, both qualitatively and quantitatively. One of the quantitative pieces is offer sizes, broken down by cash, equity, and bonuses.\n\nBelow is average total compensation (initial offers, before negotiating, just for public companies, as we don’t presume to accurately value private company equity) among our users over time. As you can see, it has largely stayed flat, despite the downturn.\n\n![Average initial offers over time](https://strapi-iio.s3.us-west-2.amazonaws.com/Average_initial_offers_over_time_e4f1c58592.png)\n\nSource: Proprietary interviewing.io data\n\nWe were also fortunate to get a data set from the folks at [comprehensive.io](https://comprehensive.io), a source of comp data from the maker of [layoffs.fyi](https://layoffs.fyi). What we really liked about this data set is that it had engineering salaries for the kinds of eng roles that our users were actually targeting, in the locations where they were likely to work (if you look at broader aggregators, you’ll see that the numbers are much lower than you’d expect because they factor in engineering-adjacent positions in non-tech hubs).\n\n![Software engineer salaries stayed flat throughout 2023](https://strapi-iio.s3.us-west-2.amazonaws.com/Eng_salaries_stayed_flat_throughout_2023_45916eef35.png)\n\nSource: [comprehensive.io](http://comprehensive.io) (from the maker of layoffs.fyi)\n\nAs you can see, comprehensive.io’s data backs up what we’ve seen among our negotiation users — compensation has indeed stayed flat despite the downturn.\n\nHere are a few tidbits not visible in the graphs above:\n\n* **We have NOT seen a difference in the average increase from successful negotiations.**\n* **That said, it’s now much harder to negotiate successfully without multiple offers than in H1 2022.** It used to be enough to say that you’re interviewing at a few high-profile companies and that if you get meaningfully more comp, you’ll stop (basically companies would pay you to stop interviewing). Nowadays, companies are less likely to pay to get you off the market and will insist on needing at least one other offer to justify a comp increase (as you saw above with Meta).\n* **Companies are much less likely to lead with a signing bonus.** Instead, they’ll leave them in their back pocket and only pull them out if necessary. One notable exception is scenarios where the engineer got down-leveled (more on that later). In those cases, companies might soften the blow by offering a meaningful signing bonus.\n\n> *“The market is really making things difficult. Expect things to be a bit harder and the offers to be lower than previous years. Negotiating is going to be absolutely crucial — now more than ever.”*\n\nThe changing role of recruiters, and the very practical thing it means for you\n------------------------------------------------------------------------------\n\nTake a look at the graph below. The black curve is the same one we first shared above, when we graphed it against total open tech jobs — it’s total mock interview purchases on interviewing.io, which we have found to be a good proxy for the state of eng hiring (we’re using this graph and not the one from trueup.io because the latter doesn’t specifically show engineering jobs, and seeing engineering jobs specifically matters for the point we’re about to make). As before, we’ve hidden the y-axis values to protect our business.\n\nSince the start of the year, mock interview purchases (and resulting eng jobs) have grown by 58%.\n\n![Software engineer jobs are recovering, recruiter jobs are not](https://strapi-iio.s3.us-west-2.amazonaws.com/Eng_jobs_vs_recruiter_jobs_a1cdbe8e46.png)\n\nSource: interviewing.io proprietary data and [lightcast.io](https://lightcast.io)\n\nThe dark blue line is new. It’s the number of open tech recruiter jobs over time. As you can see, engineering jobs are recovering, but recruiter jobs are not (and because this isn’t our proprietary data, we’ve included numbers on the right y-axis). In fact, since the start of the year, open recruiter jobs have shrunk by 52%.\n\nWe’ll talk about the implications of this disparity in a moment, but before we do that, let’s look at one more graph:\n\n![Rise in inbound applications in business and technical roles](https://strapi-iio.s3.us-west-2.amazonaws.com/business_and_technical_roles_apps_ef78404990.png)\n\nSource: [Ashby’s 2023 Trends Report | Applications Per Job](https://www.ashbyhq.com/talent-trends-report/reports/2023-trends-report-applications-per-job?utm_source=adwords&utm_medium=ppc&utm_campaign=Ashby+Keyword&utm_term=ashbyhq))\n\nThis graph comes from Ashby, a new applicant tracking system (ATS) that’s used by a growing number of startups and tech companies. Because they’re an ATS, they have a treasure trove of data about who applies to what jobs, when. As you can see, the number of applicants for technical roles held steady from 2021 to 2022, and then it began to creep up before it hit a serious growth inflection point at the end of 2022. Though Ashby’s graph only goes up to April 2023, since the start of 2023, inbound applications for technical positions have doubled.\n\n**Plainly put, the number of eng positions recruiters are responsible for filling is increasing, and the volume of applicants is growing, while the number of recruiters among whom that work is distributed is shrinking.**\n\nYou might argue that recruiters might not end up having to do more work because this disparity will force them to rely even more on automated filtering of applications. This may very well be true. However, whether increasingly harried recruiters will take less time to review inbound applicants or whether this is an opportunity for a new wave of resume-filtering solutions, one thing is clear. While applying online has always been a pretty BAD IDEA — because it’s the moral equivalent of shouting into a black hole — it’s more true now than ever before.\n\nIn addition to the market data above, we have some anecdotal info and survey results. In our survey that preceded this post, 17% of senior engineers cited that getting in the door was the hardest part of their job search, even harder than technical interviews.\n\nFinally, eng leaders we’ve spoken to have confirmed their recruiting teams (and sometimes their eng teams) are overwhelmed with inbound applications.\n\n**So, do not apply online in this climate, full stop.** Instead, reach out to hiring managers — recruiters are not incentivized to break rules and will likely not get back to you, especially if you don’t look perfect on paper. Moreover, the numbers will be on your side, as relatively few candidates are targeting hiring managers directly. We plan to write a full blog post on how to do this kind of outreach well, but this CliffsNotes version will get you started:\n\n* Get a LinkedIn Sales Navigator account\n* Make a target list of hiring managers at the companies you’re interested in\n* Figure out their emails (you can use a tool like RocketReach), and send them something personalized. Do not use LinkedIn. The same way that you don’t live in LinkedIn, eng managers don’t either.\n\nSome advice from our users about getting in the door:\n\n> *“Be prepared, and also be proactive. Don't just wait for the recruiters to contact you.”*\n\n> *“Networking is key. If you know the name of the person who will be interviewing you, research them online to try to get a feel for who they are and what types of questions they might ask.”*\n\nDown-leveling\n-------------\n\nAs I mentioned above, Meta has been brazen in their down-leveling of candidates. It usually manifests as a down-level (e.g., you interview for an E5 role and then end up at an E4). This kind of down-leveling has happened to almost every negotiation client we've had who has interviewed at Meta.\n\nMaking matters worse, we've found that it's virtually impossible to negotiate without another offer on the table. It doesn't necessarily have to be from another FAANG, but it needs to be from a notable FAANG-adjacent company.\n\nBasically Meta's approach post-downturn has been as follows:\n\n* Interview someone\n* Down-level them, citing poor performance in system design interviews\n* Make them a lowball offer even for that band but compensate for it with a decent signing bonus (~$50-70k) as a way to soften the blow and make it more likely candidates will accept quickly\n* Refuse to negotiate unless candidates show them other offers: \"I can't take this to the compensation committee without a compelling reason, i.e., another offer.\"\n\nThough we’d argue Meta has drawn the hardest line with respect to negotiation, as discussed above, they’re not the only company that’s down-leveling candidates. **According to our survey, of the people who got offers in 2023, 33% were down-leveled.**\n\n**We didn’t have enough data for Apple and Amazon, but using our survey results, we’ve computed the probability of getting down-leveled at the rest of the FAANGs, given that you receive an offer there.** At Netflix, it’s 50%. At Meta, it’s 55%. At Google, it’s 59%, and finally at Microsoft it’s a whopping 66%. Again, although Meta’s down-leveling percentage isn’t as high as Microsoft’s, they’re hiring far more engineers, so their actions have a much greater effect on the marketplace.\n\n![Probability of being down-leveled by FAANG companies](https://strapi-iio.s3.us-west-2.amazonaws.com/Probability_of_being_down_leveled_by_FAANG_74be6f98df.png)\n\nSource: interviewing.io survey data\n\n**For all other companies (small startups, large startups, and FAANG-adjacent), the probability of getting down-leveled is about 37%.** Almost everyone we heard from was down-leveled by just one level, but we saw a handful of people drop by two levels.\n\nAs you’d expect with the rise down-leveling, the bar for interview performance has been going up as well, and this rising bar accounts for some of the down-leveling we’re seeing (though we expect that cost savings for the employers who are doing it accounts for much of the rest).\n\nThe rising bar\n--------------\n\nThe rising bar makes sense — after all, more engineers are competing for relatively fewer positions. Because we have a bunch of interview data, we are actually able to quantify the increase.\n\nSpecifically, after every interview on our platform, whether it’s a mock or a real one, the interviewer leaves feedback. The feedback form looks like this, and it includes a yes/no question for whether they’d move the candidate forward, as well as star ratings of their performance on a scale of 1 to 4, where 4 is best. These star ratings cover 3 areas: coding ability, problem-solving ability, and communication skills.\n\n![interviewing.io feedback form](https://strapi-iio.s3.us-west-2.amazonaws.com/feedback_93fb5e447f.png)\n\nTo track how the bar has changed, we can look at the average coding and problem-solving ratings for successful interviews over time (we haven’t focused on communication ability because we previously discovered that it doesn’t significantly affect interview outcomes except for very senior roles).\n\n![The bar for coding ability and problem solving continue to rise](https://strapi-iio.s3.us-west-2.amazonaws.com/Bar_for_coding_ability_and_problem_solving_ed03a59d2f.png)\n\nSource: Proprietary interviewing.io data, based on tens of thousands of interviews\n\nAs you can see above, the coding and problem-solving scores for successful interviews have risen steadily since the start of 2022. To make these numbers concrete, you now need to perform 22% better in technical interviews than you did at the start of last year (with respect to both coding and problem solving). [The last time we wrote about the rising bar was in November 2022](https://interviewing.io/blog/you-now-need-to-do-15-percent-better-in-technical-interviews), and back then you had to do 15% better. The bar has only continued to go up.\n\nWe came up with these numbers after bucketing interview performance into percentiles (more realistic than just looking at average raw scores because relative differences in raw scores may amount to much larger or smaller differences when competing against other engineers). At the start of 2022, you had to be in the 65th percentile to pass interviews, namely outperform 65% of engineers. At the end of 2022, you had to be in the 78th percentile, and now you have to be in the 83rd.\n\nThis data is corroborated by our users’ experience with the job market. In our survey, we asked our users if they felt that the bar was higher and/or the interview process was more difficult than the last time they were interviewing, 77% said yes.\n\nBonus prediction about AI\n-------------------------\n\n**Prediction #7: For a long time in the future, we won’t see major changes to interview styles, despite the advent of ChatGPT. We will, however, see changes to interview questions.** Specifically:\n\n* Companies will have to move away from asking verbatim LeetCode questions because of how easy it is to cheat.\n* Similarly, we expect that async coding tests will go away for the same reason.\n* There will be a premium on good human interviewers.\n\nThis final prediction might feel a bit random, as we haven’t discussed AI at all till now, but we’ve been doing some experimentation on how tools like ChatGPT are going to change technical interviewing. We’ll publish details soon, and we decided to include the AI predictions in this post because the topic is too salient not to.\n\nPredictions recap\n-----------------\n\nWe’re currently in the eye of an unstable period. It’s simply not sustainable for most FAANGs to just be backfilling while Meta and Netflix plow ahead with 2022-level hiring volume. Though Amazon did something similar in 2022, unstable equilibria like this don’t last. Sure, you could make the argument that Meta and Netflix will just freeze again (like Amazon did, after their hiring spree), but between the recent rise in mock interview bookings overall, the increase in salary negotiation requests, and the huge drop in layoffs, it seems likelier that next year the other FAANGs will join Meta and Netflix in actively hiring again.\n\nHere are some key data points that support our predictions:\n\n* Meta and Netflix mock interview bookings, and subsequent hiring, are way up (+800% and +300% respectively). Again, having only 2 FAANGs hiring is untenable because this type of unstable equilibrium can’t last long.\n* Open tech jobs are growing (+14% since Feb 2023).\n* Tech layoffs are down, and specifically the number of people laid off is -93% since Jan 2023.\n* Mock interview bookings are growing (+58% since Jan 2023).\n* Salary negotiation is up 100% (trailing indicator).\n\nEven though hiring is going to come back, it’s not going to feel quite like the heady days of 2021 and early 2022. The bar is going to stay high for the foreseeable future — as you saw, it’s up 22% over the start of 2022, and walking that kind of thing back takes a long time.\n\nSimilarly, until most of the FAANGs are back to their pre-downturn hiring velocity, which will probably take at least 6 months, we will continue to see down-leveling, not just because of the rising bar but, frankly, because companies can get away with it while candidates have less leverage. For the same reason, salaries will continue to stay flat — they’ve been flat for so long, despite market changes, that until we’re firing on all cylinders again, inertia will keep them there.\n\nFinally, as hiring recovers, more recruiters will get hired back, but that, too, will take some time. For at least the next 6 months, recruiters will continue to be overworked, which means that, for the foreseeable future, applying online is going to be a fool’s errand.\n\nActionable advice and resources\n-------------------------------\n\nWe have every reason to believe that by H2 of 2024, hiring will be back to normal, or close to it. In the meantime, though hiring is coming back, it’s not back to normal quite yet. During this liminal period, you can’t approach your job search the same way you did during the boom.\n\nAs such, even though this was a very long post, the main takeaways are surprisingly short: It’s more important than ever to have to get creative with how you get in the door (so you don’t get lost in the shuffle), to be even more prepared for interviews than before (especially system design), and to do your best to get multiple offers (if you don’t, it will be much harder to negotiate. Here are some specific resources and tips to help you do these things:\n\n* If you’re targeting specific companies but don’t have connections there, don’t apply online and don’t wait. Instead, reach out to hiring managers directly. And don’t do it on LinkedIn; email is a much better channel.\n* Use our [Company Intros](https://start.interviewing.io/company-introductions) feature if you have access — we get you in front of a decision-maker human immediately.\n* Use our [company process guides](https://interviewing.io/topics#companies) to help you plan your job searches and to prepare for specific companies. And use our [Learning Center](https://interviewing.io/learn) too. It has technical topic explanations, a bunch of interview replays, and a whole lot more.\n* Prepare for your interviews. You don’t have to use us. Just find a way to do some mock interviews before your real ones.\n  + Our [AI Interviewer](https://start.interviewing.io/interview-ai) is a great, free resource that’s more effective than LeetCoding on your own — it lets you practice algorithmic questions with a human-like interviewer in CoderPad and gives you feedback at the end.\n  + If you don’t have an interviewing.io account, you can also try out our [Technical Interviewer custom GPT](https://chat.openai.com/g/g-Fhk16eFON-technical-interviewer-by-interviewing-io) (it does system design interviews as well!).\n* Study system design (we have the [best system design guide that’s out there](https://interviewing.io/guides/system-design-interview)). Lackluster performance in system design interviews is the main reason people are getting down-leveled.\n* In this climate, negotiating is harder. Don’t set yourself up to fail. [Read this post immediately](https://interviewing.io/blog/sabotage-salary-negotiation-before-even-start) to set yourself up for success and to prevent making mistakes at the beginning of the process.\n\nWe’ll close this post with some more independent advice from our users:\n\n> *\"Make sure to practice system design as much or more than ds/a.”*\n\n> *“Learn what the process involves at each company you're applying to. Aggressively plan and prepare for each. Do a lot of mock interviews, and be ready for the unexpected.”*\n\n> *“After waves of layoffs, there are so many candidates with great experiences and skills who are looking for jobs. Even if you ace the interview, they may fill the position with someone else who did better, so don't be too discouraged if that happens, and keep trying!”*\n\n> *“Be honest with yourself about how much you actually need to prepare. And give yourself that time. Prepare prepare, prepare!”*\n\nFootnotes:\n----------\n\nFootnotes\n---------\n\n1. Though we didn’t have data on what portion of layoffs were engineers specifically, we did some unscientific spot checking, and generally engineers comprised 20-30% of layoffs at a given company. [↩](#user-content-fnref-1)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/when-is-hiring-coming-back-predictions-for-2024",
      "author": "",
      "user_id": ""
    },
    {
      "title": "You probably don’t factor in engineering time when calculating cost per hire. Here’s why you really should.",
      "content": "Whether you’re a recruiter yourself or an engineer who’s involved in hiring, you’ve probably heard of the following two recruiting-related metrics: time to hire and cost per hire. Indeed, these are THE two metrics that any self-respecting recruiting team will track. Time to hire is important because it lets you plan — if a given role has historically taken 3 months to fill, you’re going to act differently when you need to fill it again than if it takes 2 weeks. And, traditionally, cost per hire has been a planning tool as well — if you’re setting recruiting budgets for next year and have a headcount in mind, seeing what recruiting spent last year is super helpful.\n\nBut, with cost per hire (or CPH, as I’ll refer to it from now on in this post) in particular, there’s a problem. CPH is typically blended across ALL your hiring channels and is confined to recruiting spend alone. Computing one holistic CPH and confining it to just the recruiting team’s spend hides problems with your funnel and doesn’t help compare the quality of all your various candidate sources. And, most importantly, it completely overlooks arguably the most important thing of all — how much time your team is actually spending on hiring. **Drilling down further, engineering time, specifically, despite being one of the most expensive resources, isn’t usually measured as part of the overall cost per hire.** Rather, it’s generally written off as part of the cost of doing business. **The irony, of course, is that a typical interview process puts the recruiter call at the very beginning of the process precisely to save eng time, but if we don’t measure eng time spent and quantify, then we can’t really save it.**\n\nFor what it’s worth, the Twitterverse (my followers are something like 50/50 engineers and recruiters) seems to agree. Here are the results (and some associated comments) of a poll I conducted on this very issue:\n\n> Quick poll! Should \"cost per hire\" calculations include engineering time spent on interviewing candidates?\n>\n> — Aline Lerner (@alinelernerLLC) [March 17, 2019](https://twitter.com/alinelernerLLC/status/1107073450013650944?ref_src=twsrc%5Etfw)\n\nAnd yet, most of us don’t do it. Why? Is it because it doesn’t measure the things recruiters care about? Or is it because it’s hard? Or is it because we can’t change anything, so why bother? After all, engineers need to do interviews, both phone screens and onsites, and we already try to shield them as much as possible by having candidates chat with recruiters or do coding challenges first, so what else can you do?\n\n**If you’d like to skip straight to how to compute a better, more inclusive CPH, you can skip down to our handy spreadsheet. Otherwise read on!**\n\nI’ve worked as both an engineer and an in-house recruiter before founding interviewing.io, so I have the good fortune of having seen the limitations of measuring CPH, from both sides of the table. As such, in this post, I’ll throw out two ways that we can make the cost per hire calculation more useful — by including eng time and by breaking it out by candidate source — and try to quantify exactly why these improvements are impactful… while building better rapport between recruiting and eng (where, real talk, relationships can be somewhat strained). But first, let’s talk about how CPH is typically calculated.\n\nHow is CPH typically calculated, and why does it omit eng time?\n---------------------------------------------------------------\n\nAs I called out above, the primary purpose of calculating cost per hire is to plan the recruiting department’s budget for the next cycle. With that in mind, below is the formula that you’ll find if you google how to calculate cost per hire (pulled from [Workable](https://resources.workable.com/tutorial/faq-recruitment-budget-metrics)):\n\n![A sketch outline of the cost per hire equation](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffc5ff_screenshot_2019_04_22_14_42_10_cdf4827348.webp&w=1920&q=75 \"The Cost per Hire equation\")\n\nTo figure out your CPH, you add up all the external and internal costs incurred during a recruiting cycle and divide by the number of hires.\n\n“External” refers to any money paid out to third parties. Examples include job boards, tools (e.g. sourcing, assessment, your ATS), agency fees, candidate travel and lodging, and recruiting events/career fairs.\n\n“Internal” refers to any money you spend within your company: recruiting team salaries, as well as any employee referral bonuses paid out over the course of the last cycle.\n\nNote that internal costs don’t include eng salaries, as engineering and recruiting teams typically draw from different budgets. Hiring stuff is the domain of the recruiting team, and they pay for it out of their pockets… and engineers pay for… engineering stuff.\n\nWhat’s problematic is that, while being called “cost per hire” this metric actually tells us what recruiting spends rather than what’s actually being spent as a whole. While tracking recruiting spend makes sense for budget planning, this metric, because of its increasingly inaccurate name, often gets pulled into something it ironically wasn’t intended for: figuring out how much the company is actually spending to make hires.\n\nWhy does factoring in engineering time matter?\n----------------------------------------------\n\nAs you saw above, not only is this the way we compute CPH inaccurate because it doesn’t factor in any time or resource expenditure outside the recruiting team (with eng being the biggest one). But, does engineering time really matter?\n\nYes, it matters a lot, for the following three reasons:\n\n1. Way more eng time than recruiting time goes into hiring (as you’ll see in this post!)\n2. Eng time is more expensive\n3. Eng time expenditure can vary wildly by channel\n\nTo establish that these things are (probably) true, let’s look at a typical eng hiring funnel.[1](#user-content-fn-1) For the purposes of this exercise, we’ll start the funnel at the recruiter screen and assume that the costs of sourcing candidates are fixed.[2](#user-content-fn-2)\n\n![Screenshot of the hiring funnels howing pass-through rates at each stage](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F6c093_screenshot_2019_03_23_17_36_16_1_c18d98de26.webp&w=3840&q=75 \"The hiring funnel\")\n\nThe green arrows are conversion rates between each step (e.g. 50% of people who get offers accept and get hired). The small gray text at the bottom of each box is how long that step takes for an engineer or recruiter (or both, in the case of an onsite). And the black number is how many times that needs to happen to ultimately make 1 hire, based on the green-arrow conversion rates.\n\nSo, with that in mind, to make one hire, let’s see how much time both eng and recruiting need to spend to make 1 hire and how much that time costs. Note that I’m assuming $100/hour is a decent approximation for recruiting comp and $150/hour is a decent approximation for eng comp.\n\nIs eng time spent on recruiting really that costly?\n---------------------------------------------------\n\nBased on the funnel above, here’s the breakdown of time spent by both engineering and recruiting to make 1 hire. The parentheticals next to each line of time spent are based on how long that step takes times the number of times it needs to happen.\n\n**RECRUITING – 15 total hours**  \n**10** hours of recruiter screens (20 screens needed \\* 30 min per screen)  \n**4** hours of onsites (4 onsites needed \\* 1 hour per onsite)  \n**1** hour of offers (2 offer calls needed \\* 30 min per offer call)\n\nTo make 1 hire, it takes 15 recruiting hours or $1500.\n\n**ENGINEERING – 40 total hours**  \n**16** hours of phone screens (16 screens needed \\* 1 hour per screen)  \n**24** hours of onsites (4 onsites needed \\* 6 hours per onsite)\n\nFor 1 hire, that’s a total of 40 eng hours, and on the face of it, it’s $6,000 of engineering time, but there is one more subtle multiplier on eng time that doesn’t apply to recruiting time that we need to factor in. Every time you interrupt an engineer from their primary job, which is solving problems with code, it takes time to refocus and get back into it. If you’re an engineer, you know this deep in your bones. And if you’re not, interruptions are very likely something you’ve heard your engineering friends decry… because they’re so painful and detrimental to continued productivity. Back when I was writing code on a regular basis, it would take me 15 minutes of staring at my IDE (or, if I’m honest, occasionally reading Hacker News or Reddit) to let my brain ease back into doing work after coming back from an interview. And it would take me 15 minutes before an interview to read a candidate’s resume and get in the mindset of whatever coding or design question I was going to ask. I expect my time windows are pretty typical, so it basically ends up being a half hour of ramping up and back down for every hour spent interviewing.\n\nTherefore, with ramp-up and ramp-down time in mind, it’s more like $9,000 in eng hours.[3](#user-content-fn-3)\n\n**Ultimately, for one hire, we’re paying a total of $10,500, but eng incurs 6X the cost that recruiting does during the hiring process.**\n\nWhy does breaking out cost per hire by source matter?\n-----------------------------------------------------\n\nSo, hopefully, I’ve convinced you that engineering time spent on hiring matters and that it’s the biggest cost you incur. But, if there’s nothing we can do to change it, and it’s just the cost of doing business, then why factor it in to CPH calculations? **It turns out that eng time spent IS a lever you can pull, and its impact becomes clear when you think about cost per hire by candidate source.**\n\nTo make that more concrete, let’s take a look at 2 examples. In both cases, we’ll pretend that one of our candidate sources has a different conversion rate than the overall rate at some step in the funnel. Then we’ll change up the conversion rate at one step in the funnel and try to guess that the financial implications of that are… and then actually calculate it. You might be surprised by the results.\n\n### What happens when you increase TPS to onsite conversion to 50%?\n\nAs you can see in the funnel above, a decent TPS to onsite conversion rate is 25%. Let’s say one of your sources could double that to 50% (by doing more extensive top-of-funnel filtering, let’s say). What do you think this will do to cost per hire?\n\nIn this model, we’re spending a total of 10 recruiting hours (worth $1000) and 32 eng hours (worth $7200).[4](#user-content-fn-4) Unlike in the first example, we’re now paying a total of $8200 to make a hire.\n\nIn this case, you’ve reduced your recruiting time spent by 30% and your eng time spent by 20%, ultimately saving $2300 per hire. If one of your sources can get you this kind of efficiency gain, you probably want to invest more resources into it. And though doubling conversion from tech screen to onsite sounds great and perhaps something you would have known already about your source, without computing the cost per hire for this channel, it’s not intuitively clear just how much money a funnel improvement can save you, end to end.\n\n### What happens when you cut your offer acceptance rate in half?\n\nAnother possibility is that one of your sources does pretty well when it comes to candidate quality all the way to offer, but for some reason, those candidates are twice as hard to close. In this scenario, you double both the eng and recruiting time expenditure and ultimately pay an extra $7500 per hire for this source (which you’ll likely want to deallocate resources from here on out).[5](#user-content-fn-5)\n\nIn either of the examples above, until you break out CPH by source and see exactly what each is costing you, it’s a lot harder to figure out how to optimize your spend.\n\nHow to actually measure cost per hire (and include eng time of course!)\n-----------------------------------------------------------------------\n\nThe usual way to calculate cost per hire is definitely useful for setting recruiting budget, as we discussed above, but if you want to figure out how much your whole company is actually spending on hiring, you need to factor in the most expensive piece — engineering time.\n\nTo do this, we propose a different metric, one that’s based on time spent by your team rather than overall salaries and fixed costs. Let’s call it “cost per hire prime” or *CPH prime*.\n\nCPH prime doesn’t factor in fixed costs like salaries or events, which you can still do using the formula above… but it is going to be instrumental in helping you get a handle on what your spend actually looks like and will help you compare different channels.\n\n**To make your life easier, we’ve created a [handy spreadsheet for you to copy and then fill in your numbers](https://docs.google.com/spreadsheets/d/1a3Oq1hkP9NfEbjU6pJiQEKCvLUzB9UTZOcqBxMiwDc0/edit#gid=0), like so**:\n\n![A spreadsheet calcuation of cost savings per hire](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F90740_screenshot_2019_04_24_09_50_29_3f7f158b1f.webp&w=1200&q=75 \"Calculating cost savings per hire\")\n\n=>As you can see, once you fill the highlighted cells with your own conversion numbers (and optionally your hourly wages if yours differ much from our guesses), we’ll compute CPH prime for you.\n\nAnd because we’re a business and want you to hire through us, we’ve included the average savings for companies hiring through our platform. We provide two big value-adds: we can pretty drastically improve your TPS to onsite conversion — about 65% of our candidates pass the tech screen at companies on average. From there, they get offers and accept them at the same rate as you’d see in your regular funnel.\n\nClosing thoughts on building bridges between eng and recruiting\n---------------------------------------------------------------\n\nSo, why does being cognizant of eng time in your CPH calculations matter? I’ve already kind of beaten it into the ground that it’s the biggest cost sink. However, there’s another, more noble reason, to care about eng time. In my career, having sat on all different sides of the table, I’ve noticed one unfortunate, inalienable truth: engineering and recruiting teams are simply not aligned.\n\nEngineers tend to harbor some resentment toward recruiters because recruiters are the arbiters of how eng spends their time when it comes to hiring without a set of clear metrics or goals that help protect that time.\n\nRecruiters often feel some amount of resentment toward engineers who tend to be resistant to interruptions, toward putting in the time to provide meaningful feedback about candidates so that recruiting can get better, and toward changes in the process.\n\nIn our humble opinion, much of the resentment on both sides could be cured by incorporating recruiting and engineering costs together in a specific, actionable way that will reduce the misalignment we’re seeing. Recruiters tend to hold the cards when it comes to hiring practices, so we’d love to see them take the lead to reach across the aisle by proactively factoring in eng time spent during hiring and ultimately incorporating recruiting and eng costs together in one metric that matters. Once that’s in place, recruiting can use the data they gather to make better decisions about how to use eng time, and in the process, rebuild much of the rapport and love that’s lost between the two departments.\n\nAppendix\n--------\n\n### Table 1.\n\n**RECRUITING – 15 total hours or $1500**\n\n* **5** hours of recruiter screens (10 screens needed \\* 30 min per screen)\n* **4** hours of onsites (4 onsites needed \\* 1 hour per onsite)\n* **1** hour of offers (2 offer calls needed \\* 30 min per offer call)  \n  **ENGINEERING – 32 total hours or $7200**\n* **8** hours of phone screens (8 screens needed \\* 1 hour per screen)\n* **24** hours of onsites (4 onsites needed \\* 6 hours per onsite)\n\n### Table 2.\n\n**RECRUITING – 30 total hours or $3000**\n\n* **20** hours of recruiter screens (40 screens needed \\* 30 min per screen)\n* **8** hours of onsites (8 onsites needed \\* 1 hour per onsite)\n* **2** hours of offers (4 offer calls needed \\* 30 min per offer call)  \n  **ENGINEERING – 80 total hours or $18,000**\n* **32** hours of phone screens (32 screens needed \\* 1 hour per screen)\n* **48** hours of onsites (8 onsites needed \\* 6 hours per onsite)\n\nFootnotes\n---------\n\n1. We’re basing these numbers on a mix of ATS reporting ([Lever’s recruiting metrics report](https://www.lever.co/resources/recruiting-metrics-for-startups-and-smbs-report/) in particular) and what we’ve heard from our customers. [↩](#user-content-fnref-1)\n2. We’re assuming sourcing costs are fixed for purposes of simplicity and because this post is largely about the importance of eng time factored in to the funnel. Of course, if you have channels that reduce sourcing time significantly, you’ll want to weigh that when deciding its efficacy. [↩](#user-content-fnref-2)\n3. Really though, the value of an hour of work for an engineer is intangible and much higher than an hourly wage. There ARE inefficiencies and overhead to having a larger staff, not every hour is effective, and most likely it’s your best people who are conducting interviews. The reality is that the money spent on salaries is probably only a fraction of the true cost to the company, particularly for engineers (as opposed to recruiters). [↩](#user-content-fnref-3)\n4. See *Appendix: Table 1* for our work in figuring out how much recruiting and eng time it takes to make a hire when your TPS to onsite conversion rate is 50%. [↩](#user-content-fnref-4)\n5. See *Appendix: Table 2* for our work in figuring out how much recruiting and eng time it takes to make a hire when you cut your offer acceptance rate in half. [↩](#user-content-fnref-5)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/you-probably-dont-factor-in-engineering-time-when-calculating-cost-per-hire-heres-why-you-really-should",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Announcing the interviewing.io Technical Interview Practice Fellowship",
      "content": "I started interviewing.io because I was frustrated with how inefficient and unfair hiring was and how much emphasis employers placed on resumes.\n\nBut the problem is bigger than resumes. We’ve come to learn that interview practice matters just as much. The resume gets you in the door, and your interview performance is what gets you the offer. But, even though technical interviews are hard and scary for everyone — many of our users are senior engineers from FAANG who are terrified of getting back out there and code up the kinds of problems they don’t usually see at work while someone breathes down their neck — interview prep isn’t equitably distributed.\n\nThis inequity never really sat right with me (that’s why interviewing.io exists), but when [we started charging for interview practice post-COVID](https://interviewing.io/blog/interviewing-io-is-out-of-beta-anonymous-technical-interview-practice-for-all), it *really* didn’t sit right with me.\n\nAs you may have read, if you follow interviewing.io news, COVID-19 turned our world upside down. In its wake, the pandemic left a deluge of hiring slowdowns and freezes. For a recruiting marketplace, this was an existential worst nightmare — in a matter of weeks, we found ourselves down from 7-figure revenue to literally nothing. Companies didn’t really want or need to pay for hiring anymore, and we were screwed.\n\nThen, we pivoted and started charging our users, who had previously been able to practice on our platform completely for free (albeit with some strings, more on that in a moment). While this pivot was the right thing to do — without it, we would have had to shut down the company, unable to provide any practice at all — charging people, especially those from underrepresented backgrounds, didn’t sit right with us, and in our last post announcing our model, we made the following promises:\n\n* We’d ALWAYS have a free tier\n* **We’d immediately start working on a fellowship for engineers from underrepresented backgrounds or in a visa crisis/experiencing financial hardship ←** *That’s what this post is about!*\n* We’d find a way to let people defer their payments\n\nWe launched with a free tier, and it’s still there and going strong. We’re still working on deferred payments and are in the thick of user research and price modeling.\n\nBut, the rest of this post is about the 2nd promise. To wit, I’m so proud to tell you that we’ve officially launched the first (pilot) cohort of the interviewing.io Technical Interview Practice Fellowship. This cohort will be focused on engineers from backgrounds that are underrepresented in tech. We are acutely aware, of course, that our first cohort couldn’t capture everyone who’s underrepresented, that gender and race isn’t enough, and that we need to do more for our users who can’t afford our price tags, regardless of who they are or where they come from.\n\nOur hope is to expand this Fellowship to anyone who needs it.\n\nWe’re also working on the much harder problem of how to navigate the visa situation we’re in right now (different than when we wrote the first post, sadly… but especially important to me, given that I’m an immigrant myself).\n\n### What is the Fellowship, and why does it exist?\n\nBefore we tell you a little bit about the Fellows in our inaugural cohort and what the Fellowship entails, a quick word about why this matters.\n\nIn order to get a job as a software engineer, it’s not enough to have a degree in the field from a top school. However you learned your coding skills, you also have to pass a series of rigorous technical interviews, focusing on analytical problem solving, algorithms, and data structures.\n\nThis interview style is controversial, in part because it’s not entirely similar to the work software engineers do every day but also because 1) like standardized testing, it’s a learned skill and 2) unlike standardized testing, interview results are not consistent or repeatable — the same candidate can do well in one interview and fail another one in the same day. According to our data, only about [25% of candidates are consistent in their performance from interview to interview](https://interviewing.io/blog/after-a-lot-more-data-technical-interview-performance-really-is-kind-of-arbitrary) and [women quit 7X more often than men after a poor performance](https://interviewing.io/blog/voice-modulation-gender-technical-interviews).\n\nTo account for both of these limitations, the best strategy to maximize your chances of success is to practice a lot so you can 1) get better and 2) accept that the results of a single interview are not the be-all and end-all of your future aptitude as a software engineer and that it’s ok to keep trying.\n\nThe main problem created by modern interview techniques is that, despite interview practice being such a critical prerequisite to success in this field, access to practice isn’t equitably distributed. We want to fix this, and we’re well equipped to do so. Based on our data, engineers are twice as likely to pass a real interview after they’ve done 3-5 practice sessions on our platform.\n\nOur Fellows will get these practice sessions completely for free. These will be 1:1 hour-long sessions with senior engineers from a top company who have graciously volunteered their time and expertise. Huge thank you and a big shout-out to them all.\n\nAfter each session, Fellows will get actionable feedback that will help them in their upcoming job search, and we will be helping Fellows connect with top companies as well.\n\nNote: We’d like to be able to offer even more support – and are actively seeking more partners to do so. Please see the *How you can help* section below if you or your organization would like to get involved!\n\n### Why now?\n\nThe world seems to be in a place, now more than ever, to have the conversation about race, gender, socioeconomic, and other kinds of equity, in hiring. This is our small part of that conversation.\n\n### Who are the Fellows?\n\nAfter opening up our application process, we close to 1,000 submissions in a week, and (though it was really, really hard) we culled those down to 56 Fellows.\n\nOur first cohort is:\n\n* **82%** Black, Latinx, and/or Indigenous\n* **53%** women\n* **55%** senior (4+ years of experience) & **45%** junior (0-3 years of experience)\n\nHere are some of their (anonymized) stories. There were a lot of stories like these.\n\n> *My goal is to keep pressing as well as to share and give to underrepresented communities because the journey in tech can be isolating. Often I am the only one. It is critical that there are more people that look like me that are engineers \\*and\\* ascend the leadership ladder.*\n\n> *My parents immigrated from [redacted] to The Bronx without a formal education. I’m the first individual in my household to graduate from college and I’m the only Software Engineer in my family. I grew up in a poor neighborhood where many individuals had limited economic and educational opportunity. I aim to make the path to become a Software Engineer easier for those who were in my situation.*\n\n> *My journey to becoming a software engineer almost never happened. Throughout my undergraduate studies I was faced with having to drop out multiple times, due to the immigration status of my parents…. I was tasked with assisting in my family’s living situation and paying for school. I worked full time and started my own construction company in order to take care of my family and studies. It was always tough having to work 8-10 hours a day and then going to class or doing homework… Becoming a software engineer was always a goal of mine, and realizing that goal was well worth the struggle, given the struggle my parents went through to bring us here in the first place.*\n\n> *I spent 5 years in public education working directly with marginalized communities in the struggle for equity. My journey through software engineering is a continuation of this spirit of advocacy and changemaking. Software engineering is a tool to be put at the service of advocacy.*\n\n### What can I do to help?\n\nThere are a number of ways you can help and get involved!\n\n#### Help sponsor future Fellowship cohorts & create scholarships for underrepresented engineers!\n\nEvery Fellow in this first cohort represents at least 100X who are not. We have the tech to scale the hell out of this program, and all we need is backing and resources from people or organizations who recognize there’s a need (donations are tax-deductible). **Please email [fellowship-sponsors@interviewing.io](mailto:fellowship-sponsors@interviewing.io) if you’d like to get involved or want more information.**\n\n#### Hire through us!\n\nDespite mounting evidence that resumes are poor predictors of aptitude, companies were obsessed with where people had gone to school and worked previously. On interviewing.io, software engineers, no matter where they come from or where they’re starting, can book anonymous mock interviews with senior interviewers from top companies. We use data from these interviews to identify top performers much more reliably than a resume, and fast-track them to real job interviews with employers on our platform through the same anonymous, fair process. Because we use data, not resumes, our candidates end up getting hired consistently by companies like Facebook, Uber, Twitch, Lyft, Dropbox, and many others, **and 40% of the hires we’ve made to date have been candidates from non-traditional backgrounds. Many of our candidates have literally been rejected based on their resumes by the same employer who later hired them when they came through our anonymous platform** (one notable candidate was rejected 3 times from a top-tier public company based on his resume before he got hired at that same company through our anonymous interview format).\n\n**Please email [sales@interviewing.io](mailto:sales@interviewing.io) to get rolling.**\n\n#### Buy an individual practice session for someone who can’t afford it\n\n**If you know individual engineers who need interview practice but can’t afford it, use our [handy interview gifting feature](https://interviewing.io/gift-practice-interviews).** Interviews are $100 each. They’re not cheap, but we have to price them that way to pay for interviewer time (interviewers are senior FAANG engineers) and cover our costs. Sadly that means practice interviews are not affordable to everyone. Even if you can’t get involved to help us fund interviews at scale, if you know someone who needs practice but can’t afford it, you can buy them an anonymous mock interview or two individually. It’s the best gift you can give to an engineer who’s starting their job search.",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/announcing-the-interviewing-io-technical-interview-practice-fellowship",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Stop trying to make recruiters think, or, why your resume is bad and how to fix it",
      "content": "Years ago, Steve Krug wrote a book about web design called *Don’t Make Me Think*. It’s a classic, and the main point is that good design should make everything painfully obvious to users without demanding anything of them.\n\nResumes are just the same. Your resume shouldn’t make recruiters think. It should serve up the most important things about you on a platter that they can digest in 30 seconds or less.\n\nBefore I share some resume tips, there’s something important I want to reiterate: **Don’t spend a lot of time on your resume**. You can read my [piece about how resume writing is snake oil](https://interviewing.io/blog/why-resume-writing-is-snake-oil), but the TL;DR is that recruiters spend a median of 30 seconds looking at resumes, and most of that is spent looking for top-tier companies. If you don’t have top-tier companies (and in some cases niche skills), wordsmithing your bullets or rearranging your sections or changing your layout won’t help. If you do have top-tier companies, sometimes doing some wordsmithing and rearrangement will help… *if your top company experience or niche skills are buried.*\n\nIf you don’t have top-tier brands, the best bang for your buck is to switch from online applications to hiring manager outreach. [Here’s how to do it.](https://interviewing.io/blog/how-to-get-in-the-door-at-top-companies-cold-out-reach-to-hiring-managers-part-2)\n\nWith that said, I know that no matter what I say, people will still grind on their resumes instead of doing outreach. Grinding on resumes is safe. Outreach is scary and opens you up to personal (rather than impersonal) rejection. So, look, if you’re going to do *something* to your resume, let’s make sure that that something is low-effort and high-return. Unlike the endless resume tweaking that most candidates do, these changes directly address how recruiters actually read resumes.\n\nHere we go.\n\nStop putting filler buzzwords in your \"About\" section. Use it to spell out the most impressive things about you.\n----------------------------------------------------------------------------------------------------------------\n\nYour \"About\" or \"Summary\" section is prime real estate. Yet so many candidates fill this section with meaningless jargon like \"passionate self-starter\" or \"detail-oriented team player.\"\nInstead, use this section to explicitly tell recruiters the 2-3 most impressive things about you in plain English. This is your chance to control the narrative. *Want recruiters to take something away from reading your resume?* Don’t assume they’ll figure it out. They’re not reading it long enough to intuit anything. Spell it out for them verbatim in this section.\nDo this, not that:\n\n❌ Results-driven full-stack engineer with a passion for scalable systems and user-centric design  \n✅ Senior engineer with 3 years at Amazon, promoted twice in 3 years (2X the company average)[1](#user-content-fn-1)\n\nDon’t include your GPA if it’s under 3.8\n----------------------------------------\n\nThis is simple but effective: only include your GPA if it's 3.8 or higher[2](#user-content-fn-2). A middling GPA doesn't help your case and might inadvertently signal academic mediocrity.\n\nIf your GPA isn't stellar, focus on other academic achievements: hackathons, technical competitions, fellowships or scholarships. These provide better signals about your capabilities than a so-so GPA.\n\nContext matters for lesser-known companies\n------------------------------------------\n\nIf you've worked at Google or Facebook, recruiters instantly get what kind of company you're coming from. But when you have \"TechStartup123\" on your resume, they have no idea what they're looking at or how impressive it might be.\n\nFor lesser-known companies, include a one-line description explaining what the company does, along with any impressive metrics or investors:\n\n❌ \"Software Engineer, DevTools Inc.\"  \n✅ \"Software Engineer, DevTools Inc. ($50M Series B from Sequoia, 2M+ active users)\"\n\nThis simple addition provides crucial context that helps recruiters evaluate your experience properly. Without it, they might discount valuable experience simply because they don't recognize the company name.\n\nAvoid the \"job-hopper\" misperception\n------------------------------------\n\nHere's a common mistake: listing each role at the same company as if they were separate jobs. This can make recruiters think you've job-hopped, which is often seen as a red flag.\n\nInstead, group different roles under the same company heading:\n\n❌ Listing separate entries for \"Junior Developer at XYZ\" and \"Senior Developer at XYZ\"  \n✅ \"XYZ Company - Senior Developer (2021-Present) - Junior Developer (2019-2021) *Promoted in 2 years vs. company average of 3.5 years*\"\n\nThe second format clearly shows growth within a single company and explicitly highlights faster-than-average promotion, which is a strong positive signal. (You may also want to carry over your promotion cadence into your “About” section, as you saw above.)\n\nBe crystal clear about your work authorization status\n-----------------------------------------------------\n\nThis one is particularly crucial if you have a foreign-sounding name and/or education outside the US. I've seen many qualified candidates get passed over because recruiters assumed they needed visa sponsorship when they actually didn't. Don't leave this to chance.\n\nMake your work status explicit in your header or summary section:\n\n❌ No mention of work authorization (leaving recruiters to guess)  \n✅ \"US Citizen\" or \"Green Card Holder\" or \"Authorized to work in the US without visa sponsorship\"\n\nCareer changers: provide context about the change\n-------------------------------------------------\n\nIf you've switched careers, your resume can look confusing without proper context. Recruiters might struggle to understand why someone with your background is applying for this role, or they might not recognize how your previous experience translates to your current trajectory.\n\nAddress this head-on in your “About” section.\n\n❌ Listing previous career experience with no explanation of your transition  \n✅ \"Transitioned from marketing to software engineering in 2021 after completing a bootcamp\" or \"Former accountant who pivoted to data science through self-study and online courses while continuing full-time work\"\n\nThis context helps recruiters understand your timeline and puts your current title and achievements in perspective. Without it, you risk serious misinterpretation:\n\n1. Recruiters might think you're far more junior than you actually are in your new field (potentially ruling you out for appropriate-level positions)\n2. Or conversely, they might assume you have years of relevant experience in your new field (and then wonder why you haven't achieved more in that time)\n\nBoth misinterpretations can be fatal to your application. By providing a clear timeline of your transition, you help recruiters accurately gauge your experience level and set appropriate expectations. This transparency also demonstrates valuable traits like adaptability and determination.\n\nAnd here's another key point for career changers: you don't need to list all your previous positions before the transition... unless they're impressive. Be selective about what pre-transition experience you include:\n\n❌ DON'T include mundane or irrelevant details from your previous career that add nothing to your current narrative. Your three years as a retail associate before becoming a developer probably won't strengthen your software engineering application.  \n✅ DO highlight prestigious achievements from your previous career. If you were, say, a concert pianist, a lawyer who graduated from a top-tier law school, or a management consultant at McKinsey, absolutely include that. These signal that you're smart and high-achieving, regardless of domain.\n\nIn conclusion\n-------------\n\nIf you do all these things, you may or may not see a return. After all, even the impact of these tweaks pales in comparison to having top brands on your resume. But, given that these will take you a few minutes to do, it doesn’t hurt. Here’s the TL;DR:\n\n![A list of high-ROI resume tweaks](https://strapi-iio.s3.us-west-2.amazonaws.com/high_roi_resume_tweaks_f23aa49f90.png)\n\nFootnotes:\n\nFootnotes\n---------\n\n1. I have no idea what the average promotion cadence is at Amazon, and this example is meant to be illustrative rather than accurate, though maybe my readers will tell me the cadence now. [↩](#user-content-fnref-1)\n2. I realize this diverges from the advice in *Beyond Cracking the Coding Interview*, where Gayle recommends including it if it’s 3.0 or more. This is one of the cases where the authors had differing opinions. We’re (mostly) human. [↩](#user-content-fnref-2)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/stop-trying-to-make-recruiters-think-or-why-your-resume-is-bad-and-how-to-fix-it",
      "author": "",
      "user_id": ""
    },
    {
      "title": "No engineer has ever sued a company because of constructive post-interview feedback. So why don’t employers do it?",
      "content": "One of the things that sucks most about technical interviews is that they’re a black box—candidates (usually) get told whether they made it to the next round, but they’re rarely told why they got the outcome that they did. Lack of feedback, or feedback that doesn’t come right away, isn’t just frustrating to candidates. It’s bad for business. We did a [whole study on this](https://interviewing.io/blog/own-interview-performance). It turns out that candidates chronically underrate and overrate their technical interview performance, like so:\n\n![](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F7138d_screenshot_2016_12_27_20_50_08_6bc70ae8e8.webp&w=1920&q=75 \"Perceived vs. Actual interview performance\")\n\nWhere this finding starts to get actionable is that there’s a statistically significant relationship between whether people think they did well in an interview and whether they’d want to work with you. In other words, in every interview cycle, some portion of interviewees are losing interest in joining your company just because they don’t think they did well, even when they actually did. It makes sense… when you suspect you might not have done well, you’re prone to embark on a painful bout of self-flagellation, and to make it stop, you’ll rationalize away the job by telling yourself that you *totally didn’t want to work there anyway*.\n\n**Practically speaking, giving instant feedback to successful candidates can do wonders for increasing your close rate.**\n\nIn addition to making candidates you want today more likely to join your team, feedback is crucial for the candidates you might want down the road. Technical interview outcomes are highly non-deterministic. According to our data, only about 25% of candidates perform consistently from interview to interview. Why does this matter? If interview outcomes are erratic, it means that the same candidate you reject this time might be someone you want to hire in 6 months. It’s in your interest to forge a good relationship with them now and be cognizant of and humble about the flaws in your hiring process.\n\nI thought this tweet captured my sentiments particularly well.\n\n> Great teams treat candidate rejections with just as much care as offers.  \n>   \n> It’s insane to see folks screw this up, especially with younger talent.  \n>   \n> Why? You have no idea how they’ll grow in 18 months.  \n>   \n> For all you know, you just benched MJ in high school 🙏🏼\n>\n> — Danny Trinh (@dtrinh) [February 4, 2020](https://twitter.com/dtrinh/status/1224757036447125504?ref_src=twsrc%5Etfw)\n\nSo, despite the benefits, why do most companies persist in giving slow feedback or none at all? I surveyed founders, hiring managers, recruiters and labor lawyers (and also put out some questions to the Twitterverse) to understand why anyone who’s ever gone through interviewer training has been told in no uncertain terms to not give feedback.\n\nAs it turns out, feedback is discouraged primarily because companies are scared of getting sued… and because interviewers fear defensive candidate backlash. In some cases, giving feedback is avoided just because companies view it as a no-upside hassle.\n\nThe sad truth is that hiring practices have not caught up with market realities. Many of the hiring practices we take for granted today originated in a world where there was a surplus of candidates and a shortage of jobs. This extends to everything from painfully long take-home assignments to poorly written job descriptions. And post-interview feedback is no exception. As Gayle Laakmann McDowell, author of *Cracking the Coding Interview*, [explains on Quora](https://www.quora.com/When-Google-or-Facebook-rejects-a-candidate-why-don%E2%80%99t-they-give-him-her-a-simple-explanation-for-the-rejection-to-help-the-candidate-work-on-the-gaps-in-their-knowledge/answer/Gayle-Laakmann-McDowell?ch=10&share=ecef86e3&srid=hQGW):\n\n*“Companies are not trying to create the most perfect process for you. They are trying to hire—ideally efficiently, cheaper, and effectively. This is about their goals, not yours. Maybe when it’s easy they’ll help you too, but really this whole process is about them… Companies do not believe it helps them to give candidates feedback. Frankly, all they see is downside.”*\n\nLook, I’m guilty of this, too. Here’s a rejection email I wrote when I was head of technical recruiting at TrialPay. This email makes me want to go back in time and punch myself in the face and then wish myself the best in my future endeavors to not get punched in the face.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/b9a7b_rejection_6827fc9744.webp)\n\nThese kinds of form letter rejections (which I guess *is* better than just leaving the person hanging) make a lot of sense when you have a revolving door of disposable candidates. They are completely irrational in this brave new world where candidates have more leverage than companies. **But, because HR is fundamentally a cost center tasked with risk mitigation (rather than a profit center tasked with, you know, making stuff better), and because engineers on the ground only have so many cognitive cycles to tackle hard stuff outside their job descriptions, we continue to march forward on autopilot, perpetuating outdated and harmful practices like this one.**\n\nIn this hiring climate, companies should move toward practices that give candidates a better interview experience. Is fear of litigation and discomfort legit enough to keep companies from giving feedback? Does optimizing for fear and a few bad actors in lieu of candidate experience make sense in the midst of a severe engineering shortage? Let’s break it down.\n\nDoes the fear of getting sued even make sense?\n----------------------------------------------\n\nWhile researching this piece, I spoke to a few labor lawyers and ran some Lexis Nexis searches to see just how often a company’s constructive feedback (i.e. not “durrrr we didn’t hire you because you’re a woman”) to a rejected eng candidate has resulted in litigation.\n\n**Hey, guess what? IT’S ZERO! THIS HAS NEVER HAPPENED. EVER.**[1](#user-content-fn-1)\n\nAs some of my lawyer contacts pointed out, a lot of cases get settled out of court, and that data is much harder to get. But in this market, creating poor candidate experience to hedge against something that is *highly* unlikely seems… irrational at best and destructive at worst.\n\nWhat about candidates getting defensive?\n----------------------------------------\n\nAt some point, I stopped writing trite rejection emails like the one above, but I was still beholden to my employer’s rules about written feedback.[2](#user-content-fn-2) As an experiment, I tried giving candidates verbal feedback over the phone.\n\nFor context, I had a unique, hybrid role at TrialPay. Though my title was Head of Technical Recruiting, which meant I was accountable for normal recruiter stuff like branding and sourcing and interview process logistics, my role had one unique component. Because I had previously been a software engineer, to take the heat off the long-suffering eng team, I was often the first line of defense for technical interviews and conducted something like 500 of them that year.\n\nAfter doing a lot of interviews day in and day out, I became less shy about ending them early when it was clear that a candidate wasn’t qualified (e.g. they couldn’t get through the brute force solution to the problem, let alone optimize). Did ending interviews early cause candidates to fly off the handle or feel particularly awkward, as many people suspect?\n\n![](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F48730_screenshot_2020_01_13_14_56_32_0c924829ae.webp&w=1200&q=75 \"A tweet reply\")\n\nIn my experience, cutting things off and saying nothing about why is a lot more awkward and leads to more defensiveness than letting candidates know what the deal is. Some candidates will get defensive (at which point you can politely end the call), but if you offer **constructive feedback**—let them know what went wrong, make some recommendations about books to read, point them to problem repositories like Leetcode[3](#user-content-fn-3), etc.—most will be grateful. My personal experience with giving feedback has been overwhelmingly positive. I used to love mailing books to candidates, and I formed lasting relationships with many. Some became early interviewing.io users a few years later.\n\nAnyway, the way to avoid negative reactions and defensiveness from candidates is to practice giving feedback in a way that’s constructive. We’ll cover this next.\n\nSo if giving feedback isn’t actually risky and has real upsides, how does one do it?\n------------------------------------------------------------------------------------\n\nWhen I started interviewing.io, it was the culmination of what I had started experimenting with at TrialPay. It was clear to me that feedback is a Good Thing and that candidates liked it… which in this market means it’s also good for companies. But, we still had to grapple with prospective customers’ (pretty irrational) fears about the market being flooded with defensive candidates with a lawyer on speed dial.\n\nFor context, interviewing.io is a hiring marketplace. Before talking to companies, engineers can practice technical interviewing anonymously, and if things go well, unlock our jobs portal, where they can bypass the usual top-of-funnel cruft (applying online, talking to recruiters or “talent managers,” finding friends who can refer them) and book real technical interviews with companies like Microsoft, Twitter, Coinbase, Twitch, and many others… often as early as the next day.\n\nThe cool thing is that both practice and real interviews with companies take place within the interviewing.io ecosystem, and wrt feedback, you’ll see why this matters in a moment.\n\n**Before we started working with employers, we spent some time building out our practice platform and getting the mechanics right.** For practice interviews, our post-interview feedback forms looked like this:\n\n![Screenshot showing interviewing.io interview feedback form](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F6f86b_screenshot_2017_11_29_09_10_46_6a2be62174.webp&w=1920&q=75 \"Interviewing.io feedback form\")\n\nThe feedback form that an interviewer fills out\n\nAfter each practice interview, interviewers fill out the form above. Candidates fill out a similar form rating their interviewer. When both parties fill out their forms, they can see each other’s responses.\n\nIf you’re curious, you can watch people practicing and read real feedback that they got in [our public showcase](https://interviewing.io/mocks). Here’s a snapshot:\n\n![Screenshot of a Showcase interview summary](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fd163a_screenshot_2020_01_31_17_36_12_63d389ef47.webp&w=1920&q=75 \"A Showcase interview summary\")\n\nCheck out our [showcase](https://interviewing.io/mocks). It's cool.\n\nWhen we started letting employers hire on our platform, we just recycled this post-interview feedback format, told them they should leave feedback to help us calibrate and because it’s good for candidate experience, and fervently hoped that they wouldn’t have an issue with it.\n\n**To our surprise and delight, employers were eminently willing to leave feedback. On our platform, candidates were able to see whether they passed or not and exactly how they did, just a few minutes after the interview was over, stopping the rising tide of post-interview anxiety and self-flagellation in its tracks, and, as we’ve said, increasing the likelihood that a great candidate will accept an offer.**\n\n![Screenshot of a real interview feedback form](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F680bb_screenshot_2020_02_01_12_40_49_546970b519.webp&w=3840&q=75 \"Genuine feedback for a real company interview\")\n\nA real, successful company interview on interviewing.io\n\nAnd if a candidate failed an interview, they got to see exactly why they failed and what they needed to work on, probably for the first time ever in interview history.\n\n![Screenshot of a feedback form for a failed interview](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ff8ecd_feedback_e28094_interviewing_io_10f09e88b2.webp&w=3840&q=75 \"Actionable feedback for a failed company interview\")\n\nA real, failed company interview on interviewing.io\n\nAnonymity makes it easier to give feedback\n------------------------------------------\n\n**On interviewing.io, interviews are anonymous: an employer knows literally nothing about the candidate before and during the interview** (employers can even enable [our real-time voice masking feature](https://interviewing.io/blog/voice-modulation-gender-technical-interviews)). Candidates’ identities are only revealed after a successful interview, i.e. after the employer has already submitted feedback.\n\nWe insist on anonymity because about 40% of our top-performing candidates are non-traditional, and we don’t want lack of pedigree or an unusual background to open them up to bias. Because interviews are anonymous, it’s often impossible to discriminate on the basis of age or gender or background. **Therefore, feedback has to be constructive, by design, because the only info the interviewer has to go on is how well the candidate is performing in the interview.** In addition to helping candidates get a fair shot, this anonymity provides something of a safety net for employers—it’s harder to build a discrimination case out of the feedback when the employer doesn’t know the identity of the candidate.\n\nIn many other contexts, anonymity can be destructive because of reduced accountability. But in the interview process, we’ve discovered over and over that anonymity sets free the better angels of our nature and creates a kinder, more inclusive interview experience for candidates and employers alike.\n\nBuilding post-interview feedback into your eng hiring process\n-------------------------------------------------------------\n\nSo, how can you fold these learnings into your process? Of course, the easiest way to get your feet wet is to start using interviewing.io. We’ll get you candidates you wouldn’t be sourcing without us, and we’ll empower you to give them the best interview experience you can.\n\nBut even if you don’t use us, based on how unlikely it is that you’re going to get sued or deal with angry candidates, we strongly recommend having your interviewers provide constructive feedback (like in the examples above) after every interview for all candidates, whether they pass or not, over email.\n\nHere are a few strategies for delivering constructive feedback:\n\n1. Be clear that it’s a no-go. Ambiguity is psychologically difficult in a stressful situation. For instance: *Thank you for interviewing with us. Unfortunately, you didn’t pass the interview.*\n2. After you make it clear that it’s a no-go, tell them something nice. Find something about their performance—an answer they gave, or the way they thought through a problem, or how they asked the right questions—and share it with them. They’ll be more receptive to the rest of your feedback once they know that you’re on their side. For instance: *Despite the fact that it didn’t work out this time, you did {x,y,z} really well, and I think that you can do much better in the future. Here are some suggestions for what you can work on.*\n3. When you give suggestions, be specific and constructive. Don’t tell them that they royally screwed the whole operation and need to rethink their line of work. Instead, focus on specific things they can work on. Or, to put it another way, “Hey, familiarize yourself with big O notation. It’s not as scary as it sounds bc it comes up a lot in these kinds of interviews.”[4](#user-content-fn-4) doesn’t say “you’re dumb and your work experience is dumb and you should feel bad” or “you seem like an asshole.” It says you should familiarize yourself with big O notation.\n4. Make recommendations. Is there a book they could read? *If they’re promising but just lack knowledge, it’s a really nice gesture to ship said book to them.*\n5. If you think the candidate is on their way to becoming a great engineer (especially if they take your recommendations and advice!), let them know that they can contact you again in a few months. You’ll build goodwill with someone who, even if they don’t work with you in the future, will talk about you to others. And when they do improve, you’ll be in a better position to bring them onto your team.\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. If you know of such a case, please tell me, and I’ll update this post/associated content accordingly ASAP. [↩](#user-content-fnref-1)\n2. This is a rule pretty much every company has, ever. It’s not just TrialPay, which was a great place to work and whose defensive HR policies, like every other company’s, were in no way indicative of their workplace culture. [↩](#user-content-fnref-2)\n3. Whether algorithmic problems of the type you’d find on Leetcode are the best way to interview is a question worth asking… and I’ve since come to feel pretty strongly that many of them are not. But that’s out of scope for this piece. [↩](#user-content-fnref-3)\n4. I recently discovered an amazing online book that makes big O approachable, practical, and not scary (all without talking down to the reader): *[Grokking Algorithms](https://livebook.manning.com/book/grokking-algorithms/)* by Aditya Bhargava. [↩](#user-content-fnref-4)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/no-engineer-has-ever-sued-a-company-because-of-constructive-post-interview-feedback-so-why-dont-employers-do-it",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Can fake names create bias? An exploration into interviewing.io’s pseudonym generator",
      "content": "Hello everyone, my name is Atomic Artichoke, and I’m the newest employee of the interviewing.io team, having joined a couple months ago as a Data Scientist.\n\nAtomic Artichoke isn’t my real name, of course. That’s the pseudonym the interviewing.io platform gave me, right before I took my final interview with the company. If you’ve never used interviewing.io before (and hey, if you haven’t already, [why not sign up now?](https://interviewing.io/signup)), it’s a platform where you can practice technical interviewing anonymously with experienced engineers (and do real job interviews anonymously too).\n\n![Screenshot showing how an Interviewing.io user can pick their anonymous pseudonym](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fpseudonym_handle_3d625b5bb7.png&w=1920&q=75 \"Choosing a pseudonym\")\n\nWhen it’s time to interview, you and your partner meet in a collaborative coding environment with voice, text chat, and a whiteboard (check out [recordings of real interviews](https://interviewing.io/mocks) to see this process in action). During interviews, instead of your name, your partner will see your pseudonym, like so:\n\n![Screenshot showing how a user's anonymous pseudonym appears during a practice interview](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fpseudonym_code_2ee6deedd1.png&w=1080&q=75 \"Pseudonyms are always used unless an interviewer and interviewee have chosen to de-mask and reveal their names to each other\")\n\nIn my opinion, “Atomic Artichoke” is a pretty cool name. It sounds like a Teenage Mutant Ninja Turtles villain, and alliterative phrases are always cool. However, I had some reservations about that handle, because I feel like the pseudonym represented me in ways with which I didn’t identify. I don’t know how to eat or cook an artichoke, I never really understood atoms much, and I possess no mutant superpowers.\n\nBut I wondered, how did the interviewer perceive me? Did this person think “Atomic Artichoke” was a cool name? If so, did that name influence his or her perception of me in any way? More importantly, **did my pseudonym have any influence in me getting hired?** If I had a different, less cool name, would I have gotten this job?\n\nI know, it’s a silly question. I’d like to think I was hired because of my skills, but who really knows? I was curious, so I ~~wasted~~ invested a few days to investigate.\n\nWhat we already know about names in the hiring process\n------------------------------------------------------\n\nYou might be asking, “Why does interviewing.io have pseudonyms, anyway?” Anonymity. We want candidates to be assessed on their actual skills, not on proxies of skill like the colleges they’ve attended, the notoriety of their social circles, or prior companies they’ve worked at. If a hiring manager knows a person’s name and knows how to use the Internet, it’s easy to find this information.\n\nI’m not the first to wonder about names and hiring. Plenty of academic literature exists exploring the impact of name choice on various life outcomes. I’ll briefly touch on a handful of those perspectives.\n\n* A 1948 paper concluded people with unique names tended to have [lower academic performance than those with more common names](https://www.tandfonline.com/doi/abs/10.1080/00224545.1948.9918930?journalCode=vsoc20).\n* A 2003 study observing the relationship between “black” names and life outcomes concluded that after for controlling for other factors, [name choice did not affect life outcomes](https://www2.nber.org/papers/w9938.pdf?new_window=1).\n* Finally, a 2004 paper specifically focused on the jobs market suggested that resumes containing [“black” names received fewer callbacks than those with “white” names](https://www.aeaweb.org/articles?id=10.1257/0002828042002561), even after controlling for resume quality.\n\nAs you can see, academic opinions differ. However, in the case that name-based bias actually exists, maybe we can implement a cheap-enough solution to eliminate the bias completely. Randomly-generated pseudonyms fits that bill nicely.\n\nBut as I wondered before, maybe the pseudonym name generator creates a different kind of bias, leaving us in a similarly biased place that using real people’s names leaves us. I first needed to understand how pseudonyms get generated, so I dug into some code.\n\nExploring code\n--------------\n\nAfter dusting off what little Javascript knowledge I acquired 6 years ago, I found the 13 lines of code that generates pseudonyms. Mechanics-wise it’s simple: there are two lists, one containing adjectives and one containing nouns. The pseudonym generator randomly pulls one adjective and one noun from each list, and mashes them together in that order, with a space in between. The generator outputs some sweet sounding pseudonyms like:\n\n* Serpentine Gyroscope\n* Moldy Parallelogram\n* Frumious Slide Rule\n* Supersonic Llama\n\nBut they can also come up with less memorable, more commonplace, and more boring phrases like:\n\n* Ice Snow\n* Warm Wind\n* Red Egg[1](#user-content-fn-1)\n* Infinite Avalanche\n\nAfter running through a few example pseudonyms, anecdotally I felt the first list was more attractive to me than the second. It sparked more joy in me, one could say. I just couldn’t articulate why.\n\nThat’s when I noticed that certain themes kept recurring. For example, there were multiple Alice in Wonderland references, a bunch of animals, and many types of foods listed. At first glance the chosen words seemed odd. But after getting to know my co-workers better, the list of words began to make a lot more sense.\n\nThe co-worker sitting across from me is a huge Alice in Wonderland fan. Our founders seem to love animals, since they bring their dogs to work most days. Finally, food and restaurant discussions fuel most lunchtime arguments. Just in my first month, I had heard more discussion about chicken mole and Olive Garden than I ever had in my life.\n\nWhile it’s true the pseudonym generator chooses words randomly, the choice of which words get onto the list isn’t necessarily random. If anything, the choice of words reflects the interests of the people who built the application. Might it be possible that the first list appealed to me because they reference math concepts, and I happen to like math-y things?\n\nThe hypothesis\n--------------\n\nThis insight helped me craft my hypothesis more concretely: all else equal, do some candidates receive better ratings on interviews, **because interviewers happen to associate positively with users whose pseudonyms reference the interviewers’ personal interests?**\n\nThis hypothesis rests upon the assumption that people are drawn to stuff that’s similar to themselves. This seems intuitive: when individuals share common interests or backgrounds with others, chances are they’ll like each other. Therefore, is it possible that interviewers like certain candidates more because they find commonality with them, even though we manufactured that commonality? And did that likability translate to better interview ratings?\n\nTo test this, I categorized users into one of the following 6 categories based on the noun part of their pseudonym, which will be called Noun Category going forward.\n\n* Animal\n* Fantasy\n* Food\n* History\n* Object\n* Science\n\nThese broad categories aimed to differentiate among interest areas that might appeal differently to different interviewers. Among these 6 groups, I wanted to observe differences in interview performance. And knowing the pseudonym generator assigns names randomly, we would not expect to find a difference.\n\nTo proxy for interview performance, I used the “Would You Hire” response from the interviewer on the interviewee, which is the first item on the interviewer’s post-interview questionnaire.\n\n![Screenshot of the interview feedback form for an anonymous company round](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffake_names_bias_f8cd1ac082.png&w=1080&q=75 \"The interview feedback form for anonymous company rounds asks if the user wants to share their info\")\n\nThese two pieces of data led to a clear, testable null hypothesis: **there should exist no relationship between Noun Category and the Would You Hire response.** If we reject this null hypothesis, we would have evidence suggesting our pseudonyms can impact hiring decisions.\n\nData analysis and interpretation\n--------------------------------\n\nI pulled data on a sample of a few thousand interviewing.io candidates’ first interview on our platform, and performed a Chi-Squared test against the observed frequencies of the 6 “Noun Categories” and 2 “Would You Hire” interviewer responses. Each cell of the 6 x 2 matrix contained at least 40 observations.\n\nBelow are the mean percentage of candidates who received a Yes from their interviewer, broken out by Noun Category. While most of the categories seemed to clump around a similar pass rate, the History group seemed to under-perform while the Fantasy group over-performed.\n\n![Chart showing Would You Hire Pass % vs Noun Category](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fd1730_would_you_hire_pass_vs_noun_category3_c020791d7b.webp&w=1200&q=75 \"Would You Hire Pass % vs Noun Category\")\n\nThe Chi-Square test rejected the null hypothesis at a 5% significance level.\n\nThese results suggest **a relationship might exists between Noun Category and an interviewer’s Would You Hire response**. Which again, should *not* occur because a candidate’s Noun Category was randomly assigned![2](#user-content-fn-2)\n\nWhat next?\n----------\n\nWhile this analysis doesn’t predict outcomes for specific individuals, the result suggests it isn’t totally crazy to believe I may gotten lucky on my interview. Maybe I don’t suffer from imposter syndrome, maybe I am an imposter. How depressing.\n\nSo what now? Fortunately (or unfortunately) for my new company, if we want to eliminate this bias, I can suggest potential next steps.\n\nOne solution might be to pander to an interviewer’s interests. We could randomly generate a new pseudonym for candidates every time they meet a different interviewer, ensuring that pseudonym creates positive associations with the interviewer. Similarly, we could generate more pseudonyms referencing Lord of the Rings and Warcraft, if we know our interviewer pool tends to be fantasy-inclined.\n\nAn alternative solution might be to give candidates pseudonyms with no meaning at all. For example, we could generate random strings, similar to what password managers generate for you. This would eliminate any real world associations, but we’d lose some whimsy and human readability that the current pseudonyms provide.\n\nYet another alternative solution could be to do more analysis before acting. The analysis didn’t quantify the magnitude of the bias, so we could construct a new sample to test a more specific hypothesis about bias size. It’s possible the practical impact of the bias isn’t huge, and we should focus our energy elsewhere.\n\nZooming out\n-----------\n\nOn the face of it, this pseudonym bias seems trivial, and in the universe of all biases that could exist, that’s probably true. However, it makes me wonder how many other hidden biases might exist elsewhere in life.\n\nI think that’s why I was hired. I’m obsessed with bias. Though I’ll be doing normal business-y Data Scientist stuff, my more interesting responsibilities will be poking at all aspects of the hiring market and examining the myriad of factors, mechanisms, and individuals that make the hiring market function, and perhaps not function effectively for some people.\n\nGoing a step further than identifying hiring biases, I’d like to shift discussions toward action. It’s great that the tech industry talks about diversity more, but I think we can facilitate more discussions around which concrete actions are being taken, and whether those actions actually achieve our goals, whatever those goals may be.\n\nI think it all starts with being introspective about ourselves, and investigating whether something as innocuous as a randomly generated phrase could ever matter.\n\nAtomic Artichoke\n(Ken Pascual)\n\n![Atomic Artichoke's personalized Interviewing.io sticker](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F28d03_image_from_ios_3_e1235e7732.webp%3Fupdated_at%3D2022-12-08T13%3A55%3A14.749Z&w=640&q=75 \"Atomic Aritichoke\")\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. This is the shortest pseudonym possible on interviewing.io [↩](#user-content-fnref-1)\n2. This is not entirely true. Users can re-generate a random pseudonym as often as they want, meaning a user can choose their name if they re-generate a lot. However, there's no evidence this happens often, because we found no significant difference in the observed and theoretical randomized distribution of Noun Categories. [↩](#user-content-fnref-2)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/interview-bias-pseudonyms",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Technical interview performance is kind of arbitrary. Here’s the data.",
      "content": "*Note: Though I wrote most of the words in this post, there are a few people outside of interviewing.io whose work made it possible. **[Ian Johnson](https://twitter.com/enjalot)**, creator of [d3 Building Blocks](http://blockbuilder.org/), created the graph entitled Standard Dev vs. Mean of Interviewee Performance (the one with the icons) as well as all the interactive visualizations that go with it. **[Dave Holtz](https://twitter.com/daveholtz)** did all the stats work for computing the probability of people failing individual interviews. You can see more about his work on [his blog](https://daveholtz.net/).*\n\n[interviewing.io](https://interviewing.io/) is a platform where people can practice technical interviewing anonymously and, in the process, find jobs. In the past few months, we’ve amassed data from hundreds of interviews, and **when we looked at how the same people performed from interview to interview, we were really surprised to find quite a bit of volatility, which, in turn, made us question the reliability of single interview outcomes.**\n\nThe setup\n---------\n\nWhen an interviewer and an interviewee match on our platform, they meet in a collaborative coding environment with voice[1](#user-content-fn-1), text chat, and a whiteboard and jump right into a technical question. Interview questions on the platform tend to fall into the category of what you’d encounter at a phone screen for a back-end software engineering role, and interviewers typically come from a mix of large companies like Google, Facebook, and Yelp, as well as engineering-focused startups like Asana, Mattermark, KeepSafe, and more. If you’d like to see an interview an action, head over to our [public recordings](https://interviewing.io/mocks) page for a few examples.\n\nAfter every interview, interviewers rate interviewees on a few different dimensions, including technical ability. Technical ability gets rated on a scale of 1 to 4, where 1 is “meh” and 4 is “amazing!” ([you can see the feedback form here](https://strapi-iio.s3.us-west-2.amazonaws.com/49613_interviewer_feedback_64893c98f9.png)). On our platform, a score of 3 or above has generally meant that the person was good enough to move forward.\n\nAt this point, you might say, that’s nice and all, but what’s the big deal? Lots of companies collect this kind of data in the context of their own pipelines. Here’s the thing that makes our data special: **the same interviewee can do multiple interviews, each of which is with a different interviewer and/or different company, and this opens the door for some pretty interesting and somewhat controlled comparative analysis**.\n\nPerformance from interview to interview is pretty volatile\n----------------------------------------------------------\n\nLet’s start with some visuals. In the graph below, every tiny human icon represents the mean technical score for an individual interviewee who has done 2 or more interviews on the platform.[2](#user-content-fn-2) The y-axis is standard deviation of performance, so the higher up you go, the more volatile interview performance becomes. If you hover over each , you can drill down and see how that person did in each of their interviews. Anytime you see bolded text with a dotted underline, you can hover over it to see relevant data viz. Try it now to expand everyone’s performance. You can also hover over the labels along the x-axis to drill into the performance of people whose means fall into those buckets.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_22_at_11_45_36_2x_9d9ce0d21d.png)\n\nAs you can see, roughly 25% of interviewees are consistent in their performance, and the rest are all over the place.[3](#user-content-fn-3) If you look at the graph above, despite the noise, you can probably make some guesses about which people you’d want to interview. **However, keep in mind that each represents a *mean*. Let’s pretend that, instead, you had to make a decision based on just one data point. That’s where things get dicey.** For instance:\n\n* Many people who scored at least one 4 also scored at least one 2.\n* If we look at high performers (mean of 3.3 or higher), we still see a fair amount of variation.\n* Things get really murky when we consider “average” performers (mean between 2.6 and 3.3).\n\n**To me, looking at this data and then pretending that I had to make a hiring decision based on one interview outcome felt a lot like peering into some beautiful, lavishly appointed parlor through a keyhole.** Sometimes you see a piece of art on the wall, sometimes you see the liquor selection, and sometimes you just see the back of the couch.\n\nAt this point you might say that it’s erroneous and naive to compare raw technical scores to one another for any number of reasons, not the least of which is that one interviewer’s 4 is another interviewer’s 2. We definitely share this concern and address it in the appendix of this post. It does bear mentioning, though, that most of our interviewers are coming from companies with strong engineering brands and that correcting for brand strength didn’t change interviewee performance volatility, nor did correcting for interviewer rating.\n\nSo, in a real life situation, when you’re trying to decide whether to advance someone to onsite, you’re probably trying to avoid two things — false positives (bringing in people below your bar by mistake) and false negatives (rejecting people who should have made it in). Most top companies’ interviewing paradigm is that false negatives are less bad than false positives. This makes sense right? With a big enough pipeline and enough resources, even with a high false negative rate, you’ll still get the people you want. With a high false positive rate, you might get cheaper hiring, but you do potentially irreversible damage to your product, culture, and future hiring standards in the process. And of course, the companies setting the hiring standards and practices for an entire industry ARE the ones with the big pipelines and seemingly inexhaustible resources.\n\nThe dark side of optimizing for high false negative rates, though, rears its head in the form of our current engineering hiring crisis. Do single interview instances, in their current incarnation, give enough signal? Or amidst so much demand for talent, are we turning away qualified people because we’re all looking at a large, volatile graph through a tiny keyhole?\n\nSo, hyperbolic moralizing aside, **given how volatile interview performance is, what are the odds that a good candidate will fail an individual phone screen?**\n\nOdds of failing a single interview based on past performance\n------------------------------------------------------------\n\nBelow, you can see the distribution of mean performance throughout our population of interviewees.\n\nIn order to figure out the probability that a candidate with a given mean score would fail an interview, we had to do some stats work. First, we broke interviewees up into cohorts based on their mean scores (rounded to the nearest 0.25). Then, for each cohort, we calculated the probability of failing, i.e. of getting a score of 2 or less. Finally, to work around our starting data set not being huge, we [resampled](https://www.burns-stat.com/documents/tutorials/the-statistical-bootstrap-and-other-resampling-methods-2/) our data. In our resampling procedure, we treated an interview outcome as a multinomial distribution, or in other words, pretended that each interview was a roll of a weighted, 4-sided die corresponding to that candidate’s cohort. We then re-rolled the dice a bunch of times to create a new, “simulated” dataset for each cohort and calculated new probabilities of failure for each cohort using these data sets. Below, you can see the results of repeating this process 10,000 times.\n\nAs you can see, a lot of the distributions above overlap with one another. This is important because these overlaps tell us that there may not be statistically significant differences between those groups (e.g. between 2.75 and 3). Certainly, with the advent of LOT more data, the delineations between cohorts may become clearer. On the other hand, if we do need a huge amount of data to detect differences in failure rate, it might suggest that people *are* intrinsically highly variable in their performance. **At the end of the day, while we can confidently say that there is a significant difference between the bottom end of the spectrum (2.25) versus the top end (3.75), for people in the middle, things are murky.**\n\nNevertheless, using these distributions, we did attempt to compute the probability that a candidate with a certain mean score would fail a single interview (see below — the shaded areas encapsulate a 95% confidence interval). The fact that people who are overall pretty strong (e.g. mean ~= 3) can mess up technical interviews as much as 22% of the time shows that there’s definitely room for improvement in the process, and this is further exacerbated by the general murkiness in the middle of the spectrum.\n\n\n\n---\n\nGenerally, when we think of interviewing, we think of something that ought to have repeatable results and carry a strong signal. However, the data we’ve collected, meager though it might be, tells a different story. And it resonates with both my anecdotal experience as a recruiter and with the sentiments we’ve seen echoed in the community. Zach Holman’s [Startup Interviewing is Fucked](https://zachholman.com/posts/startup-interviewing-is-fucked/) hits on the disconnect between interview process and the job it’s meant to fill, the fine gentlemen of TripleByte [reached similar conclusions](https://triplebyte.com/blog/who-y-combinator-companies-want) by looking at their own data, and one of the more poignant expressions of inconsistent interviewing results recently came from [rejected.us](https://rejected.us/).\n\nYou can bet that many people who are rejected after a phone screen by Company A but do better during a different phone screen and ultimately end up somewhere traditionally reputable are getting hit up by Company A’s recruiters 6 months later. And despite everyone’s best efforts, the murky, volatile, and ultimately stochastic circle jerk of a recruitment process marches on.\n\nSo yes, it’s certainly one possible conclusion is that technical interviewing itself is indeed fucked and doesn’t provide a reliable, deterministic signal for one interview instance. Algorithmic interviews are a hotly debated topic and one we’re deeply interested in teasing apart. One thing in particular we’re very excited about is tracking interview performance as a function of interview type, as we get more and more different interviewing types/approaches happening on the platform. Indeed, one of our long-term goals is to really dig into our data, look at the landscape of different interview styles, and make some serious data-driven statements about what types of technical interviews lead to the highest signal.\n\n**In the meantime, however, I am leaning toward the idea that drawing on aggregate performance is much more meaningful than a making such an important decision based on one single, arbitrary interview.** Not only can aggregative performance help correct for an uncharacteristically poor performance, but it can also weed out people who eventually do well in an interview by chance or those who, over time, submit to the beast and memorize *Cracking the Coding Interview*. I know it’s not always practical or possible to gather aggregate performance data in the wild, but at the very least, in cases where a candidate’s performance is borderline or where their performance differs wildly from what you’d expect, it might make sense to interview them one more time, perhaps focusing on slightly different material, before making the final decision.\n\nAppendix: The part where we tentatively justify using raw scores for comparative performance analysis\n-----------------------------------------------------------------------------------------------------\n\nFor the skeptical, inquiring minds among you who realize that using raw coding scores to evaluate an interviewee has some pretty obvious problems, we’ve included this section. The issue is that even though our interviewers tend to come from companies with high engineering bars, raw scores are still comprised of just one piece of feedback, they don’t adjust for interviewer strictness (e.g. one interviewer’s 4 could be another interviewer’s 2), and they don’t adjust well to changes in skill over time. Internally, we actually use a more complex and comprehensive rating system when determining skill, and if we can show that raw scores align with the ratings we calculate, then we don’t feel so bad about using raw scores comparatively.\n\nOur rating system works something like this:\n\n1. We create a single score for each interview based on a weighted average of each feedback item.\n2. For each interviewer, we pit all the interviewees they’ve interviewed against one another using this score.\n3. We use a Bayesian ranking system (a modified version of [Glicko-2](https://www.npmjs.com/package/glicko2)) to generate a rating for each interviewee based on the outcome of these competitions.\n\nAs a result, each person is only rated based on their score as it compares to other people who were interviewed by the same interviewer. That means one interviewer’s score is never directly compared to another’s, and so we can correct for the hairy issue of inconsistent interviewer strictness.\n\nSo, why am I bringing this up at all? You’re all smart people, and you can tell when someone is waving their hands around and pretending to do math. Before we did all this analysis, we wanted to make sure that we believed our own data. We’ve done a lot of work to build a ratings system we believe in, so we correlated that with raw coding scores to see how strong they are at determining actual skill.\n\nThese results are pretty strong. Not strong enough for us to rely on raw scores exclusively but strong enough to believe that raw scores are useful for determining approximate candidate strength.\n\n*Thanks to Andrew Marsh for co-authoring the appendix, to [Plotly](https://plotly.com/) for making a terrific graphing product, and to everyone who read drafts of this behemoth.*\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. While listening to interviews day in and day out, I came up with a drinking game. Every time someone thinks the answer is hash table, take a drink. And every time the answer actually is hash table, take two drinks.[4](#user-content-fn-4) [↩](#user-content-fnref-1)\n2. This is data as of January 2016, and there are only 299 interviews because not all interviews have enough feedback data and because we threw out everyone with less than 2 interviews. Moreover, one thing we don’t show in this graph is the passage of time, so you can [see people’s performance over time](https://chart-studio.plotly.com/~aline_interviewingio/858/interviewee-performance-over-time-299-interviews-w-67-interviewees/?share_key=JFfMkYzESYN6gMH2E7vlJC) — it’s kind of a hot mess. [↩](#user-content-fnref-2)\n3. We were curious to see if volatility varied at all with people’s mean scores. In other words, were weaker players more volatile than strong ones? The answer is no — when we ran a regression on standard deviation vs. mean, we couldn’t come up with any meaningful relationship (R-squared ~= 0.03), which means that people are all over the place regardless of how strong they are on average. [↩](#user-content-fnref-3)\n4. I almost died. [↩](#user-content-fnref-4)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/technical-interview-performance-is-kind-of-arbitrary-heres-the-data",
      "author": "",
      "user_id": ""
    },
    {
      "title": "The Eng Hiring Bar: What the hell is it?",
      "content": "Recursive Cactus has been working as a full-stack engineer at a well-known tech company for the past 5 years, but he’s now considering a career move.\n\nOver the past 6 months, Recursive Cactus (that’s his anonymous handle on [interviewing.io](https://interviewing.io/signup)) has been preparing himself to succeed in future interviews, dedicating as much as 20-30 hours/week plowing through LeetCode exercises, digesting algorithms textbooks, and of course, practicing interviews on our platform to benchmark his progress.\n\n**Recursive Cactus’s typical weekday schedule**\n\n| **Time** | **Activity** |\n| --- | --- |\n| 6:30am – 7:00am | Wake up |\n| 7:00am – 7:30am | Meditate |\n| 7:30am – 9:30am | Practice algorithmic questions |\n| 9:30am – 10:00am | Commute to work |\n| 10:00am – 6:30pm | Work |\n| 6:30pm – 7:00pm | Commute from work |\n| 7:00pm – 7:30pm | Hang out with wife |\n| 7:30pm – 8:00pm | Meditate |\n| 8:00pm – 10:00pm | Practice algorithmic questions |\n\n**Recursive Cactus’s typical weekend schedule**\n\n| **Time** | **Activity** |\n| --- | --- |\n| 8:00am – 10:00am | Practice algorithmic questions |\n| 10:00am – 12:00pm | Gym |\n| 12:00pm – 2:00pm | Free time |\n| 2:00pm – 4:00pm | Practice algorithmic questions |\n| 4:00pm – 7:00pm | Dinner with wife & friends |\n| 7:00pm – 9:00pm | Practice algorithmic questions |\n\nBut this dedication to interview prep has been taking an emotional toll on him, his friends, and his family. Study time crowds out personal time, to the point where he basically has no life beyond work and interview prep.\n\n“It keeps me up at night: what if I get zero offers? What if I spent all this time, and it was all for naught?”\n\nWe’ve all been through the job search, and many of us have found ourselves in a similar emotional state. But why is Recursive Cactus investing so much time, and what’s the source of this frustration?\n\nHe feels he can’t meet the engineer hiring bar (aka “The Bar”), that generally accepted minimum level of competency that all engineers must exhibit to get hired.\n\nTo meet “The Bar,” he’s chosen a specific tactic: to look like the engineer that people want, rather than just be the engineer that he is.\n\nIt seems silly to purposefully pretend to be someone you’re not. But if we want to understand why Recursive Cactus acts the way he does, it helps to know what “The Bar” actually is. And when you think a little more about it, you realize there’s not such a clear definition.\n\nDefining “The Bar”\n------------------\n\nWhat if we look at how the [FAANG companies (Facebook, Amazon, Apple, Netflix, Google)](https://interviewing.io/blog/how-know-ready-interview-faang) define “The Bar?” After all, it seems those companies receive the most attention from pretty much everybody, job seekers included.\n\nFew of them disclose specific details about their hiring process. Apple doesn’t publicly share any information. [Facebook](https://www.facebookcareers.com/facebook-life/how-we-hire/?page=1) describes the stages of their interview process, but not their assessment criteria. [Netflix](https://jobs.netflix.com/culture) and [Amazon](https://www.amazon.jobs/en/landing_pages/assessments) both say they hire candidates that fit their work culture/leadership principles. Neither Netflix nor Amazon describes exactly how they assess against their respective principles. However, Amazon does share how interviews get conducted as well as software [topics that could be discussed for software developer](https://www.amazon.jobs/en/landing_pages/software-development-topics) positions.\n\nThe most transparent of all FAANGs, [Google publicly discloses](https://careers.google.com/how-we-hire/interview/) its interview process with the most detail, with Laszlo Bock’s book [*Work Rules!*](https://www.amazon.com/Work-Rules-Insights-Inside-Transform/dp/1455554790) adding even more insider color about how their interview process came to be.\n\nAnd speaking of tech titans and the recent past, Aline (our founder) mentioned the 2003 book [*How Would You Move Mount Fuji?*](https://www.amazon.com/How-Would-Move-Mount-Fuji-ebook/dp/B000Q67H6I) in a prior [blog post,](https://interviewing.io/blog/you-cant-fix-diversity-in-tech-without-fixing-the-technical-interview) which recounted Microsoft’s interview process when they were the pre-eminent tech behemoth of the time.\n\nIn order to get a few more data points about how companies assess candidates, I also looked at Gayle Laakmann McDowell’s “Cracking the Coding Interview”, which is effectively the Bible of interviewing for prospective candidates, as well as Joel Spolsky’s [Guerilla Guide to Interviewing 3.0](https://www.joelonsoftware.com/2006/10/25/the-guerrilla-guide-to-interviewing-version-30/), written by an influential and well-known figure within tech circles over the past 20-30 years.\n\n### Definitions of “The Bar”\n\n| **Source** | **Assessment Criteria** |\n| --- | --- |\n| Apple | Not publicly shared |\n| [Amazon](https://www.amazon.jobs/en/landing_pages/assessments) | Assessed against [Amazon’s Leadership principles](https://interviewing.io/guides/amazon-leadership-principles) |\n| Facebook | Not publicly shared |\n| Netflix | Not publicly shared |\n| [Google](https://careers.google.com/how-we-hire/interview/#onsite-interviews) | 1. General cognitive ability 2. Leadership 3. “Googleyness” 4. Role-related knowledge |\n| [*Cracking the Coding Interview*](https://www.crackingthecodinginterview.com/) – Gayle Laakmann McDowell | – Analytical skills – Coding skills – Technical knowledge/computer science fundamentals – Experience – Culture fit |\n| [Joel Spolsky](https://www.joelonsoftware.com/2006/10/25/the-guerrilla-guide-to-interviewing-version-30/) | – Be smart – Get things done |\n| [Microsoft](https://www.amazon.com/How-Would-Move-Mount-Fuji-ebook/dp/B000Q67H6I) (circa 2003) | – “The goal of Microsoft’s interviews is to assess a general problem-solving ability rather than a specific competency.” – “Bandwidth, inventiveness, creative problem-solving ability, outside-the-box thinking” – “Hire for what people can do rather than what they’ve done” – Motivation |\n\nDefining “Intelligence”\n-----------------------\n\nIt’s not surprising that coding and technical knowledge would be part of any company’s software developer criteria. After all, that is the job.\n\nBut beyond that, the most common criteria shared among all these entities is a concept of intelligence. Though they use different words and define the terms slightly differently, all point to some notion of what psychologies call “cognitive ability.”\n\n| **Source** | **Definition of cognitive ability** |\n| --- | --- |\n| Google | *“General Cognitive Ability. Not surprisingly, we want smart people who can learn and adapt to new situations. Remember that this is about understanding how candidates have solved hard problems in real life and how they learn, not checking GPAs and SATs.”* |\n| Microsoft (circa 2003) | *“The goal of Microsoft’s interviews is to assess a general problem-solving ability rather than a specific competency… It is rarely dear what type of reasoning is required or what the precise limits of the problem are. The solver must nonetheless persist until it is possible to bring the analysis to a timely and successful conclusion.”* |\n| Joel Spolsky | *“For some reason most people seem to be born without the part of the brain that understands pointers.”* |\n| Gayle Laakmann McDowell | *“If you’re able to work through several hard problems (with some help, perhaps), you’re probably pretty good at developing optimal algorithms. You’re smart.”* |\n\nAll these definitions of intelligence resemble early 19th-century psychologist [Charles Spearman’s theory of intelligence](https://en.wikipedia.org/wiki/Two-factor_theory_of_intelligence), the most widely acknowledged framework for intelligence. After performing a series of cognitive tests on school children, Spearman found that those who did well in one type of test tended to also perform well in other tests. This insight led Spearman to theorize that there exists a single underlying general ability factor (called “g” or “g factor”) influencing all performance, separate from specific, task-specific abilities (named “s”).\n\nIf you believe in the existence of “g” (many do, some do not… there exist [different theories of intelligence](https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences)), finding candidates with high measures of “g” aligns neatly with the intelligence criteria companies look for.\n\nWhile criteria like leadership and culture fit matter to companies, “The Bar” is not usually defined in those terms. **“The Bar” is defined as having technical skills but also (and perhaps more so) having general intelligence.** After all, candidates aren’t typically coming to interviewing.io to specifically practice leadership and culture fit.\n\nThe question then becomes how you measure these two things. Measuring technical skills seems tough but doable, but how do you measure “g?”\n\nMeasuring general intelligence\n------------------------------\n\nMentioned in Bock’s book, Frank Schmidt’s and John Hunter’s 1998 paper “[The Validity and Utility of Selection Methods in Personnel Psychology](https://www.researchgate.net/publication/232564809_The_Validity_and_Utility_of_Selection_Methods_in_Personnel_Psychology)” attempted to answer this question by analyzing a diverse set of 19 candidate selection criteria to see which predicted future job performance the best. The authors concluded general mental ability (GMA) best predicted job performance based on a statistic called “predictive validity.”\n\n![Table showing candidate selection criteria and validity in predicting job performance](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F4_Table1_1_bd3ec75215.png&w=1920&q=75 \"Candidate selection criteria and validity in predicting job performance\")\n\nIn this study, a GMA test referred to an IQ test. But for Microsoft circa 2003, puzzle questions like “How many piano tuners are there in the world?” appear to have taken the place of IQ tests for measuring “g”. Their reasoning:\n\n> *“At Microsoft, and now at many other companies, it is believed that there are parallels between the reasoning used to solve puzzles and the thought processes involved in solving the real problems of innovation and a changing marketplace. Both the solver of a puzzle and a technical innovator must be able to identify essential elements in a situation that is initially ill-defined.”*\n>\n> – “How Would You Move Mount Fuji?” – page 20\n\nFast forward to today, Google [denounces this practice](https://rework.withgoogle.com/guides/hiring-use-structured-interviewing/steps/avoid-brainteasers/), concluding that “performance on these kinds of questions is at best a discrete skill that can be improved through practice, eliminating their utility for assessing candidates.”\n\nSo here we have two companies who test for general intelligence, but who also fundamentally disagree on how to measure it.\n\nAre we measuring specific or general intelligence?\n--------------------------------------------------\n\nBut maybe as Spolsky and McDowell have argued, the traditional algorithmic and computer science-based interview questions are themselves effective tests for general intelligence. Hunter & Schmidt’s study contains some data points that could support this line of reasoning. Among all single-criteria assessment tools, work sample tests possessed the highest predictive validity. Additionally, when observing the highest validity regression result of two-criteria assessment tool (GMA test plus work sample test), the standardized effect size on the work sample rating was larger than that of the GMA rating, suggesting a stronger relationship with future job performance.\n\nIf you believe algorithmic exercises function as work sample tests in interviews, then the study suggests traditional algorithm-based interviews could predict future job performance, maybe even more than a GMA/IQ test.\n\nRecursive Cactus doesn’t believe there’s a connection.\n\n> *There’s little overlap between the knowledge acquired on the job and knowledge about solving algorithmic questions. Most engineers rarely work with graph algorithms or dynamic programming. In application programming, lists and dictionary-like objects are the most common data structures. However, interview questions involving those are often seen as trivial, hence the focus on other categories of problems.*\n>\n> – Recursive Cactus\n\nIn his view, algorithms questions are similar to Microsoft’s puzzle questions: you learn how to get good at solving interview problems, which to him don’t ever show up in actual day-to-day work, which, if true, wouldn’t actually fit with the Schmidt & Hunter study.\n\nDespite Recursive Cactus’s personal beliefs, interviewers like Spolsky still believe these skills are vital to being a productive programmer.\n\n> *A lot of programmers that you might interview these days are apt to consider recursion, pointers, and even data structures to be a silly implementation detail which has been abstracted away by today’s many happy programming languages. “When was the last time you had to write a sorting algorithm?” they snicker.*\n>\n> *Still, I don’t really care. I want my ER doctor to understand anatomy, even if all she has to do is put the computerized defibrillator nodes on my chest and push the big red button, and I want programmers to know programming down to the CPU level, even if Ruby on Rails does read your mind and build a complete Web 2.0 social collaborative networking site for you with three clicks of the mouse.*\n>\n> – Joel Spolsky\n\nSpolsky seems to concede that traditional tech interview questions might not mimic actual work problems, and therefore wouldn’t act as work samples. Rather, it seems he’s testing for general computer science aptitude, which is general in a way, but specific in other ways. General intelligence within a specific domain, one might say.\n\nThat is, unless you believe intelligence in computer science *is* general intelligence. McDowell suggests this:\n\n> *There’s another reason why data structure and algorithm knowledge comes up: because it’s hard to ask problem-solving questions that don’t involve them. It turns out that the vast majority of problem-solving questions involve some of these basics.*\n>\n> – Gayle Laakmann McDowell\n\nThis could be true assuming you view the world primarily through computer science lenses. Still, it seems pretty restrictive to suggest people who don’t speak the language of computer science would have more difficulty solving problems.\n\nAt this point, we’re not really talking about measuring general intelligence as Spearman originally defined it. Rather, it seems we’re talking about specific intelligence, defined or propagated by those grown or involved in traditional computer science programs, and conflating that with general intelligence (Spolsky, McDowell, Microsoft’s Bill Gates, and 4 of 5 FAANG founders studied computer science at either some Ivy League university or Stanford).\n\nMaybe when we’re talking about “The Bar,” we’re really talking about something subjective, based on whoever is doing the measuring, and whose definition might not be consistent from person-to-person.\n\nLooking at candidate assessment behavior from interviewers on the interviewing.io platform, you can find some evidence that supports this hypothesis.\n\n“The Bar” is subjective\n-----------------------\n\nOn the interviewing.io platform, people can practice technical interviews online and anonymously, with interviewers from top companies on the other side. Interview questions on the platform tend to fall into the category of what you’d encounter at a phone screen for a back-end software engineering role, and interviewers typically come from companies like Google, Facebook, Dropbox, Airbnb, and more. Check out our [interview showcase](https://interviewing.io/mocks) to see how this all looks and to watch people get interviewed. After every interview, interviewers rate interviewees on a few different dimensions: technical skills, communication skills, and problem-solving skills. Each dimension gets rated on a scale of 1 to 4, where 1 is “poor” and 4 is “amazing!”. You can see what our feedback form looks like below:\n\n![Screenshot of the Interviewing.io interview feedback form](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F5a595_screenshot_2017_11_29_09_13_30_1ca42f5eec.png&w=750&q=75 \"Interviewing.io interview feedback form\")\n\nIf you do well in practice, you can bypass applying online/getting referrals/talking to recruiters and instead immediately book real technical interviews directly with our partner companies (more on that in a moment).\n\nWhen observing our most frequent practice interviewers, we noticed differences across interviewers in the percent of candidates that person would hire, which we call the passthrough rate. Passthrough rates ranged anywhere between 30% and 60%. At first glance, certain interviewers seemed to be a lot stricter than others.\n\n![Chart showing passthrough rates by interviewer](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F08ff4_blog_post_passthrough_rates_by_interviewer_3488f7303d.png&w=1080&q=75 \"Passhrough Rates by Interviewer\")\n\nBecause interviewees and interviewers are anonymized and matched randomly[^1], we wouldn’t expect the quality of candidates to vary much across interviewers, and as a result, wouldn’t expect interviewee quality to explain the difference. Yet even after accounting for candidate attributes like experience level, differences in passthrough rates persist.[^2]\n\nMaybe some interviewers choose to be strict on purpose because their bar for quality is higher. While it’s true that candidates who practiced with stricter interviewers tended to receive lower ratings, they also tended to perform better on their next practice.\n\nThis result could be interpreted in a couple of ways:\n\n* Stricter interviewers might systematically underrate candidates\n* Candidates get so beat up by strict interviewers that they tended to improve more between practices, striving to meet their original interviewer’s higher bar\n\nIf the latter were true, you would expect that candidates who practiced with stricter interviewers would perform better in real company interviews. However, we did not find a correlation between interviewer strictness and future company interview passthrough rate, based on real company interviews conducted on our platform.[^3]\n\n![Chart showing interviewer's passthrough rate vs. interviewee's future performance](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffb594_blog_post_practice_vs_company_passthrough_ea25d930f7.png&w=1080&q=75 \"Interviewer's Passthrough Rate vs. Interviewee's Future Performance\")\n\nInterviewers on our platform represent the kinds of people a candidate would encounter in a real company interview, since those same people also conduct phone screens and onsites at the tech companies you’re all applying to today. And because we don’t dictate how interviewers conduct their interviews, these graphs could be describing the distribution of opinions about your interview performance once you hang up the phone or leave the building.\n\nThis suggests that, independent of your actual performance, **whom you interview with could affect your chance of getting hired**. In other words, “The Bar” is subjective.\n\nThis variability across interviewers led us to reconsider our own internal definition of “The Bar,” which determined which candidates were allowed to interview with our partner companies. Our definition strongly resembled Spolsky’s binary criteria (“be smart”), heavily weighing an interviewer’s Would Hire opinion way more than our other 3 criteria, leading to the bimodal, camel-humped distribution below.\n\n![Chart showing Interviewing.io's Old Internal Score Distribution](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F09df4_blog_post_old_score_distribution_af49004df0.png&w=1080&q=75 \"Interviewing.io's Old Internal Score Distribution\")\n\nWhile our existing scoring system correlated decently with future interview performance, we found that an interviewer’s Would Hire rating wasn’t as strongly associated with future performance as our other criteria were. We lessened the weight on the Would Hire rating, which in turn improved our predictive accuracy.[^4] Just like in *“Talledega Nights”* when [Ricky Bobby learned there existed places other than first place and last place in a race](https://www.youtube.com/watch?v=20iio0wLpPA), we learned that it was more beneficial to think beyond the binary construct of “hire” vs. “not hire,” or if you prefer, “smart” vs. “not smart.”\n\nOf course, we didn’t get rid of all the subjectivity, since those other criteria were also chosen by the interviewer. And this is what makes assessment hard: an interviewer’s assessment is itself the measure of candidate ability.\n\nIf that measurement isn’t anchored to a standard definition (like we hope general intelligence would be), then the accuracy of any given measurement becomes less certain. It’s as if interviewers used measuring sticks of differing lengths, but all believed their own stick represented the same length, say 1 meter.\n\nWhen we talked to our interviewers to understand how they assessed candidates, it became even more believable that different people might be using measuring sticks of differing lengths. Here are some example methods of how interviewers rated candidates:\n\n* Ask 2 questions. Pass if answer both\n* Ask questions of varying difficulty (easy, medium, hard). Pass if answers a medium\n* Speed of execution matters a lot, pass if answers “fast” (“fast” not clearly defined)\n* Speed doesn’t matter much, pass if have a working solution\n* Candidates start with full points. When candidates make mistakes, start docking points\n\nHaving different assessment criteria isn’t necessarily a bad thing (and actually seems totally normal). It just introduces more variance to our measurements, meaning our candidates’ assessments might not be totally accurate.\n\nThe problem is, when people talk about “The Bar,” that uncertainty around measurement usually gets ignored.\n\nYou’ll commonly see people advising you only to hire the highest quality people.\n\n> *A good rule of thumb is to hire only people who are better than you. Do not compromise. Ever.*\n>\n> – Laszlo Bock\n\n> *Don’t lower your standards no matter how hard it seems to find those great candidates.*\n>\n> – Joel Spolsky\n\n> *In the Macintosh Division, we had a saying, “A player hire A players; B players hire C players”–meaning that great people hire great people.*\n>\n> – Guy Kawasaki\n\n> *Every person hired should be better than 50 percent of those currently in similar roles – that’s raising the bar.*\n>\n> – Amazon Bar Raiser blog post\n\nAll of this is good advice, assuming “quality” could be measured reliably, which as we’ve seen so far, isn’t necessarily the case.\n\nEven when uncertainty does get mentioned, that variance gets attributed to the candidate’s ability, rather than the measurement process or the person doing the measuring.\n\n> *[I]n the middle, you have a large number of “maybes” who seem like they might just be able to contribute something. The trick is telling the difference between the superstars and the maybes, because the secret is that you don’t want to hire any of the maybes. Ever.*\n>\n> …\n>\n> *If you’re having trouble deciding, there’s a very simple solution. NO HIRE. Just don’t hire people that you aren’t sure about.*\n>\n> – Joel Spolsky\n\nAssessing candidates isn’t a fully deterministic process, yet we talk about it like it is.\n\nWhy **“The Bar” is so high**\n----------------------------\n\n“Compromising on quality” isn’t really about compromise, it’s actually about decision-making in the face of uncertainty. And as you see from the quotes above, the conventional strategy is to only hire when certain.\n\nNo matter what kind of measuring stick you use, this leads to “The Bar” being set really high. Being really certain about a candidate means minimizing the possibility of making a bad hire (aka “false positives”). And companies will do whatever they can to avoid that.\n\n> *A bad candidate will cost a lot of money and effort and waste other people’s time fixing all their bugs. Firing someone you hired by mistake can take months and be nightmarishly difficult, especially if they decide to be litigious about it.*\n>\n> – Joel Spolsky\n\nHunter and Schmidt quantified the cost of a bad hire: “The standard deviation… has been found to be at minimum 40% of the mean salary,” which in today’s terms would translate to $40,000 assuming a mean engineer salary of $100,000/year.\n\nBut if you set “The Bar” too high, chances are you’ll also miss out on some good candidates (aka “false negatives”). McDowell explains why companies don’t really mind a lot of false negatives:\n\n> *“From the company’s perspective, it’s actually acceptable that some good candidates are rejected… They can accept that they miss out on some good people. They’d prefer not to, of course, as it raises their recruiting costs. It is an acceptable tradeoff, though, provided they can still hire enough good people.”*\n\nIn other words, it’s worth holding out for a better candidate if the difference in their expected output is large, relative to the recruiting costs from continued searching. Additionally, the costs of HR or legal issues downstream from potentially problematic employees also tilt the calculation toward keeping “The Bar” high.\n\nThis is a very rational cost-benefit calculation. But has anyone ever done this calculation before? If you have done it, we’d love to hear from you. Otherwise, it seems difficult to do.\n\nGiven that nearly everyone is using hand-wavy math, if we do the same, maybe we can convince ourselves that “The Bar” doesn’t have to be set quite so high.\n\nAs mentioned before, the distribution of candidate ability might not be so binary, so Spolsky’s nightmare bad hire scenario wouldn’t necessarily happen with all “bad” hires, meaning the expected difference in output between “good” and “bad” employees might be lower than perceived.\n\nRecruiting costs might be higher than perceived because finding and employing +1 standard deviation employees gets increasingly difficult. By definition, fewer of those people exist as your bar rises. Schmidt and Hunter’s “bad hire” calculation only compares candidates within an applicant pool. The study does not consider the relative cost of getting high-quality candidates into the applicant pool to begin with, which tends to be the more significant concern for many of today’s tech recruiting teams. And when you consider that other tech companies might be employing the same hiring strategy, competition would increase the average probability that offers get rejected, extending the time to fill a job opening.\n\nEstimating the expected cost of HR involvement is also difficult. No one wants to find themselves interacting with HR. But then again, not all HR teams are as useless as [Toby Flenderson](https://theoffice.fandom.com/wiki/Toby_Flenderson).\n\nTaken together, if the expected output between “good” and “bad” candidates were less than expected, and the recruiting costs were higher than perceived, it would make less sense to wait for a no-brainer hire, meaning “The Bar” might not have to be set so high.\n\nEven if one does hire an underperformer, companies could adopt the tools of training and employee management to mitigate the negative effects from some disappointing hires. After all, people can and do become more productive over time as they acquire new skills and knowledge.\n\nEmployee development seems to rarely get mentioned in conjunction with hiring (Laszlo Bock makes a few connections here and there, but the topics are mostly discussed separated). But when you add employee development into the equation above, you start to see the relationship between hiring employees and developing employees. You can think of it as different methods for acquiring more company output from different kinds of people: paying to train existing employees versus paying to recruit new employees.\n\nYou can even think of it as a trade-off. Instead of developing employees in-house, why not outsource that development? Let others figure out how to develop the raw talent, and later pay recruiters to find them when they get good. Why shop the produce aisle at Whole Foods and cook at home when you can just pay Caviar to deliver pad thai to your doorstep? Why spend time managing and mentoring others when you can spend that time doing “real work” (i.e. engineering tasks)?\n\nPerhaps “The Bar” is set high because companies don’t develop employees effectively, which puts more pressure on the hiring side of the company to yield productive employees.\n\nTherefore, companies can lower their risk by shifting the burden of career development onto the candidates themselves. In response, candidates like Recursive Cactus have little choice but to train themselves.\n\nInitially, I thought Recursive Cactus was a crazy outlier in terms of interview preparation. But apparently, he’s not alone.\n\nCandidates are training themselves\n----------------------------------\n\nLast year we surveyed our candidates about how many hours they spent preparing for interviews. Nearly half of the respondents reported spending 100 hours or more on interview preparation.[^5]\n\n![Chart showing responses to the survey question: how many hours spent preparing for interviews](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fb81df_blog_post_survey_hours_65812fae4a.png&w=1080&q=75 \"Survey: Hours Spent Preparing For Interviews\")\n\nWe wondered whether hiring managers and recruiters had the same expectations for candidates they encounter, Aline asked a similar question on Twitter, and results suggest they vastly underestimate the work and effort candidates endure prior to meeting with a company.\n\n> Are you a hiring manager or recruiter? If so, please vote on my poll! Results will be shared in an upcoming, titillating blog post.  \n>   \n> How many dedicated hours of interview prep do you believe candidates should invest in their job search?\n>\n> — Aline Lerner (@alinelernerLLC) [July 3, 2019](https://twitter.com/alinelernerLLC/status/1146468041212907520?ref_src=twsrc%5Etfw)\n\nDecision makers clearly underestimate the amount of work candidates put into job hunt preparation. The discrepancy seems to reinforce the underlying and unstated message pervading all these choices around how we hire: If you’re not one of the smart ones (whatever that means), it’s not our problem. You’re on your own.\n\n“The Bar” revisited\n-------------------\n\nSo this is what “The Bar” is. “The Bar” is a high standard set by companies in order to avoid false positives. It’s not clear whether companies have actually done the appropriate cost-benefit analysis when setting it, and it’s possible it can be explained by an aversion to investing in employee development.\n\n“The Bar” is in large part meant to measure your general intelligence, but the actual instruments of measurement don’t necessarily follow the academic literature that underlies it. You can even quibble about the academic literature.[^6] “The Bar” does measure specific intelligence in computer science, but that measurement might vary depending on who conducts your interview.\n\nDespite the variance that exists across many aspects of the hiring process, we talk about “The Bar” as if it were deterministic. This allows hiring managers to make clear binary choices but discourages them to think critically about whether their team’s definition of “The Bar” could be improved.\n\nAnd that helps us understand why Recursive Cactus spends so much time practicing. He’s training himself partially because his current company isn’t developing his skills. He’s preparing for the universe of possible questions and interviewers he might encounter because hiring criteria varies a lot, which cover topics that won’t necessarily be used in his day-to-day work, all so he can resemble someone that’s part of the “smart” crowd.\n\nThat’s the system he’s working within. Because the system is the way it is, it’s had significant impact on his personal life.\n\n> *My wife’s said on more than one occasion that she misses me. I’ve got a rich happy life, but I don’t feel I can be competitive unless I put everything else on hold for months. No single mom can be doing what I’m doing right now.*\n>\n> – Recursive Cactus\n\nThis impacts his current co-workers too, whom he cares about a lot.\n\n> *This process is sufficiently demanding that I’m no longer operating at 100% at work. I want to do the best job at work, but I don’t feel I can do the right thing for my future by practicing algorithms 4 hours a day *and* do my job well.*\n>\n> *I don’t feel comfortable being bad at my job. I like my teammates. I feel a sense of responsibility. I know I won’t get fired if I mail it in, but I know that it’s them that pick up the slack.*\n>\n> – Recursive Cactus\n\nIt’s helpful to remember that all the micro decisions made around false positives, interview structure, brain teasers, hiring criteria, and employee development add up to define a system that, at the end of the day, impacts people’s personal lives. Not just the lives of the job hunters themselves, but also all the people that surround them.\n\nHiring is nowhere near a solved problem. Even if we do solve it somehow, it’s not clear we would ever eliminate all that uncertainty. After all, projecting a person’s future work output after spending an hour or two with them in an artificial work setting seems kinda hard. While we should definitely try to minimize uncertainty, it might be helpful to accept it as a natural part of the process.\n\nThis system can be improved. Doing so requires not only coming up with new ideas, but also revisiting decades-old ideas and assumptions, and expanding upon that prior work rather than anchoring ourselves to it.\n\nWe’re confident that all of you people in the tech industry will help make tech hiring better. We know you can do it, because after all, you’re smart.",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/the-eng-hiring-bar-what-the-hell-is-it",
      "author": "",
      "user_id": ""
    },
    {
      "title": "How well do LeetCode ratings predict interview performance? Here's the data.",
      "content": "![Author avatar](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F15_2024_03_08_Mroczka_Headshots_Ray_Glaser_Photography_DSC_3702_81cf562c2c.jpg&w=384&q=75 \"Mike Mroczka\")\n\nMike Mroczka\n\nMike Mroczka, a former senior SWE (Google, Salesforce, GE), is the primary author of [Beyond Cracking the Coding Interview](https://www.beyondctci.com/)—the official sequel to Gayle McDowell's original CTCI. He works as a tech consultant and has a decade of experience helping engineers land their dream jobs. He’s a top-rated mentor (interviewing.io, Karat, Pathrise, Skilled.inc) and the author of viral technical content on system design and technical interview strategies featured on HackerNews, Business Insider, and Wired. He also sometimes writes technical content for interviewing.io (like this piece) and was one of the authors of interviewing.io’s [A Senior Engineer's Guide to the System Design Interview](https://interviewing.io/guides/system-design-interview).\n\nYou can find him online at [mikemroczka.com](https://www.mikemroczka.com/), [LinkedIn](https://www.linkedin.com/in/michael-mroczka/), and [X](https://x.com/mike_mroczka).\n\nHave you ever wondered if you should spend more time on LeetCode, participate in those contests, or focus on solving harder problems? A [popular Reddit post](https://www.reddit.com/r/leetcode/comments/1eg8060/you_probably_need_700_questions_to_actually_crack/) suggests you need 700+ questions and a LeetCode rating between 1800-2000 to pass FAANG interviews. Is this really what the data supports? To answer these questions and more, we looked at our users' LeetCode ranks and ratings and tied them back to interview performance on our platform and whether those users worked at FAANG.\n\nIn this post, we’ll share what we’ve learned.\n\nThe experiment\n--------------\n\ninterviewing.io is an interview practice platform and recruiting marketplace for engineers. Engineers use us for mock interviews. Companies use us to hire top performers. Hundreds of thousands of engineers have used our platform to prepare for interviews, and we have performance data for over 100k technical interviews (split between real interviews and mocks).\n\nWe surveyed almost 700 of our users and asked them to share their LeetCode and LinkedIn profiles. From those profiles, we pulled our users’ employment history, as well as their LeetCode data: number of problems worked, ratings, and, if they had it, contest performance. Finally, we cross-referenced all this data with their performance in mock and real interviews on interviewing.io.\n\nThis data set allowed us to start asking interesting questions. How much LeetCoding is useful? What kinds of problems are most useful to practice? How does LeetCode performance relate to performance in interviews with real people? Do LeetCode contest scores predict interview performance, and is a competitive coder likelier to work at a big tech company? Are hard questions worth solving or should we just stick with mediums? Let's find out!\n\nResults\n-------\n\nThe matrix below summarizes our findings. In addition to all LeetCode fields listed below, we also looked into global ratings and contest ratings. It was harder to find a large enough sample size to see any effect of those who did contests. Since the results were not large enough to be informative we have excluded them from the rest of this piece. All findings listed below are statistically significant, with lighter squares indicating stronger correlations.\n\nFor each profile, we looked at the following attributes:\n\n* **Total questions**: the total number of questions solved on LeetCode\n* **Hard questions**: the number of \"hard\" questions solved on LeetCode\n* **Medium questions**: the number of \"medium\" questions solved on LeetCode\n* **Easy questions**: the number of \"easy\" questions solved on LeetCode\n* **Worked at FAANG**: whether or not the user has ever worked at a FAANG company\n* **interviewing.io percentile**: how an interviewing.io user stacks up against other users of the platform, after having completed at least one interview\n\n![How much Leetcode activity correlates with interview performance and FAANG employment](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fleetcode_activity_relationship_to_interview_performance_and_faang_empoyment_b84cc797d6.png&w=1200&q=75)\n\nHow LeetCode attributes relate to having worked at FAANG and performance in mock and real interviews on interviewing.io. Correlations range from 1 (directly positively correlated) to -1 (directly negatively correlated). 0 means there’s no relationship.\n\nThis matrix summarizes the correlations between LeetCode attributes (y axis) with 1) whether people worked at a FAANG and 2) how well they performed in interviews on interviewing.io (x axis). The higher the number (and the darker the color), the stronger the relationship.\n\nFor instance, the number of questions a user solved correlates with working at a FAANG company and a user’s percentile ranking on interviewing.io. We see that it is a stronger predictor of interview performance than it is for working at a FAANG company (0.27 and 0.17, respectively).\n\nLet's dig into the most exciting findings!\n\n### The total number of questions you have completed matters!\n\n**Unsurprisingly, the total number of questions a user has completed correlates with having FAANG on their resume and doing well in technical interviews.** Those who work at FAANG companies appear to have completed more questions than those who do not.\n\n![FAANG engineers question volume compared to nonfaang engineers](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffaang_engineers_question_volume_compared_to_nonfaang_engineers_301c4b7405.png&w=1200&q=75)\n\nDon't fret, though, as this doesn't mean you're doomed to be behind people who have started sooner and have amassed thousands of questions. The vast majority of our top users stop LeetCoding after they've hit about 500 questions. Only a fraction of top users have done more than ~500 questions, and only about 10 did more than a thousand questions. **As you might expect, there are seriously diminishing returns associated with doing more than 500 questions. Those who continued past the 500 questions performed only marginally better than those who had stopped near 500.**\n\n*In the zero-sum interviewing game, those who complete the most LeetCode questions generally get the highest interview scores and work at the most desirable tech companies, but completing ~500 questions will put you among the top talent in the current market.*\n\n### The difficulty level you attempt matters, too!\n\nIt is common wisdom on the LeetCode discussion board and subreddits to \"do medium questions, not hard questions,\" with the argument that companies don't ask LeetCode hards. Does this advice stand up to scrutiny? **Mostly no.**\n\nThe data shows a clear bias towards those who solved more challenging problems, the exact point at which this matters is up for debate. At interviewing.io we find that before tackling difficult questions, you just need to first start doing problems. **Getting started matters more than attempting to complete a particular question difficulty, so don't focus arbitrarily on medium/hard questions, just focus on practicing**!\n\n**Tip 1: Start with anything. Getting started matters more than picking the perfect set of questions.** For those just starting, you might think, \"I'm a straight-A student and a fast learner, so I should be able to manage hard questions.\" Don't make this mistake, or you'll burn out quickly. Instead, just focus on doing questions and getting into the habit of translating your thoughts into code efficiently.\n\n![probability of faang question difficulty](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fprobability_of_faang_question_difficulty_a08c9d52a1.png&w=1200&q=75)\n\nThose who solved more challenging questions needed to do far fewer questions to have the same chance of getting into a FAANG company\n\n\n\n![interviewing io percentile vs total questions by difficulty](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Finterviewing_io_percentile_ee5c50d4f0.png&w=1200&q=75)\n\nThose who solved more challenging questions needed to do far fewer questions to have a high interviewing.io percentile\n\nBefore tackling mediums and hards, we need to first get good at easy questions. **Once we can tackle mediums, for each additional 50 questions you complete, you increase your score of passing your interviews by three percentage points.**\n\n**On the other hand, a LeetCode hard carries over *twice* the benefits of a medium question. Completing 50 hard questions increases your interview score by seven percentage points!** Our data shows that you'd need to complete close to 233 medium questions to get the same benefits that 100 hard questions would give. Consider these two people (fictitious but inspired by real examples in our data):\n\n| Question Difficulty | Alex | Kara |\n| --- | --- | --- |\n| Easy | 50 | 25 |\n| Medium | 630 | 50 |\n| Hard | 10 | 135 |\n| Total | 690 | 210 |\n\nThese two people are statistically likely to have similar interview scores, yet Kara completed half the number of questions Alex did. While it goes against standard advice, harder questions having a higher benefit make sense. A LeetCode medium tends to require you to do one thing (perform a DFS, scan an array, etc.) to get the correct answer. In contrast, hard questions require you to do multiple things (memoize results while performing a DFS, tally prefix sums while scanning an array, etc.). **The compounding nature of multiple tasks in harder questions necessitates deeper learning in fewer questions.**\n\nAdmittedly, there is a chicken-and-egg scenario going on here. Naively, one might think everyone should just do hard questions, but that doesn't work in practice. Jumping straight to hard questions is an infeasible leap for most people, and the reason many people can complete hard questions is precisely *because* they have finished many medium questions first. Our advice is not to jump to hard questions immediately. Instead, you should be mindful and not stay at the medium difficulty level longer than necessary. It isn't that the advice to \"do medium questions\" is wrong—it is just overused. It is natural to want to be able to complete a question regularly in a short period of time. Still, if you find yourself completing LeetCode mediums quickly, it indicates that you have stagnated and would be better off moving on to hards. If you have solved 100 or more medium questions, you'll likely get more bang for your buck by transitioning to hard.\n\n**Tip 2: Don't ignore hard questions.** Balance your medium questions with the hard ones. The compounding nature of multiple tasks in harder questions necessitates deeper learning in fewer questions.\n\nFinally, we must also acknowledge that not all hard questions are created equal. This tip is my gut feeling, and we don't have data to back it up, but intuitively, we've all done hard questions that seem *impossible*, hard questions that seemed like they should have been rated *medium*, and even medium questions that seem like they should have been *hard*. Question difficulty is somewhat subjective, so if you focus on hard questions, choose questions with a high acceptance rating and/or ones that are \"frequently asked.\" These questions tend to be *achievable* (they don't have a single impossible trick you need to just have known), *realistic* (they don't test nonsense algorithms no one knows like Manacher’s algorithm), and *solution-flexible* (there is often more than one valid approach).\n\n**Tip 3: When choosing hard questions, pick popular questions.** They are more likely to be *achievable*, *realistic*, and *solution-flexible*, which will likely result in more learning per question.\n\n### Contests and rankings don't matter\n\nSurprisingly, we found no correlation between LeetCode ratings and their interviewing percentile. Those who were great at contests also didn't appear more likely to have FAANG on their resume. Two confounding variables that might have caused these unexpected results are:\n\n1. **Small contest selection size:** Despite our best efforts, we received a relatively small group of candidates that attended any number of contests, let alone several. The data set may be too small to find a correlation.\n2. **Biased candidate selection:** We incentivized candidates with free interviews, so those who regularly complete contests (and therefore have a lot of practice and likely feel confident in their skills) might not be interested in the rewards and weren't studied in this experiment.\n\n**Tip 4: Don't worry about contests unless you enjoy them.** They provide a way to practice managing your time and keep yourself honest with how fast you are moving, but they don't contribute much to your success apart from containing more questions for you to attempt.\n\nConclusion\n----------\n\nIn the end, LeetCode questions are a reasonable proxy for predictors of interview performance. While ratings and contest scores don't seem to matter,[1](#user-content-fn-1) questions —and question difficulty—do. Focus on tackling increasingly difficult questions until you've hit that point of diminishing returns near ~500. Don't stagnate with medium questions; pick popular and frequently asked hard questions, too. And don't stress if your friend has a higher LeetCode score than you do; turns out it doesn't mean much!\n\nFootnotes:\n\nFootnotes\n---------\n\n1. A caveat here is that, strictly speaking, we're reporting correlations—not necessarily causal relationships. This leaves open the possibility that what we were measuring was not exactly a return to LeetCode but rather the effect of some third variable that increases both LeetCode activity and job prospects. For example, say MIT has a course that emphasizes coding puzzles, and MIT grads also get better jobs. In that case, what might look like a LeetCode benefit is actually an MIT benefit, and controlling for that would make the apparent benefit disappear. At the very least, though, these patterns are informative about the typical profile of successful engineers, which is likely a useful thing to emulate in general. Also, remember that our data still shows solving a healthy number of questions regardless of their difficulty can boost your chances of getting through difficult interviews. By definition, many people can pass interviews with far fewer problems completed, and many will pass after doing more problems. [↩](#user-content-fnref-1)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/how-well-do-leetcode-ratings-predict-interview-performance",
      "author": "",
      "user_id": ""
    },
    {
      "title": "interviewing.io is finally out of beta. Anonymous technical interview practice for all!",
      "content": "I started [interviewing.io](https://interviewing.io/) 5 years ago. After working as both an engineer and a recruiter, my frustration with how inefficient and unfair hiring had reached a boiling point. What made me especially angry was that despite mounting evidence that resumes are [poor predictors of aptitude](http://blog.alinelerner.com/resumes-suck-heres-the-data/%22%20target), employers were obsessed with where people had gone to school and worked previously. In my mind, any great engineer, regardless of how they look on paper, should have the opportunity to get their foot in the door wherever they choose.\n\nSo, we set out to build a better system. On interviewing.io, software engineers can book anonymous mock interviews with senior engineers from companies like Facebook, Google, and others, and if they do well in practice, get fast-tracked with top employers regardless of how they look on paper. Fast-tracking means that you bypass resume screens, scheduling emails, and recruiter calls, and go straight to the technical interview (which, by the way, is still anonymous [1](#user-content-fn-1)) at companies of your choice. **Because we use interview data, not resumes, our candidates end up getting hired consistently by companies like Facebook, Uber, Twitch, Lyft, Dropbox, and many others, and 40% of the hires we’ve made to date have been candidates from non-traditional backgrounds.** What’s nuts is that many of our candidates have literally been rejected based on their resumes by the same employer who later hired them when they came through interviewing.io. **One notable candidate was rejected *three* times from a top-tier public company based on his resume before he got hired at that same company through us.**\n\nOver the past 5 years, we’ve hosted over 50,000 technical interviews (both practice and real) on our platform. Our [YouTube channel](https://www.youtube.com/channel/UCNc-Wa_ZNBAGzFkYbAHw9eg), where you can watch other people interview, has gotten over 3.5M views, and, most importantly, we have helped thousands of engineers get great jobs.\n\n![Screenshot of a live and anonymous interview](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F7ff46_screenshot_2020_06_03_15_57_12_894fcecad3.webp&w=1920&q=75 \"A live and anonymous interview\")\n\nAll practice interviews are completely anonymous and include actionable, high-fidelity feedback\n\n**Despite that for our entire, multi-year existence, we’ve been in beta. Over the past year or so, this increasingly inaccurately named “beta” became kind of a smoke screen.** Our product was stable, we had plenty of interviewers, but sadly, we couldn’t serve many of the people who needed us most. Because we made money by charging companies for hires, despite a growing waitlist of >180,000 engineers, we could only serve the ones whom we had a shot at placing, i.e. engineers who 1) were located in a city where we had customers and 2) had 4 or more years of experience⁠—sadly, despite our best efforts, employers across the board were not willing to pay for junior hires.\n\nThen, COVID-19 happened and with it, a deluge of hiring slowdowns and freezes. In a matter of weeks, we found ourselves down from 7-figure revenue to literally nothing. Companies didn’t really want or need to pay for hiring anymore.\n\nBut these hiring slowdowns freezes weren’t just affecting us. In parallel, we saw a growing sea of layoffs, and we realized that, soon, more and more candidates would be vying for a shrinking number of jobs. **On top of that, because a disproportionate amount of layoffs targeted recruiters, an overworked in-house skeleton recruiting team would go back to relying on resumes and other old-school proxies, unintentionally marginalizing non-traditional candidates once again**. We also realized that many of the folks getting laid off would be here on visas, which meant that they’d have a paltry 60 days to find their next job or risk deportation.\n\nSo, we made a hard call. You may know that historically, we’ve offered *completely free* mock interviews. What you may not know, is that we pay our professional interviewers, as it’s the only way to ensure that we have seasoned, experienced engineers delivering a consistent, realistic, and high-quality experience. This is often our largest expense.\n\nSince we previously funded practice by charging companies for hires, we had to find another revenue stream to continue. In the face of either shutting down the company, unable to provide any practice at all, or to begin to charge for premium practice interviews, we made the choice to launch a paid tier. But, because charging for practice felt anathema to our mission, we knew we needed some ground rules in place. I started this company to fix hiring, after all, and that’s why the people who work at interviewing.io are here, too.\n\nAfter 50,000 interviews, our data shows that [where someone goes to school has no correspondence to their interview performance](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt). Despite that, we are all too aware that just because aptitude is uniformly distributed among populations, resources are not. We understand that paying for interviews will be prohibitive to many of the people who need help most, so our ground rules and goals for this pivot were as follows:\n\n* We’d ALWAYS have a free tier\n* We’d immediately start working on a fellowship for engineers from underrepresented backgrounds or in a visa crisis experiencing financial hardship\n* We’d find a way to let people defer their payments\n\nThere is an upside to our new model though. Now that we’re no longer strictly beholden to employers, we’re able to open up interviewing.io to engineers of all seniority levels and many more locations. And because our revenue isn’t coming from placements but directly from practice, we don’t have to constrain our users to a limited number of practice interviews.\n\n**So, as of today, interviewing.io is open to engineers of all experience levels in North America and the UK**. Engineers who sign up can book mock interviews 24 hours out, and top performers will still get fast-tracked for great jobs.[2](#user-content-fn-2)\n\n![Screenshot of the interview scheduler wizard - picking an interviewer type, the type of interview and a timeslot](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fedfb7_dashboard_1_71ade2a5c3.webp&w=1920&q=75 \"Scheduling an interview\")\n\nYou can book either free (peer practice with other users) or premium interviews with experienced interviewers from FAANG, as early as 24 hours out\n\nHere’s how it works:\n\n* **If you’re able to pay, you can now book unlimited interviews with professional interviewers from FAANG and other top companies.** We’re no longer constraining you to 2 or 3, and we have the interviewer supply to make this work. Interviews cost between $100 and $225.\n* **If you can’t pay, there’s a free tier where you can do peer practice with other users.**\n\nWhat about the goals and ground rules above? We’ve already made some headway against these. The free tier was there from day one. Moreover, a number of our professional interviewers have stepped up and volunteered their time to help the people who need it most prepare for free (if you’d like to volunteer your time to help engineers from underrepresented groups practice interviewing, please email [interviewers@interviewing.io](mailto:interviewers@interviewing.io) with the subject line ‘Volunteer’), and a formal fellowship is in the works. Lastly, we’re working on a deferred payment plan, where users who buy premium interviews will not have to pay us for them until they find their next job.\n\n![Screenshot of a zoom meeting with the whole team](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F73996_teamzoom_1aeec85ecf.webp&w=1920&q=75 \"The team\")\n\nCOVID-19 has changed a lot of things for our team (that’s us above doing the remote work thing). But not how much we want to fix hiring.\n\nLook, whether you got laid off, are a student who’s reeling from your internship being withdrawn, lost your visa, whether you’re fortunate enough to still be employed but worry about your job stability, or if you just want to help making hiring fairer, we hope you’ll [take us for a spin](https://interviewing.io/). Technical interviews are hard, and hiring is broken. And whether you’re new to interviewing or are just rusty after being off the market, and whether you can pay or not, we have your back.\n\nThe coming months are going to be hard. We know that in the current climate more people than ever are feeling helpless, and the world feels like it’s burning. And it might not even seem like technical interviews matter that much. But they do to us… because this is our way of creating a world where access to opportunity isn’t determined by who you are or where you come from but by what you can do.\n\nP.S. If you don’t need practice for yourself but you or your organization want to help others get awesome at technical interviews, check out our [new gifting feature](https://interviewing.io/gift-practice-interviews).\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. \"Unlike other hiring marketplaces, we will always be an anonymous platform. No public profiles, no sharing of candidate information, and no one but you gets access to your interview history. You get to decide after each interview if you want to unmask, one interview at a time.\" [↩](#user-content-fnref-1)\n2. If you were waitlisted previously and have been patiently waiting to get your invite, we’re issuing thousands of invites a day and we should get to you within a few days. [↩](#user-content-fnref-2)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/interviewing-io-is-out-of-beta-anonymous-technical-interview-practice-for-all",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Exactly what to say when recruiters ask you to name the first number… and other negotiation word-for-words",
      "content": "There are a lot of resources out there that talk about salary negotiation but many tend to skew a bit theoretical. In my experience, one of the hardest things about negotiating your salary is knowing what to say in tough, ambiguous situations with a power balance that’s not in your favor. What’s OK? What’s rude? What are the social norms? And so on.\n\nBefore I started [interviewing.io](https://interviewing.io/), I’ve worked as a software engineer, an in-house recruiter, and an agency recruiter, so I’ve literally been on all sides of the negotiating table. For the last few years, I’ve been guest-lecturing MIT’s 6.UAT, a class about technical communication for computer science majors. Every semester, negotiation is one of the most-requested topics from students. In this post, I’m sharing the content of that lecture, which is targeted toward students, but has served seasoned industry folks just as well. You’re never too young or too old to advocate for yourself.\n\nBtw, if you don’t like reading and prefer long, rambly diatribes in front of an unsightly glass wall, I covered most of this material (and other stuff) in a webinar I did with the fine people at Udacity (where I used to run hiring) a few months ago. So, pick your poison.\n\nWhy negotiate my salary at all, especially if I’m junior?\n---------------------------------------------------------\n\nIf you’re early in your career, you might say that negotiation isn’t worth the hassle — after all, junior roles have pretty narrow salary bands. There are a few reasons this view is short-sighted and wrong. First, though it’s pretty unlikely in the grand scheme of things, if you’re applying to a startup, there might come a magical day when your equity is worth something. This is especially true if you’re an early employee — with a good exit, a delta of a few tenths of a percent might end up being worth a down payment on a home in San Francisco.\n\nBut, let’s get real, your equity is likely worthless (except interviewing.io’s equity… that’s totes gonna be worth something), so let me give you a better, more immediate reason to learn to haggle early in your career, precisely because that’s when the stakes are low. Humans are frighteningly adaptable creatures. Scared of public speaking? Give 3 talks. The first one will be gut-wrenchingly horrific, the stuff of nightmares. Your voice will crack, you’ll mumble, and the whole time, you’ll want to vomit. The next one will be nerve-wracking. The last one will mostly be OK. And after that, you’ll be just fine. Same thing applies to approach anxiety, mathematical proofs, sex, and, you guessed it, salary negotiation!\n\nSo, make all the awkward, teeth-cringing mistakes now, while it doesn’t matter, and where failure will cost you $5K or $10K a year. Because the further along you get in your career, the bigger the upside will be… and the bigger the downside will be for not negotiating. Not only will the salary bands be wider for senior roles, but as you get more senior, more of your comp comes from equity, and equity has an even wider range for negotiating. Negotiating your stock well can make 6-figure differences and beyond (especially if you apply some of these same skills to negotiating with investors over term sheets, should you ever start your own company)… so learn these skills (and fail) while you’re young, because the older you get, the harder it’s going to be to start and the more high-stakes it’s going to be.\n\nSo, below, as promised, I’ll give you a few archetypal, stress-inducing situations and what to say, word-for-word in each one. But first, let me address the elephant in the room…\n\nWill my offer be rescinded if I try to negotiate my salary?\n-----------------------------------------------------------\n\nAs I mentioned earlier, this blog post is coming out of a lecture I give at MIT. Every semester, I start the negotiation portion of the lecture with the unshakeable refrain that no one will ever rescind your offer for negotiating. Last semester was different, though. I was just starting to feel my oats and get into my talk (the negotiation piece comes about halfway through) and smugly recited the bit about offers never being rescinded, followed by my usual caveat… “unless you act like a douche while negotiating.” Then, a hand shot up in the back of the room. Ah ha, I thought to myself, one of the non-believers. Ready to placate him, I called on the gentleman in the back.\n\n“My offer got rescinded for negotiation.”\n\nThe class broke out into uproarious laughter. I laughed too. It was kind of funny… but it was also unnerving, and I wanted to get to the bottom of it.\n\n“Were you a giant jerk when you negotiated?”\n\n“Nope.” Shit, OK, what else can I come up with…\n\n“Were you applying at a really small company with maybe one open role?” I asked, praying against hope that he’d say yes.\n\n“Yes.”\n\n“Thank god.”\n\nSo, there’s the one exception I’ve found so far to my blanket statement. After working with hundreds and hundreds of candidates back when I was still a recruiter, I had never heard or seen an offer get rescinded (and none of my candidates acted like douches while negotiating, thank god), until then. So, if you’re talking to a super small company with one role that closes as soon as they find someone, yes, then they might rescind the offer.\n\nBut, to be honest, and I’m not just saying this because I was wrong in front of hundreds of bloodthirsty undergrads, an early startup punishing a prospective employee for being entrepreneurial is a huge red flag to me.\n\nOK, so, now onto the bit where I tell you exactly what to say.[1](#user-content-fn-1)\n\nWhat to say when asked to name the first number\n-----------------------------------------------\n\nThere will come a time in every job search where a recruiter will ask you about your compensation expectations. This will likely happen very early in said search, maybe even during the first call you’ll ever have with the company.\n\nI think this is a heinous, illaudable practice fraught with value asymmetry. Companies know their salary ranges and roughly what you’re worth to them before they ever talk to you (barring phenomenal performance in interviews which kicks you into a different band). And they know what cost of living is in your area. So they already have all the info they need about you, while you have none about them or the role or even your market value. Sure, there are some extenuating circumstances where you are too expensive, e.g. you’re like an L6 at Google and are talking to an early stage startup that can only afford to pay you 100K a year in base, but honestly even in that situation, if the job is cool enough and if you have the savings, you might take it anyway.\n\nSo, basically, telling them something will only hurt you and never help you. So don’t do it. Now, here’s exactly what to say when asked to name the first number.\n\nAt this point, I don’t feel equipped to throw out a number because I’d like to find out more about the opportunity first – right now, I simply don’t have the data to be able to say something concrete. If you end up making me an offer, I would be more than happy to iterate on it if needed and figure out something that works. I also promise not to accept other offers until I have a chance to discuss them with you.\n\nTADA!\n\nWhat to say when you’re handed an exploding offer\n-------------------------------------------------\n\nExploding offers, in my book, are the last bastion of the incompetent. The idea goes something like this… if we give a candidate an aggressive deadline, they’ll have less of a chance to talk to other companies. Game theory for the insipid.\n\nHaving been on the other side of the table, I know just how arbitrary offer deadlines often are. Deadlines make sense when there is a limited number of positions and applicants all come in at the same time (e.g. internships). They do not make any sense in this market, where companies are perpetually hiring all the time — therefore it’s an entirely artificial construct. Joel Spolsky, the creator of Trello and Stack Overflow, had something particularly biting to say on the matter of exploding offers many years ago (the full post, [Exploding Offer Season](https://www.joelonsoftware.com/2008/11/26/exploding-offer-season/), is really good):\n\n*“Here’s what you’re thinking. You’re thinking, well, that’s a good company, not my first choice, but still a good offer, and I’d hate to lose this opportunity. And you don’t know for sure if your number one choice would even hire you. So you accept the offer at your second-choice company and never go to any other interviews. And now, you lost out. You’re going to spend several years of your life in some cold dark cubicle with a crazy boss who couldn’t program a twenty out of an ATM, while some recruiter somewhere gets a $1000 bonus because she was better at negotiating than you were.”*\n\nEven in the case of internships, offer deadlines need not be as aggressive as they often are, and I’m happy to report that many college career centers have taken stands against exploding offers. Nevertheless, if you’re not a student or if your school hasn’t outlawed this vile practice, here’s exactly what to say if it ever happens to you.\n\nI would very much appreciate having a bit more time. I’m very excited about Company X. At the same time, choosing where I work is extremely important to me. Of course, I will not drag things out, and I will continue to keep you in the loop, but I hope you can understand my desire to make as informed of a decision as possible. How about I make a decision by…?\n\nThe reverse used car salesman… or what to say to always get more\n----------------------------------------------------------------\n\nAt the end of the day, the best way to get more money is to have other offers. I know, I know, interviewing sucks and is a giant gauntlet-slog, but in many cases, having just one other offer (so, I don’t know, spending a few extra days of your time spread over a few weeks) can get you at least $10K extra. It’s a pretty rational, clear-cut argument for biting the slog-bullet and doing a few more interviews.\n\nOne anecdote I’ll share on the subject goes like this. A few years ago, a close friend of mine who’s notoriously bad at negotiation and hates it with a passion was interviewing at one of the big 4 companies. I was trying to talk him into getting out there just a little bit, for the love of god, and talk to at least one more company. I ended up introducing him to a mid-sized startup where he quickly got an onsite interview. Just mentioning that he had an onsite at this company to his recruiter from the bigco got him an extra $5K in his signing bonus.\n\nOffers are, of course, better than onsites, but in a pinch, even onsites will do… because every onsite increases your odds of not accepting the offer from the company you’re negotiating with. So, let’s say you do have some offers. Do you reveal the details?\n\nThe answer is that it depends. If the cash parts of the offers you have are worth more than the one you have in hand, then you can reveal the details. If they’re worth more in total but less in cash, it’s a bit dicier because equity at smaller companies is kind of worthless… you can still use it as leverage if you tell the story that that equity is worth more to YOU, but that’s going to take a bit more finesse, so if you’ve never negotiated before, you might want to hold off.\n\nIf the cash part of your equity is not worth more, it’s sufficient to say you have offers and when pressed, you can simply say that you’re not sharing the details (it’s ok not to share the details).\n\nBut whether you reveal details or not, here’s the basic formula for getting more. See why I call it the reverse used car salesman?\n\nI have the following onsites/offers, and I’m still interviewing at Company X and Company Y, but I’m really excited about this opportunity and will drop my other stuff and **SIGN TODAY** if…\n\nSo, “if” what? I propose listing 3 things you want, which will typically be:\n\n* Equity\n* Salary\n* Signing/relocation bonus\n\nThe reason I list 3 things above isn’t because I expect you’ll be able to get all 3, but this way, you’re giving the person you’re negotiating with some options. In my experience, you’ll likely get 2 out of the 3.\n\nSo, what amounts should you ask for when executing on the reverse used car salesman? It’s usually easier to get equity and bonuses than salary (signing bonus is a one-off rather than something that repeats every year). Therefore, it’s not crazy to ask for 1.5X-2X the equity and an extra 10-15% in salary. For the bonus portion, a lot depends on the size of the company, but if you’re talking to a company that’s beyond seed stage, you can safely ask for at least 20% of your base salary as a signing bonus.[2](#user-content-fn-2)\n\nWhat if the company says no to all or most of these and are a big enough brand to where you don’t have much of a leg to stand on? You can still get creative. One of our users told me about a sweet deal he came up with — he said he’d sign today if he got to choose the team he could join and had a specific team in mind.\n\nOther negotiation resources\n---------------------------\n\nAs I mentioned at the beginning of this post, there are plenty of blog posts and resources on the internets about negotiation, so I’ll just mention two of my favorites. The first is a riveting, [first-hand account of negotiation adventures](https://www.freecodecamp.org/news/how-not-to-bomb-your-offer-negotiation-c46bb9bc7dea) from one of my favorite writers in this space, Haseeb Qureshi. In his post, Haseeb talks about how he negotiated for a 250K (total package) offer with Airbnb and what he learned along the way. It’s one of the most honest and thoughtful accounts of the negotiation process I’ve ever read.\n\nThe second post I’ll recommend is a [seminal work in salary negotiation by Patrick McKenzie](https://www.kalzumeus.com/2012/01/23/salary-negotiation/) (patio11 on Hacker News, in case that’s more recognizable). I read it back when I was still an engineer, and it was one of those things that indelibly changed how I looked at the world. I still madly link anyone and everyone who asks me about negotiation to this piece of writing, and it’s still bookmarked in my browser.\n\nIf you’re an interviewing.io user and have a job offer or five that you’re weighing and want to know exactly what to say when negotiating in your own nuanced, unique situation, please email me, and I’ll whisper sweet, fiscal nothings in your ear like a modern-day Cyrano de Bergerac wooing the sweet mistress that is capitalism.[3](#user-content-fn-3)\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. If you’re interviewing at interviewing.io, USE THESE ON ME. IT'LL BE GREAT. And while you’re at it, [use these on me](https://blog.alinelerner.com/how-to-interview-your-interviewers/) as well. [↩](#user-content-fnref-1)\n2. Some of the larger tech companies offer huge signing bonuses to new grads (~100K-ish). Obviously this advice is not for that situation. [↩](#user-content-fnref-2)\n3. An increasing number of our customers pay us on subscription, so we don't get more money if you do.[4](#user-content-fn-4) And for the ones who don't, salary and recruiting fees typically come out of a different budget. [↩](#user-content-fnref-3)\n4. In the early days of interviewing.io, we tried to charge a flat per-hire fee in lieu of a percentage of salary, precisely for this reason -- we wanted to set ourselves up as an entirely impartial platform where lining up with our candidates' best interests was codified into our incentive structure. Companies were pretty weirded out by the flat fee, so we went back to doing percentages, but these days we're moving over as many of our customers to subscription as possible -- it's cheaper for them, better for candidates, and I won't lie that I like to see that recurring revenue. [↩](#user-content-fnref-4)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/negotiate-salary-recruiter",
      "author": "",
      "user_id": ""
    },
    {
      "title": "How many engineers have gotten laid off in 2022 and 2023 so far?",
      "content": "I recently ran a Twitter poll asking my followers to estimate how many engineers had been laid off from US-based startups and tech companies in 2022 and 2023 so far.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/image_9749decedb.png)\n\n### \n\nAs you can see, 75% of respondents thought that 50K or more engineers had been laid off, and over a third of respondents thought it was at least 100K. Only about a quarter of respondents guessed 10K or fewer.\n\nAs it turns out, most people were off by roughly an order of magnitude – the real number is likely somewhere between 7.5K and 10K… which means that the majority of people are operating on incorrect information when they make important career decisions. For instance, how might your answers change to questions like these if you knew that layoffs aren’t as pervasive as you thought?\n\n*How do I act at work? Should I even look for a job; it probably makes sense to hunker down and keep my head down, right?? If I’m in the middle of a job search, should I take the first offer? If I just got an offer, should I even try to negotiate… won’t they rescind it if I ask for more money?*\n\nWe’ll attack some of these questions, head on, in a future post, but in this post, we’ll just share our estimate and how we got there, and hopefully we’ll dispel some of the panic around eng layoffs and help engineers make decisions based on data rather than fear.\n\nTo get to our much lower estimate, we first looked at how many people total were laid off in 2022 and 2023 so far, independent of department. Then we did some analysis to figure out how many of those people were engineers.\n\nHow many total people got laid off in 2022 and 2023 (so far)?\n-------------------------------------------------------------\n\nTo figure out how many people got laid off, we looked at layoffs.fyi. At some point in the last 2.5 years, you’ve probably visited layoffs.fyi. It was launched by Roger Lee in February 2020, just when concerns that this COVID-19 thing might affect the economy went mainstream. The site does exactly what it sounds like – it tracks layoffs at tech companies. Every time a company conducts a public round of layoffs, it gets added to a growing list. Each entry includes a layoff count, and a small subset of entries include a link to a list (usually in a Google doc) of actual people who were let go, as well as some info about them (name, LinkedIn, geography, title, and so on).\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/Layoff_lists_on_layoffs_fyi_1_2ad6abbed9.png)\n\nThe meat of layoffs.fyi is a giant Airtable embedded into the site, which means you can filter the data, like so (I had to go city by city rather than just saying “United States” because many rows had the country mislabeled):\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/unnamed_f610896c24.png)\n\nOnce you filter on just the U.S., you get 163,296 layoffs total in 2022 and 50,263 layoffs total in 2023 so far, making for a grand total of 213,559.\n\nWe saw this estimate roughly corroborated in a [recent post by Crunchbase News](https://news.crunchbase.com/startups/tech-layoffs). They also used layoffs.fyi as a source but ended up with a more conservative estimate of 153,000.\n\n**So, TL;DR somewhere between 150K and 200K people got laid off total**.\n\nHow many of those people are engineers?\n---------------------------------------\n\nWe recently did an analysis, where we looked at a mix of layoffs.fyi and LinkedIn data to figure out [how much layoffs affected engineers vs. other departments](https://interviewing.io/blog/2022-layoffs-engineers-vs-other-departments). As it turned out, about 5% of total layoffs were engineers, as of the publication date of that post (October 2022)[1](#user-content-fn-1). I expect that this figure has held up over the past few months.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/of_total_layoffs_each_department_constituted_in_2022_68b8b4e6d6.png)\n\n**If we put all this info together, we’re looking at something like 7.5K-10K engineers who were affected by layoffs between the start of 2022 and now, which is a far cry from the 50K-100K+ that most people had guessed.**[2](#user-content-fn-2)\n\nWhile our hearts go out to all those affected, especially in cases when the layoff was a function of poor planning by the company rather than anything to do with the engineer’s performance, it’s important to put these numbers in perspective and temper the rising urge to panic with some cold, hard data.\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. If you’re curious, even though only 5% of total layoffs were engineers, about 10% of engineers were laid off. [↩](#user-content-fnref-1)\n2. A few high-profile companies with layoff lists are likely outliers: Meta and Coinbase. Both of those companies had engineers comprise more than 5% of layoffs, but in both cases, the lists were primarily composed of junior engineers. That doesn’t make it any less painful for the people involved, of course, but from what we saw, eng layoffs were not generally so skewed toward junior engineers. For a really useful analysis of Meta layoffs specifically, please see this [Github repo](https://github.com/cjporteo/meta-layoffs-analysis). [↩](#user-content-fnref-2)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/how-many-engineers-laid-off-in-2022-and-2023-so-far",
      "author": "",
      "user_id": ""
    },
    {
      "title": "We analyzed 100K technical interviews to see where the best performers work. Here are the results.",
      "content": "At interviewing.io, we’ve hosted over 100K technical interviews, split between mock interviews and real ones.\n\nAs it happens, we know where many of our users currently work – they tell us that when they sign up to avoid getting matched for mock interviews with people they work with.\n\nGiven that we have this data AND given that we know how well people do in their interviews, we thought it would be interesting to see which companies’ engineers are especially good at technical interviews. We looked at which companies have engineers with the best overall performance, as well as those that shine specifically on technical ability, problem solving ability, and communication skills.\n\nThe resulting top ten lists are below!\n\nThe setup\n---------\n\nOn interviewing.io, engineers can practice technical interviewing anonymously. If things go well, they skip right to the technical interview at real companies (which is also fully anonymous). We started interviewing.io because resumes suck and because we believe that anyone, regardless of how they look on paper, should have the opportunity to prove their mettle.\n\nWhen an interviewer and an interviewee match on our platform, they join a collaborative coding environment with voice, text chat, and a whiteboard, and jump right into a technical interview. After each interview, both parties leave feedback, and once they’ve both submitted, each one can see what the other person said and how they were rated.\n\nHere’s what the feedback form that interviewers fill out looks like:\n\n![Screenshot of the Interviewing.io feedback form](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FScreen_Shot_2022_03_30_at_5_06_30_PM_1511096ff8.png&w=750&q=75 \"Interviewing.io feedback form\")\n\nTo arrive at the top ten rankings in this post, we took post-interview feedback data from our users and grouped them by the company where they work.\n\nAbout 60% of our users work at FAANG and FAANG-adjacent companies (e.g. Dropbox, Lyft, Uber, Square, etc). The rest work at other large tech companies, startups of all stages and sizes, digital agencies. A small portion are students, some are entrepreneurs, some are career changers, and some are unemployed (as you’ll see in a spicy tidbit below). For context, our average (and median) active user has about 7 years of experience.\n\nFor this analysis, we only included companies where we had at least 50 employees on interviewing.io (which meant that several companies were notably missing because they were just shy of that mark, including Quora, Asana, Stripe, and a few others).\n\nA few words about statistical significance\n------------------------------------------\n\nIn all the sections below, we’ve listed the top 10 companies in each category. But how different is the 1st from the 10th, really? As it happens:\n\n1. Membership in the top 10 is indeed highly statistically significant. The companies that made it into our top ten lists do indeed have significantly better interviewees than the ones that did not.\n\n2. Most comparisons *within* the top 10 are largely insignificant. Though it varies a bit from list to list, in general, the top company is significantly different from the 8-10th ranked companies, except with the communication score, which is very noisy. In other words, a lot of the intermediate ranks could have gone either way.\n\nThat being said, here are the lists!\n\nBest overall performance\n------------------------\n\nWe looked to see which 10 companies had the highest % of people passing interviews on interviewing.io. The average across all of our users is 54%.\n\n![Chart showing companies whose engineers had the highest interview pass rates](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FCompanies_whose_engineers_had_the_highest_interview_pass_rates_1_64103d0dbb.png&w=2048&q=75 \"Companies whose engineers had the highest interview pass rates\")\n\nBest technical\n--------------\n\nWe also looked to see which 10 companies had the highest average technical scores in interviews on our platform. The average across all of our users is 2.85 out of 4.\n\n![Chart showing companies whose engineers had the best technical scores](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FCompanies_whose_engineers_had_the_best_technical_scores_1_102f54f16e.png&w=2048&q=75 \"Companies whose engineers had the best technical scores\")\n\nBest problem solving\n--------------------\n\nWe also looked to see which 10 companies had the highest average problem solving scores in interviews on our platform. The average across all of our users is 2.79 out of 4.\n\n![Chart showing companies whose engineers had the best problem solving scores](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FCompanies_whose_engineers_had_the_best_problem_solving_scores_1_711241b216.png&w=2048&q=75 \"Companies whose engineers had the best problem solving scores\")\n\nBest communication\n------------------\n\nFinally, we looked to see which 10 companies had the highest average communication scores in interviews on our platform. The average across all of our users is 3.22 out of 4. It was surprising to see how high “Unemployed” ranked (that is exactly what it sounds like – these are users who indicated they didn’t have a job when they joined interviewing.io).\n\n![Chart showing companies whose engineers had the best communication scores](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FCompanies_whose_engineers_had_the_best_communication_scores_1_03760e4995.png&w=2048&q=75 \"Company engineeers' average communication score\")\n\nLimitations and closing thoughts\n--------------------------------\n\nIt bears mentioning that there are some limitations to our data. As you saw at the beginning, where people work is self-reported, and though we’ve done enough spot checking over the years to be confident that, *for the most part*, our users are honest about where they work, it’s not a perfect system. People also don’t always update their employer when they switch jobs.\n\nThere’s also the issue of selection bias. Maybe we’re just getting people who really feel like they need practice and aren’t an indicative slice of engineers at that company. After having talked to our users for years and after seeing how they perform in real interviews later on, I’m not too sure that’s true, but hey, it’s totally possible.\n\nFinally, there’s the obvious quantity issue. As I mentioned earlier, we’re only including companies where at least 50 of their engineers practiced on interviewing.io. When we lowered that limit, the list started to look different… but we weren’t confident enough in those results (yet) to publish them.\n\nLimitations aside, what does this all mean? I’m tempted to speculate about why Dropbox dominated these rankings and what about their engineers is distinct from many of the other companies with great brands that our users hail from. Dropbox does also hire on interviewing.io, and from what we’ve seen, they have an extremely high bar – many users who’ve done well with other companies on our platform have failed their Dropbox interview. However, I will refrain from speculating beyond that, and I will also refrain from speculating about why certain companies do well on one score but not the others – looking from the outside, we simply don’t know enough about these companies’ inner workings or the nuances of the types of engineers they attract to come up with a credible hypothesis. However, I hope to hear from you, dear reader, if you have visibility into these things and can comment on them.\n\nYou might be wondering how this list of companies and scores resolves with interviewing.io’s ongoing refrain about how resumes and pedigree don’t tell you very much about whether someone is a good engineer. We’ve seen over and over that [where you go to school doesn’t matter](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt) (and in fact, interview performance among students from elite schools doesn’t meaningfully differ from that of students in state schools), but we have seen consistently that where you’ve worked in the past does have some bearing on how you do in interviews.\n\nSo, yes, where you work matters. The good news, though, is that it’s not the whole story. We’ve seen in the past that [people who take a bunch of Coursera and Udacity classes](https://interviewing.io/blog/lessons-from-3000-technical-interviews) on topics related to algorithms and data structures tend to perform better than people from top companies who have not. And we’ve seen that after 5 mock interviews on interviewing.io, regardless of where you started or where you work, your [chances of passing real interviews will (on average) double](https://interviewing.io/blog/how-know-ready-interview-faang).\n\nOf course, the really interesting question in all of this is the holy grail of technical recruiting: *Does performance in interviews reliably predict on-the-job performance?* While we diligently gather data on the subject, I’d love to hear from you. If you’ve hired engineers from some of the companies in this post, have they performed better than others? Are there any patterns or anti-patterns that you have noticed?",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/we-analyzed-100k-technical-interviews-to-see-where-the-best-performers-work-here-are-the-results",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Read nine chapters of Beyond Cracking the Coding Interview for free",
      "content": "Along with Gayle Laakmann McDowell, Mike Mroczka, and Nil Mamano, I wrote the official sequel to *Cracking the Coding Interview*. It's fittingly called [*Beyond Cracking the Coding Interview.*](https://www.amazon.com/dp/195570600X).\n\nNow, we're releasing [9 chapters of the book for free](https://bctci.co/free-chapters)! There are two PDFs in the linked folder:\n\n* **The first seven chapters of the book**, covering topics such as why technical interviews are broken, what recruiters won't tell you, why not to spend a lot of time on resumes, and how to get in the door at companies without a referral.\n* **Two technical chapters: Sliding Windows and Binary Search**. Our new take on Binary Search teaches one template that works for every binary search problem on LeetCode, with only a single-line change you need to remember. The Sliding Windows chapter features 6 unique sliding window templates that make off-by-one errors a thing of the past.\n\nTake a look, and let me know what you think. You can reach me at [aline@interviewing.io](mailto:aline@interviewing.io).",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/nine-free-chapters-of-beyond-cracking-the-coding-interview",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Are recruiters better than a coin flip at judging resumes? Here's the data.",
      "content": "*This post is a very exciting first for interviewing.io because it’s about a proper experiment run by a real, live academic research lab. If you’ve been reading my work for the past decade, you know that I’ve always been something of an armchair researcher. I ran some experiments before starting interviewing.io, and since then, my team and I have [kept it up](https://interviewing.io/blog/category/data-deep-dives).*\n\n*One of the experiments I ran before I founded interviewing.io was an attempt to figure out how good recruiters were at judging candidate quality based on resumes. I ran it 10 years ago and discovered that not only was everyone bad at judging resumes (about as accurate as flipping a coin), [they all disagreed with each other about what a good candidate looked like](https://blog.alinelerner.com/resumes-suck-heres-the-data/).*\n\n*Even though these results were shocking at the time, the study had some serious limitations. First, I had no objective measures for which candidates were actually good. I was working as a recruiter at the time, so I knew whom I had been able to place, but that’s obviously not the be-all and end-all of engineering ability. Second, I had a non-representative sample of software engineers. Due to my brand, I had managed to attract a lot of excellent, non-traditional candidates — engineers who were actually very good but didn’t look good on paper. These types of resumes are the hardest for recruiters to judge, and the data was full of them. Finally, my sample size wasn’t that big: I ended up with 716 data points in total, only about half of which came from recruiters (the rest came from engineers and hiring managers — my original hypothesis was that they might be better at the task, but I was wrong… everyone was bad at judging resumes).*\n\n*So, now that I’m CEO of interviewing.io, with access to a lot more data, resources, and a team of excellent academics at [Learning Collider](https://www.learningcollider.org/), we decided to run this study again, but with a more rigorous treatment and better conditions, to see if we could replicate the results. This time, we focused just on recruiters, given that they’re most often the gatekeepers who decide which candidates get an interview.*\n\n***Below are all the details, but here’s the TL;DR: we reproduced my results from 10 years ago! Our new study showed that recruiters were only a bit better than a coin flip at making value judgments, and they still all disagreed with each other about what a good candidate looks like.***\n\n*In this piece, we also talk about:*\n\n* *How far off recruiters were in their predictions and how much they disagreed with each other*\n* *What recruiters say they look for vs. what the data shows they actually look for*\n* *Why recruiters taking more time to parse resumes would lead to better outcomes (median parse time is just 31 seconds)*\n* *Whether AI can do a better job at judging resumes (spoiler: yes, it can)*\n\n*The rest of this piece is co-authored by Peter Bergman, Tushar Kundu, and Kadeem Noray of Learning Collider.*\n\n---\n\nThe setup\n---------\n\nIn the real world, resumes (or LinkedIn profiles) are evaluated by recruiters in minutes — even seconds — and these evaluations are THE thing that determines who gets an interview.\n\nBut what do these word walls tell recruiters? How predictive are their evaluations of actual interview success? Ultimately, how good are recruiters at judging resumes?\n\nTo answer these questions, we designed a study approximating technical recruiters’ decisions in the real world. We asked[1](#user-content-fn-1) 76 technical recruiters (both agency and in-house) to review and make judgments about 30 engineers’ resumes each, just as they would in their current roles.\n\nThey answered two questions per resume:\n\n* Would you interview this candidate?[2](#user-content-fn-2) (Yes or No)\n* What is the likelihood this candidate will pass the technical interview (as a percentage)?\n\nWe ended up with nearly 2,200 evaluations of over 1,000 resumes.\n\nThe resumes in this study belonged to interviewing.io users (with their consent) — actual engineers currently on the job market.\n\nCollaborating on this study with interviewing.io is an ideal scenario, precisely because outcome data were available for comparison purposes. Each engineer in this study has completed multiple mock interviews on the platform. Performance in these interviews is quite predictive of performance in real interviews: top performers (roughly the top 5% of users) on interviewing.io are 3X more likely to pass technical interviews at top-tier companies than candidates from other sources. Even passing a single interview on interviewing.io is a strong predictor of outcomes; it's associated with a 32% increase in the chance of working at a FAANG company post-interview.\n\nOnce we had recruiters’ evaluations of the resumes, we compared them to how those engineers actually performed on interviewing.io: skills scores, feedback from interviewers, and ultimately, whether they passed or failed their mock interviews.\n\nRecruiters’ resume judgments are just slightly better than a coin flip\n----------------------------------------------------------------------\n\n### Question #1: Would you interview this candidate?\n\nIn aggregate, recruiters in the study recommended 62% of candidates for an interview. But how did recruiter evaluations stack up against candidates’ performance on the platform?\n\n**We calculated recruiter accuracy by treating each candidate’s first interview (pass/fail) as the truth, and recruiters’ decision to interview as a prediction. It turns out that recruiters chose correctly 55% of the time, which is just slightly better than a coin flip.**\n\n### Question #2: What is the likelihood this candidate will pass the technical interview?\n\nRecruiters predicted the likelihood that each candidate would pass the technical interview. In most hiring processes, the technical interview follows the recruiter call and determines whether candidates proceed to the onsite. Being able to accurately predict which candidates will succeed at this stage is important and should inform the decision about whether to interview the candidate or not.\n\n**What we found most surprising is how far their predictions were from the truth:**\n\n*** When recruiters predicted the lowest probability of passing (0-5%), those candidates actually passed the technical interview with a 47% probability.*** **When recruiters predicted the highest probability of passing (95-100%), those candidates actually passed with a 64% probability.**\n\nBelow is a graph that shows recruiter predictions vs. actual performance. The x-axis is the bucketed recruiter rating. In other words, the first point is all the candidates that recruiters assigned a 0-5% likelihood of passing. The y-axis is the average interviewing.io pass rate for those candidates. The red dotted line represents 100% accuracy – in an ideal world, the higher a recruiter's ranking of a candidate, the higher their actual performance would be. The orange line represents reality – as you can see, there isn’t much correspondence between how recruiters predicted candidates would perform and their actual performance.\n\n![recruiter predictions vs interviewing.io performance](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fpredictions_0e401e8d92.png&w=1200&q=75)\n\nRecruiters’ predictions below 40% underestimate these candidates by an average of 23 percentage points. Above 60%, they’re overestimating by an average of 20 percentage points. **If this was predicting student performance, recruiters would be off by two full letter grades.**\n\nRecruiters can’t agree on what a good candidate looks like\n----------------------------------------------------------\n\nClearly, there is lots of noise in resume evaluations. Were recruiters’ noisy judgments at least consistent when reviewing the same resumes?\n\nNearly 500 resumes were evaluated by more than one recruiter. Based on a random selection of two evaluations per resume, the overall likelihood of two recruiters agreeing to either interview or not interview a given candidate was 64%.\n\nSince recruiters also guess the probability a candidate will pass the technical interview, we can compare how different these guesses are for a given candidate. **The average differential between two randomly selected recruiters’ evaluations of the same resume was 41 percentage points. So, let’s say one recruiter predicts a 30% probability the candidate would pass; another recruiter evaluating the same resume would predict, on average, a 71% probability of passing.**\n\nTo further understand just how prevalent the disagreement is, we looked at the standard deviations for across-candidate evaluations and same-candidate evaluations:\n\n* 0.34 across different candidates\n* 0.32 across the same candidates\n\n**So, when two recruiters are asked to judge the same candidate, their level of disagreement is nearly the same as if they evaluated two completely different candidates.**\n\nThe most sought-after resume attributes\n---------------------------------------\n\n**Despite the noise and variability in the study’s resume evaluations, there were some characteristics that recruiters consistently favored: experience at a top-tier tech[3](#user-content-fn-3) company (FAANG or FAANG-adjacent) and URM (underrepresented minority) status (in tech, this means being Black or Hispanic).**\n\nMost predictive for Question #1 (whether a recruiter would want to interview that candidate) was experience at a top company — these candidates were 35% more likely to be picked. Black or Hispanic candidates are also associated with an increased likelihood a recruiter would interview a candidate — by 21%.[4](#user-content-fn-4)\n\nWith Question #2 (how likely the candidate was to pass a technical interview), having a top company on your resume is associated with a 21% increase in the likelihood that recruiters believe the candidate will pass the interview. Compared to the actual pass rates, recruiters’ predictions of FAANG candidates are generally accurate (average 4 percentage point overestimate).[5](#user-content-fn-5) Unlike the presence of a top company, URM status didn't appear to influence recruiter decisions here.\n\nHow do recruiters’ stated reasons for rejecting candidates line up with actual rejection reasons?\n-------------------------------------------------------------------------------------------------\n\nSo, we know what recruiters tend to favor, whether they’d admit to it or not: 1) FAANG/FAANG-adjacent experience and 2) URM status. But what’s even more interesting than why a recruiter would say yes is why they would say no.\n\nWhen we asked recruiters to judge a resume, we also asked them WHY they made that decision.[6](#user-content-fn-6) Below are recruiters’ stated reasons for rejecting candidates. As you can see, “missing skill” is the main reason by far, with “no top firm” a distant third.\n\n![Bar chart of recruiter's stated reasons for rejection](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Frej_reasons_full_a70e7af161.png&w=1200&q=75)\n\nSo, then, we wondered… How do recruiters’ stated reasons for rejecting candidates line up with reality? To figure that out, we analyzed the resumes that ended up in the rejected pile and looked at common traits.\n\n**Below is a graph of actual rejection reasons, based on our analysis. The main rejection reason isn’t “missing skill” — it’s “no top firm.” This is followed, somewhat surprisingly, but much less reliably (note the huge error bars), by having an MBA. “No top school” and having a Master’s degree come in at third and fourth. Note that these top four rejection reasons are all based on a candidate’s background, NOT their skill set.**\n\n![Predictors of recruiter rejections](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Frej_regression_8776500012.png&w=1200&q=75)\n\nThe y-axis is the coefficient from regressing rejection on that variable. So, a coefficient of Y for a given trait means that trait is associated with a Y\\*100% percentage point increase in the likelihood of being rejected.\n\nSlowing down is associated with better decisions\n------------------------------------------------\n\nAnother key piece of this study is time. In hiring settings, recruiters make decisions quickly. Moving stacks of candidates through the funnel gives little room to second-guess or even wait before determining whether or not to give a candidate the opportunity to interview.\n\nIn our study, **the median time spent on resume evaluations was just 31 seconds**. Broken down further by Question #1 — whether or not the recruiter would interview them — the median time spent was:\n\n* 25 seconds for those advanced to a technical interview\n* 44 seconds for those placed in the reject pile\n\n![Distribution of time taken to evaluate candidates](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ftimetaken_yesno_b30ffa76f7.png&w=1200&q=75)\n\nGiven the weight placed on single variables (e.g., experience at a top firm), how quickly recruiters make judgments isn’t surprising. But might they be more accurate if they slowed down? **It turns out that spending more time on resume evaluations, notably >45 seconds, is associated with more accurate predictions — just spending 15 more seconds appears to increase accuracy by 34%.[7](#user-content-fn-7) It could be that encouraging recruiters to slow down might result in more accurate resume screening.**\n\n![Recruiter accuracy vs time taken](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Frecruiter_accuracytime_f308373dfa.png&w=1200&q=75)\n\nCan AI do better?\n-----------------\n\nAs a gaggle of technologists and data geeks, we tested whether algorithms could quiet the noise and inconsistencies in recruiters’ predictions.\n\nWe trained two local, off-the-rack machine-learning models.[8](#user-content-fn-8)\n\nJust like human recruiters, the models were trained to predict which candidates would pass technical interviews. The training dataset was drawn from interviewing.io and included anonymized resume data (years of experience, whether they had worked at a top firm, and whether they had attended a top 10 school for either grad or undergrad), candidates’ race and gender, and interview outcomes.[9](#user-content-fn-9)\n\n**Despite the very limited types of data we input into both models, when presented with out-of-sample candidate profiles, both models made predictions more accurately than human recruiters.**\n\nRandom Forest was somewhat more accurate than recruiters when predicting lower performing candidates. XGBoost, however, was more accurate across the board than both the Random Forest model AND recruiters.\n\n![predictions vs interviewing.io performance](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fprediction_comp_e3cf3757f5.png&w=1200&q=75)\n\nWhere does this leave us?\n-------------------------\n\nIn this section, when we say “we,” we are speaking as interviewing.io, not as the researchers involved in this study. Just FYI.\n\n### Advice for candidates\n\nAt interviewing.io, we routinely get requests from our users to add resume review to our list of offerings. So far, we have declined to build it. Why? Because we suspected that recruiters, regardless of what they say publicly, primarily hunt for name brands on your resume. Therefore, highlighting your skills or acquiring new skills is unlikely to make a big difference in your outcomes.\n\nWe are sad to see the numbers back up our intuition that it mostly is about brands.[10](#user-content-fn-10) As such, here’s an actionable piece of advice: maintain a healthy skepticism when recruiters advise you to grow your skill set. Acquiring new skills will very likely make you a better engineer. But it will very likely NOT increase your marketability.\n\nIf enhancing your skill set won’t help, what can you do to get in front of companies? We’re in the midst of a brutal market, the likes of which we haven’t seen since the dot-com crash in 2000. According to anecdotes shared in our Discord community, even engineering managers from FAANGs are getting something like a 10% response rate when they apply to companies online. If that’s true, what chance do the rest of us have?\n\nWe strongly encourage anyone looking for work in this market, especially if you come from a non-traditional background, to stop spending energy on applying online, full stop. Instead, reach out to hiring managers. The numbers will be on your side there, as relatively few candidates are targeting hiring managers directly. We plan to write a full blog post on how to do this kind of outreach well, but this CliffsNotes version will get you started:\n\n* Get a LinkedIn Sales Navigator account\n* Make a target list of hiring managers at the companies you’re interested in\n* Figure out their emails (you can use a tool like RocketReach), and send them something short and personalized. Do not use LinkedIn. The same way that you don’t live in LinkedIn, eng managers don’t either. Talk about the most impressive thing you’ve built. Ask them about their work, if you can find a blog post they’ve written or a project they’ve worked on publicly. Tie those two things together, and you’ll see a much higher response rate. Writing these personalized emails takes time, of course, but in this market, it’s what you need to do to stand out.\n\n### Advice for recruiters\n\nWe know that recruiting is a tough job, especially in the current climate, where there are more applicants than ever and fewer recruiters to parse through them. So, it rationally makes sense to us that a recruiter would spend no more than 30 seconds per resume and focus primarily on looking for top brands.\n\nWe hope, though, that this piece may have given a measure of pause about your approach, and we’d like to leave you with two actionable pieces of advice. First, if you do nothing else, please slow down. As you saw above, taking just 15 extra seconds to read a resume could improve your accuracy by 34%.[11](#user-content-fn-11)\n\nOur second piece of advice is this. Freada Kapor Klein from Kapor Capital coined the term “distance traveled” more than two decades ago. It refers to what someone accomplished, in the context of where they started. For instance, Kapor Klein recommends that, in their admissions processes, universities should consider not just the number of AP tests a candidate has passed but the number of AP tests divided by the total number offered at their high school. For example, if an applicant took 5 AP tests and their school offered 27, that paints a very different picture from another applicant who also took 5 AP tests when that’s the total number offered at their school. Kapor Capital uses distance traveled as one of their metrics for determining which entrepreneurs to fund. One can easily apply this concept to hiring as well.\n\nTake a look at the resume below. \"John\" (name has been changed; scrubbed resume shared with permission) studied chemical engineering and worked his way into software engineering by starting as a service engineer focused on pen testing. In the meantime, he completed a bootcamp, attended the Bradfield School of Computer Science (a school dedicated to teaching computer science at a depth beyond what many university programs, and certainly most bootcamps, offer), and ended up with a senior title in just three years.\n\nJohn was consistently rated poorly by recruiters but is one of the top performers on interviewing.io.\n\n![anonymized resume](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fanonymized_resume_55980a6854.png&w=1200&q=75)\n\nIt takes just a bit more time, so please spend a little longer reading resumes, and evaluate candidates’ achievements in the context of where they came from. Think about the denominator. But don’t think for a moment that we recommend that you lower the bar — absolutely not. On interviewing.io, we regularly see candidates like John objectively outperforming their FAANG counterparts.\n\n### What this means for our industry\n\nThe last time I did this research, I wrote about how being bad at judging resumes isn’t anything to be ashamed about and that comes down to the resume itself being a low-signal and not-very-useful document.\n\nI held that same opinion for the last decade (and even wrote a [recent post about how AI can’t do recruiting](https://interviewing.io/blog/why-ai-cant-do-hiring))… right up until we ran this study and successfully built two ML models that outperformed recruiters.\n\nSo, I stand corrected.\n\nAs you saw above, both models were limited – they were looking at the same types of features that recruiters do when they quickly scan a resume, certainly fewer features than recruiters have access to. But, despite that, the AI models still outperformed humans. What happens then, if you can build a model that behaves like a recruiter who really slows down and reads everything? These results make me believe that resumes do carry some signal, and you can uncover it if you carefully read what people write about their jobs and themselves and also analyze how they write it. Unfortunately, this takes more time and effort to uncover than most human recruiters are able to devote. And, in retrospect, that’s a good task for AI. Though we haven’t built a model like that for this post, I’m optimistic that we may be able to do it in the future.\n\nAs I said in the AI piece I linked above, in order for AI to do useful recruiting work, rather than just perpetuating the biases that human recruiters hold, it needs a data set that contains some objective measure of performance. Most recruiting AI models today do one of three things: glorified keyword matching, training on what recruiters prefer (the outcome is whether a recruiter would want to talk to the candidate, NOT whether the candidate is good), or live on top of existing tools like ChatGPT (which [we recently showed doesn’t perform very well and is biased against non-traditional candidates](https://interviewing.io/blog/refuting-bloombergs-analysis-chatgpt-isnt-racist)). These three approaches just result in the wrong thing being done, faster.\n\nI hope that, in the not too distant future, we can use AI to make less-biased decisions, using meaningful performance data. And I hope that this type of AI solution can get adoption among the recruiting community.\n\nFootnotes:\n\nFootnotes\n---------\n\n1. Participating technical recruiters were paid a base rate and then received additional $$ for each accurate prediction. [↩](#user-content-fnref-1)\n2. Different roles have different requirements. To correct for that, we asked each candidate to specify which eng role they were applying for: Software Engineer (back-end or full-stack), Mobile Engineer, Front-end Engineer, ML Engineer, Data Engineer, or Engineering Manager. Then we prompted recruiters to evaluate them specifically for that role. If no role was specified by the candidate, the default role to evaluate for was Software Engineer (back-end or full-stack). [↩](#user-content-fnref-2)\n3. Top firms = Airbnb, Amazon, Anthropic, AWS, Apple, Asana, Atlassian, Bloomberg LP, Checkr, Coinbase, Coursera, Cruise, Dropbox, Etsy, Facebook, Flexport, GitHub, Google, Gusto, HashiCorp, Instacart, Instagram, Jane Street, Jump Trading, Khan Academy, LinkedIn, Lyft, Medium, Microsoft, Mozilla, Netflix, Oculus, OpenAI, Palantir, Peloton, Pinterest, Postmates, Quora, Reddit, Robinhood, Roblox, Salesforce, Segment, Slack, Snap, Snowflake, SpaceX, Spotify, Square, Stripe, Tesla, Thumbtack, TikTok, Twilio, Twitch, Twitter, Two Sigma, Uber, Udemy, Waymo, Whatsapp, Yelp, and Zoom. [↩](#user-content-fnref-3)\n4. We corrected by FAANG & FAANG-adjacent experience (and all of our other variables) before making this statement, i.e., the effect existed for engineers from underrepresented backgrounds who did not have FAANG/FAANG-adjacent companies on their resumes. We expect that recruiters favor underrepresented minority candidates because of guidelines from their employers to focus on sourcing these types of candidates, as part of DEI initiatives. Discussion about the magnitude of this effect and its implications is out of scope of this piece. [↩](#user-content-fnref-4)\n5. Interestingly, recruiters might penalize, for example, alternative education. Candidates with only alternative education pathways post-high school — coding bootcamps or digital certifications — appeared to be penalized by recruiters in this study. However, with limited observations (n=11), it’s inconclusive without further study. [↩](#user-content-fnref-5)\n6. That field was optional, so most of the reasons recruiters provided were in cases when they said no — presumably because the reasons for saying yes may have seemed self-evident. [↩](#user-content-fnref-6)\n7. It’s not that recruiters who generally take their time make more accurate judgements. Any recruiter slowing down might make them better at judging resumes! [↩](#user-content-fnref-7)\n8. It’s important to stress that neither algorithm was custom-built. The models, one using a Random Forest algorithm and the other an XGBoost algorithm, are distinct but interrelated approaches akin to [Decision Tree algorithms](https://medium.com/@brandon93.w/decision-tree-random-forest-and-xgboost-an-exploration-into-the-heart-of-machine-learning-90dc212f4948). Decision trees sort data into groups based on features. Random forest algorithms combine multiple decision trees to improve predictions. XGBoost builds multiple decision trees one after another, with each new tree focusing on prediction errors from the previous trees. [↩](#user-content-fnref-8)\n9. Training data excluded data in this study. We take user privacy very seriously, and we want to stress that all models were local and anonymized and that no data in this study was shared with cloud LLMs. [↩](#user-content-fnref-9)\n10. To see a particularly egregious example of recruiters favoring brands over substance, take a close look at [this fake resume that got a bunch of recruiter responses](https://www.reddit.com/r/recruitinghell/comments/qhg5jo/this_resume_got_me_an_interview/). [And this one too](https://twitter.com/JerryJHLee/status/1778484920593055763). [↩](#user-content-fnref-10)\n11. We haven’t proven causality here, but when we just scoped our analysis to the same person, it appeared that taking more time did help (in other words, it’s not just that recruiters who spend more time usually are more accurate; it’s the added time). Still, this is something that merits more work, and we'll try to investigate it causally in the future. [↩](#user-content-fnref-11)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/are-recruiters-better-than-a-coin-flip-at-judging-resumes",
      "author": "",
      "user_id": ""
    },
    {
      "title": "User Story: Jamie White, MIT alum and engineering manager at Roblox",
      "content": "*Hey, Aline (founder of interviewing.io) here. This is the inaugural post in our new User Stories series.*\n\n*One of the biggest misconceptions about technical interview practice is that it's just for students. However, our data tells a different story. Experienced engineers are 25% less likely than new grads to pass their first mock interviews, according to our data. Moreover, in the current market, where the bar has consistently gone up since the downturn, practice matters more than ever. You don't have to practice with us, but you should practice... and don't forget system design — a lackluster performance in those interviews is one of the main reason people get down-leveled.*\n\n*My heartfelt thanks to my friend Jamie White for sharing his story (including two replays of failed mock interviews) and for coming to us for his prep in the first place. If you'd like to share your story on our blog, we’d love to hear from you. Please email me at [aline@interviewing.io](mailto:aline@interviewing.io).*\n\n![Author avatar](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fjamie_headshot_a23fd91b6d.png&w=384&q=75 \"Jamie White\")\n\nJamie White\n\nJamie is a two-time founder and perennial engineer. His work has spanned medical devices (Common Sensing + MIT), photon detectors (MIT), and labor unions (Unit of Work). Most recently, he’s been working on creativity tools at Roblox, where he’s an engineering manager. Jamie likes tech but loves humans, spending weekends playing video games with friends and organizing queer community events. You can find him on [Twitter](https://twitter.com/jamieearlwhite) and [LinkedIn](https://www.linkedin.com/in/jamieearlwhite/).\n\nRight out of grad school, my friend and I started a company. I originally said I'd be there for a year or two and then go get my PhD. Seven years later, I left that company to start another. After three years, I shut the second company down and was deciding what to do next. The largest team I'd ever had at that point was around 30 people (including part-time + contractors). I was ready for a new experience–a larger team, more financial security, the chance to dive deeper into technical topics. I decided I wanted to be a principal software engineer, and not for a 30-person startup. In the search for this experience, I found the Big Tech Interview.\n\nI had interviewed many people by this point, but none of the companies I ran had emulated the Big Tech Interview. I was completely new to it. The first super-important thing I learned is **just because you're qualified for the job, doesn't mean you can pass the Big Tech Interview**. I'm really smart. I got a 4.9 GPA at MIT. By this time I had worked with a pretty wide range of different software languages, tools, frameworks, ideologies, etc. I had managed 30+ people and could also plan and build complex systems solo when needed. I felt very qualified for the positions I was applying for. On my first Big Tech Interview, I did terribly.\n\nLuckily, it wasn't high on my list. I took advice from a brilliant friend and applied to my least favorite job first. The interview began. I'm a great communicator, breaking the ice and getting started was easy. I asked a few questions and got started, but I was already doomed. I hadn't written code in plain-text in ages. I missed some basic questions you should always ask. I didn't properly consider my edge cases. I forgot what a heap sort was, and there wasn't enough time to Google. My timing was off, and I wasn't sure how much I should talk vs code vs test. Didn't get the job.\n\nAn internal recruiter at a separate company had faith in me and gave me some great advice: \"Our interviews are really hard; you should go online and practice before you move on to that stage - I'll wait\". This is when it clicked. **The Big Tech Interview has a meta: video game lingo for a strategy that is agreed on by the community to be most effective**.\n\n**Not only do Big Tech Interviews have a meta, but because in this hiring game you are competing against other candidates, not learning that meta puts you at a huge disadvantage, even if you're a great match for the job**. I called around my friends who had been hired by Big Tech companies or hired for them, asking how to play the game most effectively. One particular friend blew my mind with how much she knew about Big Tech hiring, including the interview meta: Aline Lerner. Aline founded a company–interviewing.io–to help people like me. Because I'm such an awesome friend, I got started for free, although I ultimately also paid instead of begging Aline for more freebies. It was that good.\n\nAline led me to the two key features at interviewing.io that would ultimately prevent me from bombing again: practice interviews and recorded interviews. Practice interviews pair you with someone who actually interviews for a Big Tech company at the level you're shooting for. You enter a blind, simulated remote interviewing environment. You can set the collaborative editor to plain-text or one of many programming languages. A diagramming whiteboard mimics similar tools that companies provide during their interviews. After a brief introduction, the interviewer asks if there is anything particular you want to focus on, and then for the next hour you are in a very realistic-feeling Big Tech Interview. I found these interviews to be slightly harder than the real-life ones, a great direction to err. When time is up, your interviewer transitions to giving immediate feedback and tips, all of which are added to an overall dashboard for the interview, along with a single rating from 1-4.\n\nHere are a few of my early failed interviews. You can watch them if you want, too. The first one is algorithmic, and the second interview is system design. Huge thank you to Jamie’s excellent interviewers in these interviews, Continuous Hedgehog (algorithmic) and Indelible Hawk (system design), for their willingness to share these as well.\n\nAlgorithmic interview\n---------------------\n\n![Feedback of Jamie White's mock algorithmic interview](https://strapi-iio.s3.us-west-2.amazonaws.com/jamie_failed_algo_928f505a54.png)\n\nSystem design interview\n-----------------------\n\n![Feedback of Jamie White's mock system design interview](https://strapi-iio.s3.us-west-2.amazonaws.com/jamie_failed_sys_design_87e6a9d20a.png)\n\nOnce finished, you can revisit that interview any time you want through the interview recording. The recording not only includes synced-up audio, code, and diagramming but also includes a secret notes panel where your interviewer is jotting down their thoughts in real-time.\n\n![Interviewing.io interviewer notes panel real-time](https://strapi-iio.s3.us-west-2.amazonaws.com/user_story_jamie_white_comments_final_9f47e3c642.gif)\n\nWhen an interview is complete, you have the option to opt-in and publish your anonymous interview for all interviewing.io users to see. After Aline showed me this feature, I was hooked. I would pull up [interviewing.io’s replay showcase](https://interviewing.io/mocks) and watch an interview over lunch, listen to one in the background while I worked out, or deep-dive into one, pausing to attempt answering myself first.\n\nOver about 10 live and 20 recorded interviews, I could see my ratings progressing upward. Weak spots became apparent, and I focused on them. I was grokking the meta, and I reached out to the recruiter to go forward with the tech interview. I did one more live practice session the day before the real interview I was warned about.\n\nI aced it. I still had nerves; I don’t think you can necessarily rid yourself of those. And the problems were novel to me. I hadn’t encountered them specifically in my practice. But thanks to my mock interviews, I was able to **quickly understand and solve these coding/systems problems, which meant my interviewer and I could move on to more advanced topics and talk about my work instead of traversing binary trees**. This also gave me time to learn more about the company and interviewer and have some fun, albeit nerve-wracking fun.\n\nUltimately, I got amazing offers, including from the recruiter who gave me that warning. All noted that my interview performance was high. If I had walked in and performed like my first ill-advised interview above, I would have had to settle lower down my list.\n\nIf you've been reading, you won't be surprised at the takeaway here. No matter how good at the job you are, you need to practice for the Big Tech Interview meta. interviewing.io was instrumental in getting me there, and I recommend them to everyone now. You'll be giving your job ~2000 hours of your time every year; it's totally worth the time and money to make sure you end up where you want.",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/roblox-eng-manager-practices-on-interviewingio",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Want to work from home? That’ll be $46,282.",
      "content": "**TL;DR: We sacrifice a lot to get the job we want, but the data says it’s not more than $50k.**\n\nSoftware engineering jobs come with a lot of perks. We have modern offices, our hours are flexible, and recruiters assure us that our projects will change the world. But that doesn’t mean our jobs are perfect. Work requires us to commute, reduces the time we can spend with family, increases our stress levels, and forces us to deal with stressed-out teammates. And sometimes we work for companies with questionable morals and use technologies we don’t enjoy.\n\nFor some of us, it’s worth trading cash for a job that fits into our lives better. But figuring out just how much money to give up is difficult without any data.\n\nSo we got the data.\n\nWe asked the interviewing.io audience about times they accepted a job offer with lower total compensation because they prioritized other aspects of the job. Surprisingly, it turns out that most engineers have never accepted an offer with lower monetary compensation. And for the engineers who have taken lower pay for higher values, we found that $50k was as much as most had sacrificed.\n\nIn this post, we’re going to dive into what drives our compensation choices and how that can help you in your next job hunt.\n\nWhen do we make compensation tradeoffs?\n---------------------------------------\n\nLet’s say you’re on your way to getting a new job. You’ve applied to enough companies, performed well in your interviews, and now you have a stack of exciting offers in your inbox. Of course, you’re negotiating your offers to make sure you’re getting the best deals you can, but your first choice company is still offering you the third most compensation. Should you take the deal?\n\nOr, instead, imagine that you’re working with a team you can’t stand. Before starting to look for a new gig, wouldn’t it be nice to figure out how much it costs to find a better team?\n\nWe all value perks differently, but it’s important to know the market value of our values before making major life decisions.\n\nHow did you calculate $46,282?\n------------------------------\n\nWe sent a survey to interviewing.io users that asked about times they’d received multiple offers and had taken the offer with lower compensation. We wanted to know *why* they selected the lower offer and how much money they left on the table.\n\nFor respondents who had never taken a lower-comp offer, we asked what they *would* sacrifice compensation for. The results below are based on the primary (first ranked) reason respondents said they did (or would) accept a lower offer. We also asked everyone if they had advice for themselves for their next job hunt in a free-form response, which gave us some insight into how people make career decisions.\n\nLet’s dive into the results to see which quality-of-life factors are valued the most.\n\n![Pie chart showing responses to a survey asking whether respondents had ever accepted a job offer with lower total compensation, with 70.4% selecting 'No' and 29.6% selecting 'Yes'.](https://strapi-iio.s3.us-west-2.amazonaws.com/ever_accepted_6455051b5c.png)\n\nOf the 480 responses, 70% have never taken an offer with lower compensation. It's clear that while money doesn’t solve all of our problems, it certainly must solve a lot of them. There’s no shame in taking the money — the majority of people do.\n\nSome respondents were in the fortunate position where their highest cash offer was also most aligned with other aspects of the job. If you find yourself in this scenario, consider yourself lucky.\n\nThe remaining 30% of respondents sacrificed an average of $47k to get something that was important to them.\n\n![Bar chart showing average pay cut based on primary reason for accepting lower job offer: Mission $51,300, Commute/Remote $46,282, Work/Life Balance $42,563, Technology $42,250, Prestige $41,214, Team $36,737](https://strapi-iio.s3.us-west-2.amazonaws.com/pay_Cut_Avg_Reason_c1a932f1f7.png)\n\nThe differences in sacrificed pay between different reasons are quite small. The only significant difference was between Team ($37k) and Mission ($51k), which had the smallest and largest average loss in pay.\n\nMission may have required the largest pay drop because the type of companies that have positive missions tend to have less funding and profit (unfortunately). Their payrolls simply aren’t as big as their “evil” competitors’ payrolls.\n\nTeam may have required a significantly lower pay cut because it’s not as concrete of a value as the others, so it’s less worth paying for. Unlike commute time saved by working from home, it’s much harder to quantify an increase in the likelihood you’ll get along with your new teammates. We expect that makes it harder to justify a reduction in compensation.\n\n![Bar chart titled 'Pay difference bucketed by amount for all reasons' showing percentage of respondents per pay cut size range: 18% took cuts of $0-$9k and $20k-$24k, 13% took $30k-$34k cuts, 21% took $40k-$44k cuts, 9% took $50k-$54k cuts, 4% took $60k-$64k cuts, 1% each took $70k-$74k, $80k-$84k, $90k-$94k cuts, 6% took $95k-$99k cuts, 8% took over $100k pay cuts.](https://strapi-iio.s3.us-west-2.amazonaws.com/pay_Bucket_Reason_61625cfd30.png)\n\nAround the $50k mark, we see a considerable drop in the number of people who have taken lower pay for their values. There’s a limit to how big of a pay cut we will accept for any reason, and that limit appears to be about $50k.\n\nWhile the reason for a pay cut appears to not be very significant, the *size* of the pay cut plays a huge role. We know that [money matters when recruiters reach out to us](https://blog.alinelerner.com/what-i-learned-from-reading-8000-recruiting-messages/), so it’s not surprising that it also matters when we’re accepting offers.\n\nWhat about visas?\n-----------------\n\nSurely visa sponsorship doesn’t abide by the $50k cutoff, right? After all, many of the items above could be considered a luxury, but visa sponsorship is not, and in some cases, you may not have many options when making this decision (e.g., if you were laid off and then have 60 days to find a new job before you have to leave the country[1](#user-content-fn-1)). After computing our initial set of results above, we realized that we had inadvertently left visas out, so we created a second survey about pay sacrifices for visa sponsorships. We asked our audience if they had ever taken a lower paying job to secure a visa, and we asked them to specify what type of visa they received: H-1B, H-1B transfer, non-H-1B, or non-H-1B transfer.\n\nWe expected visa sponsorship needs would lead to larger pay cuts than the less tangible requirements in our first survey. Surprisingly, we still found ourselves around the $50k mark.\n\n![Bar chart titled 'Average pay cut for visa types and non-visa' showing average difference from maximum offer in thousands of dollars: $51,428 for New Non H-1B visas, $49,051 for Transfer H-1B visas, $46,680 for New H-1B visas, $46,060 for Non-Visa situations.](https://strapi-iio.s3.us-west-2.amazonaws.com/pay_Cut_Avg_Visa_d4c90718d8.png)\n\nThe averages for new H-1B, new non-H-1B, and transferred H-1B visas were very close to each other and also not significantly different from the average pay cut for intangible, “non-visa” reasons (from the first part of the post). Once again we see a cliff around $50k, beyond which we find few engineers willing to take a pay cut even for immigration status changes.\n\n![Bar chart titled 'Pay difference bucketed by amount for visa seekers' showing percentage of respondents per pay cut size range: 3% took $0-$4k cuts, 17% took $10k-$14k and $20k-$24k cuts, 22% took $40k-$44k cuts, 14% took $30k-$34k cuts, 8% took $50k-$54k cuts, 4% took $70k-$74k cuts, 1% took $80k-$84k cuts, 6% each took $90k-$94k and $95k-$99k cuts, 3% took over $100k pay cuts.](https://strapi-iio.s3.us-west-2.amazonaws.com/pay_Diff_Bucket_Visa_0271d34130.png)\n\nThe scarcity of accepted offers beyond a $50k pay cut shows us how highly engineers value cash. Both quality-of-life factors and more concrete rewards like visas are equally unlikely to entice them to accept less.\n\nHow do our projections compare to our actions?\n----------------------------------------------\n\nFinally, we wanted to know if the reasons engineers actually took lower paying jobs reflected the reasons that engineers said they would take lower jobs. For the engineers who had accepted a lower offer, we grouped them by the primary reason they stated for accepting that offer. For engineers who had never accepted a lower offer, we asked them what reasons, if any, they would hypothetically accept a lower offer, and we grouped those engineers by their primary reason.\n\n![Two pie charts comparing the primary reasons engineers have accepted or would accept lower paying job offers including work-life balance, commute, prestige, team, mission, and technology](https://strapi-iio.s3.us-west-2.amazonaws.com/reason_compare_41f6b761e9.png)\n\nThe reasons we *actually* accept lower-paying jobs appear to line up closely with the reason we *think* we would accept those jobs. Work/Life Balance is still the clear leader, but Commute/Remote takes up a much larger slice in the hypothetical situation than the real one. That may be a symptom of companies moving back into the office while their engineers want to stay remote. If you just lost your WFH flexibility, consider asking yourself how much you would pay to get it back.\n\nYour next job hunt\n------------------\n\nSo, what can you do with this information? At the beginning of your next job hunt, think critically about your values and consider how much you would pay to keep them. Most engineers in your shoes haven’t sacrificed more than $50k for any reason, so be wary of extending beyond that limit.\n\nIf you value working from home but your WFH option pays $10k less, you’re actually getting a great deal compared to your peers. If, on the other hand, you have an offer for a team you’d like to work with more, but it comes with a $80k pay cut, consider that you’re losing roughly twice the average of other engineers looking for the same thing.\n\nIf you set yourself up in a strong negotiating position by getting multiple offers, leverage them to make your favorite company compensate you as well as your not-quite-as-great-but-higher-paying choice. If you can’t get them to match, at least try to get them within $50k.\n\nLife isn’t all about money, but it’s up to you to make sure you’re paid what you’re worth.\n\n![Author avatar](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FElliot_Headshot_a87ff0cadc.jpeg&w=384&q=75 \"Elliot Plant\")\n\nElliot Plant\n\nElliot Plant enjoys everything about building. He has a degree in mechanical engineering, a job writing software, and he tinkers with electronics in his free time. Elliot currently works for Anthropic, and previously spent time at Google and Tesla as well his own startup. When the weather is nice, you can find Elliot walking his dog in the Oakland hills. Online, you can find him on [GitHub](https://github.com/elliotaplant), [LinkedIn](https://linkedin.com/in/elliotplant), or [Twitter](https://twitter.com/plant_elliot).\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. This is terrible, and we wholeheartedly believe these laws need to change. [↩](#user-content-fnref-1)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/cost-of-working-remotely-and-other-reasons-we-leave-money-on-the-table",
      "author": "",
      "user_id": ""
    },
    {
      "title": "How to write (actually) good job descriptions",
      "content": "We’ve been doing a lot of hiring at interviewing.io recently, which means we’ve been writing a lot of job descriptions. We’ve written before about [how important good copy is and how it can elevate less well-known brands](https://interviewing.io/blog/3-exercises-to-create-the-kind-of-employer-brand-that-actually-makes-engineers-want-to-work-for-you), but the exercise of having to write job descriptions for ourselves made me realize that giving advice is a lot easier than following it and that even we, who make a living from hiring, tend to fall into some of the same traps that we tell others to avoid.\n\nSo I thought I’d write this guide to keep us honest and to hopefully say some useful stuff to others undertaking this somewhat herculean task.\n\nBefore I get into the practical do’s and don’ts, a bit of theory.\n\nHow to avoid the “audience paradox”\n-----------------------------------\n\nWhen you start writing a job description, the first question you should ask yourself is, *Am I trying to attract the right people, or am I trying to keep the wrong people out?*\n\nThen, once you answer it, write for that audience deliberately, because it’s really hard to write for both (if you try, you’ll find that your job description will end up being way too long, and then you’ll attract… neither!)\n\nWhy is it hard to write for both? When you’re trying to attract talent, your tone is going to be different. You’re going to focus on cool stuff about the company, projects they’re going to do, the impact they’ll have, initiatives and outcomes that they’re going to own.\n\nWhen you try to keep people out, the focus tends to be more on a long laundry list of requirements.\n\nMost job descriptions that I’ve seen tend to focus on keeping the wrong people out, though I doubt that doing so was a deliberate choice — when people write these, it’s generally their first instinct to optimize for less noise. Why? Because when you post job descriptions, you inevitably get a bunch of unqualified candidates in. If you’re spending any meaningful time on hiring, others are too, and you’re probably in a hot market. In these markets, more and more unqualified candidates try to get in because salaries go up.\n\nAt the same time, in hot markets, fewer and fewer qualified candidates apply for jobs because companies start going after them (this is why sourcers and recruiters exist).\n\nThe paradox here is that, despite the noise, writing job descriptions that aim to reduce it will actually get you *fewer* qualified candidates (and will not cut the unqualified ones).\n\nUnqualified candidates have nothing to lose by applying to you, and no amount of bullet points will keep them out. On the other hand, writing job descriptions that, first and foremost, get people excited might actually get you some good people. Below is a succinct way of putting it.\n\n![Chart: the audience paradox](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fjob_descriptions_graph_9d898422e8.webp&w=1200&q=75 \"The Audience Paradox\")\n\nSo, no matter what, your goal should be to attract talent rather than worrying about the noise.\n\nIf they mostly attract noise, why even write job descriptions?\n--------------------------------------------------------------\n\nIt’s true that in hot markets, most hires don’t come from inbound applicants. However, writing good ones still matters. Here’s why:\n\n1. They help you collect your thoughts and align internally on what you need\n2. They help you figure out how you’re going to talk about the job to candidates\n3. Many candidates who come from outbound sources (i.e. you go out to get them rather than them coming to you) will still want to see the job description so they can have something concrete to reference when they’re trying to grok the job\n\nThe anatomy of a good job description\n-------------------------------------\n\n### An “About us” section\n\n*(1 paragraph)* Come up with a good general company summary. In a previous post, we listed some [exercises that will help you distill what makes your company special](https://interviewing.io/blog/3-exercises-to-create-the-kind-of-employer-brand-that-actually-makes-engineers-want-to-work-for-you).\n\n*(1 paragraph)* Contextualize the above for the specific team/department you’re hiring for. In other words, how does this specific team/department fit in to what you’re working on as a whole? What has made that department/team special, unique, or impactful til now? For instance, if your eng team is punching above their weight because it’s a small team producing outsized results, mention that. If you’re using some programming language that has a big community around it or you’re an early adopter of a cool language, mention that. If this is a design job description, and you’re a design-first culture or the founder is a designer, mention that. And so on.\n\n### An “About you & what you’ll do here” section\n\n#### About you\n\nList your requirements, but only list actual objective deal-breakers. People tend to get really carried away in this section and come up with huge laundry lists full of subjective things like “Follows coding best practices”. Imagine someone reading the job description, and put yourself in their shoes. Will you really weed someone out because reading that bullet will be the watershed moment in their life, where they honestly admit that they have not, in fact, been following best practices? Remember, the point of a job description is to sell good people, not keep out bad ones! Do you think that putting a bullet about following best practices is going to get someone excited to work for you? Probably not. Following best practices is table stakes, as are most other requirements of that ilk.\n\nHere are examples of good bullets. Note that they’re specific to the role at hand and that not being able to do them would actually be a dealbreaker for the role in question:\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_16_at_17_09_09_2x_e733d9c740.png)\n\nAnd here are examples of bad bullets (pulled from real job descriptions). Note that these are bad not because they’re not true (they are!) but because they won’t keep anyone out and just create noise and detract attention away from the parts that attract the best people:\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_16_at_17_09_14_2x_b8f55142ca.png)\n\n#### What you’ll do here\n\nList all the cool things that this person will get to do and work on. This is really important. Do NOT put generic things like “Will work with key stakeholders to drive KPIs”. Put very specific things that will excite the best candidates. The purpose is to get the best people excited, nothing more.\n\nHere are examples of good bullets:\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_16_at_17_09_34_2x_d448bfa95d.png)\n\nAnd here are examples of bad bullets (pulled from real job descriptions). Again, these are bad not because they’re untrue but because they’re generic and unmemorable. A good litmus test is, “Can most companies say this about the work they’re doing?”. Below you can see bullets that pertain to almost every eng role ever:\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_16_at_17_09_42_2x_6b1b3cfd8e.png)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/how-to-write-good-job-descriptions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "What do the best interviewers have in common? We looked at thousands of real interviews to find out.",
      "content": "At interviewing.io, we’ve analyzed and written at some depth about what makes for a good interview from the perspective of an interviewee. However, despite the inherent power imbalance, interviewing is a two-way street. I wrote a while ago about how, in this market, [recruiting isn’t about vetting as much as it is about selling](http://blog.alinelerner.com/building-a-product-in-the-technical-recruiting-space-read-this-first/), and not engaging candidates in the course of talking to them for an hour is a woefully missed opportunity. But, just like solving interview questions is a learned skill that takes time and practice, so, too, is the other side of the table. Being a good interviewer takes time and effort and a fundamental willingness to get out of autopilot and engage meaningfully with the other person.\n\nOf course, everyone and their uncle has strong opinions about what makes someone a good interviewer, so instead of waxing philosophical, we’ll present some data and focus on analytically answering questions like… Does it matter how strong of an engineering brand your company has, for instance? Do the questions you ask actually help get candidates excited? How important is it to give good hints to your candidate? How much should you talk about yourself? And is it true that, at the end of the day, what you say is way less important than how you make people feel?[1](#user-content-fn-1) And so on.\n\nBefore I delve into our findings, I’ll say a few words about interviewing.io and the data we collect.\n\nThe setup\n---------\n\n[interviewing.io](https://interviewing.io/) is an anonymous technical interviewing platform. On interviewing.io, people can practice technical interviewing anonymously, and if things go well, unlock real (still anonymous) interviews with companies like Lyft, Twitch, Quora, and more.\n\nThe cool thing is that both practice and real interviews with companies take place within the interviewing.io ecosystem. As a result, we’re able to collect quite a bit of interview data and analyze it to better understand technical interviewing. One of the most important pieces of data we collect is feedback from both the interviewer and interviewee about how they thought the interview went and what they thought of each other. If you’re curious, you can watch a real interview on our [recordings](https://interviewing.io/mocks) page, and see what the feedback forms for interviewers and interviewees look like below — in addition to one direct yes/no question, we also ask about a few different aspects of interview performance using a 1-4 scale. We also ask interviewees some extra questions that we don’t share with their interviewers, one of which is their own take on how they thought they did.\n\n![Screenshot of the interviewing.io interview feedback form for interviewers](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F5a595_screenshot_2017_11_29_09_13_30_6712d25e2a.png&w=1200&q=75 \"Interviewing.io interview feedback form for interviewers\")\n\nInterviewer feedback form\n\n\n\n![Screenshot of the interviewing.io interview feedback form for interviewees](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F9fb48_screenshot_2017_11_29_09_02_07_7d4a011c3d.png&w=1080&q=75 \"Interviewing.io interview feedback form for interviewees\")\n\nInterviewee feedback form\n\nIn this post, we’ll be analyzing feedback and outcomes of thousands of real interviews with companies to figure out what traits [the best interviewers](https://interviewing.io/blog/our-business-depends-on-having-the-best-interviewers-so-we-built-an-interviewer-rating-system-and-you-can-too) have in common.\n\nBefore we get into the nitty-gritty of individual interviewer behaviors, let’s first put the value of a good interviewer in context by looking at the impact of a company’s brand on the outcome. After all, if brand matters a lot, then maybe being a good interviewer isn’t as important as we might think.\n\nBrand strength\n--------------\n\nSo, does brand really matter for interview outcomes? One quick caveat before we get into the data: every interview on the platform is user-initiated. In other words, once you unlock our jobs portal (you have to do really well in practice interviews to do so), you decide who you talk to. So, candidates talking to companies on our platform will be predisposed to move forward because they’ve chosen the company in the first place. And, as should come as no surprise to anyone, companies with a very strong brand have an easier time pulling candidates (on our platform and out in the world at large) than their lesser-known counterparts. Moreover, many of the companies we work with do have a pretty strong brand, so our pool isn’t representative of the entire branding landscape. However, all is not lost — in addition to working with very recognizable brands, we work with a number of small, up-and-coming startups, so we hope that if you, the reader, are coming from a company that’s doing cool stuff but that hasn’t yet become a household name, our findings likely apply to you. And, as you’ll see, getting candidates in the door isn’t the same as keeping them.\n\nTo try to quantify brand strength, we used three different measures: the company’s [Klout Score](https://klout.com/corp/score) (yes, that still exists), its Mattermark [Mindshare Score](https://mattermark.com/first-official-company-rankings-update/#:~:text=The%20Mindshare%20Score%20is%20the,gain%20and%20retain%20attention%20online), and its score on [Glassdoor](https://www.glassdoor.com/Reviews/index.htm) (under general reviews).[2](#user-content-fn-2)\n\nWhen we looked at interview outcomes relative to brand strength, its impact was not statistically significant. In other words, **we found that brand strength didn’t matter at all when it came to either whether the candidate wanted to move forward or how excited the candidate was to work at the company**.\n\nThis was a bit surprising, so I decided to dig deeper. Maybe brand strength doesn’t matter overall but matters when the interviewer or the questions they asked aren’t highly rated? In other words, can brand buttress less-than-stellar interviewers? Not so, according to our data. Brand didn’t matter even when you corrected for interviewer quality. In fact, of the top 10 best-rated companies on our platform, half have no brand to speak of, 3 are mid-sized YC companies that command respect in Bay Area circles but are definitely not universally recognizable, and only 2 have anything approaching household name status.\n\nSo, what’s the takeaway here? **Maybe the most realistic thing we can say is that while brand likely matters *a lot* for getting candidates in the door, once they’re in, no matter how well-branded you are, they’re yours to lose.**\n\nChoosing the interview question\n-------------------------------\n\nIf brand doesn’t matter once you’ve actually gotten a candidate in the door, then what does? Turns out, the questions you ask matter a TON. As you recall, feedback on interviewing.io is symmetric, which means that in addition to the interviewer rating the candidate, the candidate also rates the interviewer, and one of the things we ask candidates is how good the question(s) they got asked were.\n\n**Question quality was extremely significant (p < 0.002 with an effect size of 1.25) when it came to whether the candidate wanted to move forward with the company. This held both when candidates did well and when they did poorly.**\n\nWhile we obviously can’t share the best questions (these are company interviews, after all), we can look at what candidates had to say about the best and worst-rated questions on the platform.\n\n### The good\n\n> *Always nice to get questions that are more than just plain algorithms.*\n\n> *Really good asking of a classic question, opened my mind up to edge cases and considerations that I never contemplated the couple of times I’ve been exposed to the internals of this data structure.*\n\n> *This was the longest interviewing.io interview I have ever done, and it is also the most enjoyable one! I really like how we started with a simple data structure and implemented algorithms on top of it. It felt like working on a simple small-scale project and was fun.*\n\n> *He chose an interesting and challenging interview problem that made me feel like I was learning while I was solving it. I can’t think of any improvements. He would be great to work with.*\n\n> *I liked the question — it takes a relatively simple algorithms problem (build and traverse a tree) and adds some depth. I also liked that the interviewer connected the problem to a real product at [Redacted] which made it feel like less like a toy problem and more like a pared-down version of a real problem.*\n\n> *This is my favorite question that I’ve encountered on this site. it was one of the only ones that seem like it had actual real-life applicability and was drawn from a real (or potentially real) business challenge. And it also nicely wove in challenges like complexity, efficiency, and blocking.*\n\n### The bad\n\n> *Question wasn’t straightforward and it required a lot of thinking/understanding since functions/data structures weren’t defined until a lot later. [Redacted] is definitely a cool company to work for, but some form of structure in interviews would have been a lot more helpful. Spent a long time figuring out what the question is even asking, and interviewer was not language-agnostic.*\n\n> *I was expecting a more technical/design question that showcases the ability to think about a problem. Having a domain-specific question (regex) limits the ability to show one’s problem-solving skills. I am sure with enough research one could come up with a beautiful regex expression but unless this is something one does often, I don’t think it [makes for] a very good assessment.*\n\n> *This is not a good general interview question. A good interview question should have more than one solution with simplified constraints.*\n\n### Anatomy of a good technical interview question\n\n1. Layer complexity (including asking a warmup)\n2. No trivia\n3. Real-world components/relevance to the work the company is doing are preferable to textbook algorithmic problems\n4. If you’re asking a classic algorithmic question, that’s ok, but you ought to bring some nuance and depth to the table, and if you can teach the interviewee something interesting in the process, even better!\n\nAsking the question\n-------------------\n\nOne of the other things we ask candidates after their interviews is how helpful their interviewer was in guiding them to the solution. Providing your candidate with well-timed hints that get them out of the weeds without giving away too much is a delicate art that takes a lot of practice (and a lot of repetition), but how much does it matter?\n\nAs it turns out, being able to do this well matters a ton. **Being good at providing hints was extremely significant** (p < 0.00001 with an effect size of 2.95) **when it came to whether the candidate wanted to move forward with the company (as before, we corrected for whether the interview went well)**.\n\nYou can see for yourself what candidates thought of their interviewers when it came to their helpfulness and engagement below. Though this attribute is a bit harder to quantify, it seems that hint quality is actually a specific instance of something bigger, namely the notion of turning something inherently adversarial into a collaborative exercise that leaves both people in a better place than where they started.[3](#user-content-fn-3)\n\nAnd if you can’t do that every time, then at the very least, be present and engaged during the interview. And no matter what the devil on your shoulder tells you, no good will ever come of opening Reddit in another tab.[4](#user-content-fn-4)\n\nOne of the most memorable, pithy conversations I ever had about interviewing was with a seasoned engineer who had spent years as a very senior software architect at a huge tech company before going back to what he’d always liked in the first place, writing code. He’d conducted a lot of interviews over a career spanning several decades, and after trying out a number of different interview styles, what he settled on was elegant, simple, and satisfying. **According to him, the purpose of any interview is to “see if we can be smart together.”** I like that so much, and it’s advice I repeat whenever anyone will listen.\n\n### The good\n\n> *I liked that you laid out the structure of the interview at the outset and mentioned that the first question did not have any tricks. That helped set the pace of the interview so I didn’t spend an inordinate amount of time on the first one.*\n\n> *The interview wasn’t easy, but it was really fun. It felt more like making a design discussion with a colleague than an interview. I think the question was designed/prepared to fill the 45 minute slot perfectly.*\n\n> *I’m impressed by how quickly he identified the issue (typo) in my hash computation code and how gently he led me to locating it myself with two very high-level hints (“what other tests cases would you try?” and “would your code always work if you look for the the pattern that’s just there at the beginning of the string?”). Great job!*\n\n> *He never corrected me, instead asked questions and for me to elaborate in areas where I was incorrect – I very much appreciate this.*\n\n> *The question seemed very overwhelming at first but the interviewer was good at helping to break it down into smaller problems and suggest we focus on one of those first.*\n\n### The bad\n\n> *[It] was a little nerve-wracking hearing you yawn while I was coding.*\n\n> *What I found much more difficult about this interview was the lack of back and forth as I went along, even if it was simple affirmation that “yes, that code you just wrote looks good”. There were times when it seemed like I was the only one who had talked in the past five minutes (I’m sure that’s an exaggeration). This made it feel much more like a performance than like a collaboration, and my heart was racing at the end as a result.*\n\n> *While the question was very straightforward, and [he] was likely looking for me to blow through it with no prompting whatsoever in order to consider moving forward in an interview process, it would have been helpful to get a discussion or even mild hinting from him when I was obviously stuck thinking about an approach to solve the the problem. While I did get to the answer in the end, having a conversation about it would have made it feel more like a journey and learning experience. That would have also been a strong demonstration of the collaborative culture that exists while working with teams of people at a tech company, and would have sold me more vis-a-vis my excitement level.*\n\n> *If an interview is set to 45 minutes, the questions should fit this time frame, because people plan accordingly. I think that if you plan to have a longer interview you should notify the interviewee beforehand, so he can be ready for it.*\n\n> *One issue I had with the question though is what exactly he was trying to evaluate from me with the question. At points we talking about very nitty-gritty details about python linked list or array iteration, but it was unclear at any point if that was what he was judging me on. I think in the future he could outline at the beginning what exactly he was looking for with the problem in order to keep the conversation focused and ensure he is well calibrated judging candidates.*\n\n> *Try to be more familiar with all the possible solutions to the problem you choose to pose to the candidate. Try to work on communicating more clearly with the candidate.*\n\n### Anatomy of a good interview\n\n1. Set expectations, and control timing/pacing\n2. Be engaged!\n3. Familiarity with the problem and its associated rabbit holes/garden paths\n4. Good balance of hints and letting candidate think\n5. Turn the interview into a collaborative exercise where both people are free to be smart together\n\nThe art of storytelling… and the importance of being human\n----------------------------------------------------------\n\nBeyond choosing and crafting good questions and being engaged (but not overbearing) during the interview, what else do top-rated interviewers have in common?\n\nThe pervasive common thread I noticed among the best interviewers on our platform is, as above, a bit hard to quantify but dovetails well with the notion of being engaged and creating a collaborative experience. It’s taking a dehumanizing process and elevating it to an organic experience between two capable, thinking humans. Many times, that translates into revealing something real about yourself and telling a story. It can be sharing a bit about the company you work at and why, out of all the places you could have landed, you ended up there. Or some aspect of the company’s mission that resonated with you specifically. Or how the projects you’ve worked on tie into your own, personal goals.\n\n### The good\n\n> *I like the interview format, in particular how it was primarily a discussion about cool tech, as well as an honest description of the company… the discussion section was valuable, and may be a better gauge of fit anyway. It’s nice to see a company which places value on that 🙂*\n\n> *The interviewer was helpful throughout the interview. He didn’t mind any questions on their company’s internal technology decisions, or how it’s structured. I liked that the interviewer gave me a good insight of how the company functions.*\n\n> *Extremely kind and very generous with explaining everything they do at [redacted]. Really interested in the technical challenges they’re working on. Great!*\n\n> *Interesting questions but the most valuable and interesting thing were the insights he gave me about [redacted]. He sounded very passionate about engineering in general, particularly about the challenges they are facing at [redacted]. Would love to work with him.*\n\n### The bad\n\n> *[A] little bit of friendly banter (even if it’s just “how are you doing”?) at the very beginning of the interview would probably help a bit with keeping the candidate calm and comfortable.*\n\n> *I thought the interview was very impersonal, [and] I could not get a good read on the goal or mission of the company.*\n\nAnd, as we wrote about in a previous post, one of the most genuine, human things of all is [giving people immediate, actionable feedback](https://interviewing.io/blog/people-cant-gauge-their-own-interview-performance-and-that-makes-them-harder-to-hire). As you recall, during the feedback step that happens after each interview, we ask interviewees if they’d want to work with their interviewer. As it turns out, there’s a very statistically significant relationship (p < 0.00005) between whether people think they did well and whether they’d want to work with the interviewer.[5](#user-content-fn-5) This means that when people think they did poorly, they may be a lot less likely to want to work with you. And by extension, it means that in every interview cycle, some portion of interviewees are losing interest in joining your company just because they didn’t think they did well, despite the fact that they actually did.\n\nHow can one mitigate these losses? **Give positive, actionable feedback immediately (or as soon as possible)! This way people don’t have time to go through the self-flagellation gauntlet that happens after a perceived poor performance, followed by the inevitable rationalization that they totally didn’t want to work there anyway.**\n\n### How to be human\n\n1. Talk about what your company does… and what specifically about it appealed to you and made you want to join\n2. Talk about what you’re currently working on and how that fits in with what you’re passionate about\n3. When you like a candidate, give positive feedback as quickly as you can to save them from the self-flagellation that they’ll likely go through otherwise… and which might make them rationalize away wanting to work with you\n4. And, you know, be friendly. A little bit of warmth can go a long way.\n\nBecoming a better interviewer\n-----------------------------\n\nInterviewing people is hard. It’s hard to come up with good [interview questions](https://interviewing.io/questions), it’s hard to give a good interview, and it’s especially hard to be human in the face of conducting a never-ending parade of interviews. But, being a good interviewer is massively important. As we saw, while your company’s brand will get people in the door, once they’ve reached the technical interview, the playing field is effectively level, and you can no longer use your brand as a crutch to mask poor questions or a lack of engagement. And in this market, where the best candidates have a ton of options, when wielded properly, a good interview that elevates a potentially cold, transactional interaction into something real and genuine can become the selling point that gets great engineers to work for you, whether you’re a household name or a startup that just got its first users.\n\nGiven how important it is to do interviews well, what are some things you can do to get better right away? **One thing I found incredibly useful for coming up with good, original questions is to start a shared doc with your team where every time someone solves a problem they think is interesting, no matter how small, they jot down a quick note.** These notes don’t have to be fleshed out at all, but they can be the seeds for unique interview questions that give candidates insight into the day-to-day at your company. Turning these disjointed seeds into interview questions takes thought and effort — you have to prune out a lot of the details and distill the essence of the problem into something it doesn’t take the candidate a lot of work/setup to grok, and you’ll likely have to iterate on the question a few times before you get it right — but they payoff can be huge.\n\nAnother thing you can do to get actionable feedback like the kind you saw in this post (and then immediately level up) is to get on interviewing.io as an interviewer. **If you interview people in our double-blind practice pool, no one will know who you are or which company you represent, which means that you get a truly unbiased take on your interviewing ability, which includes your question quality, how excited people would be to work with you, and how good you are at helping people along without giving away too much.** It’s also a great way to go beyond your team, which can be pretty awkward, and try out new questions on a very engaged, high-quality user base. You’ll also get to keep replays of your interviews so you can revisit crucial moments and figure out exactly what you need to do to get better next time.\n\n![Screenshot of the interviewer overall performance report](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffb028_screenshot_2017_11_29_11_52_16_528b62c8c9.png&w=1080&q=75 \"Overall performance\")\n\n*Want to hone your skills as an interviewer? Want to help new interviewers at your company warm up before they officially get added to your interview loops? You can [sign up to our platform as an interviewer](https://interviewing.io/signup), or (especially for groups) ping us at [interviewers@interviewing.io](mailto:interviewers@interviewing.io).*\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. “People will forget what you said, people will forget what you did, but people will never forget how you made them feel.” - Maya Angelou [↩](#user-content-fnref-1)\n2. It’s important to call out that brand and *engineering* brand are two separate things that can diverge pretty wildly. For instance, Target has a strong brand overall but probably not the best engineering brand (sorry). Heap, on the other hand, is one of the better-respected places to work among engineers (both on interviewing.io and off), but it doesn’t have a huge overall brand. Both the Klout and Mattermark Mindshare scores aren’t terrible for quantifying brand strength, but they’re not amazing at engineering brand strength (they’re high for Target and low for Heap). The Glassdoor score is a bit better because reviewers tend to skew engineering-heavy, but it’s still not that great of a measure. So, if anyone has a better way to quantify this stuff, let me know. If I were doing it, I’d probably look at GitHub repos of the company and its employees, who their investors are, and so on and so forth. But that’s a project that’s out of scope for this post. [↩](#user-content-fnref-2)\n3. If you’re familiar with [Dan Savage’s campsite rule](https://en.wikipedia.org/wiki/Savage_Love#Campsite_rule) for relationships, I think there should be a similar for interviewing… leave your candidates in better shape than when you found them. [↩](#user-content-fnref-3)\n4. Let us save you the time: Trump is bad, dogs are cute, someone ate something. [↩](#user-content-fnref-4)\n5. This time with even more significance! [↩](#user-content-fnref-5)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/best-technical-interviews-common",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Lessons from 3,000 technical interviews… or how what you do after graduation matters way more than where you went to school",
      "content": "The first blog post I published that got any real attention was called “[Lessons from a year’s worth of hiring data](https://blog.alinelerner.com/lessons-from-a-years-worth-of-hiring-data/)“. It was my attempt to understand what attributes of someone’s resume actually mattered for getting a software engineering job. Surprisingly, as it turned out, where someone went to school didn’t matter at all, and by far and away, the strongest signal came from the number of typos and grammatical errors on their resume.\n\nSince then, I’ve discovered (and written about) how [useless resumes are](https://blog.alinelerner.com/resumes-suck-heres-the-data/), but ever since writing that first post, I’ve been itching to do something similar with interviewing.io’s data. For context, interviewing.io is a platform where people can practice technical interviewing anonymously and, in the process, find jobs — do well in practice, and you get guaranteed (and anonymous!) technical interviews at companies like Uber, Twitch, Lyft, and more. Over the course of our existence, we’ve amassed performance data from thousands of real and practice interviews. **Data from these interviews sets us up nicely to look at what signals from an interviewee’s background might matter when it comes to performance.**\n\nAs often happens, what we found was surprising, and some of it runs counter to things I’ve said and written on the subject. More on that in a bit.\n\nThe setup\n---------\n\nWhen an interviewer and an interviewee match on our platform, they meet in a collaborative coding environment with voice, text chat, and a whiteboard and jump right into a technical question (check out our [recordings](https://interviewing.io/mocks) page to see this in action). Interview questions on the platform tend to fall into the category of what you’d encounter at a phone screen for a back-end software engineering role, and interviewers typically come from a mix of large companies like Google, Facebook, and Uber, as well as engineering-focused startups like Asana, Mattermark, KeepSafe, and more.\n\nAfter every interview, interviewers rate interviewees on a few different dimensions, including technical ability. Technical ability gets rated on a scale of 1 to 4, where 1 is “poor” and 4 is “amazing!”. On our platform, a score of 3 or above has generally meant that the person was good enough to move forward. You can see what our feedback form looks like below:\n\n![Screenshot of the interview feedback form](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffeedback_circled_40095dd6e4.png&w=1200&q=75 \"Example feedback form\")\n\n**To run the analysis for this post, we cross-referenced interviewees’ average technical scores** (circled in red in the feedback form above) **with the attributes below to see which ones mattered most**.[1](#user-content-fn-1)\n\n* Attended a top computer science school\n* Worked at a top company\n* Took classes on Udacity/Coursera[2](#user-content-fn-2)\n* Founded a startup\n* Master’s degree\n* Years of experience\n\n**Of all of these, only 3 attributes emerged as statistically significant: top school, top company, and classes on Udacity/Coursera.** Apparently, as the fine gentlemen of Metallica once said, nothing else matters. In the graph below, you can see the [effect size](https://en.wikipedia.org/wiki/Effect_size) of each of the significant attributes (attributes that didn’t achieve significance don’t have bars).\n\nAs I said at the outset, these results were quite surprising, and I’ll take a stab at explaining each of the outcomes below.\n\nTop school & top company\n------------------------\n\nGoing into this, I expected top company to matter but not top school. The company thing makes sense — you’re selecting people who’ve successfully been through at least one interview gauntlet, so the odds of them succeeding at future ones should be higher.\n\nTop school is a bit more muddy, and it was indeed the least impactful of the significant attributes. Why did schooling matter in this iteration of the data but didn’t matter when I was looking at resumes? I expect the answer lies in the disparity between performance in an isolated technical phone screen versus what happens when a candidate actually goes on site. With the right preparation, the technical phone interview is manageable, and top schools often have rigorous algorithms classes and a culture of preparing for technical phone screens (to see why this culture matters and how it might create an unfair advantage for those immersed in it, see my post about how we need to [rethink the technical interview](https://interviewing.io/blog/you-cant-fix-diversity-in-tech-without-fixing-the-technical-interview)). Whether passing an algorithmic technical phone screen means you’re a great engineer is another matter entirely and hopefully the subject of a future post.\n\nUdacity/Coursera\n----------------\n\nMOOC participation (Udacity and Coursera in particular, as those were the ones interviewing.io users gravitated to most) mattering as much as it did (and mattering way more than pedigree) was probably the most surprising finding here, and so it merited some additional digging.\n\nIn particular, I was curious about the interplay between MOOCs and top schools, so I partitioned MOOC participants into people who had attended top schools vs. people who hadn’t. When I did that, something startling emerged. **For people who attended top schools, completing Udacity or Coursera courses didn’t appear to matter. However, for people who did not, the effect was huge, so huge, in fact, that it dominated the board.** Moreover, interviewees who attended top schools performed significantly worse than interviewees who had not attended top schools but HAD taken a Udacity or Coursera course.\n\nSo, what does this mean? Of course (as you’re probably thinking to yourself while you read this), correlation doesn’t imply causation. As such, rather than MOOCs being a magic pill, I expect that people who gravitate toward online courses (and especially those who might have a chip on their shoulder about their undergrad pedigree and end up drinking from the MOOC firehose) already tend to be abnormally driven. But, even with that, I’d be hard pressed to say that completing great online CS classes isn’t going to help you become a better interviewee, especially if you didn’t have the benefit of a rigorous algorithms class up until then. Indeed, a lot of the courses we saw people take focused around algorithms, so it’s no surprise that supplementing your preparation with courses like this could be tremendously useful. Some of the most popular courses we saw were:\n\n**Udacity**\n\n* [Design of Computer Programs](https://www.udacity.com/course/design-of-computer-programs--cs212)\n* [Intro to Algorithms](https://www.udacity.com/course/intro-to-algorithms--cs215)\n* [Computability, Complexity & Algorithms](https://www.udacity.com/course/computability-complexity-algorithms--ud061)\n\n**Coursera**\n\n* [Algorithms Specialization](https://www.coursera.org/specializations/algorithms)\n* [Functional Programming Principles in Scala](https://www.coursera.org/learn/progfun1)\n* [Machine Learning](https://www.coursera.org/learn/machine-learning)\n* [Algorithms on Graphs](https://www.coursera.org/learn/algorithms-on-graphs)\n\nFounder status\n--------------\n\nHaving been a founder didn’t matter at all when it came to technical interview performance. This, too, isn’t that surprising. The things that make one a good founder are not necessarily the things that make one a good engineer, and if you just came out of running a startup and are looking to get back into an individual contributor role, odds are, your interview skills will be a bit rusty. This is, of course, true of folks who’ve been in industry but out of interviewing for some time, as you’ll see below.\n\nMaster’s degree & years of experience\n-------------------------------------\n\nNo surprises here. I’ve ranted quite a bit about the [disutility of master’s degrees](https://blog.alinelerner.com/how-different-is-a-b-s-in-computer-science-from-a-m-s-in-computer-science-when-it-comes-to-recruiting/), so I won’t belabor the point.\n\nYears of experience, too, shouldn’t be that surprising. For context, our average user has about 5 years of experience, with most having between 2 and 10. I think we’ve all anecdotally observed that the time spent away from your schooling doesn’t do you any favors when it comes to interview prep. You can see a scatter plot of interview performance vs. years of experience below as well as my attempt to fit a line through it (as you can see, the R^2 is piss poor, meaning that there isn’t a relationship to speak of).\n\nClosing Thoughts\n----------------\n\nIf you know me, or even if you’ve read some of my writing, you know that, in the past, I’ve been quite loudly opposed to the concept of pedigree as a useful hiring signal. With that in mind, I feel like I owe clearly acknowledge, up front, that we found this time runs counter to my stance. But that’s the whole point, isn’t it? You live, you get some data, you make some graphs, you learn, you make new graphs, and you adjust. **Even with this new data, I’m excited to see that what mattered way more than pedigree was the actions people took to better themselves (in this case, rounding out their existing knowledge with MOOCs), regardless of their background.**\n\nMost importantly, these findings have done nothing to change interviewing.io’s core mission. We’re creating an efficient and meritocratic way for candidates and companies to find each other, and as long as you can code, we couldn’t care less about who you are or where you come from. In our ideal world, all these conversations about which proxies matter more than others would be moot non-starters because coding ability would stand for, well, coding ability. And that’s the world we’re building.\n\n*Thanks to Roman Rivilis for his help with data annotation for this post.*\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. For fun, we tried relating browser and operating system choice to interview performance, (smugly) expecting Chrome users to dominate. Not so. Browser choice didn’t matter, nor did what OS people used while interviewing. We got this data from looking at interviewees’ LinkedIn profiles. [↩](#user-content-fnref-1)\n2. We got this data from looking at interviewees' LinkedIn profiles. [↩](#user-content-fnref-2)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/lessons-from-3000-technical-interviews",
      "author": "",
      "user_id": ""
    },
    {
      "title": "How hard is it to cheat in technical interviews with ChatGPT? We ran an experiment.",
      "content": "*Edit: This article originally contained a TikTok video of someone cheating in an interview with ChatGPT. Videos like this still can be found online, but companies hate them, so they don't stay up for long*\n\nChatGPT has revolutionized work as we know it. From helping small businesses automate their administrative tasks to coding entire React components for web developers, its usefulness is hard to overstate.\n\nAt interviewing.io, we've been thinking a lot about how ChatGPT will change technical interviewing. **One big question is: Does ChatGPT make it easy to cheat in interviews?** You've probably started to hear concerns about students cheating on their homework with ChatGPT, and we are certain that some people have tried to cheat in interviews with it, too!\n\nInitial responses to cheating software have been pretty much in line with what you’d expect:\n\n* Redditors state that “[ChatGPT is the end of coding as we know it.](https://www.reddit.com/r/singularity/comments/12zyela/chatgpt_spells_the_end_of_coding_as_we_know_it/)”\n* YouTubers announce that “[Software engineering is dead. ChatGPT killed it.](https://www.youtube.com/watch?v=OeebS-VcSH0)”\n* X (formerly Twitter) questions if “[​​ChatGPT spell(s) the end for Coding Interviews?](https://twitter.com/intx_podcast/status/1635396953109561344)”\n\nIt seems clear that ChatGPT can assist people during their interviews, but we wanted to know:\n\n* How much can it help?\n* How easy is it to cheat (and get away with it)?\n* Will companies that ask LeetCode questions need to make significant changes to their interview process?\n\nTo answer these questions, we recruited some of our professional interviewers and users for a cheating experiment! Below, we’ll share everything we discovered and explain what it means for you. As a little preview, just know this: companies need to change the types of interview questions they are asking—immediately.\n\nThe experiment\n--------------\n\ninterviewing.io is an interview practice platform and recruiting marketplace for engineers. Engineers use us for mock interviews. Companies use us to hire top performers. We have thousands of professional interviewers in our ecosystem, and hundreds of thousands of engineers have used our platform to prepare for interviews.[1](#user-content-fn-1)\n\n### Interviewers\n\nInterviewers came from our pool of professional interviewers. They were broken into three groups, with each group asking a different type of question. **The interviewers had no idea that the experiment was about ChatGPT or cheating; we told them that \"[this] research study aims to understand the trends in the predictability of an interviewer’s decisions over time – especially when asking standard vs. non-standard interview questions.\"**\n\nThese were the three question types:\n\n1. **Verbatim LeetCode questions**: questions pulled directly from LeetCode at the interviewer's discretion with no modifications to the question.\n\nExample: The [Sort Colors](https://leetcode.com/problems/sort-colors/) LeetCode question is asked exactly as it is written.\n\n2. **Modified LeetCode questions**: questions pulled from LeetCode and then modified to be similar to the original but still notably different from it.\n\nExample: The [Sort Colors](https://leetcode.com/problems/sort-colors/) question above but modified to have four integers (0,1,2,3) instead of just three integers (0,1,2) in the input.\n\n3. **Custom questions**: questions that aren’t directly tied to any question that exists online.\n\nExample: You are given a log file with the following format:\n- `<username>: <text> - <contribution score>`\n- Your task is to identify the user who represents the median level of engagement in a conversation. Only consider users with a contribution score greater than 50%. Assume that the number of such users is odd, and you need to find the one right in the middle when sorted by their contribution scores. Given the file below, the correct answer is SyntaxSorcerer.\n\n```\nLOG FILE START\nNullPointerNinja: \"who's going to the event tomorrow night?\" - 100%\nLambdaLancer: \"wat?\" - 5%\nNullPointerNinja: \"the event which is on 123 avenue!\" - 100%\nSyntaxSorcerer: \"I'm coming! I'll bring chips!\" - 80%\nSyntaxSorcerer: \"and something to drink!\" - 80%\nLambdaLancer: \"I can't make it\" - 25%\nLambdaLancer: \"🙁\" - 25%\nLambdaLancer: \"I really wanted to come too!\" - 25%\nBitwiseBard: \"I'll be there!\" - 25%\nCodeMystic: \"me too and I'll brink some dip\" - 75%\nLOG FILE END\n\n```\n\nFor more information about question types and about how we designed this experiment, please read the [Interviewer Experiment Guidelines](https://docs.google.com/document/u/0/d/1UdWZHUQfeLR8oUiNY4JfwgES42HTlAQL5z_VfQJPPKk/edit) doc that we shared with participating interviewers.\n\n### Interviewees\n\nInterviewees came from our pool of active users and were invited to participate in a short survey. We selected interviewees who:\n\n* Were actively looking for a job in today's market\n* Had 4+ years of experience and were applying to senior-level positions\n* Rated their “ChatGPT while coding” familiarity as moderate to high\n* Identified themselves as someone who thought they could cheat in an interview without getting caught\n\nThis selection helped us skew the candidates toward people who could feasibly cheat in an interview, had the motivation to do so, and were already reasonably familiar with ChatGPT and coding interviews.\n\n**We told interviewees that they had to use ChatGPT in the interview, and the goal was to test their ability to cheat with ChatGPT.** They were also told not to try to pass the interview with their own skills — the point was to rely on ChatGPT.\n\nWe ended up conducting 37 interviews overall, 32 of which we were able to use (we had to remove 5 because participants didn’t follow directions):\n\n* 11 with the “verbatim” treatment\n* 9 with the “modified” treatment\n* 12 with the “custom” treatment\n\nA quick disclaimer. Because our platform allows for anonymity, our interviews have audio but no video. We’re anonymous because we want to create a safe space for our users to fail and learn quickly without judgment. It’s great for our users, but we acknowledge that not having video in these interviews makes our experiment less realistic. In a real interview, you will be on camera with a job on the line, which makes cheating harder — but does not eliminate it.\n\nAfter the interviews, both interviewers and interviewees had to complete an exit survey. We asked interviewees about the difficulties of using ChatGPT during the interview, and interviewers were given multiple chances to express concerns about the interview — we wanted to see how many interviewers would flag their interviews as problematic and report that they suspected cheating.\n\n![Post-survey interviewee questions](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FInterviewee_Collage_Survey_88a5b0a095.jpg&w=1200&q=75 \"Post-survey interviewee questions\")\n\nPost-survey interviewee questions\n\n\n\n![Post-survey interviewer questions](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FInterviewer_Collage_Survey_a0e0cf80fb.jpg&w=1200&q=75 \"Post-survey interviewer questions\")\n\nPost-survey interviewer questions\n\nWe had no idea what would happen in this experiment, but we assumed that if half the candidates that cheated got away with it and passed the interview, it would be a telling result for our industry.\n\nResults\n-------\n\nAfter removing interviews where participants did not follow instructions[2](#user-content-fn-2), we got the following results. Our control was how candidates performed in interviewing.io mock interviews outside the study: 53%.[3](#user-content-fn-3) Note that most mock interviews on our platform are LeetCode-style questions, which makes sense because that's primarily what FAANG companies ask. We'll come back to this in a moment.\n\n![Pass Rate by Question Type with the Control group at 53%, the Verbatim group at 73%, the Modified group at 67%, and the Custom group at 25%](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FPass_rate_by_question_type_when_trying_to_cheat_with_Chat_GPT_c20c87712e.png&w=1920&q=75 \"Pass Rate by Question Type with the Control group at 53%, the Verbatim group at 73%, the Modified group at 67%, and the Custom group at 25%\")\n\n'Verbatim' questions passed significantly more often, compared to both our platform average and to 'custom' questions. 'Verbatim' and 'modified' questions were not statistically significantly differnt from each other. 'Custom' questions had a significantly lower pass rate than any of the other groups.\n\n### “Verbatim” questions\n\nPredictably, the verbatim group performed the best, passing 73% of their interviews. Interviewees reported that they got the perfect solution from ChatGPT.\n\nThe most notable comment from the post-interview survey for this group is below — we think it is particularly telling of what was going on in many of the interviewers’ minds:\n\n> *“It's tough to determine if the candidate breezed through the question because they're actually good or if they've heard this question before. Normally, I add 1-2 unique twists to the problem to ascertain the difference.”*\n\nNormally, this interviewer would have followed up with a modified question to get more signal, so let’s examine the “modified” group next to see if interviewers actually got more signal by adding a twist to their questions.\n\n### “Modified” questions\n\nRemember, this group may have had a LeetCode question given to them, which was standard but modified in a way that was not directly available online. This means ChatGPT couldn’t have had a direct answer to this question. Hence, the interviewees were much more dependent on ChatGPT's actual problem-solving abilities than its ability to regurgitate LeetCode tutorials.\n\nAs predicted, the results for this group weren’t too different from the “verbatim” group, with 67% of candidates passing their interviews. As it turns out, this difference was not statistically significantly different from the \"verbatim\" group, i.e., “modified” and “verbatim” are essentially the same. This result suggests that ChatGPT can handle minor modifications to questions without much trouble. Interviewees did notice, however, that it took more prompting to get ChatGPT to solve the modified questions. As one of our interviewees said:\n\n> *“Questions that are lifted directly from LeetCode were no problem at all. A follow-up question that was not so much directly LeetCode-style was much harder to get ChatGPT to answer.”*\n\n### “Custom” questions\n\nAs expected, the “custom” question group had the lowest pass rate, with only 25% of candidates passing. **Not only is it statistically significantly smaller than the other two treatment groups, it's significantly lower than the control! When you ask candidates fully custom questions, they perform worse than they do when they're not cheating (and getting asked LeetCode-style questions)!**\n\nNote that this number, when initially calculated, was marginally higher, but after reviewing the custom questions in detail, we discovered a problem with this question type we hadn’t anticipated, which had skewed the results minorly toward a higher pass rate. Read the section below called \"Companies: Change the questions you are asking immediately!\" to find out what that problem was.\n\nNo one was caught cheating!\n---------------------------\n\nIn our experiment, interviewers were not aware that the interviewees were being asked to cheat. As you recall, after each interview, we had interviewers complete a survey in which they had to describe how confident they were in their assessments of candidates.\n\n**Interviewer confidence in the correctness of their assessments was high, with 72% saying they were confident in their hiring decision.** One interviewer felt so strongly about an interviewee's performance that they concluded we should invite them to be an interviewer on the platform!\n\n> *“The candidate performed very well and demonstrated knowledge of a strong Amazon L6 (Google L5) SWE... and could also be considered to be an interviewer/mentor on interviewing.io.”*\n\nThat is a lot of confidence after just one interview — probably too much!\n\nWe’ve long known that [engineers are bad at gauging their own performance](https://interviewing.io/blog/own-interview-performance), so perhaps it shouldn’t come as a shock to find that interviewers also overestimate the effectiveness of the questions that they ask.\n\nOf the interviewers who were not confident in their hiring choice (28%), we asked them why. This was the frequency distribution of their reasons.\n\n![Interviewers weren't confident in their assessment because the candidate gave suboptimal solutions, missed edge cases, had messy code, was inarticulate, or occasionally had other niche issues, but no interviewer was called out cheating as a concern](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FInterviewer_confidence_61b40b9ce1.png&w=1920&q=75 \"Note that cheating isn’t mentioned anywhere!\")\n\nNote that cheating isn’t mentioned anywhere!\n\nMost interviewers concerned about their hiring decision expressed specific reasons for their lack of confidence. These issues typically included suboptimal solutions, missed edge cases, messy code, or poor communication. We specifically included an “Other Issue” category to see if they would express a concern that the interviewee was cheating, but digging deeper revealed only minor grievances like “personality issues” and “they need to speed up their coding.”\n\nIn addition to having this opportunity to call out cheating, interviewers were prompted three additional times to note any other concerns they had with the interview, including free-form text boxes and several multiple-choice questions with options to explain their concerns.\n\nWhen an interviewee bombed because they didn’t understand the ChatGPT response, the interviewer chalked up the interviewee’s odd behavior and stilted responses to a lack of practice — not cheating. One interviewer thought the candidate's problem-solving was fine but commented that they were slow and needed to consider edge cases more carefully.\n\n> *“Candidate did not seem prepared for any LeetCode questions.\"*\n\n> *“Candidate's approach lacked clarity, and they jumped into the coding too early.”*\n\n> *“The candidate was not prepared to tackle even the most basic coding questions on LeetCode.”*\n\n> *“Good problem solving in general, but the candidate needs to be faster at coding and identifying critical edge cases.”*\n\nSo, who reported concerns about cheating? And who got caught?\n\n**As it turns out, not a single interviewer mentioned concerns about any of the candidates cheating!**\n\nWe were stunned to discover that interviewers reported no suspicions of cheating, and interestingly, interviewees were largely confident that they were getting away with it, too. 81% reported no concerns about being caught, 13% thought they might have tipped off the interviewer, and an astonishingly small 6% of participants thought the interviewer suspected them of cheating.\n\n![81% of interviewees were not worried, 13% were slightly worried, and just 6% were very worried about being caught](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FInterviewee_worry_level_6d1445e631.png&w=1920&q=75 \"Interviewees were mostly sure that their cheating went undetected\")\n\nInterviewees were mostly sure that their cheating went undetected\n\nThe candidates who worried they were caught did have abnormal comments from the interviewers in the post-survey analysis, but they still were not suspected of cheating. **To summarize, most candidates thought they were getting away with cheating — and they were right!**\n\nCompanies: Change the questions you are asking immediately!\n-----------------------------------------------------------\n\nThe obvious conclusion from these results is that **companies need to start asking custom questions immediately, or they are at serious risk of candidates cheating during interviews (and ultimately not getting useful signal from their interviews)!**\n\nChatGPT has made verbatim questions obsolete; anyone relying on them will be naively leaving their hiring processes up to chance. Hiring is [already tricky enough](https://interviewing.io/blog/we-ran-the-numbers-and-there-really-is-a-pipeline-problem-in-eng-hiring) without worrying about cheating. If you’re part of a company that uses verbatim LeetCode questions, please share this post internally!\n\nUsing custom questions isn’t just a good way to prevent cheating. It filters out candidates who have memorized a bunch of LeetCode solutions (as you saw, our custom question pass rate was significantly lower than our control). It also meaningfully improves candidate experience, which makes people way more likely to want to work for you. A while ago, we did an [analysis of what makes good interviewers good](https://interviewing.io/blog/best-technical-interviews-common). Not surprisingly, asking good questions was one of the hallmarks, and our best-rated interviewers were the ones who tended to ask custom questions! Question quality was extremely significant in our study, regarding whether the candidate wanted to move forward with the company. It was much more important than the company’s brand strength, which mattered for getting candidates in the door but didn’t matter relative to question quality once they were in process.\n\nAs some of our interviewees said…\n\n> *“Always nice to get questions that are more than just plain algorithms.”*\n\n> *“I liked the question — it takes a relatively simple algorithms problem (build and traverse a tree) and adds some depth. I also liked that the interviewer connected the problem to a real product at [Redacted], which made it feel less like a toy problem and more like a pared-down version of a real problem.”*\n\n> *“This is my favorite question that I’ve encountered on this site. It was one of the only ones that seemed to have real-life applicability and was drawn from a real (or potentially real) business challenge. And it also nicely wove in challenges like complexity, efficiency, and blocking.”*\n\nOne more somewhat subtle piece of advice for companies who decide to move to more custom questions. You might be tempted to take verbatim LeetCode questions and change up the wording or some of the window dressing. That makes sense, because it’s certainly easier than coming up with questions from scratch. Unfortunately, that doesn’t work.\n\nAs we mentioned earlier, we discovered in this experiment that just because a question looks like a custom question, doesn’t mean it is one. Questions can appear custom and still be identical to an existing LeetCode question. **When making questions to ask candidates, it isn’t enough to obscure an existing problem.** You need to ensure that the problem has unique inputs and outputs to be effective at stopping ChatGPT from recognizing it!\n\nThe questions that interviewers ask are confidential, and we cannot share the exact questions that our interviewers used in the experiment. However, we can give you an indicative example. Below is a “custom question” with this critical flaw, which is easy for ChatGPT to beat:\n\n```\nFor her birthday, Mia received a mysterious box containing numbered cards \nand a note saying, \"Combine two cards that add up to 18 to unlock your gift!\" \nHelp Mia find the right pair of cards to reveal her surprise.\n\nInput: An array of integers (the numbers on the cards), and the target sum (18). \narr = [1, 3, 5, 10, 8], target = 18\n\nOutput: The indices of the two cards that add up to the target sum. \nIn this case, [3, 4] because index 3 and 4 add to 18 (10+8).\n\n```\n\nDid you spot the issue? While this question appears “custom” at first glance, its objective is identical to the popular [TwoSum](https://leetcode.com/problems/two-sum/) question: finding two numbers that sum to a given target. The inputs and outputs are identical; the only thing “custom” about the question is the story added to the problem.\n\nSeeing that this is identical to known problems, it shouldn’t be a surprise to learn that ChatGPT does well on questions that have inputs and outputs identical to existing known problems — even when they have a unique story added to them.\n\n### How to actually create good custom questions\n\nOne thing we’ve found incredibly useful for coming up with good, original questions is to start a shared doc with your team where every time someone solves a problem they think is interesting, no matter how small, they jot down a quick note. These notes don’t have to be fleshed out at all, but they can be the seeds for unique interview questions that give candidates insight into the day-to-day at your company. Turning these disjointed seeds into interview questions takes thought and effort — you have to prune a lot of the details and distill the essence of the problem into something that doesn’t take the candidate a lot of work/setup to grok. You’ll also likely have to iterate on these home-grown questions a few times before you get them right — but the payoff can be huge.\n\nTo be clear, we’re not advocating the removal of data structures and algorithms from technical interviews. DS&A questions have gotten a bad reputation because of bad, unengaged interviewers and because of companies lazily rehashing LeetCode problems, many of them bad, which have nothing to do with their work. In the hands of good interviewers, those questions are powerful and useful. If you use the approach above, you’ll be able to come up with new data structure & algorithmic questions that have a practical foundation and component that will engage candidates and get them excited about the work you’re doing.\n\nYou’ll also be moving our industry forward. It’s not OK that memorizing a bunch of LeetCode questions gives candidates an edge in today’s interview process, nor is it OK that interviews have gotten to a state where cheating starts to look like a rational choice. The solution is more work on the employer’s part to come up with better questions. Let’s all do it together.\n\nReal talk for job seekers\n-------------------------\n\nAll right, now, for all of you who are actively looking for work, listen up! Yes, a subset of your peers will now be using ChatGPT to cheat in interviews, and at companies that ask LeetCode questions (sadly, many of them), those peers will have an edge… for a short while.\n\n**Right now, we’re in a liminal state where companies’ processes have not caught up to reality. They will, soon enough, either by moving away from using verbatim LeetCode questions entirely (which will be a boon for our entire industry) or by returning to in-person onsites (which will make cheating largely impossible past the technical screen) or both.**\n\nIt sucks that other candidates cheating is another thing to worry about in an [already difficult climate](https://interviewing.io/blog/you-now-need-to-do-15-percent-better-in-technical-interviews), but we cannot, in good conscience, endorse cheating to “level the playing field.”\n\nIn addition, interviewees who used ChatGPT uniformly reported how much more difficult the interview was to complete while juggling the AI.\n\nBelow, you can view one interviewee stumbling through their time complexity analysis after giving a perfect answer to an interview question. The interviewer is confused as the interviewee scrambles to explain how they got to their incorrect time complexity (secretly provided by ChatGPT).\n\nWhile no one was caught during the study, their cameras were off, and cheating was still difficult for many of our skilled candidates, as evidenced by this clip.\n\nEthics aside, cheating is difficult, stressful, and not entirely straightforward to implement. Instead, we advise investing that effort into practice, which will serve you well once companies change their processes, which hopefully should be soon. Ultimately, we hope the advent of ChatGPT will be the catalyst that finally moves our industry’s interview standards away from grinding and memorizing to actually testing engineering ability.\n\n![Author avatar](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F15_2024_03_08_Mroczka_Headshots_Ray_Glaser_Photography_DSC_3702_81cf562c2c.jpg&w=384&q=75 \"Mike Mroczka\")\n\nMike Mroczka\n\nMike Mroczka, a former senior SWE (Google, Salesforce, GE), is the primary author of [Beyond Cracking the Coding Interview](https://www.beyondctci.com/)—the official sequel to Gayle McDowell's original CTCI. He works as a tech consultant and has a decade of experience helping engineers land their dream jobs. He’s a top-rated mentor (interviewing.io, Karat, Pathrise, Skilled.inc) and the author of viral technical content on system design and technical interview strategies featured on HackerNews, Business Insider, and Wired. He also sometimes writes technical content for interviewing.io (like this piece) and was one of the authors of interviewing.io’s [A Senior Engineer's Guide to the System Design Interview](https://interviewing.io/guides/system-design-interview).\n\nYou can find him online at [mikemroczka.com](https://www.mikemroczka.com/), [LinkedIn](https://www.linkedin.com/in/michael-mroczka/), and [X](https://x.com/mike_mroczka).\n\n*Special thanks to Dwight Gunning and Liz Graves for their help with this experiment. And of course a big thank you to all the interviewees and interviewers who participated!*\n\nFootnotes:\n----------\n\nFootnotes\n---------\n\n1. To be an interviewer on our platform, you have to have at least 4 years of experience and have conducted at least 20 interviews on behalf of a FAANG or FAANG-adjacent company). [↩](#user-content-fnref-1)\n2. Five interviews needed to be removed because they did not meaningfully use ChatGPT. In two instances, the interviewee was familiar with the question and chose to solve the problem themselves. In one interview, the interviewee wanted to just try the question on their own and didn't prompt ChatGPT, ignoring our instructions. The last two interviews were \"custom\" interview questions that were problematic for reasons we’ll outline later in this article. [↩](#user-content-fnref-2)\n3. This is a higher passthrough rate than you'd see in the wild. We think it comes down to two factors: selection bias and pre-interview prep. The users who invest in interview prep are a specific, self-selected slice of the population. Moreover, many of our users practice on their own before practicing with a human. [↩](#user-content-fnref-3)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/how-hard-is-it-to-cheat-with-chatgpt-in-technical-interviews",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Impostor syndrome strikes men just as hard as women... and other findings from thousands of technical interviews",
      "content": "The modern technical interview is a rite of passage for software engineers and (hopefully!) the precursor to a great job. But it’s also a huge source of stress and endless questions for new candidates. Just searching “how do I prepare for a technical interview” turns up millions of Medium posts, coding bootcamp blogs, Quora discussions, and entire books.\n\nDespite all this conversation, people struggle to know how they’re even doing in interviews. [In a previous post](https://interviewing.io/blog/own-interview-performance), we found that a surprisingly large number of interviewing.io’s users consistently underestimate their performance, making them more likely to drop out of the process and ultimately harder to hire. Now, and with considerably more data (over 10k interviews led by real software engineers!), we wanted to go deeper: **what seems to make candidates worse at gauging their own performance?**\n\nWe know some general facts that make accuracy a challenge: people aren’t always great at assessing or even remembering their performance on difficult cognitive tasks like writing code.[1](#user-content-fn-1) Technical interviews can be particularly hard to judge if candidates don’t have much experience with questions with no single right answer. Since many companies don’t share any kind of detailed post-interview feedback (beyond a yes/no) with candidates for liability reasons, many folks never get any sense of how they did, what they did well, or what could have been better.[2](#user-content-fn-2),[3](#user-content-fn-3) Indeed, pulling back the curtain on interviewing, *across the industry,* was one of the primary motivators for building interviewing.io!\n\nBut to our knowledge there’s little data out there looking specifically at how people feel after real interviews on this scale, across different companies–so we gathered it, giving us the ability to test interesting industry assumptions about engineers and coding confidence.\n\nOne big factor we were interested in was **impostor syndrome**. Impostor syndrome resonates with a lot of engineers[4](#user-content-fn-4), indicating that many wonder whether they truly match up to colleagues and discount even strong evidence of competence as a fluke. Impostor syndrome can make us wonder whether we can count on the positive performance feedback that we’re getting, and how much our opportunities have come from our own effort, versus luck. Of particular interest to us was whether this would show up for women on our platform. There’s a lot of research evidence that candidates from underrepresented backgrounds experience a greater lack of belonging that feeds impostor syndrome[5](#user-content-fn-5), and this could show up as inaccuracy about judging your own interview performance.\n\nThe setup\n---------\n\ninterviewing.io is a platform where people can practice technical interviewing anonymously, and if things go well, get jobs at top companies in the process. We started it because resumes suck and because we believe that anyone, regardless of how they look on paper, should have the opportunity to prove their mettle.\n\nWhen an interviewer and an interviewee match on interviewing.io, they meet in a collaborative coding environment with voice, text chat, and a whiteboard and jump right into a technical question (feel free to [watch this process in action on our interview recordings page](https://interviewing.io/mocks)). After each interview, people leave one another feedback, and each party can see what the other person said about them once they both submit their reviews.\n\nHere’s an example of an interviewer feedback form:\n\n![Screenshot of the Interviewing.io interviewer feedback form highlight the question: How were their technical skills?](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F9fdaa_new_interviewer_feedback_circled_8e5be18c42.png&w=750&q=75 \"How were their technical skills?\")\n\nFeedback form for interviewers\n\nImmediately after the interview, candidates answered a question about how well they thought they’d done on the same 1-4 scale:\n\n![Screenshot of the Interviewing.io interviewee feedback form highlight the question: How well do you think you did?](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F15c64_new_interviewee_feedback_circled_edb300c741.png&w=640&q=75 \"How well do you think you did?\")\n\nFeedback form for interviewees\n\nFor this post, we looked at over 10k technical interviews led by real software engineers from top companies. In each interview, a candidate was rated by an interviewer on their problem-solving ability, technical ability, and communication skills, as well as whether the interviewer would advance them to the next round. This gave us a measure of how different someone’s self-rating was from the rating that the interviewer actually gave them, and in which direction. In other words, how skewed was their estimation from their true performance?\n\nGoing in, we had some hunches about what might matter:\n\n* **Gender.** Would women be harder on their coding performance than men?\n* **Having been an interviewer before.** It seems reasonable that having been on the other side will pull back the curtain on interviews.\n* **Being employed at a top company.** Similar to above.\n* **Being a top-performing interviewee** on interviewing.io — people who are better interviewees overall might have more confidence and awareness of when they’ve gotten things right (or wrong!)\n* **Being in the Bay Area** or not. Since tech is still so geographically centered on the Bay Area, we considered that folks who live in a more engineering-saturated culture could have greater familiarity with professional norms around interviews.\n* **Within the interview itself, question quality and interviewer quality.** Presumably, a better interviewer is also a better communicator, whereas a confusing interviewer might throw off a candidates’ entire assessment of their performance. We also looked at whether it was a practice interview, or for a specific company role.\n* For some candidates, we could also look at few measures of their **personal brand** within the industry, like their number of GitHub and Twitter followers. Maybe people with a strong online presence are more sure of themselves when they interview?\n\nSo what did we find?\n--------------------\n\n### Women are just as accurate as men at assessing their technical ability\n\nContrary to expectations around gender and confidence, we *didn’t* find a reliable statistically significant gender difference in accuracy. At first, it looked like female candidates were more likely to underestimate their performance, but when we controlled for other variables, like experience and rated technical ability, it turned out **the key differentiator was experience.** More experienced engineers are more accurate about their interview performance, and men are more likely to be experienced engineers, but experienced female engineers are just as accurate about their technical ability.\n\nBased on previous research, we hypothesized that impostor syndrome and a greater lack of belonging could result in female candidates penalizing their interview performance, but we didn’t find that pattern[6](#user-content-fn-6). However, our finding echoes [a research project from the Stanford Clayman Institute for Gender Research](https://gender.stanford.edu/sites/gender/files/climbing_the_technical_ladder.pdf), which looked at 1,795 mid-level tech workers from high tech companies. They found that women in tech aren’t necessarily less accurate when assessing their own abilities, but do have significantly different ideas about what success requires (e.g., long working hours and risk-taking). In other words, **women in tech may not doubt their own abilities but might have different ideas about what’s expected**. [And a survey from Harvard Business Review](https://hbr.org/2014/08/why-women-dont-apply-for-jobs-unless-theyre-100-qualified)  asking over a thousand professionals about their job application decisions also made this point. Their results emphasized that gender gaps in evaluation scenarios could be more about **different expectations for how scenarios like interviews are judged.**\n\nThat said, we did find one interesting difference: women went through fewer practice interviews overall than men did. The difference was small but statistically significant, and harkens back to our earlier finding that women leave interviewing.io roughly 7 times as often as men do, after a bad interview.\n\nBut in that same earlier post, we also found that masking voices didn’t impact interview outcomes. This whole cluster of findings affirms what we suspected and what the folks doing [in-depth studies of gender in tech](https://gender.stanford.edu/sites/gender/files/climbing_the_technical_ladder.pdf) have found: **it’s complicated.** Women’s lack of persistence in interviews can’t be explained only by impostor syndrome about their *own* abilities, but it’s still likely that they’re interpreting negative feedback more severely and making different assumptions about interviews.\n\nHere’s the distribution of accuracy distance for both female and male candidates on our platform (zero indicates a rating that matches the interviewer’s score, while negative values indicate underestimated score, and positive values indicate an overestimated score). The two groups look pretty much identical:\n\n### What else didn’t matter?\n\nAnother surprise: **having been an interviewer didn’t help**. Even people who had been interviewers themselves don’t seem to get an accuracy boost from that. **Personal brand was another non-finding**. **People with more GitHub followers weren’t more accurate** than people with few to no GitHub followers. **Nor did interviewer rating matter** (i.e. how well an interviewer was reviewed by their candidates), although to be fair, interviewers are generally rated quite highly on the site.\n\n### So what was a statistically significant boost to accurate judgments of interview performance? Mostly, experience.\n\nExperienced engineers have a better sense for how well they did in interviews, compared with engineers earlier in their careers.[7](#user-content-fn-7) But it doesn’t seem to *just* be that you’re better at gauging your interview performance because you’re better at writing code; although there is a small lift from this, with higher rated engineers being more accurate. But when you look at junior engineers, **even top-performing junior candidates struggled to accurately assess their performance**.[8](#user-content-fn-8)\n\nOur data mirrors a trend seen in [Stack Overflow’s 2018 Developer survey](https://insights.stackoverflow.com/survey/2018#connection-and-competition). They asked respondents several questions about confidence and competition with other developers, and noted that more experienced engineers feel less competitive and more confident.[9](#user-content-fn-9) This isn’t necessarily surprising: experience is correlated with skill level, after all, and highly skilled people are likely to be more confident. But our analysis let us control for performance and code skill within career groups, and we *still* found that experienced engineers were better at predicting their interview scores. There are probably multiple factors here: experienced engineers have been through more interviews, have led interviews themselves, and have a stronger sense of belonging, all of which may combat impostor syndrome.\n\n**Insider knowledge and context also seems to help:** Being in the Bay Area and being at a top company both made people more accurate. Like the experienced career group, engineers who seem more likely to have *contextual industry knowledge* are also more accurate. We found small but statistically significant lifts from factors like being located in the Bay Area and working at a top company. However, the lift from working at a top company seems to mostly measure a lift from overall technical ability: being at a top company is essentially a proxy measure for being a more experienced, higher quality engineer.\n\n**Finally, as you get better at interviewing and move into company interviews, you do get more accurate.** People were more accurate about their performance in company interviews compared to practice interviews, and their overall ranking on the interviewing.io site also predicted improved accuracy: interviewing.io also gives users an overall ranking, based on their performance over multiple interviews and weighted toward more recent measures. People who scored in the top 25% were more likely to be accurate about their interview performance.\n\nIn general, how are people at gauging their interview performance overall? [We’ve looked at this before](https://interviewing.io/blog/own-interview-performance), with roughly a thousand interviews, and now, with ten thousand, the finding continues to hold up. Candidates were accurate about how they did in only 46% of interviews, and underestimated themselves in 35% of interviews (and the remaining 19%, of course, are the overestimators). Still, candidates are generally on the right track — it’s not like people who score a 4 are always giving themselves a 1.[10](#user-content-fn-10) Self-ratings *are* statistically significantly predictive for actual interview scores (and positively correlated), but that relationship is noisy.\n\nThe implications\n----------------\n\nAccurately judging your own interview performance is a skill in its own right and one that engineers need to learn from experience and context in the tech industry. But we’ve also learned that **many of the assumptions we made about performance accuracy didn’t hold up to scrutiny —** female engineers had just as accurate a view of their own skills as male ones, and engineers who had led more interviews or were well known on GitHub weren’t particularly better at gauging their performance.\n\nWhat does this mean for the industry as a whole? First off, impostor syndrome appears to be the bleary-eyed monster that attacks across gender ability, and how good you are, or where you are, or how famous you are isn’t that important. Seniority does help mitigate some of the pain, but impostor syndrome affects everyone, regardless of who they are or where they’re from. So, maybe it’s time for a kinder, more empathetic interviewing culture. And a culture that’s kinder to everyone, because though marginalized groups who haven’t been socialized in technical interviewing are [hit the hardest by shortcomings in the interview process](https://interviewing.io/blog/you-cant-fix-diversity-in-tech-without-fixing-the-technical-interview), no one is immune to self-doubt.\n\nWe’ve previously discussed what makes someone a good interviewer, and [empathy plays a disproportionately large role](https://interviewing.io/blog/best-technical-interviews-common). And we’ve seen that [providing immediate post-interview feedback is really important for keeping candidates from dropping out](https://interviewing.io/blog/own-interview-performance). So, whether you’re motivated by kindness and ideology or cold, hard pragmatism, a bit more kindness and understanding toward your candidates is in order.\n\n*[Cat Hicks](https://www.drcathicks.com/), the author of this guest post, is a researcher and data scientist with a focus on learning. She’s published empirical research on learning environments, and led research on the cognitive work of engineering teams at Google and Travr.se. She holds a PhD in Psychology from UC San Diego.*\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. Self-assessment has been explored in a number of domains, and often used to measure learning. One important criticism is that it’s highly impacted by people’s motivation and emotional state at the time of asking. See: Sitzmann, T., Ely, K., Brown, K. G., & Bauer, K. N. (2010). Self-assessment of knowledge: A cognitive learning or affective measure?. *Academy of Management Learning & Education*, *9*(2), 169-191. [↩](#user-content-fnref-1)\n2. Designing a good technical interview is no small task on the interviewer side. For an informal discussion of this, [see this post.](https://erikbern.com/2018/05/02/interviewing-is-a-noisy-prediction-problem.html)  [↩](#user-content-fnref-2)\n3. For some anecdotal conversation about interview self-assessment, [see this one](https://www.gayle.com/blog/2011/03/31/why-your-interview-performance-is-impossible-to-judge). [↩](#user-content-fnref-3)\n4. E.g., [this article](https://hired.com/blog/candidates/truth-imposter-syndrome-tech-workers/) and [this one.](https://www.cnet.com/science/tech-employees-likely-to-suffer-from-impostor-syndrome/) [↩](#user-content-fnref-4)\n5. Some examples of further reading in social science research: Good, C., Rattan, A., & Dweck, C. S. (2012). Why do women opt out? Sense of belonging and women’s representation in mathematics. *Journal of personality and social psychology*, *102*(4), 700. Master, A., Cheryan, S., & Meltzoff, A. N. (2016). Computing whether she belongs: Stereotypes undermine girls’ interest and sense of belonging in computer science. *Journal of Educational Psychology*, *108*(3), 424. [↩](#user-content-fnref-5)\n6. One complication for our dataset is the representation of experienced female engineers: we simply didn’t have very many, which is true to the demographics of the tech industry, but also means that selection biases in the small group of experienced female engineers we do have are more likely to be present, and this isn’t the be-all and end-all of exploring for group differences. We’d like to continue looking at interviews with female participants to explore this fully. [↩](#user-content-fnref-6)\n7. These effects and the previous non-findings were all explored in a linear mixed model. Significant results for the individual effects are all p < .05 [↩](#user-content-fnref-7)\n8. Experienced engineers have an average skew of -.14; Junior engineers have an average skew of -.22, New Grads have an average skew of -.25. [↩](#user-content-fnref-8)\n9. See also: <https://www.dice.com/career-advice/imposter-syndrome-tech-pros-age> [↩](#user-content-fnref-9)\n10. Another wrinkle with the design behind this data is that there’s a floor and a ceiling on the scale: people who always score a 4, for example, *can’t* ever overrate themselves, because they’re already at the top of the scale. We dealt with this a couple of ways: by excluding people at the floor and ceiling and re-running analyses on the middle subset, and by binning skew into either accurate or not and looking at that. The findings hold up across this. [↩](#user-content-fnref-10)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/impostor-syndrome-strikes-men-just-as-hard-as-women",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Engineers can't gauge their own interview performance. And that makes them harder to hire.",
      "content": "[interviewing.io](https://interviewing.io/) is an anonymous technical interviewing platform. We started it because [resumes suck](https://blog.alinelerner.com/resumes-suck-heres-the-data/) and because we believe that anyone, regardless of how they look on paper, should have the opportunity to prove their mettle. **In the past few months, we’ve amassed over 600 technical interviews along with their associated data and metadata.** Interview questions tend to fall into the category of what you’d encounter at a phone screen for a back-end software engineering role at a top company, and interviewers typically come from a mix of larger companies like Google, Facebook, and Twitter, as well as engineering-focused startups like Asana, Mattermark, KeepSafe, and more.\n\nOver the course of the next few posts, we’ll be sharing some { unexpected, horrifying, amusing, ultimately encouraging } things we’ve learned. **In this blog’s heroic maiden voyage, we’ll be tackling people’s surprising inability to gauge their own interview performance and the very real implications this finding has for hiring.**\n\nFirst, a bit about setup\n------------------------\n\nWhen an interviewer and an interviewee match on our platform, they meet in a collaborative coding environment with voice, text chat, and a whiteboard and jump right into a technical question. After each interview, people leave one another feedback, and each party can see what the other person said about them once they both submit their reviews. If both people find each other competent and pleasant, they have the option to unmask. **Overall, interviewees tend to do quite well on the platform, with just under half of interviews resulting in a “yes” from the interviewer.**\n\nIf you’re curious, we have a few [public recordings](https://interviewing.io/mocks) of interviews done on the platform, so you can watch and see what an interview is really like. In addition to these, our feedback forms are attached below. There is one direct yes/no question, and we also ask about a few different aspects of interview performance using a 1-4 scale. We also ask interviewees some extra questions that we don’t share with their interviewers, and one of those questions is about how well they think they did. **In this post, we’ll be focusing on the technical score an interviewer gives an interviewee and the interviewee’s self-assessment (both are circled below).** For context, a technical score of 3 or above seems to be the rough cut-off for hirability.\n\n![](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F49613_interviewer_feedback_2025030f47.webp%3Fupdated_at%3D2022-12-14T11%3A48%3A18.080Z&w=1920&q=75 \"Feedback form for interviewers\")\n\nFeedback form for interviewers\n\n\n\n![](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F8ddad_interviewee_feedback_fd66b6f2fd.webp%3Fupdated_at%3D2022-12-14T11%3A48%3A18.267Z&w=1920&q=75 \"Feedback form for interviewees\")\n\nFeedback form for interviewees\n\nPerceived versus actual performance\n-----------------------------------\n\nBelow, you can see the distribution of people’s actual technical performance (as rated by their interviewers) and the distribution of their perceived performance (how they rated themselves) for the same set of interviews.[1](#user-content-fn-1)\n\nYou might notice right away that there is a little bit of disparity, but things get interesting when you plot perceived vs. actual performance *for each interview*. Below, is a heatmap of the data where the darker areas represent higher interview concentration. For instance, the darkest square represents interviews where both perceived and actual performance was rated as a 3. You can hover over each square to see the exact interview count (denoted by “z”).\n\nIf you run a regression on this data[2](#user-content-fn-2), you get an R-squared of only 0.24, and once you take away the worst interviews, it drops down even further to a 0.16. For context, [R-squared](https://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit) is a measurement of how well you can fit empirical data to some mathematical model. It’s on a scale from 0 to 1 with 0 meaning that everything is noise and 1 meaning that everything fits perfectly. **In other words, even though some small positive relationship between actual and perceived performance does exist, it is not a strong, predictable correspondence.**\n\nYou can also see there’s a non-trivial amount of impostor syndrome going on in the graph above, which probably comes as no surprise to anyone who’s been an engineer.\n\nGayle Laakmann McDowell of *Cracking the Coding Interview* fame has written quite a bit about [how bad people are at gauging their own interview performance](https://www.gayle.com/blog/2011/03/31/why-your-interview-performance-is-impossible-to-judge), and it’s something that I had noticed anecdotally when I was doing recruiting, so it was nice to see some empirical data on that front. In her writing, Gayle mentions that it’s the job of a good interviewer to make you feel like you did OK even if you bombed. I was curious about whether that’s what was going on here, but when I ran the numbers, there wasn’t any relationship between how highly an interviewer was rated overall and how off their interviewees’ self-assessments were, in one direction or the other.\n\nUltimately, this isn’t a big data set, and we will continue to monitor the relationship between perceived and actual performance as we host more interviews, but we did find that this relationship emerged very early on and has continued to persist with more and more interviews — R-squared has never exceeded 0.26 to date.\n\nWhy this matters for hiring\n---------------------------\n\nNow here’s the actionable and kind of messed up part. As you recall, during the feedback step that happens after each interview, we ask interviewees if they’d want to work with their interviewer. **As it turns out, there’s a very statistically significant relationship** (*[p](https://en.wikipedia.org/wiki/P-value)* < 0.0008) **between whether people think they did well and whether they’d want to work with the interviewer. This means that when people think they did poorly, they may be a lot less likely to want to work with you**[3](#user-content-fn-3). And by extension, it means that in every interview cycle, some portion of interviewees are losing interest in joining your company just because they didn’t think they did well, *despite the fact that they actually did*.\n\n**How can one mitigate these losses? Give positive, actionable feedback immediately (or as soon as possible)!** This way people don’t have time to go through the self-flagellation gauntlet that happens after a perceived poor performance, followed by the inevitable rationalization that they *totally* didn’t want to work there anyway.\n\n*Lastly, a quick shout-out to [Statwing](https://www.statwing.com) and [Plotly](https://plotly.com/) for making terrific data analysis and graphing tools respectively.*\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. There are only 254 interviews represented here because not all interviews in our data set had comprehensive, mutual feedback. Moreover, we realize that raw scores don’t tell the whole story and will be focusing on standardization of these scores and the resulting rat’s nest in our next post. That said, though interviewer strictness does vary, we gate interviewers pretty heavily based on their background and experience, so the overall bar is high and comparable to what you’d find at a good company in the wild. [↩](#user-content-fnref-1)\n2. Here we are referring to linear regression, and though we tried fitting a number of different curves to the data, they all sucked. [↩](#user-content-fnref-2)\n3. In our data, people were 3 times less likely to want to work with their interviewers when they thought they did poorly. [↩](#user-content-fnref-3)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/people-cant-gauge-their-own-interview-performance-and-that-makes-them-harder-to-hire",
      "author": "",
      "user_id": ""
    },
    {
      "title": "How much have 2022 layoffs affected engineers vs. other departments? We dug into the data to find out.",
      "content": "Over the past few months, I’ve seen a number of fear-mongering pieces in the press about how the recession is driving tech layoffs and how tech employees (and engineers specifically) are losing their leverage as a result. Here are a few recent examples:\n\n* [The balance of power is shifting in the tech industry](https://www.cnn.com/2022/07/07/tech/tech-layoffs-workers-silicon-valley/index.html) (CNN)\n* [The hot market for tech employees is ending, giving the power back to companies like Meta and Google to set and reduce pay](https://www.businessinsider.com/tech-layoffs-salary-cuts-workers-2022-8) (Business Insider)\n* [Snap’s Web3 team among 20% workforce reduction after first single-digit-growth quarter since going public](https://fortune.com/crypto/2022/09/01/snap-shutters-web3-team-layoffs-tech-evan-spiegel/) (Fortune)\n\nArticles like these drive engineers to speculate [on Reddit](https://www.reddit.com/r/ExperiencedDevs/comments/um58oq/have_any_engineers_been_affected_by_recent_layoffs/), in the internet equivalent of hushed whispers, about whether they’re next. Unfortunately, none of the pieces I’ve seen have bothered to draw a distinction between “tech” and “software engineering,” and many even use these terms interchangeably (like the CNN piece above).\n\nThe problem is that “tech” can mean anyone working at a tech company. You’re an engineer? Of course, you’re tech. You do ops? Great, you’re tech. You do marketing? You, too, are tech! These are all critical roles, and what I take umbrage with isn’t the decision to label non-engineers as tech employees. It’s deliberately misleading your audience by implying that “tech” refers to engineers specifically.\n\nI don’t like imprecision, and I *really* don’t like fear-mongering. So, we at interviewing.io decided to dig into the data to see if engineers do indeed have a reason to fear. The TL;DR is that while, yes, tech layoffs have affected engineers somewhat, they are one of the least affected departments. While the existence of layoffs, in concert with eng hiring slowing down in general, does mean that engineers are losing some leverage in the market, it’s nowhere near as dire as the press makes it out to be.\n\nThe best source for layoff data\n-------------------------------\n\nAt some point in the last 2.5 years, you’ve probably visited layoffs.fyi. It was launched by [Roger Lee](https://www.rogerlee.com/) in February 2020, just when concerns that this COVID-19 thing *might* affect the economy went mainstream. The site does exactly what it sounds like – it tracks layoffs at tech companies. Every time a company conducts a public round of layoffs, it gets added to a growing list. Each entry includes a layoff count, and a small subset of entries include a list of actual people who were let go, as well as some info about them (name, LinkedIn, geography, title, and so on). These layoff lists are usually in a Google doc but sometimes in something more fancy, like this [dedicated website for Shopify layoffs](https://weworkedatshop.com).\n\n![Table of companies and number of people laid off](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FLayoff_lists_on_layoffs_fyi_1_2ad6abbed9.png&w=1920&q=75 \"# of people laid off by company\")\n\nAs far as I know, even with its limitations, layoffs.fyi is the best source for layoff data, and because the individual layoff lists actually break out layoffs by function, it’s the natural place to go if you want to disambiguate “tech” into specific departments.\n\nSo that’s where we looked. Before we talk about what we did, I’d like to call out one big limitation of the layoff lists on layoffs.fyi.\n\nOne big limitation of the data: layoff lists are opt-in\n-------------------------------------------------------\n\nFrom what we can tell, the layoff lists on layoffs.fyi are opt-in. In other words, former employees decide if they want to be included. Sometimes the difference between the layoff count reported in the press and the length of the list is substantial. Peloton, for instance, had 2800 layoffs, but their list only had 400 people.\n\nIf job losers’ willingness to opt in depends on their role, they might be overrepresented in the layoff data. Recruiters, for instance, might be more willing to sign on since they know the value of getting their names out there. Our fix is to calculate opt-in rates for each role and use these as weights in the analysis – just like when the Census upweights groups who are less likely to respond to a survey. For more detail on how we did that, please see the section called “Appendix: Methods” below.\n\nThe questions we wanted to answer\n---------------------------------\n\nTo disambiguate “tech layoffs” into actual departments and break out how each department was affected, we needed to answer the following questions:\n\n1. **What % of employees got laid off overall?**\n2. **What’s the breakdown of layoffs by department?**\n3. **How hard was each department hit?**\n\nIn the next section, you can see what we learned. If you’re curious how we got to those results, see the section called “Appendix: Methods” at the bottom of the post.\n\nThe results, or layoffs by the numbers\n--------------------------------------\n\n### What % of employees got laid off overall?\n\nBased on the companies we looked at, at companies where layoffs happened, about 19% of employees were laid off.\n\n### What’s the breakdown of layoffs by department?\n\nBelow, you can see the % of total layoffs that each department constituted. In other words, of the layoffs that happened in 2022, 20% were salespeople, 5% were engineers, 3% were finance, and so on … and that a third of newly unemployed tech workers come from sales and product.\n\n![Chart showing % of total layoffs by department in 2022](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fof_total_layoffs_each_department_constituted_in_2022_68b8b4e6d6.png&w=1920&q=75 \"Layoffs by department\")\n\n### How hard was each department hit?\n\nThe graph above doesn’t give the risk of getting laid off, however. All departments don’t have the same headcount. For instance, according to LinkedIn data, engineers make up about 20% of a company’s total headcount on average, whereas recruiters only make up 2%, about 10X less. In other words, if each department were to be hit equally, we’d expect engineers to be laid off about 10X more often than recruiters. That’s clearly not what’s going on.\n\nTo get the full picture, we need to correct for department size and see how hard each department was actually hit, which you can see in the graph below.\n\n![Chart showing % of workers laid off, by department](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F2022_layoffs_department_007b58958a.png&w=1920&q=75 \"Percentage of workers laid off, by department\")\n\nThese results show exactly why using the label “tech” isn’t good enough – there is a big disparity between different departments. According to our data, almost *half* of HR people and recruiters got laid off, as compared to 10% of engineers and only 4% of salespeople. The salespeople percentage shows the need to correct for department size, given that salespeople constituted the biggest % of overall layoffs.\n\nWhat does this mean for engineers?\n----------------------------------\n\nAlthough engineers didn’t emerge from this year’s layoffs unscathed, our data shows that lumping everyone together under the “tech” umbrella is misleading. These insights don’t change the fact that the people who lost their jobs, regardless of department, are hurting, and, unfortunately, layoffs are likely far from over.\n\nThat said, HR teams and recruiters were hit the hardest by far. Sadly these teams are often the proverbial canary in the coal mine – when a recession hits, hiring, and especially outbound hiring, is the first to get cut. Engineering, marketing, product, and design were all affected somewhat, and per capita, sales was affected least.\n\nSo what does this all mean for engineers? In our minds, the kind of anxiety produced by the news articles I referenced in the beginning is unjustified. Moreover, though hiring has clearly slowed down in the past quarter, after surveying our users, we saw that [a healthy amount of companies are still hiring engineers](https://interviewing.io/blog/companies-hiring-engineers-2022).\n\nHowever, the reality is that engineers are losing some of the leverage they had previously enjoyed. Given this climate, it’s reasonable to expect that more engineers will be competing for fewer open positions than before, which means that:\n\n1. Candidate experience is going to degrade somewhat. Why? Sadly, companies caring about candidate experience are not motivated by doing the right thing. Rather, it’s largely a function of how much leverage labor has in the market. As such, we’re likely to see…\n   * The return of homework assignments and coding quizzes early on in the process, even for senior candidates. When good candidates don’t drop out of your process despite having to jump through hoops, the hoops, which are cheaper than interviewing with a human, will return.\n   * Interview questions will degrade in quality – when selling in an interview process is less important, companies will put less effort into designing bespoke questions that give candidates a preview of what it’s like to work there and will fall back to doing the easy thing, which is probably leaning more on cookie-cutter Leetcode questions.\n2. The bar will go up. Because more candidates are competing for fewer positions, companies will be able to get more picky while still hitting their hiring goals.\n\n<https://interviewing.io/> is both a mock interview platform and an eng hiring marketplace – engineers use us for [technical interview practice](https://interviewing.io/mocks), and top performers get fast-tracked at companies. Companies actually interview our top performers anonymously, right on our platform, and leave feedback after each interview. If the candidate passes, they unmask and move to the next step (typically an onsite). Feedback is both quantitative and qualitative, and in addition to telling us if the candidate passed, companies also rate them on technical ability, communication ability, and problem solving ability. Technical ability tends to be most predictive and weighed the most heavily.\n\nAs such, to check our read on the market, we averaged the technical scores for successful interviews over the last few quarters to see where the bar has been and where it is now. The results are… compelling.\n\n![Chart showing how the engineering 'bar', in real interviews on interviewing.io, has risen over time](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FThe_engineering_bar_over_time_on_interviewing_io_1_cb7162b724.png&w=1920&q=75 \"The engineering bar\")\n\nAppendix: Methods\n-----------------\n\nBelow is the list of all companies (either US-based or with a significant number of US employees) with layoff lists, through 7/26/22 with Snap (8/31/22) added on. Sadly, there have been more layoffs in the last few months, but at some point we made the call to stop adding new ones so we could actually get this post published. That said, we made the call to include Snap (8/31/22) as a late addition because they had enough layoffs to potentially change the results. Based on layoffs.fyi data, these companies comprise about 15% of tech companies who employed people in the U.S. and had at least one round of layoffs in 2022.\n\n| Date | Company |\n| --- | --- |\n| 8/31 | Snap |\n| 7/26 | Shopify |\n| 7/20 | Capsule |\n| 7/19 | Olive |\n| 7/14 | The Mom Project |\n| 7/13 | Tonal |\n| 7/7 | Cedar |\n| 7/7 | Next Insurance |\n| 6/24 | Feather |\n| 6/23 | Netflix |\n| 6/22 | MasterClass |\n| 6/21 | Tray.io |\n| 6/16 | JOKR |\n| 6/15 | Weee! |\n| 6/14 | Redfin |\n| 6/14 | Coinbase |\n| 6/13 | Studio |\n| 6/13 | Automox |\n| 6/9 | Stitch Fix |\n| 6/7 | ID.me |\n| 6/6 | Dutchie |\n| 6/3 | Coterie |\n| 6/3 | Policygenius |\n| 5/13 | Replicated |\n| 5/31 | Tomo |\n| 5/31 | BookClub |\n| 5/27 | Terminus |\n| 5/25 | Bolt |\n| 5/25 | PeerStreet |\n| 5/23 | Klarna |\n| 5/12 | Section4 |\n| 5/4 | Cameo |\n| 5/4 | Colossus |\n| 5/4 | Ideoclick |\n| 4/27 | Robinhood |\n| 4/25 | Clyde |\n\nFor each of the companies above, we did the following to get layoff counts by department:\n\n1. Used LinkedIn to get the total number of U.S. employees at each company. In cases where a company had both U.S. and non-U.S. employees (e.g. Shopify), we looked at what portion of total employees were located in the U.S. and then adjusted the layoff count proportionately.\n2. Went through each company’s layoff list and tagged all employees with their department. We included the following departments: Software Engineering, Data Science & Analysis, Product & Design, Marketing, Sales/Account Management/Customer Success, Recruiting, HR (Non-Recruiting), Finance, and Operations.\n3. Because layoff lists are opt-in (see the section above called “One big limitation of the data: layoff lists are opt-in” for more detail), we needed another source of truth, and LinkedIn seemed like the best bet. As such, for each department:\n   * For a set of representative companies, we searched LinkedIn for the list of people who had left the company in 2022. We chose a subset of companies that included public companies, large startups, and small startups.\n   * We compared that count to the count for that department from (2) and used the difference for each department as a multiplier on the number of people in (2). When we did this, we ended up with the numbers below – each bar is our calculated probability that someone from this department would actually opt in to be included on a layoff list.\n\n![Chart showing the likelihood of people listing themselves on a layoff list, by department](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FLikelihood_of_people_listing_themselves_on_a_layoff_list_by_department_de35c9da76.png&w=1920&q=75 \"How likely is it that people self-report being laid off?\")\n\nOnce we had layoff counts by department, we also needed to know how many people worked in each department so we could figure out how hard each department was hit. For instance, Stitch Fix has about 4500 employees in the U.S. Of those, 238 are engineers. Therefore, at Stitch Fix, engineers comprise about 5% of employees. To do that, we used the same representative subset of companies as above and cross-referenced LinkedIn to see how many people worked in each department and what % of total headcount that department constituted. After we got all the numbers for the engineering department at our slice of companies, we saw that, on average, engineers constitute about 11% of employees. We repeated this process for each department. This is important information – without knowing the relative size of each department, we wouldn’t be able to reason about how each department was affected.\n\n*Thank you to Sam Jordan, Liz Graves, Santiago Munoz, the broader interviewing.io team, David Holtz, and Maxim Massenkoff for their help with this piece.*",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/2022-layoffs-engineers-vs-other-departments",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Our business depends on having the best interviewers, so we built an interviewer rating system. And you can too.",
      "content": "interviewing.io is an anonymous mock interview platform and eng hiring marketplace. Engineers use us for mock interviews, and we use the data from those interviews to surface top performers, in a much fairer and more predictive way than a resume. If you’re a top performer on interviewing.io, we fast-track you at the world’s best companies.\n\nWe make money in two ways: engineers pay us for mock interviews, and employers pay us for access to the best performers. Companies connect with our engineers through anonymous technical interviews — this is really important to us because we don’t want employers to be biased by where people went to school or where they previously worked. When employers use interviewing.io, they get great candidates, on the condition that they’re willing to talk to them anonymously. And it really is anonymous – they get zero information about the candidates ahead of time, and they agree to devote precious engineering time to interviewing people who are, from their perspective, basically randos off the internet.\n\nTo keep our engineer customers happy, we have to make sure that our interviewers deliver value to them by conducting realistic mock interviews and giving useful, actionable feedback afterwards.\n\nTo keep our employer customers happy, we have to make sure that the engineers we send them are way better than the ones they’re getting without us[1](#user-content-fn-1). Otherwise, it’s just not worth it for them.\n\n**This means that we live and die by the quality of our interviewers, in a way that no single employer does, no matter how much they say they care about people analytics or interviewer metrics or training. If we don’t have really well-calibrated interviewers, who also create great candidate experience, we don’t get paid.**\n\nFortunately, we have two pieces of data that no single employer can collect: 1) honest candidate feedback and 2) real outcomes for candidates, from multiple interviews at multiple companies.\n\nThe former helps us figure out how effective our interviewers are at engaging candidates and creating great candidate experience. The latter helps us figure out how well-calibrated our interviewers are, with a high degree of confidence.\n\nOver time, as we’ve used these metrics, our interviewer quality has gone way up.\n\nIn this post, we’ll explain exactly how we compute and use these metrics to get the best work out of our interviewers. In a [follow-up post](https://interviewing.io/blog/we-have-the-best-technical-interviewers-heres-how-we-do-it), we’ll talk about how to bring some of these approaches and metrics to your own interview process.\n\nBefore we get into all that, I’d like to provide some context about why these metrics are important, and why you can’t assume that just because an interviewer comes from a great company, they’ll be great at conducting interviews.\n\nInterviewer quality generally sucks and isn’t something companies really care about\n-----------------------------------------------------------------------------------\n\nWe host thousands of interviews a month, and that means we need a lot of interviewers. But, as you saw above, we live and die by interviewer quality, so we need those interviewers to be world-class.\n\nWhen I first started interviewing.io, I assumed that if we were very picky about whom we let be an interviewer, then all of our interviewers would be great. These are the criteria we came up with and still use today:\n\n* Engineers from FAANG and FAANG-adjacent companies (e.g., Uber, Stripe, Dropbox)\n* At least 4 years of experience (though today the average on our platform is 8)\n* Conducted at least 20 interviews at FAANG or a FAANG-adjacent company\n\nAs we quickly learned, however, even these stringent criteria weren’t enough. We found ourselves facing two problems. First, our users would regularly tell us that their interviewer didn’t seem like they were paying attention, was a jerk, or didn’t teach them anything useful. Second, over time, as interviewing.io became more popular and gained more users, we saw that our candidates weren’t performing as well in real interviews as they previously had… which meant that our interviewers weren’t as well calibrated as we thought.\n\nI could speculate at length about why just choosing interviewers from top companies isn’t good enough, but my suspicion is that interviewer training is not created equal.\n\nSome companies invest considerable time and effort into teaching their interviewers how to be present during an interview, how to give hints, how to ask good follow-up questions, and how to make the candidate walk away feeling like they spent an hour working with a smart friend on a cool problem.\n\nOthers just throw new interviewers into the fray.\n\nSome companies, like Opendoor (whose [eng manager shared with us the notion of “superforecasters”](https://interviewing.io/blog/technical-phone-screen-superforecasters) keep detailed metrics on which interviewers have the best eye for talent. But other companies shoot blind.\n\nEven with great training, some interviewers won’t put in the work to stay present and engaged during interviews. Interviewing people is polarizing — some love it and some hate it — and if you don’t love it, it’ll show.\n\nWith this said, while our criteria might sound like a pretty high barrier to entry, the reality is that interviewer quality, before we instituted some formal metrics, was all over the place, both with respect to candidate experience and calibration.\n\nEnter the money-making metrics!\n\nThe need for two metrics\n------------------------\n\nAs you saw, we care about two separate things: how good of a candidate experience our interviewers create and how well-calibrated they are.\n\n**Candidate experience matters for retention.** We want engineers to complete as many mock interviews as they need on our platform. Some users will be ready after one mock, but others will need more practice. What we don’t want is for someone to leave us before they’re ready simply because they had a negative experience with their first (or second) interviewer.\n\nWe also care about retention on a multi-year timeline. We’re a hiring marketplace, so we know that — like a dating site — if we do our job well, our users will leave us. Unlike a dating site, though, people today aren’t married to their jobs. The average tenure of a software engineer in the US is 2.4 years. We hope that once our users are considering their next move, they’ll come back to us to practice and de-rust.[2](#user-content-fn-2)\n\n**Calibration matters just as much as candidate experience.** I already talked about how the employers who hire through us are taking a leap of faith because they’re agreeing to spend eng time on our candidates, sight unseen. Calibration and candidate experience are also inextricably linked — feedback that’s too lenient might boost a candidate’s confidence in the short term, but it will also hurt their chances of doing well in interviews, which ultimately reflects poorly on us.\n\nCandidate experience and calibration are related, but they’re different enough to warrant tracking separately. We ultimately decided to create a “candidate experience” score and a “calibration” score for each interviewer and track them over time, both individually and in aggregate across the whole platform.\n\nFirst, how did we find the interviewers who drove customer success?\n\n### Metric 1: The candidate experience score\n\nAfter each interview on interviewing.io, we collect feedback from both candidates and interviewers. As you may recall, interviews are fully anonymous, and you don’t see the other person’s feedback until you submit yours. This means that on our platform, candidates are incentivized to tell us honestly how things went.\n\nThis is what our candidate feedback form looks like:\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/candidate_feedback_form_5e72f023da.png)\n\nThough we’d been qualitatively tracking candidate feedback for years, we recently quantified it and created a per-interviewer candidate experience score. Here’s what we did.\n\nRanking how well people engage customers is a wide-ranging task: similar questions could be asked about Uber drivers, Instacart shoppers, sales reps, or even schoolteachers.\n\nA core issue in customer-facing platforms is that customers are too… nice. There’s grade inflation in the feedback. For example, Uber driver ratings are compressed at the top, with most drivers clustered around 4.8 or so. In absolute terms, a 4.8 seems not that different from a 4.7, but seasoned riders have learned the difference. Despite interviews being anonymous and feedback being mutually incentivized, it’s true in our data as well: a seemingly high average in absolute terms could mask a chronic underperformer, so it’s important to use relative rankings and concrete outcomes (e.g., customer churn) in addition to customer ratings.\n\nWhat did our score need to do? We saw three overarching attributes that needed to be balanced:\n\n* **Predictive**: Scores should capture something persistent about the interviewer’s ability. Someone’s score today should tell you something about their performance tomorrow.\n* **Fair**: Scores shouldn’t feel arbitrary; they should be based on meaningful metrics and interviewers should be given space to improve.\n* **Legible**: Scores should be interpretable to both the interviewing.io ops team and to the interviewers themselves.\n\nOur first task was to check whether an interviewer-based metric was **predictive**. This wasn’t guaranteed: most of the variation in the customer experience could be driven by the attitudes of the customers or extraneous factors like the weather that day. If this were the case, any ranking of interviewers would be entirely based on luck.\n\nInstead, we found evidence that interviewers had stable skill levels — someone’s history of customer ratings was highly predictive of what the next customer would say about them. These results extended beyond just survey responses: customers who got paired with a lower-ranked interviewer were more likely to leave the platform entirely. A bad interviewer increased churn by almost 10 percentage points.\n\nThe graph below shows the result. If interviewers had no effect on churn, we would expect the fitted line to be flat. Instead, once we rank interviewers based on their past sessions, we can predict how likely a customer is to leave the platform after an interview with them. Around 30% of customers leave if they get an interviewer from the bottom fifth. But only 20% of customers leave if they get an interviewer from above the 80th percentile.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/The_tight_link_between_interviewer_quality_and_candidate_attrition_9c2a88fccb.png)\n\nSo, interviewers had stable traits which, in the worst cases, could actually scare customers off the platform. Our next step was to condense this valuable information into a single score. Machine learning methods have one suggestion for this: use the interviewer identities as features in a model predicting a customer outcome like attrition or dissatisfaction. The coefficients could then be used as a measure of quality.\n\nWhile we did estimate these, a ranking based on these numbers wouldn’t be very **legible**. At best, we could tell the interviewers that their quality scores were based on a well-calibrated model.\n\nIt turned out that averages of the more readily understood dimensions of customer feedback (how excited they are to work with them and the quality of the interview questions, hints, and communication) were closely correlated with the machine learning predictions. So to reduce the complexity in the score, we used the linear combination of the customer feedback factors that were most correlated with the output from the more complicated model.\n\nThus, we boiled down our optimal prediction into a simpler linear model that the interviewers could even calculate themselves, giving them insight into which components were holding them back. This would also help our ops team provide focused nudges to the interviewers.\n\nWas it **fair**? In general, the model’s out-of-sample predictions were more accurate when it used all past interviewers in the data to train. This is unsurprising, but it meant that veteran interviewers who had shown recent improvement might have to wait a year for this to be reflected in their score. This low-score purgatory would be too demoralizing.\n\nAnother fairness concern was that interviewers in hot water with objectively high averages could complain that they’d received only “Excellent” and “Great” ratings, much like a 4.6-star Uber driver. **We emphasized to interviewers that these scores put them in the bottom 5-10% of interviewers, with real business consequences.**\n\nOne early challenge we faced was setting the right observation window. Originally, we decided to look at interviewers who had conducted at least 5 interviews in a rolling 90-day period. But because some interviewers had 5 in that 90-day period and others had 150, this created very different sample sizes and made it possible for a single interview to derail someone’s score.\n\nAfter measuring the predictiveness of different look-back periods, we eventually decided to limit it to the most recent 30 interviews. We saw that, while more data was always better, the curve flattened around 30. Sessions older than that would be ignored so that interviewers on an upward trajectory would have this reflected in their scores.\n\n**Our final result was a simple linear function of someone’s average customer feedback, with different weights according to how predictive that factor was of the machine learning estimates, and with the limited look-back period.**\nBelow, you can see our weighted (by number of interviews) aggregate candidate experience score across all the mock interviews on the platform. We track this as an internal health metric, and it’s a remarkably useful metric that shows us how happy our users are, at a glance (and correlates very directly with NPS).\n\n### Metric 2: The accuracy score\n\nOutside of delivering great candidate experience, it’s critical to our business that our interviewers are well-calibrated. After all, if our candidates aren’t amazing, the companies who hire through us will stop giving precious eng time to interviewing our users. They’ll just go back to using resumes.\n\nSo how much better do our candidates have to be?\n\nIn a good funnel at a company with a high engineering bar, candidates pass the technical screen about 20-25% of the time. Over time and after a bunch of trial and error, we realized that we needed our candidates to pass interviews about 70% of the time.[3](#user-content-fn-3) That’s about 3X what our customers see in their funnels, and it’s the threshold where working with us started to feel like magic. After all, when companies work with us, we’re removing their ability to choose who they talk to. They just have to trust us… and that makes the psychological burden of proof higher than you might expect. 50% pass rate, for instance, wasn’t enough to feel like magic.\n\nIn a nutshell, we had to ensure that our interviewers were well-calibrated enough to achieve a 70% passthrough rate for our candidates in real interviews.\n\nFortunately, we had the data to make this happen.\n\nOn interviewing.io, engineers do mock interviews, and top performers do real ones. Both sets of interviews have an identical feedback form that the interviewer completes after each interview:\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/interviewer_feedback_form_bd43d2e5c3.png)\n\nTo figure out how “accurate” our mock interviewers were — a measure of whether their judgments tended to be too strict, too lenient, or well-calibrated, relative to the hiring decisions of companies — we compared how they rated their candidates to how those candidates performed later, in real interviews.\n\nIn other words, this was based on a similar principle as the interviewer quality score, although based on less subjective measures than customer feedback.\n\nUsing just the interviewers’ own ratings and decisions of hiring companies, we asked whether some interviewers regularly under- or overestimated their candidates compared to candidates’ subsequent interviews with real companies. The answer was yes, and this result persisted in out-of-sample testing.\n\nMuch like the customer feedback, our interviewers had a bigger problem with positivity: the overly positive interviewers indicated they would hire their candidates 75% of the time, while their candidates received a yes just 60% of the time in real interviews — a considerable gap. But a sizable portion of well-calibrated interviewers closely matched the judgments of companies, and we told them so.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/Interviewer_accuracy_and_their_disagreement_with_real_job_interviews_df13562638.png)\n\nOur accuracy calculations were a continuous score, but we wanted to condense this. We converged on a -4 to 4 scale, a small number of values that we could connect to a distinct message. People with a 0 are among the best-calibrated interviewers. People with 3s and 4s are definitely too positive: they’re almost twice as likely to want to hire someone compared to the company. People with negative values are similarly too negative.\n\nHow these scores play together, or why brutal honesty makes for great candidate experience\n------------------------------------------------------------------------------------------\n\nOnce we built out both sets of scores, we could finally answer a question that interviewers new to our platform pose regularly: Is there a tension between delivering great candidate experience and being brutally honest with candidates about their performance?\n\nAfter all, doesn’t candidate experience depend, in part, on NOT giving harsh, unvarnished feedback? And doesn’t failing people create a bad experience that makes them not want to return?\n\nFortunately, for hard questions like these, we have the data! Below is a graph of average candidate experience score as a function of interviewer accuracy, representing data from over 1,000 interviewers. As you can see, the candidate experience score peaks right at the point where interviewers are neither too strict or too lenient but are, in Goldilocks terms, just right. It drops off pretty dramatically on either side after that.[4](#user-content-fn-4)\n\nHow we used these scores to significantly improve candidate experience over time\n--------------------------------------------------------------------------------\n\nIn the graph at the bottom of the “candidate experience score” section, you can see that starting in Q1 of 2022, our aggregate candidate experience score improved dramatically. This was intentional. Once we realized how well this metric predicted both candidate experience and attrition, we started to actively drive it up. Here’s what we did:\n\n* Systematically, ruthlessly paused interviewers who fell below the bar\n* Created full transparency and visibility around both scores\n* Built automatic throttling\n* Started running monthly onboarding meetings to explain our business model to interviewers and share how both candidate experience and accurate vetting are critical to interviewing.io’s success\n\n### Ruthlessly pausing interviewers below the bar\n\nOnce we built both scores, we started to use them to filter out underperforming interviewers.\n\nFor candidate experience, we chose a line below which candidate attrition was unacceptable. If an interviewer toed that line, we would reach out and encourage them to improve their performance. If an interviewer consistently fell below that line, then we paused their ability to do interviews.\n\nWe took a similar approach to accuracy. If an interviewer was too strict or too lenient (below a -1 or above a 1), we issued a warning and ultimately paused. In a pinch, we’d keep the strict interviewers and just let go of the lenient ones.\n\nIt’s hard to frame letting people go in a really positive light, but it’s important to distinguish between the two abilities in question here. As often seen in sports, the best players are not necessarily the best coaches. Similarly, we were forced to cut some really stellar *engineers* who, regrettably, didn’t have the disposition or interpersonal skills to be great mentors.\n\nAs we started to roll out our new, ruthless policy of letting go underperformers, we realized that we had made a mistake. Though interviewers’ scores were visible to us, they were not visible to interviewers, so when we started issuing warnings, they were often blindsided and not in a place where they could take the news well.\n\n### Transparency and visibility\n\nTo fix the problem and not blindside our interviewers, we made a dashboard where interviewers could see both their “candidate experience” and “accuracy” scores in near-real time (for accuracy, we bucketed the score into “too strict,” “OK,” and “too lenient”). We also shared how we calculate both scores.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/image_15dafbf507.png)\n\nExposing the scores, as well as the logic behind them, helped our interviewer community buy into the scoring system and to see firsthand that it was built on data and not our subjective opinions. It also made it so no one was shocked when we reached out about their performance. Over time, we started encouraging interviewers to come to us if they saw their score slipping… before we had the chance to reach out to them.\n\n### Automatic throttling\n\nHaving our ops team manually review interviewer scores every week and manually pause underperforming interviewers obviously didn’t scale, so we decided to build a throttling system that would put our best interviewers in front of the most customers. That meant adjusting our interview scheduling algorithm to prioritize interviewers who maintained higher ratings. Those rated lower still got assignments but only if higher-rated interviewers weren’t available for the same type of interview or time slot. The net result is the top 20% of our interviewers administering 80% of practice interviews.\n\nNew interviewers still get the chance to prove their mettle, of course.\n\n### Monthly onboarding\n\nAs we spoke to more and more interviewers, it became clear that many of them didn’t actually know what interviewing.io did or why candidate experience and accuracy mattered so much to our bottom line.\n\nWhen I first started the company, I’d spend 30-60 minutes on the phone with every new interviewer, explaining how hiring was broken and how they played a critical part in fixing it. Then, at some point, we started growing fast, and I stopped.\n\nIt was clearly time to start again. Though it was no longer sustainable for me to do 1:1 calls, I now run a 1.25-hour monthly onboarding session along with our ops team, and every interviewer who’s joined our community since the last session has to attend.\n\nIn these sessions, we discuss a bunch of the stuff covered in this post. We talk about the mission of the company, why the two metrics matter, how to deliver harsh feedback, how important it is not to sugarcoat things, and what it means to be a good interviewer.\n\nExplaining what we do, why we do it, and how high our expectations are made a step function difference in the quality of our interviewer community. It also reaffirmed my faith in humanity. If you want people to do a great job, tell them what a great job looks like and why it matters. And if you’re fortunate enough to have talented people working for you who are committed to your mission, they will knock it out of the park.\n\nIn the next post (coming soon), we’ll talk about how to bring some of these approaches and metrics to your own interview process. Because you are a single company, not a dedicated interview platform, you may not be able to replicate what we did exactly, but you can get pretty close with just a little bit of work.\n\n*Big thank you to Maxim Massenkoff, Liz Graves, and Richard Graves for their contributions to and help with this post.*\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. Thankfully, our candidates are better. On average, interviewing.io top performers perform 3X better in technical interviews than candidates from other sources (70% pass rate compared to 20-25%, the latter being the industry standard at companies with a high bar). [↩](#user-content-fnref-1)\n2. We’re proud to say that 70% of our users return to interviewing.io for their next job search. [↩](#user-content-fnref-2)\n3. While we could go higher than 70%, we didn’t want to have too many false negatives, which aren’t great for business or for candidate experience. [↩](#user-content-fnref-3)\n4. There are some notable peculiarities in the data as well. For instance, why do interviewers with an accuracy score of -3 get rated so much worse than interviewers who are even more strict? And why does candidate experience keep improving ever so slightly, as interviewers get more and more lenient, only to drop off again at the most lenient (accuracy = 4)? I expect these are functions of needing more data and that if we had, say, 10K interviewers in our ecosystem, the curve would be a lot more smooth. [↩](#user-content-fnref-4)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/our-business-depends-on-having-the-best-interviewers-so-we-built-an-interviewer-rating-system-and-you-can-too",
      "author": "",
      "user_id": ""
    },
    {
      "title": "We ran the numbers, and there really is a pipeline problem in eng hiring.",
      "content": "If you say the words “there’s a pipeline problem” to explain why we’ve failed to make meaningful progress toward gender parity in software engineering, you probably won’t make many friends (or many hires). The pipeline problem argument goes something like this: *“There aren’t enough qualified women out there, so it’s not our fault if we don’t hire them.”*\n\nMany people don’t like this reductive line of thinking because it ignores the growing body of research that points to unwelcoming environments that drive underrepresented talent out of tech: STEM in early education being unfriendly to children from underrepresented backgrounds, lack of a level playing field and unequal access to quality STEM education (see [this study](https://www.kaporcenter.org/wp-content/uploads/2019/06/Computer-Science-in-California-Schools.pdf) on how few of California’s schools offer AP Computer Science for instance), hostile university culture in male-dominated CS programs, biased hiring practices, and ultimately non-inclusive work environments that force women and people of color to leave tech at disproportionately high rates.[1](#user-content-fn-1)\n\nHowever, because systemic issues can be hard to fix (they can take years, concerted efforts across many big organizations, and even huge socioeconomic shifts), the argument against the pipeline problem tends to get reduced to *“No, the candidates are there. We just need to fix the bias in our process.”*\n\n**This kind of reductive thinking is also not great. For years, companies have been [pumping money and resources into things like unconscious bias training](https://www.mckinsey.com/featured-insights/gender-equality/focusing-on-what-works-for-workplace-diversity) (which has been [shown not to work](https://leakytechpipeline.com/wp-content/themes/kapor/pdf/KC18001_report_v6.pdf)), anonymizing resumes, and all sorts of other initiatives, and the numbers have barely moved.** It’s no wonder tech eventually succumbs to a [“diversity fatigue”](https://www.latimes.com/business/technology/la-fi-tn-diversity-fatigue-20180604-story.html) that comes from trying to make changes and not seeing results.\n\n**We ran the numbers and learned that there really IS a pipeline problem in hiring — there really aren’t enough women to meet demand… if we keep hiring the way we’re hiring. Namely, if we keep over-indexing on CS degrees from top schools, and even if we remove unconscious bias from the process entirely, we will not get to gender parity.** And yes, there is a way to surface strong candidates without relying on proxies like a college degree. We’ll talk about that toward the end.\n\nOur findings ARE NOT meant to diminish the systemic issues that make engineering feel unwelcome to underrepresented talent, nor to diminish our ability to work together as an industry to effect change — to enact policy changes like access to CS in public schools, for instance. Our findings ARE meant to empower those individuals already working very hard to make hiring better who find themselves frustrated because, despite their efforts, the numbers aren’t moving. To those people, we say, please don’t lose sight of the systemic problems, but in the short term, there are things you can do that will yield results. We hope that, over time, by addressing both systemic pipeline issues and biases, we will get to critical mass of women from all backgrounds in engineering positions, and that these women, in turn, will do a lot to change the pipeline problem by providing role models and advocates and by changing the culture within companies.\n\nLastly, an important disclaimer before we proceed. In this post, we chose to focus on gender (and not on race). This decision was mostly due to the dearth of publicly available data around race and intersectionality in CS programs/bootcamps/MOOCs.[2](#user-content-fn-2) While this analysis does not examine race and intersectionality, it is important to note that we recognize: 1) Not all women have the same experience in their engineering journey, and 2) tech’s disparities by gender is no more important than the lack of representation of people of color in engineering. We will revisit these subjects in a future post.\n\nThe percentage of women engineers is low and likely worse than reported\n-----------------------------------------------------------------------\n\nIt’s very hard to have a cogent public conversation about diversity when there is no standardization of what statistic means what. As this post is about engineering specifically, we needed to find a way to get at how many women engineers are actually in the market and work around two big limitations in how FAAMNG (Facebook, Amazon, Apple, Microsoft, Google, and Netflix) report their diversity numbers.\n\nThe first limitation is that FAAMNG’s numbers are *global*. Why does this matter? It turns out that other countries, especially those where big companies have their offshore development offices, tend to have a higher percentage of female developers.[3](#user-content-fn-3) In India, for instance, [about 35% of developers are women](https://www.computerweekly.com/news/252437742/Why-does-India-have-a-higher-percentage-of-women-in-tech-than-the-UK); in the U.S., it’s 16%. Why are these numbers reported globally? The cynic in me says that it’s likely because the U.S. numbers, on their own, are pretty dismal, and these companies know it.[4](#user-content-fn-4) To account for this limitation and get at the U.S. estimate, we did some napkin math and conservatively cut each company’s female headcount by 20%.\n\nThe second limitation is that reported numbers are based on “technical roles,” which Facebook at least defines very broadly: “A position that requires specialization and knowledge needed to accomplish mathematical, engineering, or scientific related duties.” I expect the other giants use something similar. What are the implications of this fairly broad definition? Given that product management and UX design count as technical roles, we did some more napkin math and removed ~20% to correct for PMs and designers.[5](#user-content-fn-5)\n\nWith these limitations in mind, below is a graph comparing the makeup of the U.S. population to its representation in tech at FAAMNG companies where said data was available, as well as an estimate of women in engineering specifically.\n\n![Chart comparing the % of women working at FAANGs to the general US population](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F41a7b_theres_still_a_lot_of_work_to_do_if_we_want_to_reach_gender_parity_in_engineering_2_7a234c8e90.webp&w=1920&q=75 \"% women at FAANGs versus the general population\")\n\nThere's still a lot of work to do if we want to reach gender parity in engineering\n\nIf we want to reach gender parity in engineering, especially when we correct for women in the U.S. (and whether they’re actually software engineers), you can see that we have a long way to go.\n\n### Is it a pipeline problem?\n\nSo, are there just not enough qualified women in the hiring pool? It turns out that we’re actually hiring women at pretty much the same rate that women are graduating with CS degrees from four-year universities — out of the 71,420 students who graduated with a CS degree in 2017, 13,654, or ~20%, were women.[6](#user-content-fn-6) So maybe we just need more women to get CS degrees?\n\nTop tech companies and their non-profit arms have been using their diversity and inclusion budgets to [bolster education initiatives](https://www.google.org/billion-commitment-to-create-more-opportunity/), in the hopes that this will help them improve gender diversity in hiring. Diversity initiatives started taking off in earnest in 2014, and in 4 years, enrollment in CS programs grew by about 60%. It’s not anywhere near enough to get to gender parity.\n\nAnd even if we could meaningfully increase the number of women enrolling in CS programs overall, top companies have historically tended to favor candidates from elite universities (based on some targeted LinkedIn Recruiter searches, 60% of software engineers at FAANG hold a degree from a top 20 school). You can see enrollment rates of women in 3 representative top computer science programs below. Note that while the numbers are mostly going up, growth is linear and not very fast.\n\n![Chart showing increase in women enrolled in undergraduate computer science](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F26c4f_growth_in_womens_undergraduate_computer_science_enrollment_at_top_schools_1_8914d7d98a.webp&w=1920&q=75 \"Growth in women's undergraduate computer science enrollment at top schools\")\n\nSources: [UC-Berkeley](https://pages.github.berkeley.edu/OPA/our-berkeley/student-headcount-by-major.html), MIT [1](http://web.mit.edu/fnl/volume/305/bucciarelli.html) and [2](https://registrar.mit.edu/statistics-reports/enrollment-statistics-year)), and [Stanford](https://studentservices.stanford.edu/more-resources/office-university-registrar/everyone/enrollment-statistics) enrollment data\n\nTo see if it’s possible to reach gender parity if we remove unconscious bias but keep hiring primarily from top schools, let’s build a model. For the purposes of this model let’s focus solely on new jobs — if companies want to meet their diversity goals, at a minimum they need to achieve parity on any new jobs they’ve created. Based on the US BLS’s projections, the number of software engineering jobs is estimated to increase by 20% by 2028 (or about 1.8% annually). Today, the BLS estimates there are about 4 million computer-related jobs. This projects to about 70,000 new jobs created this year, increasing to 85,000 new jobs created in 2028.\n\nIf the goal is to hit gender parity in the workforce, our goal should be to have 50% of these new seats filled by women.\n\nTo see if this is possible, let’s project the growth of the incoming job pool over the same timeframe. Based on NCES’ 2017 survey, computer science graduates have grown annually anywhere between 7% and 11% this decade. Let’s optimistically assume this annual growth rate persists at 10%. Let’s also assume that the  \npercentage of graduates who are women remains at 20%, which has been true for the last 15 years. But, there are some gotchas.\n\nFirst, there’s no guarantee that the seats earmarked for women actually get filled by women, particularly in a world where male CS graduates will continue to outnumber females 4-to-1. Not all of these jobs will be entry-level, so some portion of these jobs will be pulling from an already female-constrained pool of senior candidates. Finally, there’s no guarantee that traditional 4-year colleges will be able to support the projected influx of computer science candidates, particularly from the top-tier universities that companies usually prefer. Below, we graph the net new seats we’d need to fill if women held half of software engineering jobs (blue line) vs. how many women are actually available to hire if we keep focusing largely on educational pedigree in our recruiting efforts (red line). **As you can see, it’s not possible to hit our goals, whether or not we’re biased against women at any point in the hiring process.**[7](#user-content-fn-7)\n\n![Chart showing a gap in the labor pool](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fa35da_there_arent_enough_women_in_the_labor_pool_to_get_to_gender_parity_based_on_how_we_hire_today_d19d624d91.webp&w=1920&q=75 \"There aren't enough women in the labor pool to get to gender parity (based on how we hire today)\")\n\nSo if the pipeline is at least partially to blame, what can we do?\n------------------------------------------------------------------\n\nYou saw above that enrollment in undergraduate computer science programs among women is growing linearly. Rising tuition costs coupled with 4-year universities’ inability to keep up with demand for computer science education have forced growing numbers of people to go outside the system to learn to code.\n\nBelow is a graph of the portion of developers who have a bachelor’s degree in computer science and the portion of developers who are at least partially self-taught, according to the Stack Overflow Developer Surveys from 2015 to 2019. As you can see, in 2015, the numbers were pretty close, and then, with the emergence of MOOCs, there was a serious spurt, with more likely to come.\n\n![Chart showing education sources: BS in CS vs at least partially self-taught](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F1086f_education_sources_for_software_engineers_3_6b49673d18.webp&w=1920&q=75 \"Education sources for software engineers\")\n\nSources: Stack Overflow Developer Surveys for [2015](https://stackoverflow.com/research/developer-survey-2015), [2016](https://stackoverflow.com/research/developer-survey-2016) and [2019](https://insights.stackoverflow.com/survey/2019#education).\n\nThe rate of change in alternative, more affordable education is rapidly outpacing university enrollment. Unlike enrollment in traditional four-year schools, enrollment in MOOCs and bootcamps is growing exponentially.\n\nIn 2015 alone, [over 35 million people have signed up for at least one MOOC course](https://www.classcentral.com/report/moocs-2015-stats/), and in 2018 MOOCs collectively had over 100M students. Of course, many people treat MOOCs as a supplement to their existing educational efforts or career rather than relying on MOOCs entirely to learn to code. This is something we factored into our model.\n\nDespite their price tag (most charge on the order of $10-20K), bootcamps seem like a rational choice when compared to the price of top colleges. Since 2013, [bootcamp enrollment has grown 9X](https://www.coursereport.com/reports/2018-coding-bootcamp-market-size-research), with a total of 20,316 grads in 2018. Though these numbers represent enrollment across all genders[8](#user-content-fn-8) and the raw number of grads lags behind CS programs (for now), below you can see that the portion of women graduating from bootcamps is also on the rise and that graduation from online programs has actually reached gender parity (as compared to 20% in traditional CS programs).\n\n![Infographic showing increase in women graduating from bootcamps](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffemale_grads_over_time_858b8b46ad.webp&w=1080&q=75 \"Female Bootcamp Grads since 2011\")\n\nSource: Course Report’s [Data Dive: Gender in Coding Bootcamps](https://www.coursereport.com/blog/data-dive-gender-in-coding-bootcamps).\n\n\n\n![Chart showing 39% of graduates from in-person bootcamps VS. 49.5% of graduates from online bootcamps are women](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fonline_vs_in_person_ce041533ef.webp&w=1920&q=75 \"Gender Parity Online vs In-Person\")\n\nSource: Course Report’s [Data Dive: Gender in Coding Bootcamps](https://www.coursereport.com/blog/data-dive-gender-in-coding-bootcamps).\n\nOf course, one may rightfully question the quality of grads from alternative education programs. We factored in [bootcamp placement rates](https://www.switchup.org/rankings/coding-bootcamp-survey) in building our updated model below.\n\nOutside of alternative education programs, the most obvious thing we can do to increase the supply of qualified women engineers is to expand our pipeline to include strong engineers who don’t hail from top schools or top companies.\n\nIn previous posts, we looked at the relationship between interview performance and traditional credentialing and found that [participation in MOOCs mattered almost twice as much](https://interviewing.io/blog/lessons-from-3000-technical-interviews) for interview performance than whether the candidate had worked at a top company. And top school was least predictive of performance and [sometimes not at all](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt). And some of my earlier research indicates that the most predictive attribute of a resume is the number of typos and grammatical errors (more is bad), rather than top school or top company. In this particular study, experience at a top company mattered a little, and a [degree from a top school didn’t matter at all](https://blog.alinelerner.com/lessons-from-a-years-worth-of-hiring-data/).\n\nBut, even if lower-tier schools and alternative programs have their fair share of talent, how do we surface the most qualified candidates? After all, employers have historically leaned so hard on 4-year degrees from top schools because they’re a decent-seeming proxy. Is there a better way?\n\nBut culling non-traditional talent is hard… that’s why we rely on pedigree and can’t change how we hire!\n--------------------------------------------------------------------------------------------------------\n\nIn this brave new world, where we have the technology to write code together remotely, and where we can collect data and reason about it, technology has the power to free us from relying on proxies, so that we can look at each individual as an indicative, unique bundle of performance-based data points. At interviewing.io, we make it possible to move away from proxies by looking at each interviewee as a collection of data points that tell a story, rather than a largely signal-less document a recruiter looks at for 10 seconds and then makes a largely arbitrary decision before moving on to the next candidate.\n\nOf course, this post lives on our blog, so I’ll take a moment to plug what we do. In a world where there’s a growing credentialing gap and where it’s really hard to figure out how to separate a mediocre non-traditional candidate from a stellar one, we can help. interviewing.io helps companies find and hire engineers based on ability, not pedigree. We give out free mock interviews to engineers, and we use the data from these interviews to identify top performers, independently of how they look on paper. Those top performers then get to interview anonymously with employers on our platform (we’ve hired for Lyft, Uber, Dropbox, Quora, and many other great, high-bar companies). And this system works. Not only are our candidates’ conversion rates 3X the industry standard (about 70% of our candidates ace their phone screens, as compared to 20-25% in a typical, high-performing funnel), **about 40% of the hires made by top companies on our platform have come from non-traditional backgrounds. Because of our completely anonymous, skills-first approach, we’ve seen an interesting phenomenon happen time and time again: when an engineer unmasks at the end of a successful interview, the company in question realizes that the student who just aced their phone screen was one whose resume was sitting at the bottom of the pile all along** (we recently had someone get hired after having been rejected by that same company 3 times based on his resume!).\n\nFrankly, think of how much [time and money you’re wasting competing for only a small pool of superficially qualified candidates](https://interviewing.io/blog/you-probably-dont-factor-in-engineering-time-when-calculating-cost-per-hire-heres-why-you-really-should) when you could be hiring overlooked talent that’s *actually* qualified. Your CFO will be happier, and so will your engineers. Look, whether you use us or something else, there’s a slew of tech-enabled solutions that are redefining credentialing in engineering, from asynchronous coding assessments like CodeSignal or HackerRank to solutions that vet candidates before sending them to you, like Triplebyte, to solutions that help you vet your inbound candidate pool, like Karat.\n\nAnd using these new tools isn’t just paying lip service to a long-suffering D&I initiative. It gets you the candidates that everyone in the world isn’t chasing without compromising on quality, helps you make more hires faster, and just makes hiring fairer across the board. And, yes, it will also help you meet your diversity goals. Here’s another model.\n\nHow does changing your hiring practices improve the pipeline?\n-------------------------------------------------------------\n\nAbove, you saw our take on status quo supply and demand of women engineers — basically how many engineers are available to hire using today’s practices versus how many we’d need to actually reach gender parity. Now, let’s see what it looks like when we include candidates without a traditional pedigree (yellow line).\n\n![Chart showing reduction in gender parity](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fd27ee_gender_parity_is_within_reach_if_we_change_how_we_hire_ed3fb681a7.webp&w=1920&q=75 \"Gender parity is within reach if we change how we hire\")\n\nAs you can see, broadening your pipeline isn’t a magic pill, and as long as demand for software engineers continues to grow, it’s still going to be really hard, systemic changes to our society notwithstanding. If we do make these changes, however, the tech industry as a whole can accelerate its path toward gender parity and potentially get there within a decade.\n\n### What about specific companies? An interactive visualization.\n\nSo far we’ve talked about trends in the industry as a whole. But, how do these insights affect individual employers? Below is an interactive model where you visualize when Google, Facebook, or your company (where you can plug in your hiring numbers) will be able to hit their goals based on current hiring practices versus the more inclusive ones we advocate in this post. Unlike the industry as a whole, built into this visualization is the idea of candidate reach, as well as hire rates — one company can’t source and hire ALL the women (as much as they might want to). Of course, the stronger your brand, the higher your response rates will be.\n\nWe made some assumptions about response rates to sourcing outreach for both Google and Facebook. Specifically, we guessed a 60%-70% response rate for these giants based on the strength of their brand and their army of sourcers — when those companies reach out and tenaciously follow up, you’ll probably respond eventually.[9](#user-content-fn-9) We also made some assumptions about their hire rates (5-10% of interviewed candidates). You can see both sets of assumptions below. And you can see that even with all the changes we propose, in our model, Google and Facebook will *still* not get to gender parity!\n\nWe also included a tab called “Your company” where you can play around with the sliders and see how long it would take your company to get to gender parity/whether it’s possible. There, we made much more conservative assumptions about response rates!\n\nAs you can see, for the giants, getting to gender parity is a tall order even with broadening your pipeline to include non-traditional candidates. And while it may be easier for smaller companies to get there without making drastic changes, when you’re small is exactly the right time to get fairer hiring into your DNA. It’s much harder to turn the ship around later on.\n\nConclusion\n----------\n\nRegardless of whether you’re a giant or a small company, as long as hiring practices largely limits itself to top schools, the status quo will continue to be fundamentally inefficient, unmeritocratic, and elitist, and any hope of reaching gender parity will be impossible. Look, there are no easy fixes or band-aids when it comes to diversifying your workforce. Rather than continuing to look for hidden sources of women engineers (I promise, we’re not all hiding somewhere, just slightly out of reach) or trying to hire all the women from top schools, the data clearly shows that the only path forward is to improve hiring for everyone by going beyond top schools and hiring the best people for the job based on their ability, not how they look on paper.\n\nI was recently in a pitch meeting where I got asked what interviewing.io’s mission is. I said that it’s to make hiring more efficient. The investors in the room were a bit surprised by this and asked, given that I care about hiring being fair, why that’s not the mission. First off, “fair” is hard to define and open to all manners of interpretation, whereas in an efficient hiring market, a qualified candidate, by definition, gets the job, with the least amount of pain and missteps. In other words, meritocracy is a logical consequence of efficiency. Secondly, and even more importantly, while I firmly believe that most people at companies want to do “the right thing”, it’s much easier to actually do the right thing in a big organization when it’s also cheaper, better, and faster.\n\nAll that’s to say that there are no shortcuts, and the most honorable (and most viable) path forward is to make hiring better for everyone and then hit your diversity goals in the process (or at least get closer to them). Software engineering is supposed to be this microcosm of the American dream — anyone can put in the work, learn to code, and level up, right? Until we own our very conscious biases about pedigree and change how we hire, that dream is a hollow pipe.\n\nAppendix\n--------\n\nTo assess whether there exists a pipeline problem, we need to estimate the number of job openings that exist, as well as the number of recent female job market entrants that could feasibly fill those roles. If a pipeline problem does exist, the number of job openings would be greater than the number of female entrants.\n\nFor this analysis, we focused on new jobs created over the next 10 years and ignored openings from existing jobs due to attrition. Unfortunately, engineering do  \nes have a significantly higher attrition rate for women than other industries, so likely the numbers are worse than they appear in our models.9\n\nThat said, if a company wants to meet its diversity goals, it seems reasonable to expect them to do so with jobs that don’t yet exist, rather than on existing jobs whose pool of candidates we know are dominated by men.\n\n### Demand: Projected new jobs created\n\nTech industry net new jobs created = (# tech industry jobs prior year) x (annual growth rate)\n\nAssumptions:\n\n* Tech jobs (2018): 4 million ([Bureau of Labor Statistics](https://www.bls.gov/oes/current/oes150000.htm))\n* 12% growth 2018-2028, or 1.2% annual growth ([Bureau of Labor Statistics](https://www.bls.gov/ooh/computer-and-information-technology/home.htm))\n\n### Supply: Projected new women in job pool from top tier universities\n\nNew women in job pool = (# CS graduates prior year) x (annual CS graduate growth rate) x (% CS graduates that are women) \\* (% CS graduates from top tier schools)\n\nAssumptions:\n\n* Current # CS graduates: 70,000 ([National Center for Education Statistics](https://nces.ed.gov/programs/digest/d18/tables/dt18_325.35.asp?current=yes))\n* CS graduate growth rate: 10% ([National Center for Education Statistics](https://nces.ed.gov/programs/digest/d18/tables/dt18_325.35.asp?current=yes))\n* % women: 20% ([National Center for Education Statistics](https://nces.ed.gov/programs/digest/d18/tables/dt18_325.35.asp?current=yes))\n* % of all CS graduates from top tier schools: 25%\n\n### Supply: Projected new women in job pool beyond top tier universities\n\nThis represents female bootcamp graduates plus female CS graduates not from top schools.\n\nNew women in job pool from beyond top schools =  \n(# bootcamp graduates prior year) x (% bootcamp graduates that are women) x (% annual bootcamp graduate growth)  \n+ (# CS graduates prior year) x (annual CS graduate growth rate) x (% women in CS grads) x (% CS graduates not from top tier schools)\n\nBootcamp assumptions:\n\n* Current # bootcamp graduates: 20,000 ([Course Report](https://www.coursereport.com/reports/coding-bootcamp-market-size-research-2019))\n* % bootcamp graduates that are women: 40% ([Switchup](https://www.switchup.org/rankings/coding-bootcamp-survey#student-demographics))\n* Bootcamp graduates growth rate: 10% ([Course Report](https://www.coursereport.com/reports/coding-bootcamp-market-size-research-2019))\n* Placement rate: 50% ([Switchup](https://www.switchup.org/rankings/coding-bootcamp-survey#employment-outcomes))\n* % CS graduates not from top tier schools: 75% (see assumption from “Supply: Projected new women in job pool from top tier universities”)\n\nAssumptions for CS graduates beyond top tier universities are the same as those found under “Supply: Projected new women in job pool from top tier universities”, but taking the remaining 75% of CS graduates excluded there.\n\n### Company-specific Demand: Projected number of candidates needed to source for job openings\n\nNumber of women to source = (# Engineers employed prior year) x (% annual growth rate) x (% diversity goal) x (1 / hire rate) x (1 / sourcing response rate)\n\nIn practice, companies typically need to contact many people for any single job opening, since there is plenty of inherent variability in the sourcing and interview process. This line describes how many people your company would have to reach to fill all new job openings created, based on assumptions about your company’s hiring practices.\n\n*Thank you to the interviewing.io data & eng team for all the data modeling, projections, and visualizations, as well as everyone who proofread the myriad long drafts.*\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. The Kapor Center has some great, data-rich reports on attrition in tech as well as systemic factors that contribute to the leaky pipeline. For a detailed study of attrition in tech, please see the [Tech Leavers Study](https://www.kaporcenter.org/wp-content/uploads/2017/08/TechLeavers2017.pdf). For a comprehensive look at systemic factors that contribute to the leaky tech pipeline (and a series of long-term solutions), please see this [comprehensive report](https://www.kaporcenter.org/wp-content/uploads/2018/02/KC18001_report_v6-1.pdf). For a survey of the deplorable state of computer science education in California schools, please see [this report](https://www.kaporcenter.org/wp-content/uploads/2019/06/Computer-Science-in-California-Schools.pdf). [↩](#user-content-fnref-1)\n2. For instance, MIT keeps their race stats behind a [login wall](https://idp.mit.edu/idp/Authn/MIT?conversation=e1s1). [↩](#user-content-fnref-2)\n3. According to a [HackerRank study](https://www.cio.com/article/222270/where-in-the-world-are-the-women-software-developers.html), “India, the United Arab Emirates, Romania, China, Sri Lanka, and Italy are the six countries with the highest percentage of women developers, [whereas the] U.S. came in at number 11.” [↩](#user-content-fnref-3)\n4. We assumed a 1:7 ratio of PMs to engineers and a 1:7 ratio of designers to engineers on product teams. Removing PMs and designers from our numbers does not mean to imply that their representation in tech doesn’t matter but rather to scope this post specifically to software engineers.) [↩](#user-content-fnref-4)\n5. The [National Center for Education Statistics](https://nces.ed.gov/programs/digest/d18/tables/dt18_322.50.asp?current=yes) doesn’t yet list graduation rates beyond 2017… the new numbers might be a bit higher, as you’ll see when you look at enrollment numbers for CS a bit further down in the post. [↩](#user-content-fnref-5)\n6. This is *not* independent of the idea that deep, systemic issues within the fabric of our society (such as the ones we mention at the beginning of the post) are keeping women from entering engineering in droves. But, as I mentioned in the intro, laying the blame at the feet of these systemic issues entirely paralyzes us and prevents us from fixing the things we can fix. [↩](#user-content-fnref-6)\n7. We couldn’t find a public record of women’s numbers by year and so are relying on graduation rates from Course Report as a proxy. [↩](#user-content-fnref-7)\n8. These rates might seem high to recruiters reading this post. They *might* be high. We did try to correct for 2 things, both of which made our estimates higher: 1) this includes university and junior candidates, who tend to be way more responsive, and 2) this isn’t per sourcing attempt but over the lifetime of a candidate, so it includes followups, outreach to candidates who had previously been declined, and so on. However, if this still seems off, please write in and tell us! [↩](#user-content-fnref-8)\n9. There are two good sources that look at attrition among women in tech. One is [Women in Tech: The Facts](https://wpassets.ncwit.org/wp-content/uploads/2021/05/13193304/ncwit_women-in-it_2016-full-report_final-web06012016.pdf), published by the National Center for Women&IT. The other is the excellent [Tech Leavers Study](https://www.kaporcenter.org/wp-content/uploads/2017/08/TechLeavers2017.pdf) by the Kapor Center. [↩](#user-content-fnref-9)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/we-ran-the-numbers-and-there-really-is-a-pipeline-problem-in-eng-hiring",
      "author": "",
      "user_id": ""
    },
    {
      "title": "How to get in the door at top companies: a data-driven, practical guide for software engineers. Part 1.",
      "content": "*In this post (part 1 of 2), we’ll share some data about which channels are most effective for getting into the door at great companies and why. In [part 2](https://interviewing.io/blog/how-to-get-in-the-door-at-top-companies-cold-out-reach-to-hiring-managers-part-2), we get very tactical and tell you exactly what to say and do to get responses.*\n\ninterviewing.io is an anonymous mock interview platform — we help engineers prepare for technical interviews by pairing them with senior FAANG and FAANG-adjacent engineers for mock interviews and feedback. In this market, many of our users are struggling with getting in the door at companies, so we ran a survey to see what’s worked well for our users and what hasn’t, in today’s difficult climate.\n\nIn our survey, we gave people the following channels for getting into companies and asked them which were the most and least effective:\n\n* In-house recruiters contact you\n* Apply online\n* Warm referrals\n* Cold referrals\n* Cold outreach to hiring managers\n* Cold outreach to recruiters\n* Agency recruiters contact you\n\nWe also asked them which types of companies they got responses from:\n\n* FAANG\n* FAANG-adjacent (e.g., Stripe, Dropbox, OpenAI, Uber)\n* Large startups\n* Small startups\n\nWe got ~500 responses. Among survey respondents, which channels were most effective was largely consistent, regardless of company type, but there were some twists depending on who the candidates were. More on that in a bit.\n\nBelow are the channels, ranked by effectiveness.[1](#user-content-fn-1) When more people found a channel effective than ineffective, it ended up in the first list. When more people found a channel to be ineffective than effective, it ended up on the second list.\n\n**Recruiting channels that our users found to be effective (ranked from most to least effective):**\n\n* Warm referrals\n* In-house recruiters contact you\n* Apply online\n* Cold outreach to hiring managers\n\n**Recruiting channels that our users found to be ineffective (ranked from least to most ineffective):**\n\n* Cold outreach to recruiters\n* Cold referrals (referrals from people you don’t know)\n* Agency recruiters contact you\n\nThis data came primarily from surveying experienced engineers (4+ years), rather than juniors (we don’t have that many juniors on our platform; average years of experience is 8). If you’re a junior engineer in this market, you already know you’re in for a tough time, and we’d advise you to take your destiny into your own hands as much as possible by reaching out directly to hiring managers (the same advice we give many of our more experienced users). More on that later in the post.\n\nInterestingly, these results were quite consistent between company types. In other words, channels that worked well for FAANGs tended to work well for startups and vice versa.\n\nOverall, the most useful channels were in-house recruiters (when they reached out to you) and warm referrals. Unfortunately, both of these channels are somewhat out of your control. You have very little control over whether internal recruiters reach out to you. There are *some* things you can do to increase the chances, but they’re all tied up in your work history and identity, neither of which you can easily change. We’ll talk more about that later on.\n\nWarm referrals (i.e., referrals from people you know), on the other hand, are very useful and are a *bit* more in your control, but they still depend on the quality of your network.\n\nBelow is a diagram comparing the utility of all the channels to how much control you have over them.\n\n![A diagram comparing the effectiveness and utility of all the recruitment/hiring channels to how much control you have over them.](https://strapi-iio.s3.us-west-2.amazonaws.com/effectiveness_and_utility_quadrant_for_channels_1c6bec37f1.png)\n\nYou might wonder why we speculatively singled out cold outreach to hiring managers as something that can be done right, versus the other channels. In our experience, that channel is both misused and underutilized and is the best bet for many candidates, provided they do it correctly. In our next post, we’ll tell you exactly what to do and what to say when you reach out to hiring managers — especially if you come from a non-traditional background and aren’t getting a lot of recruiter outreach/don’t have the network to get warm referrals, reaching out to hiring managers is your absolute best bet.\nNow let’s look at each channel in detail.\n\n### In-house recruiters contact you\n\nThis channel is one of the two where you have the least amount of control (the other is agency recruiters contacting you, though that one is way less useful).\n\nSo, how much control do you have over this channel? One bit of analysis we did on our survey data was to try to find patterns in the background of people who find in-house recruiters particularly useful. Not too surprisingly, some patterns did emerge.\n\nIn-house recruiters are most likely to contact you if:\n\n* You look good on paper, i.e., you have top-tier companies and/or schools on your resume (in our experience, companies matter more)\n* You belong to a group that’s been traditionally underrepresented in tech (i.e., you’re a woman or a person of color)\n* To *some* extent, though less than the two bullets above, if you have niche skills (e.g., ML engineering)\n\nThese results aren’t unique to just this survey. We [recently did a study](https://interviewing.io/blog/are-recruiters-better-than-a-coin-flip-at-judging-resumes) where we asked a bunch of recruiters to look at resumes and tell us which candidates they’d want to interview. While the intent of the study was to see if recruiters are good at identifying talent (spoiler: they were barely better than a coin flip), we learned some other interesting things, including what recruiters actually look for when they read a resume.\n\nThe two most sought-after resume traits were: 1) experience at a top-tier tech company (FAANG or FAANG-adjacent) and 2) URM (underrepresented minority) status (in tech, this means being Black or Hispanic).\n\nThis mirrored what we saw in our user survey when we looked at commonalities among candidates who got value from in-house recruiters.\n\nSo how do you use this information to your advantage? You obviously can’t magic FAANG/FAANG-adjacent experience or URM status out of thin air[2](#user-content-fn-2), but if you do have either, our pragmatic advice is to highlight it and make it really easy for recruiters to spot. Of course, whether you *want* to lead with URM status is a personal decision. We’ve heard differing opinions on this and are not here to judge. All we can do is share the data — do with it what you will.\n\nSo, how do you make sure that, say, your FAANG experience stands out to recruiters? Take a look at the before and after screenshots of the resume below[3](#user-content-fn-3). This resume belongs to one of our users who was kind enough to let us share it. He actually has two of the three things that recruiters look for: FAANG experience and a niche title (ML engineer). But both are buried! And the section that gets the most attention is wasted on undergraduate awards.\n\n#### Before\n\n![Before version of a resume from one of interviewing.io's users](https://strapi-iio.s3.us-west-2.amazonaws.com/resume_before_b8d5aa1c34.png)\n\nAs you can see, he spent almost 3 years at Apple, but a recruiter skimming his resume might not notice that because it was a while ago. Instead, he showcases an undergrad award and some technologies/languages that he knows. Neither of those is nearly as useful to recruiters as FAANG experience.\n\nHis current title is also ML engineer, and one at the Principal level at that. But it wasn’t always: He went from back-end to SRE to a little bit of everything to ML, and because of that, it’s possible a recruiter would miss it as well.\n\n![After version of a resume from one of interviewing.io's users](https://strapi-iio.s3.us-west-2.amazonaws.com/resume_after_c148ae3b62.png)\n\nWe edited this candidate’s resume to put all the things recruiters look for at the very top of the resume and moved the buzzword soup to the bottom. This candidate is obviously well-positioned because he has FAANG experience, several top schools, and niche skills — but before, many recruiters didn’t spot them. After he made these changes, the number of interviews he got increased by 8X.\n\n### Apply online\n\nIf you’ve ever applied to jobs online, then you know it’s kind of like screaming into a black hole. Though, according to our survey, some candidates (specifically people applying to FAANG/FAANG-adjacent companies and small startups) get some value out of this channel, it’s still a numbers game. And for large startups, it’s a losing proposition.\n\nAccording to recruiting tool Gem, applicants that come from recruiter outreach (called “outbound” in recruiter lingo) are [6 - 10X more likely to get hired](https://www.gem.com/blog/outbound-candidates-more-likely-to-be-hired) than applicants who apply online (called “inbound”).\n\nAs Lyft recruiting manager Nate Wylie put it:\n\n> Our data… showed higher pass-through rates for candidates [we reached out to] at each stage of the interview process vs. applicants via the careers page. It’s not that we want to ignore applicants; it’s just that historically we don’t get what we’re looking for — and with speed — through that channel.\n\nHaving been a recruiter myself, I can confirm that many companies do indeed ignore their online careers page. Many years ago, when I first joined the recruiting team at a top-tier startup, I spent my first few days going through the resumes of people who had applied online. I found a treasure trove of candidates, including very experienced applicants from top-tier companies.[4](#user-content-fn-4) But no one had seen these applicants because no one had been monitoring inbound activity for months!\n\nThe silver lining here is that when you don’t hear back from a company (or even when you get an automatic rejection email wishing you \"the best in your future endeavors\"), it’s not because a human looked at your resume and made a deliberate, thoughtful decision about you. It’s tempting to think that way because it plays so well into our insecurities. The reality is that a human probably never saw your resume in the first place.\n\nSo why do people apply online, despite knowing in their gut that it’s not an effective strategy? Simply put, it’s predictable and easy. You get into a routine, you upload your resume, you connect your LinkedIn, and you can knock out hundreds of applications in a matter of hours.\n\nThe other encouraging thing about this channel is that, when we analyzed specifically which types of candidates had success with it, we couldn’t find any patterns — the channel worked equally well (poorly?) for people who looked good on paper vs. not, and there was no preferential treatment for traditionally underrepresented groups in tech (e.g., women and people of color).\n\n**TL;DR Applying online doesn’t hurt… provided that you don’t take rejection personally. If you do, it’ll wear you down over time.**\n\n### Warm referrals\n\nWarm referrals are, of course, excellent. That is, assuming it's a *real* referral — someone who can actually vouch for you, and ideally your work.\n\nPer capita, referrals are most companies’ best source of candidates, and they were a great channel for our users across all company types (they were the best channel for FAANGs/FAANG-adjacents, as well as large startups, and second best for small startups, behind in-house recruiters reaching out).\n\nIf you have the network, you should absolutely use it. Of course, it’s unlikely that you’ll have meaningful connections at every company you want to work at. What do you do then?\n\n### Cold referrals\n\nShould you ask people you don't know to refer you? Our survey data says probably not. Cold referrals were net negative for both FAANG and small startups and neutral for large startups.\n\nYears ago, trying to collect cold referrals was a decent strategy. You could track down someone at the company and ask them to toss your proverbial hat into the ring\n\nEngineers were often happy to refer someone — even someone they didn't know — either to be kind, to avoid the awkwardness of declining, or to collect the potential referral bonus. They couldn't vouch for you, but the referral would ensure that a human looked at your resume.\n\nThis became so common that Blind actually spun out an entire referral marketplace called [Rooftop Slushie](https://theamericangenius.com/adj/rooftop-slushie/) (the link is to some press because the actual site is now defunct), where applicants would pay a small sum for a referral.\n\nThen, companies wised up and realized that these referrals weren't all that different from normal in-bound applicants. So why treat them differently?\nMany companies nowadays separate referrals into \"true referrals\" and \"leads.\" It’s great for maintaining the delicate dance of social dynamics, but it’s completely useless for getting hired — dropping someone’s resume into the “leads” pile is basically the same as throwing it into the inbound black hole.\n\nGiven that cold referrals aren’t zero effort, our advice is to expend that energy elsewhere. More on that shortly.\n\n### Agency recruiters\n\nAgency recruiters were the worst channel overall, according to our survey, and were net negative for all company types.\n\nFAANGs and FAANG-adjacent companies tend to rely less on agencies than startups, and when they do, it’s to fill some very specific need (rather than “Hey we need more SWEs”), so it’s not surprising that our users didn’t get much value from this channel when applying to FAANGs.\n\nWhile both large and small startups use agencies liberally, clearly the value to candidates is limited.[5](#user-content-fn-5) Out of all of our survey respondents, only a handful of our users said that agencies were useful to them, and of those who mentioned agencies, the majority said that they were the worst channel.\n\nWe won’t belabor the point, but it’s probably not in your best interest to spend much time on working with agency recruiters. It has opportunity cost and not much upside. And you can [routinely get screwed in salary negotiations when you work with an agency recruiter](https://interviewing.io/blog/sabotage-salary-negotiation-before-even-start), if you even get that far.\n\n### Cold outreach (to hiring managers vs. recruiters)\n\nNot all cold outreach is created equal, for two reasons. First, there’s your audience: hiring managers vs. recruiters. And then there’s the quality of the outreach itself. We’ll come back to how to write the kinds of messages that will get you responses. First, let’s talk about the audience.\n\nYou can see in our survey results that cold outreach to hiring managers was net positive for FAANG/FAANG-adjacent companies and neutral for the other company types. Cold outreach to recruiters, on the other hand, was net negative for both FAANG/FAANG-adjacents and small startups and neutral for large startups.\n\nIgnoring the quality of the outreach for a moment, which we expect is probably comparable for both types, why does this difference exist?\n\nIf you had to answer the question of who’s the right person to reach out to about jobs, your gut instinct might be to say it’s recruiters. After all, hiring is officially their job! However, that’s not strictly true. Recruiters are not incentivized to make hires, at least not directly. Just like everyone else, recruiters’ main priority is to keep their jobs.\n\n#### Cold outreach to recruiters doesn’t work\n\nHow does a recruiter keep their job?[6](#user-content-fn-6) By *bringing in the types of candidates that their manager tasked them with*. How is that different from hiring? Hiring implies that you’re evaluated on whether the people you bring in actually get hired, but most in-house recruiters aren’t evaluated this way… because it takes too long.\n\nInstead, recruiters are sometimes evaluated on what portion of their candidates get offers or get to onsite. However, because of drop-off and latency (getting an offer can still take months), your organization has to be pretty good at tracking. Many are not.\n\nAs such, many recruiting orgs prefer simpler, faster metrics:\n\n* Of the candidates you reached out to, how many responded?\n* Of those who responded, how many resulted in a first conversation?\n\nThe downside of measuring success in a single part of the funnel is that you don’t incentivize people to care about what happens downstream (that is, how many are hired). This would be like if marketers only paid attention to ad clicks, rather than actual purchases. But that’s how recruiting operates: individuals aren’t really incentivized to care what happens downstream.\n\nSo, if you are typically just measuring the response rates of your reports, as a recruiting manager, you have to set some guardrails for the types of candidates that you want your team to reach out to. If you don’t, they’ll end up just reaching out to people who are likely to respond instead of people who are a good fit for the job.\n\nUnfortunately, you don’t know who is a good fit for the job. You can’t just say, “Go on LinkedIn, and find me good engineers.”\n\n![Screenshot of LinkedIn search](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Flinkedin_can_code_8178d2dc43.png&w=640&q=75 \"LinkedIn search\")\n\nThat doesn’t exist. So instead, you come up with some rules that look like this:\n\n* Senior engineers\n  + Juniors and intermediate engineers would answer outreach a lot, but they already apply online in droves, so we don’t need to pay people to go out and find them. Moreover, we have a whole university department that deals with college hires once a year in September.\n* Went to a top school (in the absence of a better filter, this works OK)\n* Worked at a top company\n\nThere may be a few other items on the list if the role requires specific skills (e.g., Android development), but by and large, that’s what recruiters are tasked with, and that’s what they’re focused on.\n\nIt seems counterintuitive, but if you’re either fairly junior (fewer than 4 years of experience) or you don’t have fancy brands and schools on your resume, recruiters are not incentivized to help you because you don’t meet their criteria, and they’re not incentivized to take risks on candidates because they’re not getting rewarded when the company makes hires (or punished when the company doesn’t).\n\n**What does this mean for you? If you’re not the type of candidate that recruiters are reaching out to already (senior, well-pedigreed), they will not help you.**\n\nWith that sad reality in mind, here’s the good news: there *is* someone who’s actually incentivized to make hires and is much more open-minded: the hiring manager[7](#user-content-fn-7)!\n\n#### Cold outreach to hiring managers is effective\n\nAt this point, you might be skeptical. After all, according to the graph comparing all channels, hiring manager outreach is the worst of the best. Sure, it’s net positive for FAANG/FAANG-adjacent companies, but it lags behind in-house recruiters, warm referrals, and online applications with respect to effectiveness.\n\nHere’s the thing. Hiring manager outreach is the channel with the most untapped potential for effectiveness, while also being the one where you have the most control. Because companies often ignore them, online applications can’t come close to the same level of control, and warm referrals have a low ceiling. In-house recruiter outreach is largely out of your control (except for maybe making some limited profile tweaks, as we saw above).\n\nWhy is this the right channel?\n\nUnlike recruiters, hiring managers are actually incentivized to make hires and tend to be more open-minded about candidate backgrounds, all because hiring managers are judged on results. Specifically, they’re judged on how quickly and effectively they’re able to build stuff, and are — directly or indirectly — incentivized to grow headcount. For hiring managers, it’s not about the appearance of doing the work. It’s about the cold, hard reality of whether the work got done. And because they’re judged on actually getting stuff done, hiring managers are also much more incentivized than recruiters to take risks.\n\nOutside of needing more people to build things, hiring managers are also incentivized to hire for their teams because the better they are at recruiting and filling headcount, the more likely they are to get promoted.\n\nAs such, in our minds, when people say that hiring manager outreach hasn’t worked for them, it’s because they’re not doing it right. So, how do you do it?\n\nIn our next post, we’ll get very practical about outreach, provide a bunch of examples of good and bad outreach, and share two templates that you can steal.\n\nFootnotes:\n\nFootnotes\n---------\n\n1. First, here’s how we got to these rankings. We asked each engineer who took our survey to rank all the channels they used to get in the door, from best to worst. Then we tallied up the points (+1 for best two channels, -1 for worst two). We didn’t do a more granular point system (e.g., +2 and -2) because the difference between the top two channels wasn’t always 2X, and generally, from talking to our users, preferences were somewhat muddy. As such, these results are directionally correct, but we didn’t feel comfortable numerically comparing them to one another. Finally, we divided the total tally by the number of times that channel came up. As a result, we were able to rank channels from most effective to least effective. [↩](#user-content-fnref-1)\n2. This is why I generally view resume writers as selling snake oil. Either you have the things recruiters are looking for or you don’t. If you don’t, no amount of wordsmithing your bullet points or reorganizing the page is going to make a significant difference. Sure, [check your resume for typos](https://blog.alinelerner.com/lessons-from-a-years-worth-of-hiring-data/), and make sure that it reads decently well. Any more time invested in your resume after those basic things will have diminishing returns. Beware of anyone who tells you otherwise, and beware of any products or services who charge for resume review. [↩](#user-content-fnref-2)\n3. We realize that recruiters won’t always have access to your resume when doing outreach and are likely looking at your LinkedIn instead. The same advice stands. Make sure that your About section has all the most important tidbits about you, front and center. Also, even though we didn’t see the same strong preference for FAANGs and URM status when applying online (more on that in the next section), making these types of changes to your resume certainly won’t hurt. [↩](#user-content-fnref-3)\n4. Of course we don’t share the point of view that you can only be a good candidate if you have a brand-name company on your resume. However, many recruiters do, and they are still ignoring this channel. [↩](#user-content-fnref-4)\n5. We’d argue that the value to companies is limited as well. Though there are a handful of excellent agency recruiters out there, most are terrible. The hard thing is that, as an employer, you can’t immediately tell who’s terrible, and you end up wasting a bunch of time reviewing profiles of candidates who might look promising on the surface, but because of selection bias (these are the people who decided to work with bad agency recruiters, after all) are not a fit. That or they’re not interested in your company (and have possibly never even opted in to talk to you) or both. [↩](#user-content-fnref-5)\n6. At larger companies, recruiting is usually split into two functions: sourcing (these are the people who reach out to candidates) and recruiting (these are the people who manage candidates’ journey through the process and extend offers). In this post, for simplicity, we’re lumping them together because separating them out would change some of the details but wouldn’t change the key takeaways. [↩](#user-content-fnref-6)\n7. Note that if you’re interested in smaller startups (Series A and below), you can substitute “founder” for “hiring manager” in the steps below. Founders are the most incentivized to get shit done and take risks, regardless of company size and stage, but at larger startups, they may be less likely to read cold emails because they get bombarded with all manners of requests and sales pitches. At a Series B or C company or at public companies with fewer than, say, 3000 employees, in addition to hiring managers, you should also target Directors and VPs — they have the power to get things done and aren’t so far removed from feeling the pain of not filling roles that making an extra hire or two is out of their purview. At large public companies, targeting Directors and above doesn’t make much sense — they ARE too far removed from doing the work to make individual hires. If you do contact them, the best outcome is that they’ll pass you on to one of their direct reports. [↩](#user-content-fnref-7)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/how-to-get-in-the-door-at-top-companies-part-1",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Why AI can’t do hiring",
      "content": "The recent exciting and somewhat horrifying inflection point in AI capability, which many of us got to experience firsthand when playing with OpenAI’s ChatGPT, tipped me into finally writing this post.\n\nI’m the founder of interviewing.io, a mock interview platform and eng hiring marketplace. Engineers use us for mock interviews, and we use the data from those interviews to surface top performers, in a much fairer and more predictive way than a resume. If you’re a top performer on interviewing.io, we fast-track you at the world’s best companies.\n\nWe’re venture backed and have raised 4 rounds of funding in the last 7 years, totaling over $15M, which means that I’ve done a lot of VC pitches. I don’t know how many exactly (and a lady should never tell), but it’s in the hundreds. Once you’ve done that many pitches, you start to hear the same feedback over and over. They range from questions about whether the eng hiring is big enough (it is) to how objections about human-on-human interviews don’t scale (if 2 humans doing a thing together didn’t scale, our species would be extinct) to polite suggestions about how we’d be a much more attractive investment if we used ML/AI to match candidates to companies.\n\nI’ve heard the latter a LOT over the years, but despite the well-intentioned advice, I’m convinced that building an AI matcher is a fool’s errand. My argument is this: **It’s not that AI doing hiring is technically impossible – ChatGPT has shown us that the ceiling on what’s possible is higher than many of us had ever imagined – but that it’s impossible because you don’t have the data.** In other words, the hard part about hiring isn’t the tech. It’s having the data to make good hiring decisions in the first place.\n\n**For the purposes of this piece, I define “hiring” as being able to find candidates for specific roles and fill those roles. I am NOT referring to automating tasks like resume parsing, writing sourcing emails, or scheduling, i.e., tasks that human recruiters, sourcers, and coordinators do as part of their job. Surely those can be automated. The more interesting question is whether an AI can do the job of a recruiter better than a human.[1](#user-content-fn-1)** In other words, can it take a list of candidates, a list of job descriptions, and then use publicly available (NOT proprietary) data to match them up successfully and fill roles? After all, the reality is that neither recruiters nor burgeoning AI recruiting startups have a proprietary data set to work with. They usually have job descriptions, LinkedIn Recruiter (the granular search functionality of which isn’t publicly accessible… but the LinkedIn profiles of candidates actually are), and whatever else they can find on the internet.\n\nTo wit, this post isn’t about how AI can’t be used for hiring if you have all the data. Rather, it’s about how you can’t get access to all the data you’d need to do hiring, thereby making the training of an AI impossible.\n\nThis post also isn't about how human are great at hiring. There's nothing special about our humanity when it comes to recruiting, and humans actually suck at hiring for the same reasons as AI — we don't have the data either.\n\nA few caveats: the case for Microsoft and the question of bias\n--------------------------------------------------------------\n\nAt this point, you’re probably thinking, “Well, surely Microsoft can do this, given that they own both LinkedIn and GitHub.” In this post, you’ll see why LinkedIn and GitHub are not enough. Perhaps if Microsoft chose to buy a bunch of applicant tracking systems (ATSs) in order to get access to interview performance data, coupled with data from LinkedIn and GitHub, they’d have a fighting chance, but honestly that’s probably not enough either.\n\nMoreover, the tenuous Microsoft edge aside, the reality is that most of us do NOT have access to the kind of training data we’d need, but we still see startup after startup claiming to do AI hiring in their marketing materials.\n\nFinally, before we get into why AI can’t do hiring, I want to call out the important question of bias that results from training AI on hiring data where decisions were previously made by humans. To keep this (already long) post on task, I will not touch on the subject of the bias. To be sure, it’s a real problem, and there’s already a lot of good writing on the subject.\n\nThat caveat aside, if we’re trying to build a solution that takes candidates and jobs as inputs and produces a set of matches as output, let’s start by considering what that matcher does and how it’s trained.\n\nHow do we train an AI matcher?\n------------------------------\n\nLet’s pretend for a moment that we have built the platonic ideal of an AI matcher. It takes 2 inputs: a list of candidates and a list of companies, and a sorted list of company/role matches come out, like so:\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/AI_matcher_d024945e8d.png)\n\nTo train this matcher, we need 3 distinct pieces of data:\n\n1. Publicly available job descriptions from a bunch of tech companies\n2. Publicly available data about engineers, i.e., LinkedIn profiles, GitHub profiles & contributions, and engineers’ social graph across a bunch of different platforms\n3. A list of successful company/candidate matches, taken from the public domain, e.g., from scraping LinkedIn to see where people worked/for how long, and cross-referencing that with (1) and (2).\n\nYou may notice that in all 3 data requirements above, I called out that they’re publicly available. That might seem odd at first — after all, if you’re starting a company that’s building this matcher, your secret sauce might be your proprietary data about candidates or companies or both, and you might first run a different business model, completely divorced from AI, to collect this data, at which point, boom, you flip a switch, and all of a sudden you’re an AI company.\n\nThat’s a good strategy, but it’s actually really hard to acquire detailed, proprietary data about candidates, companies, or how well people do at companies once they’re hired, let alone all 3 at once. Most startups that try to build an AI matcher don’t start with a bunch of proprietary data. Rather, they start with the public domain. The thesis of this piece is that getting the data is the hard part, not the AI, so to reason through it, let’s assume that we have the AI already but that the only data we have is publicly available.\n\nSo now that we have all this training data, let’s get to work. We’ll go through the set of successful matches and then find more data about those candidates and those companies and see which traits carry the most signal for a good match.\n\nWe now have a trained and working matcher. So far so good. But wait, not so fast!\n\nWhat is hiring, really?\n-----------------------\n\nLet’s switch gears and forget about our matcher for a moment. Broadly speaking, regardless of how we get there, what needs to be true for someone to be a good fit for a job? There are three things:\n\n1. *Good*: They’re good enough to do the job\n2. *Looking*: They’re open to taking a new job\n3. *Interested*: They’re interested in the job/company\n\n**Let’s succinctly call those “good, looking, and interested”. These three criteria are necessary to make a hire.**\n\nThe first two items are largely independent of the company. The third is about candidate/company fit, and we’ll come back to it when we talk about matching. Before we do that, though, can we actually deduce which engineers are good and looking?\n\nI would argue that we can’t. No matter how the matcher ended up getting trained or what patterns or artifacts it detected and assigned value to, the data to tell whether someone is a good engineer (even if the definition is elastic, depending on a given company’s “bar”) simply does not exist in the public domain.\\*\\* An AI is very good at finding patterns in existing data. It is not good at magicking data out of thin air. That means that before we even get to the question of matching, we’re dead in the water.\\*\\* Let me try to convince you.\n\n### “Good”\n\nGenerally, you have 3 pieces of public data available to you for a given engineer:\n\n1. Their public-facing LinkedIn\n2. Their public-facing GitHub\n3. Their public-facing social graph\n\nSurely you can tell if someone is a good engineer from some combo of these 3?\n\n### LinkedIn\n\nLet’s look at each of the 3 data sources above, starting with LinkedIn. What data is available on engineers’ public-facing LinkedIns? Given that a LinkedIn profile is a glorified resume, it’s usually these 3 things:\n\n1. Where they’ve previously worked\n2. Where they went to school\n3. Any certifications/endorsements/skills that they have\n\nI run a hiring marketplace, and having any kind of edge in predicting which of our users are good is material to our business, so we’ve spent a good amount of time and effort trying to tie these attributes to how good an engineer is. As it turns out, an engineer’s employment history carries some signal, school carries very little to none, and LinkedIn [certifications](https://interviewing.io/blog/why-you-shouldnt-list-certifications-on-linkedIn) and [endorsements](https://interviewing.io/blog/linkedin-endorsements-useless) carry a negative signal.[2](#user-content-fn-2)\n\nKnowing which programming languages or frameworks an engineer knows can be useful, but most eng roles are either language/framework agnostic OR the language/framework is not the most significant bit when determining whether an engineer is a good fit or not — knowing the language is usually a nice to have, but it won’t get you hired if you’re not a good coder, first and foremost.\n\nOften, in the absence of being able to search for whether an engineer is good, recruiters will search for programming languages as a proxy for fit, but that’s all it is, a weak proxy.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/linkedin_filter_c6d9fc88df.png)\n\nBasically, if you’re relying on a combo of school, work history, and specific skills, you’re doing exactly the same work that armies of recruiters have been doing for decades, with very limited success. It’s really easy to search LinkedIn for past employers, schools, and skills/endorsements. They’ve built a whole business on it. However, humans are terrible at predicting engineer quality from resumes — [only about as good as a coinflip](https://interviewing.io/blog/resumes-suck-heres-the-data). **And it’s not because humans are bad and that an AI would do better. It’s because there’s minimal signal in a resume in the first place, and try hard as you might, extracting signal in this case is like squeezing water from a stone. Having a robot hand will not save you.**\n\nAI or not, we’re in a hard market. Keep your skills sharp. Sign up for anonymous mock interviews with engineers from top companies.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\n### GitHub\n\nGitHub is an interesting one. Surely if you have access to a bunch of code someone has written, you can suss out if they’re a good engineer. The GitHub approach is also appealing because it’s much more meritocratic than a resume — your good code can stand on its own, regardless of who you are or where you come from. Wouldn’t it be great if GitHub could help you surface the odd diamond in the rough or an upstart with no job experience?\n\nThis is of course, the ethical question of whether GitHub should be used for hiring in the first place and whether it’s fair to create a bias toward hobbyist engineers and/or open-source contributors.[3](#user-content-fn-3) For the purposes of this piece, I will put aside that question and focus just on whether GitHub *can* be used for hiring.\n\nThe short answer is that it can’t because most engineers don’t have public commits. Senior engineers at large tech companies don’t work on open-source projects for the most part. This is why [programmers on Reddit laugh](https://www.reddit.com/r/ProgrammerHumor/comments/11g13i6/he_is_not_qualified) at the idea of screening out candidates with unused GitHub accounts. Bjarne Stroustrup, the inventor of C++, would look unimpressive to an algorithm obsessed with GitHub activity.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/bjarne_68a4633cd6.png)\n\nThis same message is clear from data on GitHub’s users. The website [reported 100 million active developers in early 2023](https://techcrunch.com/2023/01/26/github-says-it-now-has-100m-active-users), but data on all commits in 2022 shows that only about 11 million users had any public code commits and only 8 million had more than two commits. That means that only 8-10% of engineers actually have publicly available GitHub data that our AI could use, in the absolute best case, and this counts millions of repositories with names like “hello world” and “test.”\n\nBen Frederickson did some of his digging about the utility of GitHub in hiring and published a [stellar, highly detailed report](https://www.benfrederickson.com/github-wont-help-with-hiring) in 2018. According to Frederickson, only 1.4% of GitHub users pushed more than 100 times, and only 0.15% of GitHub users pushed more than 500 times. Frederickson’s findings from 2018 roughly corroborate ours from late 2022, and in both cases, there is a clear power law – most of the commits are being done by a tiny fraction of GitHub’s users.\n\nA paper called “[The Promises and Perils of Mining GitHub](https://dl.acm.org/doi/10.1145/2597073.2597074)”, published in 2014, looked at GitHub activity as well, through the lens of projects rather than users. They found a similar power-law relationship – the most active 2.5% of projects account for the same number of commits as the remaining 97.5% projects. Moreover, this paper found that about 37% of projects on GitHub were not being used for software development, and the rest were used for other purposes (e.g., storage).\n\n**Given these limitations, the reality is that the portion of GitHub accounts that could actually be useful for hiring is likely under 1%.**\n\n### Social Graph\n\nFinally, we have the social graph. The hypothesis here is that great engineers follow other great engineers on platforms like GitHub and Twitter, so if we can identify a set of great engineers somehow, and dig deeply enough through the tangled web of whom they follow and who follows them, we’ll be able to create a reliable talent map.\n\nLet’s assume for a moment that we can seed this approach with enough great engineers (e.g., by scraping the GitHub profiles that do have enough code to extract signal). What happens next?\n\nTo get a feel for the “following” behavior among software engineers and whether they tend to follow the best people they work with, we surveyed our user base, like so:\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/social_network_survey_3a890590ac.png)\n\nBased on almost 1000 responses from our engineers (average years of experience = 8, median = 7), the social graph approach doesn’t hold water. First, engineers that we surveyed only rarely followed their most impressive colleagues on GitHub or Twitter. **The average engineer reported following just one of their top five coworkers in these sites.** The social graph was just not that connected to people’s perceptions of talent.\n\nWhat about Linkedin? \\*\\*Although many of our users did indeed follow the best engineers they’ve ever worked with, they also followed everyone else: the majority of engineers we surveyed reported that their connections had no rhyme or reason, or they just connected with anyone who tried to connect with them. \\*\\*\n\nThese muddle the graph-based approaches. Twitter and GitHub follows are still too uncommon to be a reliable signal. And LinkedIn connections capture a haphazard mix of networking approaches — the most dominant of which is either random or just driven by workplace, which any recruiter would have known in the first place.\n\n### “Looking”\n\nLet’s assume for a moment we can figure out who the good engineers are (or rate them on some kind of scale, at least). Next, we have to figure out which of these engineers are on the market right now.\n\nThis is arguably an easier problem to solve than whether an engineer is good because publicly available cues do exist. That said, a number of startups have tried to tackle this problem (most notably [Entelo, with their Sonar product](https://www.recruiter.com/recruiting/new-entelo-technology-lets-recruiters-know-which-employees-will-most-likely-quit)), though so far none have been particularly successful. The kinds of inputs that typically go into trying to figure out if an engineer is active are split between candidate-level attributes (e.g., how recently they updated their LinkedIn) and company-level attributes (e.g., how long since the company’s last round of funding). Here’s what that list could look like:\n\nCandidate-level attributes:\n\n* When did they last update their LinkedIn/have they recently started being more active on LinkedIn?\n* How long have they been in their current role? And at their current company?\n* Have they recently started being more active on social media (e.g., tweeting about engineering topics)?\n* Have they recently started a blog?\n* Have they recently started contributing to open source?\n\nCompany-level attributes:\n\n* How long has it been since the company last raised money (if not public)?\n* How has the stock price been doing (if public)?\n* Have other people been leaving the company, especially management?\n\nSome of these attributes are easier to pull than others (e.g., to track LinkedIn updates, one has to be logged in and has to repeatedly cache candidate activity, which likely violates LinkedIn’s terms of service), but I imagine that there’s enough publicly accessible data to make some guesses about who’s moving. Of course, these guesses will be fairly primitive for candidates who don’t do stuff publicly and loudly — going just off of stock price and/or fundraising history gives you a very crude first pass, but it’s not enough.\n\nWhen asked their favorite social media platforms, half of engineers in a [StackOverflow survey](https://insights.stackoverflow.com/survey/2019) reported Reddit or YouTube — which it’s hard to imagine could ever be leveraged for predictive algorithms — or not using social media at all.\n\nAs such, even if this data is easier to get than candidate quality data, figuring out who’s looking is still a data problem and not an AI tech problem.\n\nMatching: Figuring out what engineers want and whether companies have it\n------------------------------------------------------------------------\n\nLet’s say that we’re somehow able to surmise from public-facing candidate data whether and engineer is both “good” and “looking”. Now we need to figure out whether they’re actually going to be interested in a given company.\n\nTo do that, we need to have a list of company attributes that engineers could care about, and then we need to figure out 1) how each company stacks up against this list and 2) which attributes a given engineer cares about. I’ve been a recruiter for about a decade, and below is a decently representative list (in no particular order):\n\n* Compensation\n* Company mission/whether the company is mission-driven\n* Vertical\n* Size of whole team and the eng team\n* Tech stack\n* Young vs. established\n* Prestige of the brand/social proof\n* What problems are being solved/what’s coming up on the roadmap\n* Chemistry with manager and with the immediate team\n* The company’s culture and values\n\nNow that we have this list, how do we figure out how each company stacks up? Without a proprietary data set, the main resource you have is a company’s job descriptions. How much of this information can you pull from those descriptions?\n\n### Figuring out what companies have to offer\n\nCompensation is increasingly easier to get, especially given recent legislation in some states that makes it mandatory for employers to include salary ranges in their job descriptions. Of course, these ranges tend to be wide, and many aren’t super useful (see the examples below), but it’s something. The screenshot below comes from [comprehensive.io](https://comprehensive.io), a site that aggregates comp ranges from job descriptions in NY and California, where companies are legally required to disclose them. As you can see, the ranges are quite wide.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/comprehensive_io_comp_ranges_8b165c53d3.png)\n\nBelow is an example for a specific company and role: Dropbox’s open Mobile Software Engineer role in the US. As you can see, the ranges are pretty wide (a 66K spread for SF, NYC, and Seattle for instance). In my mind, like many of these ranges, all this tells you is “you’re going to get paid market for the location that you’re in”. Of course, if a company is paying below market, that’s something you need to know, but that’s the exception rather than the rule.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/dropbox_comp_0c43fb6d38.png)\n\nSome attributes like vertical, tech stack, total size, and brand prestige, are more consistent than compensation and are possible to figure out from publicly available data.\n\nWhether the company is mission-driven can sometimes be deduced from the vertical, the company’s B-corp status, and the company’s job descriptions. However, given every company’s penchant for sounding mission-driven, even when they do something as dryly mercenary as ad-serving infrastructure, this may be a bit tricky. But I’m sure the tech is there to figure this one out.\n\nProperties like eng team size, upcoming projects and roadmap, and the company’s culture/values are much harder. Sussing these out is not a technological problem but rather a data one, just like determining whether engineers are good or not. Don’t believe me? Read any job description. How do you begin to figure out from this wall of mush what it’s actually like to work at a company? Maybe some great job descriptions exist out there, and maybe you can get somewhere by scraping sites like Glassdoor, but given how low-signal both tend to be, AI will likely get just as hamstrung as the humans trying to parse these documents.\n\nIf you don’t buy that, check out [keyvalues.io](http://keyvalues.io) and look for literally any company. You’ll notice that every company seems to have pulled randomly from the same-grab bag of 20 or so lofty-sounding values that tell you nothing about what actually happens at that company day to day. Instead, you find yourself smack in the middle of a vague virtue signaling arms race.\n\nSo, some of these attributes you can figure out from the public domain, and some you can’t. In a nutshell, the data available to you on both sides looks like this:\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/candidates_and_companies_53617907f9.png)\n\nNow we have to tackle the second part of the matching problem: how do we figure out what each engineer on our list is looking for? For instance, which engineers in our database care about compensation, and what are their salary requirements? And what types of problems are they most interested in? And so on…\n\nThe reality is that there isn’t a good way to intuit most of these things from publicly available data. If you want to know what engineers care about, you have to ask them. And even when you ask them, they will very likely, albeit through no fault of their own, be lying.\n\nFiguring out what engineers value and what they’re looking for\n--------------------------------------------------------------\n\nBecause, to the best of my knowledge, there isn’t a dataset out there that compares and contrasts what engineers said they were looking for versus what jobs they ended up taking, I’ll fall back to an anecdote from earlier in my recruiting career. TL;DR Dr. House was right; everybody lies.\n\nMany years ago, I was interim head of talent at Udacity. Many of my candidates told me that it was their dream to work in ed-tech, but one in particular stood out. In addition to extolling his passion about education, he told me that one of his deal-breakers was working in advertising and that he'd never do it. He did great in our first-round technical screen, and I set him up with an onsite interview. Then, while he was in town, he interviewed at an advertising startup where one of his friends was working. That's the place he ended up choosing… because he really hit it off with the team. Though this particular example was the most stark (the one-letter difference between “ad-tech” and “ed-tech” belies the massive gulf between those verticals), instances like this, where a candidate claimed to strongly want one thing but then ended up choosing something completely different after meeting the team, are the rule rather than the exception.\n\nThe truth is, people will tell you all manner of lies about where they want to work, what they want to work on, and what's important to them. But, of course, they're not lying deliberately. It likely means you're not asking the right questions, but sometimes knowing what to ask is really hard. For many of the questions you can think of, people will have all sorts of rehearsed answers about what they want to do, but those answers are framed to the specific audience and may not reflect reality at all. Or, a lot of the time, people simply don't know what they want until they see it.\n\nIt’s hard enough sussing this stuff out when you’re talking to candidates 1:1. Imagine trying to gather this kind of nuanced information from what they say on social media or in their public blogs.\n\nPerhaps the most important thing I've learned is that, at the end of the day, one of the biggest parts of a recruiter's job is to get the right two people in a room together. Regardless of industry, or domain, or stack, or money (within reason of course), chemistry is king. Get the right two people to have the right conversation, and everything else goes out the window. Everybody lies. It’s not malicious. It’s just that chemistry is the thing that matters most, and all the rest of the attributes above are poor proxies for the magic that sometimes happens when two smart people have a good conversation.\n\nHow do you predict chemistry between people? Can an AI do it? Possibly, if that AI has access to a ton of data about candidates and companies, i.e., everything we’ve discussed in this post thus far… AND past candidate/company interactions and their outcomes.\n\nEven if you can’t get all the candidate and company data you’d need, you CAN get a history of candidate/company interactions and their outcomes from an Applicant Tracking System (ATS). But ATS data is not public. It’s the opposite — for ATSs, their data is their moat, which is what drives retention, and ATS switching costs are painful and often prohibitive.\n\nIn the absence of rich candidate and company data and the interactions between them, an AI predicting chemistry is impossible. Hell, humans can’t do it either.\n\nBut what if you do have the data?\n---------------------------------\n\nIf you have proprietary data, then you don’t need an AI. A simple non-AI program (e.g., a regression) or a human can do the job well enough. In fact, Arvind Narayan from Princeton gave an excellent talk called “[How to recognize AI snake oil](https://www.cs.princeton.edu/~arvindn/talks/MIT-STS-AI-snakeoil.pdf)”, whose crux is that, for complex questions where you need to predict social outcomes (e.g., recidivism, job performance), *no matter how much data you have*, because “AI is not substantially better than manual scoring using just a few features”.\n\nArguably, if you do have the data, you could still build out AI hiring to increase efficiency, but remember that it’s your possession of proprietary data that made an AI approach viable.\n\nConclusion\n----------\n\nSo what does this all mean for the future of recruiting? As I said in the beginning, AI is really well-suited to automating a bunch of recruiting tasks that humans do now. For instance, an AI can take the pain out of stuff like this:\n\n* Interview scheduling\n* Composing first-draft sourcing emails (though you’d need a human to truly make them sing)\n* Enriching candidate profiles with “first-order” data (years of experience, location, figuring out what programming languages they’ve used in some cases, etc.)\n* Tracking candidate progress through a funnel\n* Some amount of assessment[4](#user-content-fn-4)\n* Creating beautiful dashboards that track key recruiting metrics (time to hire, cost per hire, etc.)\n\nAs AI gets more and more sophisticated, the list above will get longer and longer, and given that most recruiters aren’t particularly good at their jobs, over time, AI will take over more and more, and there will be progressively less for human recruiters to do.\n\nHowever, the stuff above isn’t what makes recruiting hard; these are the trappings of recruiting, but not the essence. The hard thing about recruiting is figuring out who’s good and who’s looking right now, and bulldozing the way for those candidates to have as many conversations with companies as they have appetite for, in order to see if they have chemistry with that company and potentially their future team.\n\nUntil we have access to all the data that reliably predicts whether someone is a good engineer, whether they’re looking right now, and what a company offers, and whether they’re interested in that thing, having an AI will not be enough.\n\n**At the end of the day, you can’t use AI for hiring if you don’t have the data. And if you have the data, then you don’t strictly need AI.**\n\nAnd finally, because we don't have the data, humans will also continue to be bad at hiring. The difference is that good human recruiters can make some meaningful warm intros, let 2 engineers get in a room together to see if there's chemistry, and get the hell out of the way.\n\n*Huge thank you to Maxim Massenkoff, who did the data analysis for this post. Also thank you to everyone who read drafts of this behemoth and gave their feedback.*\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. If I were you, I’d be justifiably skeptical at this point. Wow, a recruiter is writing about how AI can’t do recruiting. Classic Luddite trope, right? Let’s burn down the shoe factory because it can never compete with the artisanal shoes we make in our homes. In this case, though, you’d be wrong. I walked away from a very lucrative recruiting agency that I built to start interviewing.io, precisely because I wanted to be the guy who owned the shoe factory, not the guy setting it on fire out of spite. Recruiting needs to change, it needs disintermediation, and it needs more data. It’s the only way hiring will ever become efficient and fair. I just don’t think AI is going to be that change. If I’m wrong, I’ll be the first in line to pivot interviewing.io to an AI-first solution. It’s also worth calling out that in this piece, I focus specifically on hiring engineers, as my whole career and all of interviewing.io is dedicated exclusively to technical recruiting, but I expect that my reasoning holds for many other verticals. [↩](#user-content-fnref-1)\n2. In all my years of trying to figure this out, the thing that carried the most signal is [how many typos engineers had on their resume](https://interviewing.io/blog/lessons-from-a-years-worth-of-hiring-data), which I think highlights the absurdity of this whole undertaking. See [this post](https://interviewing.io/blog/lessons-from-3000-technical-interviews) and [this post](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt) for previous data/discussion about the relative importance of employment history and pedigree in hiring. [↩](#user-content-fnref-2)\n3. See [this piece by Ashe Dryden for some good discussion about the ethics of open source labor](https://www.ashedryden.com/blog/the-ethics-of-unpaid-labor-and-the-oss-community). [↩](#user-content-fnref-3)\n4. You might be wondering at this point why I didn’t list assessments here. Arguably, AI will be able to do some amount of assessment, probably somewhere between a HackerRank/Codility and a real human interviewer. I wrote many years ago about how [recruiting engineers is fundamentally a sourcing rather than a vetting problem](https://blog.alinelerner.com/building-a-product-in-the-technical-recruiting-space-read-this-first/), so I’m very skeptical that AI-based assessments are meaningfully going to change the game – in order for assessments to be effective, engineers have to be willing to do them, which means there has to be value for both sides. This is why senior engineers typically balk at asynchronous coding tests, and the biggest attrition rates are usually at that part of the funnel. The future of technical assessments, in the age of generative AI is a noble and difficult topic, though, and it’s something we’ll tackle in a future post. [↩](#user-content-fnref-4)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/why-ai-cant-do-hiring",
      "author": "",
      "user_id": ""
    },
    {
      "title": "How to sabotage your salary negotiations efforts before you even start",
      "content": "*Note: If you’d like a practical primer on negotiation, read my [previous post on negotiation first](https://interviewing.io/blog/negotiate-salary-recruiter) — it tells you exactly what to say in a bunch of situations. This post is longer and more academic, but of course I include some practical tips and teach you what to say in a few situations, as well.*\n\nAt interviewing.io, we’ve coached hundreds of people through salary negotiation. We’re good at it — our average user gets $50k more in cash, and we have a 94% success rate.\n\nHaving done this a lot, we’ve seen our users make the same two mistakes, over and over, BEFORE they start working with us. These mistakes are costly and make it harder for us to do our jobs. Our advice is applicable to everyone, but I wrote this post primarily to share with interviewing.io’s user base, so that future clients of our negotiation service don’t shoot themselves in the foot.\n\nThese are the two things you **must** avoid. Both involve how you talk to recruiters at the start of your job search, way before there’s an offer:\n\n1. Revealing information too early in the game\n2. Negotiating before you’re ready\n\nIn this post, I’ll explain why these two mistakes routinely sabotage salary negotiation efforts and what to say to recruiters instead. In a nutshell, if you can just be in “passive information gathering” mode (more on that later) for most of your recruiter interactions, you’ll be golden. It’s hard to not to share info about your job search with your recruiter, especially as you build more rapport with them, but we’ll tell you exactly what to say instead.\n\nBefore we get into all of that, I want to go over two foundational things about negotiation.\n\n1. Recruiters are not your friend, and they don't work for you.\n2. What negotiation is and what it’s not\n\nRecruiters are not your friend, and they don’t work for you\n-----------------------------------------------------------\n\n*“It is difficult to get a man to understand something when his salary depends on his not understanding it.*”   \n-Upton Sinclair\n\nI used to be a recruiter. I ran my own agency, and I also worked in-house before starting interviewing.io. That means that I’ve had to [struggle with the tangled incentive structure that comes with being a recruiter](https://blog.alinelerner.com/if-youre-an-engineer-who-wants-to-start-a-recruiting-business-read-this-first) (see the section called “You should write down your principles”). There’s always a tension — recruiters are, by and large, good human beings who genuinely want to help their candidates, but they also have an employer they’re beholden to, as well as a comp/bonus structure that rewards certain behaviors, some of which run counter to candidates’ best interests.\n\nThere’s some distinction between in-house recruiters and third-party recruiters (recruiters who work for an agency that does placement, rather than a specific company that’s hiring engineers).\n\n### Third party recruiters\n\nMy general policy with third-party recruiters is to not tell them ANYTHING and to always deal directly with the companies they introduce you, once you establish a point of contact there. You should assume that anything you tell your recruiter is going to get back to every company you’re working with. Why? Because their primary objective is to place your butt in the seat of one of the companies they’re working with, and they will do whatever they need to do to make the deal happen. Often, those things will run counter to your interests.\n\nA big misconception that many candidates labor under is the idea that because third-party recruiters get paid every time they make a placement, their interests are fundamentally aligned. At a high level, this is kind of true, but once you dig into the details you'll see a lot of nuance.\n\nA recruiter, depending on market conditions, gets anywhere from 8%-25% of the candidate’s base salary when they make a placement. In the current climate, it’s around 10%. However, that cut is going to the recruiting agency as a whole rather than to the individual recruiter — you will almost always end up working with large agencies rather than a sole-proprietor shop where the owner gets to take all of it home.\n\nLet's say that you get an offer with a base salary of $150,000. You talk to your third-party recruiter and tell them that you would like more money. The recruiter may go to the hiring manager and try to advocate for you, but they're not going to push very hard because the incremental difference in their cut is going to be pretty small and to them the thing that matters most is getting butts in seats. After all, they're evaluated on the number of hires they make, first and foremost, independent of comp. Understanding that, let's do the math anyway. Say that they’re able to risk closing the deal and get you $165k. Before, the agency would have gotten paid $15k. Now the agency gets paid $16.5k. That incremental $1.5k isn’t worth risking a deal over (even a few thousand dollars would not justify jeopardizing the deal). On top of that, the individual recruiter is only going to maybe get a few hundred dollars total from that increase. So for them the difference really isn’t worth it. **Third party recruiters are incentivized to get the deal done, not to risk the deal by negotiating hard for you.**\n\nMoreover, because they’re incentivized to get the deal done, you should assume that your recruiter will share anything you share with them with the company or companies they’ve introduced you to. If you tell them that a company is your first choice and that you’re tempted to accept, they will likely share that with the company and may even recommend that they not raise your comp, since you’re already so enthusiastic. If you share that you’re not very interested in a company, and the recruiter has other candidates they’re presenting, they will prioritize those candidates’ experience over yours and will possibly tell the company not to invest in you as hard.\n\n### In-house recruiters\n\nWhat about in-house recruiters? In-house recruiters may or may not get a bonus for hires that happen on their watch; it depends on the company. But if they do, that bonus is generally NOT tied to your compensation, and in some cases, they may get a bigger bonus if they’re able to negotiate you down. At big companies, in particular, in-house recruiters follow a playbook. They’re trained to make offers within specific bands, and they’re trained to mobilize such that they don’t lose candidates to other big companies — if you wave a Facebook counteroffer in front of Google, they will act. If you tell them you’re interviewing at a startup, they will not, because they know that startups don’t pay as much. They’re actually evaluated on how well they follow the playbook. Because of that, there is no reason to assume that their incentives align with yours. They’re incentivized, first and foremost, to follow the rules their head of department sets for them. This is true for how they evaluate candidates, who they let through, and how they read resumes. And it’s definitely true for how they negotiate.\n\nIf you’re interested in peeking behind the curtain on how recruiters think, I interviewed three of the best ones in the industry recently. You can watch that below:\n\n  \nI’ll close this section the way I began it. Recruiters want to help, and many are rooting for their candidates. But they’re also operating inside a box, and that box isn’t set up to put your interests first.\n  \n\nWhat negotiation is and what it’s not\n-------------------------------------\n\nProbably because of bad books and airplane magazine ads (for those of you old enough to remember those), people often think that negotiation is all about saying the right thing, or how firm your handshake is, or any other amount of silly nonsense. **The reality is that negotiation is all about preparation and leverage.**\n\nDon't forget to prepare for your technical interviews too. Sign up today for anonymous mocks with senior engineers from top companies.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nPreparation and leverage means doing the work to make sure that you have multiple offers, that all your offers come in at the same time, and that you don’t tip your hand too early. Laying this foundation is 80% of the work. You’ll need to slow some companies down, speed some companies up, and hold off questions from recruiters until you’re ready to negotiate, and not before. If you do this right, the actual negotiation part will be easy and almost a foregone conclusion.\n\nIs it possible to negotiate when you don’t have multiple offers and when you haven’t done the foundational work? Sure, it is, and we’ve sometimes had success with our users doing that. But it’s much harder, and the ceiling on how much more money you can get is lower.\n\nWith all that out the way, let’s talk about how the two biggest mistakes people make and how to not make them!\n\nMistake #1: Revealing information before you’re ready to negotiate\n------------------------------------------------------------------\n\nYou’ve probably never been arrested, but if you’re like me, you’ve watched a lot of police procedurals on TV. You know the bit where they read the suspect their Miranda rights? They start like this:\n\n*You have the right to remain silent. Anything you say can and will be used against you in a court of law…*\n\nTalking to recruiters is exactly the same, and one of the biggest mistakes we see our users make is sharing information too early. This is generally the only mistake we can’t walk back — once you share information, you can’t undo it, and sharing information actually has no upside, only downside. When you’re ready to negotiate, you’re doing so deliberately because you already *know*\nthe state of the world, and you’re choosing to reveal the parts that set you up for success. Before that, you’re just revealing stuff that can be used against you.\n\n**Specifically, do not share with recruiters anything about your salary history (though it’s illegal in many states to ask this directly, there are indirect ways of asking, and many still do), your salary expectations, where else you’re interviewing, and how far along in the process you are with other companies. In short, don’t share any information about money or other interviews.**\n\nThe main question recruiters ask up front about money is: “What are your compensation expectations?” They claim that it’s because they want to make sure that you’re not so far off in your expectations that interviewing with that company would be a waste of time. This is a nonsense reason — *very few* companies pay so much below market that it would be a nonstarter. Those companies know who they are, and they know to give you a heads up that they pay below market. Moreover, with the recent advent of companies sharing salary bands, you’ll have some idea if they're grossly below market before you interview. The real reason recruiters ask about compensation expectations is so that they can use it against you later in negotiations.\n\nAs such, if you answer this question with a number, you set an artificial ceiling on your offer. Do not even utter a single number to a recruiter until you’re ready to bargain. Do not go on levels.fyi and comment on the ranges listed for your level, even if you’re currently underpaid and an average offer from them would be life changing. **Do not say a number first — ever.**\n\nYou can see exactly what to say when you get asked about compensation expectations in the section called “How to handle recruiter calls” below.\n\nThe most obvious way to lose leverage is revealing information about money. The other way to lose leverage is by sharing information about where else you’re interviewing. If you share this information, you risk prematurely scaring off smaller companies because they don’t think they can win in a bidding war with FAANG. You also risk cornering yourself into a situation where the company knows your options are limited, and they might be inclined to lowball you as a result. Finally, you risk getting an exploding offer to try to force you to make a decision before you’re ready.\n\nBelow are some examples that I hope will drive these points home.\n\n### Example #1: You’re interviewing at Google, Meta, and two startups\n\nLet’s say that you’re currently interviewing at Google, Meta, and two startups (let’s call them A and B). You’re at the onsite stage with Google, you’re doing technical phone screens at both startups, and you’re just doing your first recruiter call with Meta. This is actually a very strong position to be in!\n\nOf course, your Meta recruiter asks you about your comp expectations and where else you’re interviewing.\n\n**If you reveal your comp expectations, it will be hard to walk them back:**\n\n* Let’s say that you currently work at a startup and make $150k in cash with some amount of equity. You go to levels.fyi or a similar site and look up Facebook’s salary bands for the role you’re targeting. Let’s say those bands for total comp are $250k-$350k. Hell, that’s way more cash than you’re making now, so you decide to share that range, thinking that if those are their bands already, it does no harm. That’s reasonable, except that let’s say Google ends up making you an offer, and it’s $400k (we’ve seen this scenario happen to a bunch of our users). Now you have to walk back what you said, in which case your recruiter will invariably ask why. And now you have to reveal, before you’re ready, that you have a Google offer, which means you’ll probably end up revealing that it’s for $400k. Now you’ve set an artificial ceiling for your Facebook counteroffer to be $400k as well, when in reality that ceiling may have been closer to $450k or even $500k.\n\n**If you reveal that you’re at the onsite stage with Google and talking to some startups, here's what will happen:**\n\n* Your recruiter will do the math and start asking you in a few weeks if you got to team matching.\n* Now Meta knows that Google is the only possible offer on the table that they should be worried about. They (and other big companies) don’t take startup offers nearly as seriously because equity is monopoly money 'til it’s not… and even if you don’t reveal the cash portion of your comp, they’ll assume it’s smaller than what they’re offering.\n* If you didn’t pass the onsite, it’s going to be hard to not share that when you’re persistently asked about it, unless you lie (which I absolutely do not condone). Now you’ve lost leverage because Meta knows that you cannot possibly have any other big tech company offers.\n* If you DO perform well in your Google onsite, that’s great, but team matching can take a while (yes, I know Google is changing their process such that you’re now interviewing for a specific team at the outset, but the broader point still stands). So now if Meta is about to make you an offer, they can set an artificially fast expiration date to run out the clock.\n\n**Though you started in a strong position with multiple interviews, including at companies that are known to pay well, you’ve now weakened that position by sharing details.**\n\nHere’s another thing that could happen in this scenario. Let’s say that it’s the same set of companies as above, but this time you’re talking to the recruiter from startup A. The recruiter asks you where else you’re interviewing.\n\n**If you mention that you’re interviewing at both Google and Meta, they might get spooked.**\n\n* I’ve seen this happen a bunch. Dropping FAANG names can be a good power move, or it can shoot you in the foot, and which it’ll be really depends on the situation. Many small startups view FAANG candidates as risky because they know they can’t compete on comp and are worried that you’re going to walk the moment that you get a FAANG offer. This may or may not be true (not everyone is motivated just by comp!), but it’s not to your advantage to reveal it. YOU should be in control of if and when we play the FAANG card.\n\n### Example #2: You’re interviewing at one company and are also up for a promotion\n\nHere’s a different example. Let’s say that you work at a startup, and you’re up for a promotion soon. You figured it’d make sense to see what’s out there as well, so you’ve started interviewing with another startup.\n\nYour recruiter asks you in your first call about where else you’re interviewing and what your comp expectations are. You may be tempted to mention that you’re up for a promotion because that feels like it’ll give you leverage — if you get a promotion, the startup will have to work harder to entice you to leave, after all. Not so fast!\n\n**If you mention that you’re up for a promotion:**\n\n* Your recruiter will start checking in consistently on whether you got it. Promotions always take longer than you think, and the increase in your comp may not be what you expect. At some point, if you haven’t gotten it yet, the recruiter will assume it’s not coming, and then you actually lose leverage because they know that you’re going to be more likely to walk.\n\n**If you mention that you’re not interviewing anywhere else, that’s just a giveaway that you have no leverage:**\n\n* Promotion or not, many of our users assume that they have leverage because, “I don’t have to leave my job, so my current job is leverage.” That’s not true, though — even in this climate, and definitely in a hiring boom, an engineer having another job is table stakes. Almost every candidate you’re competing with will be currently employed. So even if having a job gives you a little bit of leverage, it gets canceled out when everyone has the same exact thing.\n\nThe details may differ in your case, but the fundamental mechanics are the same. When you reveal information before you know what hand you’ve been dealt, it can only hurt you. I’m struggling to think of a scenario where revealing something has been beneficial.\n\nI suppose the one exception to revealing information is this: Sometimes it can be useful to give your recruiter a rough estimate for when you’ll be collecting offers, e.g., “I’ve just started interviewing. I expect to get through all my interviews and onsites in the next 6 weeks and start collecting offers 2 months from now. Does that timeline work for you?”\n\nThis technique can be helpful for aligning expectations up front and then keeping recruiters off your back, as they won’t need to chronically text you to make sure you haven’t taken another offer yet (we’ll talk more about texting with recruiters in the next section). But note that even in this example, we’re not actually revealing any information about where you’re interviewing, how long it’s taking, or compensation. You’re just setting a timeline based on *hypotheticals* without giving out any details that can be used against you later. When you share the actual timeline you’re working with, you no longer control the timing of your job search, and a huge part of negotiation is controlling timing so you can make all your offers come in at the same time.\n\nMistake #2: Premature negotiation\n---------------------------------\n\n*“Don't fire until you see the whites of their eyes!*”  \n-Unknown officer at the Battle of Bunker Hill\n\nJust like not revealing information too early, you also want to avoid negotiating too early. They’re two sides of the same coin.\n\nThink of it like a hand of cards. At the beginning, you have no idea what you’re going to draw. The longer you wait to negotiate, assuming you’ve timed things correctly, the more information you have. Then, when you’re ready to negotiate, you can look at your hand and selectively share information that puts you in the strongest position. For instance, if you have a high base salary from one company, a great equity package from a public company, and a signing bonus from a third company, you can strategically share those portions of the offers without sharing the weaker parts. Each negotiation is different, and it’s hard to give catch-all advice, but that’s generally the situation you should set yourself up to be in.\n\nWith that in mind, I’m a firm believer in negotiating when you’re ready and not before. Until you know what else is on the table, it’s really hard to 1) have the bravado that comes with actually having multiple offers (this is possible to fake, but trust me, it’s hard) and 2) negotiate effectively — you will never know as well as your recruiter what salary bands are like, what market comp is, and so on. They do this all day. This may be your first or fifth time doing it, but there’s massive experience and information asymmetry. There are two ways to combat this power imbalance: have as many of your interactions be asynchronous as possible (we discussed that earlier) and do everything you can to negotiate when you’re at the point of maximum information, and not before. Daniel Biales, one of our former negotiation clients, [explained the latter really well](https://levelup.gitconnected.com/learn-how-to-negotiate-your-salary-with-expert-help-5afedd11a178).\n\n*When I received a low offer, my first inclination was to start the negotiating process. Aline helped me to realize that this was not the best course of action. The problem with this approach is that I wanted to start negotiating before receiving my highest offer. If I negotiated an increase then, I would have had to renegotiate when I received the higher offer. This will cause negotiating fatigue for you and the company. They will be less likely to negotiate a second time because they don’t know how many times you will ask them for more. First, focus on strategies to draw out your decision. Then, when you have all your offers, start negotiating. There may be a couple of back and forth communications, but they will be over a short time span rather than drawn out.*\n\n### Example #1 revisited: You're interviewing at Google, Meta, and two startups\n\nLet’s review our first example again. Imagine that you’re interviewing at Google, Meta, and two startups, A and B, just like before. Startup A makes you an offer: $160k base, 0.1% of the company in options over four years, no signing bonus. You react to it and say that you were hoping for a signing bonus. The recruiter comes back with a $10k signing bonus quickly and pressures you to make a decision, saying that they have other candidates waiting.\n\nBy starting to negotiate, you accelerated their timeline, and this is going to make it hard to go back and ask for more signing bonus.\n\nYou try to stall, and then a few days later, Google makes you an offer that includes a $25k signing bonus. You’re still excited about the startup for reasons other than compensation, but now you have to go back to them and say that you actually got a $25k signing bonus at Google. They are unlikely to move again.\n\nSo, don’t negotiate until you’re ready. It’s hard to walk things back.\n\nThat doesn’t mean it’s not possible to negotiate in stages and gradually start bringing up all your offers. In my experience, however, this is a much more difficult maneuver, takes way more experience, is much more stressful for the candidate, and often ends up with the same results as laying a solid foundation and just negotiating once at the end.\n\nSo what do you say when you get asked pointed questions by your recruiter about your comp expectations or where you’re interviewing? And how do you delay negotiation until you’re ready?\n\nHow to handle recruiter **calls:** “passive information gathering” mode\n-----------------------------------------------------------------------\n\nUntil you’re ready to negotiate, your default mode should be “passive information gathering.” This means that you listen rather than talk. I coach all of our negotiation clients to be in this mode when they get on a call with a recruiter.\n\n* **Be polite and gracious to a fault.** If it’s an offer call, thank them for the work they did to put your offer together and for advocating for you. If it’s earlier in the process, thank them for their help so far.\n* **Express genuine enthusiasm.** If it’s an early call, express excitement about the company or the interview process if there’s something idiosyncratically cool about it. If it’s an offer call, express excitement for the team, the projects, the hiring manager, whatever it is. And be genuine. Every offer will have something exciting about it.\n* If the recruiter is making an offer, do not react to what you’re being told beyond expressing enthusiasm. **Say that you need some time to process and/or talk to your {family, partner, spouse}.** Why should you not react? Because recruiters do this a lot, and you don’t. Negotiating on the phone on the fly is really hard. You’re going to need time to think, and you need a way to level the playing field. Email is the great leveler in these interactions because it’s asynchronous, and it gives you time to think and plan. No one is realistically expecting you to react to major life decisions on the fly!\n* **Use email (not phone when possible and *DEFINITELY* not texts) to interact with recruiters. We strongly advise you to leave your phone number off your resume, and if you have to fill in any required phone number fields while applying, to put a Google voice number. Also, in your first conversation with your recruiter, let them know that you are very bad at using the phone and strongly prefer email. Finally, whenever they text or call, wait a few hours, and answer with email and remind them that email is the best way to reach you (you can always say that you don’t have notifications enabled during work hours).** Why does all of this matter? Phone calls are extremely disruptive, as are texts. The only way to level the playing field in negotiation is to have time to think and to possibly ask for advice. In particular, the casual nature of texting lulls you into a false sense of security. Moreover, the fact that texts or phone calls interrupt you from something else puts you at a disadvantage — when you get interrupted, your instinct is to quickly respond to make the interruption go away. But knee-jerk responses are rarely the right ones, and you’ll find yourself giving away information you shouldn’t have. You will have to get on the phone eventually, of course, but you never have to text.\n* If a recruiter asks you pointed questions about comp expectations or where else you’re interviewing, refer to the section called “Exactly what to say” right below!\n\nExactly what to say\n-------------------\n\nIn all the snippets below, you’ll notice that they end with the same sentence: *I promise not to accept other offers until I have a chance to discuss them with you.*\n\nThis is deliberate, and it’s there because it’s disarming. Fundamentally, recruiters ask you all of these questions because they don’t want to lose out on you and have you go to another company. If you can speak to that worry head-on, there’s not much they can say back.\n\n**For questions about comp expectations at the beginning of the process:**   \n*At this point, I don’t feel equipped to throw out a number because I’d like to find out more about the opportunity first – right now, I simply don’t have the data to be able to say something concrete. If you end up making me an offer, I would be more than happy to iterate on it if needed and figure out something that works. I promise not to accept other offers until I have a chance to discuss them with you.*\n\n**For questions about comp expectations at the end of the process:**   \n*It sounds like there’s an offer coming, and I’m really excited about it. I’m not sure exactly what number I’m looking for, but if you’d be able to share what an offer package might look like, then I will gladly iterate on it with you if needed and figure out something that works. I promise not to accept other offers until I have a chance to discuss them with you.*\n\n**For questions about where else you’re interviewing at the beginning of the process:**  \n*I’m currently speaking with a few other companies and am at various stages with them. I’ll let you know if I get to the point where I have an exploding offer, and I promise not to accept other offers until I have a chance to discuss them with you.*\n\n**For questions about where else you’re interviewing at the end of the process:**  \n*I’m wrapping things up with a few companies and in process with a few more. I promise to keep you in the loop, and I promise not to accept other offers until I have a chance to discuss them with you.*\n\n**For when a recruiter provides you a salary range and asks you to comment on it, at the beginning of the process:**  \n*Thank you for sharing that with me. Right now I don’t know enough about the opportunity to value it concretely, and I honestly haven't done my market research. If you end up making me an offer, I would be more than happy to iterate on it if needed and figure out something that works. I promise not to accept other offers until I have a chance to discuss them with you.*\n\n**For when a recruiter provides you a salary range and asks you to comment on it, at the end of the process:**  \n*Thank you for sharing that with me. I haven't done my research, so I am unable to comment on that range. However, if you do make me an offer, I promise to iterate on it if needed and figure out something that works. I promise not to accept other offers until I have a chance to discuss them with you.*\n\nConclusion\n----------\n\nI said it in the beginning, and I’ll say it again. Negotiation isn’t about saying the right thing. It’s about laying a foundation: not revealing anything until you’re ready to negotiate, not negotiating too early, and making sure that you’ve set yourself up to have multiple offers.\n\nThen, once those offers come in, you swoop in with sharp precision, negotiate once (possibly with just your top choice company), and be done with it.\n\nIf you’ve set yourself up for success, done the foundational work, and haven’t made the mistakes in this post, the negotiation will feel like a foregone conclusion.\n\n*If you need some hands-on help navigating salary negotiation, [sign up for our salary negotiation package](https://start.interviewing.io/salary-negotiation). You don't pay anything unless you get more, and we’ll be with you every step of the way, for every recruiter call, every email you need ghostwritten, and every strategy discussion. Unlimited sessions, unlimited help, whatever we need to do to get you results.*",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/sabotage-salary-negotiation-before-even-start",
      "author": "",
      "user_id": ""
    },
    {
      "title": "6 red flags I saw while doing 60+ technical interviews in 30 days",
      "content": "*Hey, Aline (founder of interviewing.io) here. We’re trying something new. Up till now, all posts on this blog have been written by interviewing.io employees or contractors. Why? Frankly, it’s hard to find great content in the recruiting space. There’s so much fluff and bad advice out there, and we didn’t want any part of that.*\n\n*The other day though, I was reading Hacker News and saw* [*an article by Uduak Obong-Eren about how he did over 60 technical interviews in 30 days*](https://medium.com/@meekg33k/14-lessons-i-learned-from-doing-60-technical-interviews-in-30-days-7732e4f7608d?source=---------4------------------) *and what he learned from that gauntlet of an experience. I thought it was honest, vulnerable, well-written, and brimming with actionable advice. So, I reached out to him to see if he’d want to write something else. Fortunately, he did, and the article below is the inaugural post in what I hope will become our Guest Author series. You can read more about Uduak in the bio below.*\n\n*A quick note because this is the first time we’re doing this. One of the things I’m most excited about with this new Guest Author series is the diversity of opinions it will bring to our blog. Technical interviewing and hiring is fraught with controversy, and not everything these posts contain will be in line with my opinions or the official opinions of interviewing.io. But that’s what’s great about it. After over a decade in this business, I \\*still\\* don’t think there’s a right way to conduct interviews, and I think hiring is always going to be a bit of a mess because it’s a fundamentally human process. Even if we don’t always agree, I do promise that the content we put forth will be curated, high quality, and written by smart people who are passionate about this space.*\n\n*Now, off we go!*\n\n![Author avatar](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FUduak_Obong_Eren_0a6ec8d658.jpg&w=384&q=75 \"Uduak Obong-Eren\")\n\nUduak Obong-Eren\n\nUduak Obong-Eren is a Software Engineer based in the San Francisco Bay Area who is passionate about architecting and building scalable software systems. He has about five years of industry experience and holds a Masters in Software Engineering from Carnegie Mellon University. He is also an open source enthusiast and writes technical articles – you can view some of his writings at [https://meekg33k.dev](https://meekg33k.dev/). He especially enjoys conducting free mock technical interviews to help folks get better at technical interviewing. You can follow him on Twitter [@meekg33k](https://twitter.com/meekg33k).\n\nWhat is the one thing you would look out for if you had to join a company?\n\nSometime between January and February 2020, I wanted to change jobs and was looking to join a new company. This, among other reasons, led me to embark on a marathon of technical interviews – [60+ technical interviews in 30 days](https://www.freecodecamp.org/news/what-i-learned-from-doing-60-technical-interviews-in-30-days/).\n\nDoing that many number of interviews in such a short time meant I had an interesting mix of experiences from the various companies I interviewed with, each with their unique culture and values that often reflected in the way their interviews were conducted, intentionally or not.\n\nIn this article, I will be sharing some of the red flags I observed while I was on this marathon of technical interviews. I will not be mentioning names of any companies because that’s not the intent behind this article.\n\nThe goal of this article is also not to make you paranoid and be on the hunt for red flags in your next interview, far from it. Rather the goal is to equip you with knowledge to help you immediately identify exactly the same or similar red flags in your next interview\\*,\\* and hopefully identifying them will set you up to better handle them.\n\nEven though the stories I’ll be sharing come from my marathon of *technical* interviews, these red flags do not apply only to technical interviews. They apply to all kinds of interviews and so there’s a lot to learn here for everyone.\n\nThe Red Flags\n-------------\n\n### Your interviewer is only open to solving the problem ONE way\n\nIn the world of computing and in life generally, for any given problem, there is typically more than one way to solve that problem. For example, given a sorting problem, you could solve it using a [merge-sort](https://www.programiz.com/dsa/merge-sort) algorithm or a [heap sort](https://www.programiz.com/dsa/heap-sort) algorithm.\n\nHaving this rich number of techniques to solve a problem makes it even more interesting and the general expectation in technical interviews is that you should have the flexibility to solve a problem using your preferred technique.\n\nI had an interview where the interviewer asked me to solve an algorithmic problem. I had started solving the problem using a specific technique when the interviewer stopped me in my tracks and asked that I use another technique.\n\nWhen I probed a bit further to know why, it appeared that the reason he asked that I used the second wasn’t to test my knowledge of that second technique; it was because he was more ‘comfortable’ with that approach.\n\nIt is different if the interviewer wants to test your knowledge of something very specific. For example, given a problem that can be solved using iteration and [recursion](https://interviewing.io/recursion-interview-questions), the interviewer may want to test your knowledge of recursion and can ask you to solve the problem recursively. That wasn’t the case here.\n\nI ended up using both techniques and discussed the trade-offs but frankly that experience left a bad taste in my mouth especially because that interview was with the hiring manager — my would-be manager, who is someone that can significantly influence your career growth and trajectory.\n\n### Undue pressure to accept an offer letter\n\nIt’s very exciting and fulfilling when you go through all preliminary stages of a technical interview, through to the onsite interview (remote or in-person) and then you receive that “*Congratulations <insert name here>, we are pleased to offer you…*” email.\n\nHowever, that excitement often becomes short-lived when there is some form of pressure from your soon-to-be employer to accept the offer. It’s a bit more manageable when the pressure comes from someone in HR or the recruiter, but when it’s from the hiring manager, that can be harder to manage.\n\nThat was the case for me when I interviewed with a startup based in Palo Alto. They were a small company in terms of staff strength. My onsite interview with them had gone quite well. I had a good conversation with the hiring manager and an even better conversation with the VP of Engineering, so much so that I could tell that I was going to be extended an offer. I asked to know how long I had to accept the offer letter and I was told seventy-two hours.\n\nThe offer letter arrived later that evening, and it looked great — a six-figure offer definitely didn’t seem like a bad start. I was also at the final stage of the interview process with other companies too and thankfully, I had enough time to negotiate and accept the offer, or so I thought.\n\nThen the pressure started, incessant calls from the hiring manager and the VP of Engineering, back-to-back emails, all within the allotted time. So much was the pressure that it got to a point where I wasn’t sure I wanted to negotiate the offer anymore. I turned down the offer.\n\nI turned down the offer because the experience got me thinking about the company’s work culture. Were the methods employed by the company to get me to accept the offer indicative of their work culture? If they needed to get something done, how far would they go?\n\nNow don’t get me wrong, yes the company wants to employ you, yes the recruiting team wants to ‘close the deal’ however, it’s very important to pay attention to how the company does this. Do they remain professional about it?\n\nA company’s values go beyond what they say, it shows in what they do and how they do it.\n\n### Not enough clarity about your role\n\nAmong the many reasons why you would join a company is your desire to be involved in valuable work. I had the opportunity to join a US company based in Boulder, Colorado. They had contracted a recruiting agency to help them find someone to fill a Software Engineer position in their firm.\n\nThe hiring process started with an exploratory interview with the recruiting agency closely followed by a second interview with a recruiter from the company. In both interviews, I couldn’t get a clear sense of the details of my specific role was — what team I would be on, what kinds of projects I’d be working on, what the career growth pathway was, etc.\n\nI understand that sometimes companies can be going through restructuring, but that didn’t seem to be the case here. It seemed more like the company was focused on completing their headcount. Even though there’s nothing wrong with completing a headcount, I think there is everything wrong with not having a clear purpose for a role for a couple of reasons:\n\n* It means the role may not be critical to the company’s core business.\n* If the role isn’t that important, it may mean when a layoff comes, your position may be impacted.\n\nOn a more personal note, I don’t want to be just a number. I want to work at a place where I have the opportunity to contribute in an impactful way, and I like to believe you would too. So it’s important to get clarity about your role, for where you are today and for future career growth.\n\n### Consistent lack of interest or low morale from interviewers\n\nWhen looking to join a company, one of the things you simply must care about is the team you will end up working on. At least 25% of your waking hours will be spent interacting with that team whether in-person or virtually.\n\nInterviews offer you an opportunity to experience firsthand what it will look like to work with your prospective teammates, especially since, unless you’re interviewing at a huge company, your interviewers are likely to become your teammates.\n\nIf through all the different stages of the interview process, you experience a **consistent** lack of interest or low morale from your interviewers, you might want to pay attention .\n\nWhen I experienced that during one of my interviews, I couldn’t exactly tell what the cause was, but I knew something just wasn’t right. After some internal tussle, I decided to trust my gut feelings and ended the interview process with the company.\n\nFast forward to two months after, two of my interviewers (would-be teammates) had left the company and joined another company (no I wasn’t stalking, I just checked on LinkedIn).\n\nNow I’m not saying that during the interview process, there won’t be one or two people, who because of their busy schedules, would have preferred to be doing something else rather than interviewing. Yet, when all of the interviewers don’t want to be there, you certainly want to pay attention to that.\n\nA lack of interest or low morale could be pointers to a combination of any of the following:\n\n* Your prospective team-mates may be experiencing burnout.\n* Some internal dissatisfaction with company — culture, policies, something, anything.\n* The team isn’t that interested in you (hard pill to swallow?), maybe they don’t see you as a long-term hire.\n\nOr it could be for reasons that I have not included here, but I implore you to not ignore this red flag if you see it in your next interview.\n\n### Your interviewers aren’t prepared for the interview\n\nHave you been in an interview before where the interviewer doesn’t seem to have any questions to ask you? Trust me it can get really awkward.\n\nThat was my experience during a technical phone-screen interview with an educational technology company based in California. The interviewer wasn’t prepared for our interview and didn’t have any questions at hand. He wasn’t even sure of who he was interviewing and what role I was interviewing for. It wasn’t a pleasant experience.\n\nI understand that there are a myriad of reasons why interviewers may not be prepared for an interview. Some of which include:\n\n* Lack of proper planning by the HR/recruiting team.\n* Last-minute changes on the interviewee.\n* Busy schedules for the interviewer.\n* The interviewer just wasn’t prepared.\n\nI typically won’t act on this red flag in isolation. I will be looking for other red flags in a bid to form a cluster of patterns before making any decision.\n\n### Lack of a clear direction on where the company is headed\n\nIt’s fulfilling to be a part of a company that is involved in meaningful work that creates value for its users. Joining such a company would mean you have the desire to contribute in helping the organization meet its goals. This invariably means the organization must have some goals right?\n\nI was contacted by a startup based in San Francisco via [AngelList](https://wellfound.com/). I had a first introductory call with a recruiter from the company, closely followed by a phone screen technical interview.\n\nIn both interviews, even though the interviewers shared some details about the company, there was a lot of vagueness and about the company’s direction and where the company was headed.\n\nI particularly remember that one question I asked at the time, was about how the company would deal with its growing competition. Sadly, the answers I got didn’t seem convincing and the company later got acquired by the competition.\n\nWhen you are interviewing to join a company, you are selling more than just your skills, but also yourself — your unique experience. While it’s important to do that, I think it’s equally important that the company should be able to sell you on its vision and what it hopes to achieve.\n\nWhen I think of joining a company, I picture myself in that company for the next 2–5 years. If my vision for where I want to be in my career doesn’t align with the company’s vision, that is a mismatch that shouldn’t be ignored.\n\nConclusion\n----------\n\nWe sometimes focus more on securing the job and even though that is very important, even more important than getting the job is staying fulfilled on the job. **For me, fulfillment meant joining a company that had a clear vision of where they were headed, working in a role that was critical to the company’s business while being equipped with a lot of growth opportunities**.\n\nHopefully, these red flags I have shared will equip you to make better decisions on what companies you choose to grow your career with. I would generally not advise making a decision based on one or two red flags, but if you see a cluster of red flags, you shouldn’t ignore them. I wish you the best in your career journey.\n\nIf you ever need someone to do a mock interview with you, feel free to schedule one [here](https://interviewing.io/) or you can reach out directly to me on Twitter [@meekg33k](https://twitter.com/meekg33k).\n\nAnd if you’d like a list of things to ask companies while you’re interviewing that may help you identify these red flags (and others!) sooner, [take a look at this one](https://blog.alinelerner.com/how-to-interview-your-interviewers/).\n\n---\n\n*If you have something to say about your adventures in interviewing or hiring, write a guest post on our blog! Please email me at [aline@interviewing.io](mailto:aline@interviewing.io) to get started.*",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/6-red-flags-i-saw-while-doing-60-technical-interviews-in-30-days",
      "author": "",
      "user_id": ""
    },
    {
      "title": "LinkedIn endorsements are dumb. Here’s the data.",
      "content": "If you’re an engineer who’s been endorsed on LinkedIn for any number of languages/frameworks/skills, you’ve probably noticed that something isn’t quite right. Maybe they’re frameworks you’ve never touched or languages you haven’t used since freshman year of college. No matter the specifics, you’re probably at least a bit wary of the value of the LinkedIn endorsements feature. The internets, too, don’t disappoint in enumerating some [absurd potential endorsements](https://www.socialtalent.co/blog/endorsement-bombing) or in [bemoaning the lack of relevance of said endorsements](https://news.ycombinator.com/item?id=6292348), even when they’re given in earnest.\n\nHaving a gut feeling for this is one thing, but we were curious about whether we could actually come up with some numbers that showed how useless endorsements can be, and we weren’t disappointed. If you want graphs and numbers, scroll down to the “Here’s the data” section below. Otherwise, humor me and read my completely speculative take on why endorsements exist in the first place.\n\nLinkedIn endorsements are just noisy crowdsourced tagging\n---------------------------------------------------------\n\nPretend for a moment that you’re a recruiter who’s been tasked with filling an engineering role. You’re one of many people who pays LinkedIn ~$9K/year for a recruiter seat on their platform. That hefty price tag broadens your search radius (which is otherwise artificially constrained) and lets you search the entire system. Let’s say you have to find a strong back-end engineer. How do you begin?\n\nUnfortunately, LinkedIn’s faceted search (pictured below) doesn’t come with a “can code” filter.\n\n![Screenshot of LinkedIn search](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Flinkedin_can_code_8178d2dc43.png&w=640&q=75 \"LinkedIn search\")\n\nSo, instead of searching for what you really want, you have to rely on proxies. Some obvious proxies, [even though they’re not that great](https://interviewing.io/blog/lessons-from-3000-technical-interviews), might be where someone went to school or where they’ve worked before. **However, if you need to look for engineering ability, you’re going to have to get more specific. If you’re like most recruiters, you’ll first look for the main programming language your company uses** (despite [knowledge of a specific language not being a good indicator of programming ability](https://www.kalzumeus.com/2011/10/28/dont-call-yourself-a-programmer/) and despite most hiring managers not caring which languages their engineers know) and then go from there.\n\n**Now pretend you’re LinkedIn. You have no data about how good people are at coding, and though you do have a lot of resume/biographical data, that doesn’t tell the whole story.** You can try relying on engineers filling in their own profiles with languages they know, but given that engineers tend to be pretty skittish about filling in their LinkedIn profile with a bunch of buzzwords, what do you do?\n\n**You build a crowdsourced tagger, of course! Then, all of a sudden, your users will do your work for you.** Why do I think this is the case? Well, if LinkedIn cared about true endorsements rather than perpetuating the skills-based myth that keeps recruiters in their ecosystem, they could have written a weighted endorsement system by now, at the very least. That way, an endorsement from someone with expertise in some field might mean more than an endorsement from your mom (unless, of course, she’s an expert in the field).\n\nBut they don’t do that, or at least they don’t surface it in candidate search. It’s not worth it. Because the point of endorsements isn’t to get at the truth. It’s to keep recruiters feeling like they’re getting value out of the faceted search they’re paying almost $10K per seat for. In other words, improving the fidelity of endorsements would likely cannibalize LinkedIn’s revenue.\n\nYou could make the counterargument that despite the noise, LinkedIn endorsements still carry enough signal to be a useful first-pass filter and that having them is more useful than not having them. This is the question I was curious about, so I decided to cross-reference our users’ interview data with their LinkedIn endorsements.\n\nThe setup\n---------\n\nSo, what data do we have? First, for context, interviewing.io is a platform where people can practice technical interviewing anonymously with interviewers from top companies and, in the process, find jobs. Do well in practice, and you get guaranteed (and anonymous!) technical interviews at companies like Uber, Twitch, Lyft, and more. Over the course of our existence, we’ve amassed performance data from close to 5,000 real and practice interviews.\n\nWhen an interviewer and an interviewee match on our platform, they meet in a collaborative coding environment with voice, text chat, and a whiteboard and jump right into a technical question. Interview questions on the platform tend to fall into the category of what you’d encounter at a phone screen for a back-end software engineering role. Some examples of these interviews can be found on our [public recordings](https://interviewing.io/mocks) page.\n\nAfter every interview, interviewers rate interviewees on a few different dimensions, including technical ability. Technical ability gets rated on a scale of 1 to 4, where 1 is “poor” and 4 is “amazing!”. On our platform, a score of 3 or above has generally meant that the person was good enough to move forward. You can see what our feedback form looks like below:\n\n![Screenshot of Interviewing.io interview feedback form highlighting the question: how were their technical skills?](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ftech_skills_109973cdf5.png&w=1920&q=75 \"Interviewing.io interview feedback form\")\n\nAs promised, I cross-referenced our data with our users’ LinkedIn profiles and found some interesting, albeit not that surprising, stuff.\n\nEndorsements vs. what languages people actually program in\n----------------------------------------------------------\n\nThe first thing I looked at was whether the programming language people interviewed in most frequently had any relationship to the programming language for which they were most endorsed. It was nice that, across the board, people tended to prefer one language for their interviews, so we didn’t really have a lot of edge cases to contend with.\n\n**It turns out that people’s interview language of choice matched their most endorsed language on LinkedIn just under 50% of the time.**\n\nOf course, just because you’ve been endorsed a lot for a specific language doesn’t mean that you’re not good at the other languages you’ve been endorsed for. To dig deeper, I took a look at whether our users had been endorsed for their interview language of choice at all. It turns out that people were endorsed for their language of choice 72% of the time. This isn’t a particularly powerful statement, though, because most people on our platform have been endorsed for at least 5 programming languages.\n\nThat said, even when an engineer had been endorsed for their interview language of choice, that language appeared in their “featured skills” section only 31% of the time. This means that **most of the time, recruiters would have to click “View more”** (see below) **to see the language that people prefer to code in, if it’s even listed in the first place**.\n\n![Screenshot showing a list of Featured Skills & Endorsements](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Flinkedin_endorsement_270b56e616.png&w=1920&q=75 \"LinkedIn: Featured Skills & Endorsements\")\n\nSo, how often were people endorsed for their language of choice? Quantifying endorsements is a bit fuzzy, but to answer this meaningfully, I looked at how often people were endorsed for that language relative to how often they were endorsed for their most-endorsed language, in the cases when the two languages weren’t the same (recall that this happened about half the time). Perhaps if these numbers were close to 1 most of the time, then endorsements might carry some signal. As you can see in the histogram below, this was not the case at all.\n\nThe x-axis above is how often people were endorsed for their interview language of choice relative to their most-endorsed language. The bars on the left are cases when someone was barely endorsed for their language of choice, and all the way to right are cases when people were endorsed for both languages equally as often. All told, the distribution is actually pretty uniform, making for more noise than signal.\n\nLinkedIn endorsements vs. interview performance\n-----------------------------------------------\n\nThe next thing I looked at was whether there was any correspondence between how heavily endorsed someone was on LinkedIn and their interview performance. This time, to quantify the strength of someone’s endorsements, I looked at how many times someone was endorsed for their most-endorsed language and correlated that to their average technical score in interviews on interviewing.io.\n\nBelow, you can see a scatter plot of technical ability vs. LinkedIn endorsements, as well as my attempt to fit a line through it. **As you can see, the R^2 is piss-poor, meaning that there isn’t a relationship between how heavily endorsed someone is and their technical ability to speak of.**\n\nLinkedIn Endorsements vs. no endorsements… and closing thoughts\n---------------------------------------------------------------\n\nLastly, I took a look at whether having any endorsements in the first place mattered with respect to interview performance. If I’m honest, I was hoping there’d be a negative correlation, i.e. if you don’t have endorsements, you’re a better coder. After running some significance testing, though, **it became clear that having any endorsements at all (or not) doesn’t matter.**\n\nSo, where does this leave us? As long as there’s money to be made in peddling low-signal proxies, endorsements won’t go away and probably won’t get much better. It is my hope, though, that any recruiters reading this will take a second look at the candidates they’re sourcing and try to, where possible, look at each candidate as more than the sum of their buzzword parts.\n\n*Thanks to Liz Graves for her help with the data annotation for this post.*",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/linkedin-endorsements-useless",
      "author": "",
      "user_id": ""
    },
    {
      "title": "The unwritten rules (till now) of negotiating with Meta",
      "content": "*If you don't like reading, here's me presenting the contents of this blog post in a video. Pick your poison.*\n\n---\n\n**EDIT**: This post is based on stories from users who were interviewing for E4-E6 SWE and MLE roles at Meta. Other, more specialized roles may have different processes than what's described here.\n\n**EDIT 2**: As of Q1 2025, it looks like Meta is sometimes foregoing team matching in favor of driving candidates to their Monetization org (which apparently has a lot of open headcount). If this happens to you, after you pass the hiring committee, instead of entering team matching, your recruiter will tell you that you’ve been assigned to this particular org and, if you proceed, you’ll just get an offer. You won’t get to talk to your future manager, and you’ll find out team details a week after you join. From what we know, it’s possible to insist on team matching instead, with the downside that it’ll take an unknown amount of time rather than certainty.\n\nWhy does this matter?\n\nIf you’re looking to use your Meta offer primarily as leverage with other companies, this is a great thing because you’ll get numbers quickly. If you’re seriously interested in Meta, then this could be good or bad and depends where you are in the process and how much you want to work on monetization versus other things/how much you want to meet your manager before you work for them.\n\nSo if you’re seriously interested in Meta, whether this is good or bad depends on where you are in the process. If you need them to move quickly, it’s great because you know you won’t be stuck in team matching. However, if you need them to slow down because you aren’t as far along with other companies or because you want to work in a different org, you may want to ask your recruiter to stick with the old team matching process. In our experience, it’s ok to ask this. Your recruiter may not tell you outright that that’s an option, but if you ask, they will probably do it. Of course, you may be stuck longer than you want. But at least then you will have more of a say in what you work on.\n\n---\n\nAt interviewing.io, one of the services we offer our users is salary negotiation. Even though I’m the founder, I still do many of the sessions myself because they give me an invaluable insider’s perspective on what’s actually going on in the engineering market, what different companies’ offers look like, how companies extend offers, what kinds of deadlines they give, and how much they go up in compensation, under what circumstances.\n\nAccess to this kind of data is great because it helps me make better business decisions. But sometimes I see questionable patterns of behavior among companies. Recently, I’ve observed a string of practices at Meta that I find reprehensible, and that’s what this post is about. I’ve seen the same practices with enough candidates, and across enough different titles and positions, that it’s become clear to me that they are not isolated incidents or a rogue recruiter’s doing but rather a consistent implementation of a deliberate strategy that comes from the top.\n\nI’m writing about this for two reasons. First, if you’re negotiating with Meta, you need to know how they operate and understand the unwritten rules of the game. If you do not know the rules, you will fail — long before you even start negotiating.\n\nSecond, I’m hoping that someone at Meta sees this post and that maybe it’ll spark an internal discussion about changing the rules.\n\n*By the way, if I’m wrong, I will gladly issue a retraction and a public apology. Please contact me if you’re a recruiter at Meta and find something incorrect in this post. My email is [aline@interviewing.io](mailto:aline@interviewing.io).*\n\nLastly, if you’re about to interview there or are interviewing there already, please [read our free, long-form guide to their interview process and questions](https://interviewing.io/guides/hiring-process/meta-facebook#meta).\n\nMeta basically has a monopoly on FAANG hiring right now\n-------------------------------------------------------\n\nI mentioned above that we do salary negotiation, but our main business is mock interviews. We offer anonymous mock interviews in the style of a bunch of different companies (mostly FAANGs). This means we know how many people are practicing for interviews at Google vs. Meta vs. other FAANGs, and that lets us guess (pretty accurately) how much hiring is actually happening at these companies.\n\nYou can read in way more detail about how all the FAANGs are doing in our [recent blog post where we made 2024 predictions](https://interviewing.io/blog/when-is-hiring-coming-back-predictions-for-2024) based on our proprietary data. But while I was writing that post, I noticed something odd. Meta was hiring way more engineers than any of the other FAANGs. In fact, Meta hiring is up more than 10X since January of last year.\n![Mock interview purchases for FAANG companies on interviewing.io (as a proxy for hiring volume) in 2023](https://strapi-iio.s3.us-west-2.amazonaws.com/Mock_interview_purchases_for_FAANG_companies_on_interviewing_io_as_a_proxy_for_hiring_volume_in_2023_c7dbe9a6d7.png)\nYou can see that more recently Amazon has picked up a bit, but it’s very recent and not enough to drive major change in other companies’ behaviors (at least not yet). And, yes, Netflix is hiring too, but Meta’s eng team is more than 10X the size of Netflix’s, so in the absolute, Netflix’s hiring volume isn’t enough to balance Meta out. For all intents and purposes, Meta’s the only FAANG that’s really hiring at scale — and they’re currently getting away with treating candidates really poorly as a result.\n\nHow Meta negotiates, given their effective monopoly on eng hiring\n-----------------------------------------------------------------\n\nHere’s how Meta runs their hiring process. These practices have been consistent across every negotiation client we’ve had in the past 6 months or so.\n\nMeta’s hiring is centralized, which means that you enter one big interview process that’s completely divorced from which team you might end up on and you interview with people whom you might never work with again. If you do well, there will be a team matching component after you pass the onsite but before you get an offer[1](#user-content-fn-1). With that in mind, here’s how they run their process, once you get the green light.\n\n1. **Team matching**. This can take days or weeks, depending on how many teams you speak to and how many conversations you have with the people on each team. You'll speak with hiring managers to gauge fit and chemistry, and if you’d like, you can also talk to peers. We've heard that sometimes you get the chance to talk to a handful of teams, and sometimes it's over 10. After your team-matching conversations, your recruiter will ask you to choose a team. In order to move forward, both you and the hiring manager have to opt in[2](#user-content-fn-2).\n2. **Likely down-level you**. Sometime during team matching, you’ll probably find out that you’ve been down-leveled. Often, your recruiter will cite your performance in the system design portion (and sometimes the behavioral portion). According to a recent survey we did, something like 55% of Meta candidates get down-leveled (more likely for generalist SWE roles, less likely for more niche roles like ML).\n3. **Make a lowball offer with just a few days to make a decision**. Once you’re done with team matching, things get dicey. Your recruiter will make you a lowball offer that’s often $50k or more (!!) below the average TC on levels.fyi. Moreover, you usually just get a couple of days to make a decision. If you were down-leveled, your lowball offer may include a small signing bonus as a consolation prize.\n4. **Refuse to negotiate unless you can show them other offers from comparable companies**. Your recruiter will say something like, “If you’d like to increase your offer, I can take this to the compensation committee, but I need a compelling reason [i.e., another offer].\"\n5. **If you have other offers, they will apologize for the lowball offer, citing that it’s “automatic numbers from our computer” and raise the numbers by $100K or more (in first year’s TC)**. If you do not, you will be stuck with a lowball offer, though you may be able to negotiate a small signing bonus if the offer didn’t come with one already.\n\nHow to negotiate with Meta\n--------------------------\n\nBelow are the steps for negotiating with Meta in a hard climate where they have a monopoly on hiring. We hope that most of these won’t be necessary in the future. They are:\n\n* Don’t share anything with your recruiter\n* Make sure you have other offers\n* Slow-play team matching\n* Build rapport with every hiring manager you talk to\n* Actually negotiate (this is the easy part)\n\n### Don’t share anything with your recruiter\n\nWe wrote a whole post about how to avoid sharing information with your recruiter and why this is so vital. If you share where you’re interviewing or how far along you are, or if you start negotiating prematurely, the strategies below won’t work.\n\nPlease [read our post on not shooting yourself in the foot during negotiations](https://interviewing.io/blog/sabotage-salary-negotiation-before-even-start) before you continue!\n\nYou should also be aware of a few tricks specific to Meta recruiters. If you're not forthcoming about where else you're interviewing, they may say a few sneaky things.\n\nFirst, they may say something like, \"Well, we just want to know where you're interviewing so we can intro you to others who interviewed at those companies but ultimately chose Meta.\" Don't fall for that early in the process. It's a trap to get information out of you. You can always ask for those intros later, when you're negotiating and it's the right time information about your other offers.\n\nThe other thing they do is fish, saying something like, \"Well, in case you're interviewing at {Google, Netflix, some other big company they don't want to lose candidates to}, just so you know, they move kind of slowly, so we may not be able to wait.\" There, the Meta recruiter's goal is to get you to say, \"Oh, no, don't worry, I'm not interviewing at Google!\"\n\nNow, you've lost leverage in their eyes because that's one less competitive company they might lose you to... and they'll be more confident about lowballing you later on.\n\n### Make sure you have other offers\n\nAs you may have guessed from reading the previous section, it’s critical to have other offers, and not just any offers but ones from top-tier companies who pay very competitively[3](#user-content-fn-3).\n\nWe realize that saying “have other offers” doesn’t capture the blood, sweat, and tears that go into months of interview prep, applications, emails, recruiter calls, and interviews. We know it’s hard, but as you’ll see, it makes a huge difference in your compensation.\n\nGetting those offers doesn’t start when you’ve received your Meta offer. It starts months before. Make sure that you get enough initial conversations with other FAANGs, FAANG-adjacent companies, and late-stage sexy startups to end up with at least one other offer, ideally at least two. Depending on your interview performance, this might be anywhere from 4 to 10 initial conversations.[4](#user-content-fn-4)\n\n### Slow-play team matching\n\nHaving your offers come in around the same time is critical for [any negotiation](https://interviewing.io/blog/negotiate-salary-recruiter), but it’s especially important with Meta because they take such a hard line — without other offers, they will not meaningfully budge.\n\nObviously, you’ll want to start your conversations with other companies well in advance of your Meta interviews and do everything you can to make sure they all come in at the same time. However, even with your best efforts, it’s not guaranteed that your timing will match up. **Here’s how to make sure that your offers come in at the same time: slow-play Meta’s team-matching process**.\n\nTeam matching is actually the part of your Meta journey where you have the most leverage and power. Why leverage? At this point, they know they want you, but they can’t yet hold an offer deadline over you. We’ve already mentioned that once they make the offer, your recruiter is going to push very hard to have you accept, often giving you a deadline of something like two days. In your recruiter’s eyes, you’re a ticking time bomb, where for every day you don’t sign, the deal loses momentum, and your odds of signing drop off. Recruiters are also evaluated on how many candidates they close, so it’s in their interest to create a false sense of scarcity in order to rush you and to use high-pressure sales tactics to get you to seal the deal.\n\nAnd what power do you have? It turns out you can really control how long team matching takes, within reason. If you’re still wrapping phone screens with other companies, slow-playing is the best thing you can do. Here’s how to do it.\n\n**We’ve recently heard that Meta is now insisting that hiring manager conversations happen in series, but even if Meta lets you talk to multiple hiring managers concurrently, try to serialize those conversations as much as possible**. For instance, if you hear from your recruiter that you’re going to start team matching on a Monday, and they offer to set up some calls for Wednesday, ask to do the first call on a Friday and the next call the following Tuesday.\nWhen we advise our users to do this, we often get pushback because they’re worried that slowing things down will make them look disinterested/not serious. We promise you that’s not the case. The biggest risk you run when you slow-play team matching scheduling like this is losing the chance to work on a specific team. If you find that your recruiter has proposed what seems like the perfect team for you, you can and should prioritize doing that call as soon as possible.\n\n**In addition to serializing your hiring manager conversations, for each team, ask to talk to a few individual contributors on teams that you’re serious about**. This isn’t just a stalling tactic. These are the people you’ll be working closely with every day, and they’re the ones doing the job you may have in the future. They’re also less likely to do hard sells, and if you ask thoughtful questions, you’ll learn a lot about what to expect. We’re always surprised by how few candidates ask to speak to their future peers, out of a mistaken concern that asking for too much will make them look disinterested or unengaged.\nJust like with hiring manager calls, if you need to slow things down, we recommend scheduling calls with your peers a few days apart.\n\n### Situations where slow-playing may bite you, and how to know the difference\n\nWe have seen two instances when our advice about slow-playing could backfire. You probably remember when companies started to freeze hiring aggressively in mid-2022 — if you didn’t get matched before the ax came down, you were left out in the cold. Much more recently, we heard from some of our users that Meta put a pause on team matching for E4 roles (largely outside the Bay Area), and many candidates were stuck in a holding pattern (while Meta figured out headcount constraints, though it looks like it’s since been resolved, and picking back up in earnest. Slow-playing and then getting stuck is obviously an unfortunate situation, as is being on the wrong end of a hiring freeze, but these situations are rare, and in our humble opinion, not worth optimizing for — in most cases, you will not be dealing with an impending freeze or stalled matching. If you’re unsure about team supply or the state of hiring, you can do two things:\n\n1. Always ask your recruiter up front to share the number of teams that candidates have had the chance to talk to, on average, recently.\n2. Look at our [real-time graph of FAANG hiring volume](https://interviewingio-metabase.herokuapp.com/public/dashboard/11313b19-b9fd-4532-9a3f-d2aad7e027c3) (as opposed to the graph above, which is a monthly snapshot). In this real-time graph, you can see what portion of our mock interview volume is dedicated to practice in the style of Meta, Amazon, and Google. Historically, our purchasing behavior has lined up very well with what’s actually going on in the market, and the rate of change in this graph should give you an idea of whether Meta is slowing down. As you can see, there was a dip in February (and one in December, but that almost always happens because hiring slows down over the holidays). If you see dips like these, you’ll want to make sure that you do (1) and ask your recruiter about the situation inside. You may also choose not to slow-play for too long.\n\nThere's one more situation where slow-playing may bite you. If you match with a team and really click well with the manager, to the point where you have your heart set on it, it may be wise to accept that team instead of trying to drag things out. We've heard of times where, even if the candidate asked the manager about open headcount and confirmed they didn't have to rush, the role got filled from under them (they were able to talk to other teams afterwards but missed the chance to be on that specific team). If a specific team feels irreplaceable to you and you'd be crushed if you didn't get it, then that may be more important than maximizing your negotiation.\n\n### Build rapport with every hiring manager you talk to\n\nOutside of using the team-matching process to control your timeline, there is one other important tactical piece of advice: Do your best to build rapport with hiring managers.\n\n**As we said above, recruiters are trying to close the deal. That’s their job. Hiring managers, on the other hand, are trying to lay the groundwork for a good working relationship with you**. As a result, their interests are much more aligned with yours. Of course, they still want to close you, but it’s not worth it to them to employ high-pressure tactics, and it’s not something they’re trained in or comfortable with (in fact, many of them hated these tactics when they were on the other end of it while looking for work)[5](#user-content-fn-5).\n\nAs such, hiring managers will generally be a lot more transparent with you about how much time you actually have to make a decision, and their answers will likely be very different from the ones you get from recruiters.\n\nWe’ve advised all of our Meta candidates to ask their prospective hiring managers about when they realistically have to make a decision by, and the differences between what the hiring manager has told them (“Take your time; you have a few weeks at least.”) and what their recruiter has told them (“We’re talking to a lot of candidates for that team. To ensure your spot, you should make a decision in the next few days.”) are stark.\n\nThere’s simply no downside to building rapport with hiring managers. At worst, you make a professional connection. At best, you get a head start on a great working relationship with your new boss.\n\n**One practical note: Always ask your hiring manager for their email address in case you have more questions later**. This way, if your recruiter starts telling you that you need to make a decision by Friday, you can ping your hiring manager, explain that you’re still thinking, and ask if it’s OK to take a few more days. Almost always they will say yes.\n\n### Actually negotiate (this is the easy part)\n\nIf you’ve done everything else in this post, the negotiation is the easy part. By now, you’ve wrapped up team matching, chosen a team, and have likely gotten an aggressive offer deadline.\n\nYou have also not shared any offer details till now. As we said at the beginning of this post, the success of your strategy hinges on the recruiter not being aware of the other companies you’re interviewing with. This will be the first time they find out about it, and that will put them on their proverbial back foot.\n\n![](https://64.media.tumblr.com/4e9087066942c43406fd478dcc34ba90/f2c80ff8d660cda3-62/s540x810/9f0a2c2a8f29b0782c59e0551444d001ec1a7a26.gif)\n\nTo respond to the offer, you can send an email[6](#user-content-fn-6) that looks something like the below. The details will differ, and how much you reveal about the other offers will vary, but here’s the general idea.\n\n---\n\n*Hey [Recruiter name],*\n\n*Thank you so much for the offer, for working with me throughout this process, and for all your help with team matching. I wanted to share some details with you. I currently have offers from {Company 1}, {Company 2}, and {Company 3}.\n\n{Company 1} has offered me a significantly higher base salary of $220k. {Company 2} has a comparable base but has offered me significantly more equity: $500k. I know {Company 2} is a startup, but they’re late stage and handing out RSUs. Those RSUs are as valuable to me as public company equity.\n\nFinally, {Company 3} has thrown in a meaningful signing bonus, and their performance bonus is actually at 25%, not 15%.\n\nI’m very excited about the opportunity to work at Meta and about the team. {Insert some authentic reasons why you’re excited about the company, the team, your new boss, etc.} It’d be great to see a meaningful increase in compensation to make my decision easier.*\n\n*Thank you, and I look forward to hearing from you.*\n\n---\n\nBy the way, this isn’t the only approach you can take, and with other companies, you might have better luck with the [Reverse Used Car Salesman](https://interviewing.io/blog/negotiate-salary-recruiter). However, in our experience, if you don’t share offer details, your Meta recruiter will immediately ask you to share, so you might as well control the flow of information.\n\nIn the template above, I’ve assumed that not all of your offers are stronger than Meta’s across the board, which is why I’ve cherry-picked which pieces to share. Sometimes, if you’re lucky, you’ll have multiple offers that have a higher base, more equity, and a higher signing bonus. In that case, it’s less of a game of skill — just throw the numbers at them, and they’ll exceed the other offers without much prompting.\n\nIf you run this play, your recruiter will apologize for low-balling you, blaming the “computer” for giving them those numbers. Then, like clockwork, you will see a $50k to $150k jump in your offer (precisely how much depends on where your other offers are from and how strong they are).[7](#user-content-fn-7)\n\nNow, whether you take that offer is up to you.\n\nSome closing thoughts. I’m a capitalist. Meta’s behavior here is aggressively capitalistic, if short-sighted – once other FAANGs start meaningfully hiring again, and Meta employees figure out that there’s a $150k comp differential between people with the same job title, they’re going to pay the piper and likely see a bunch of attrition. Ultimately, the market will correct these practices. However, I also believe that individuals have the right and duty to be as informed as possible and to wield whatever weapons in their arsenal to advocate for themselves, rather than waiting on the mercy of slow, indifferent market forces.\n\nAs such, we hope this post has given you some ammunition in your negotiations and helped reduce the information asymmetry between you and Meta, a huge, aggressive player with basically a monopoly on eng hiring at the moment. And we hope that if anyone from Meta is reading this, it’ll spark some internal conversations about what’s right. And if they don’t, other FAANGs’ recruiters will swoop in soon enough.\n\nWant to know if you’re ready to interview at Meta? Do anonymous mock interviews with real Meta interviewers, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. How team matching works changed fairly recently. In the past, you’d get an offer before you matched with a team and do a 6-week “bootcamp” where you’d get up to speed on Meta’s tech stack, infrastructure, and systems, followed by a multi-week “speed dating”-esque team matching cycle. Bootcamp still exists, but now it's much shorter (2-4 weeks), and the focus is getting new engineers ramped up on generic tools. After that, new engineers continue to ramp up on their specific teams. [↩](#user-content-fnref-1)\n2. We’ve recently heard that Meta may now be insisting that team matching conversations happen in serial, i.e., you can only do one at a time. However, this doesn’t meaningfully change our advice. [↩](#user-content-fnref-2)\n3. You might say, “Aline, why can’t I just make up offers?” We could never, in good conscience, advise that. It’s unethical, and though I’d argue that while Meta’s negotiation practices are also unethical, that’s not the way to win. Outside of ethical considerations, while the risks of getting caught are low, they’re not zero. Lying about offers, in our mind, is the last refuge of the incompetent. [↩](#user-content-fnref-3)\n4. The advice in this post is orthogonal from your career goals and what you want to work on. This blog post is about navigating an unfair system filled with opaque rules while maximizing your cash. It is not about self-actualization, though we’d argue that creating the most optionality for yourself helps with self-actualization as well. You can also talk to smaller companies and use your big-co offers as leverage to increase your startup equity. There’s nothing wrong with that, but more detail on optionality and self-actualization is outside the scope of this post. [↩](#user-content-fnref-4)\n5. Of course, some hiring managers will use high-pressure sales tactics or create false timelines to try to close you. But that’s the exception rather than the rule. You can decide if that’s something that you want to weigh when judging whether or not you want to work for them. [↩](#user-content-fnref-5)\n6. We strongly urge you to avoid negotiating over the phone and over text, whenever possible. Your recruiter does 5 of these calls a day. You might do one of these calls every few years. Do the hard part over email. It’s the best way to level the playing field. [Read this post](https://interviewing.io/blog/sabotage-salary-negotiation-before-even-start) to learn how to avoid synchronous phone negotiations (just look for “phone”). [↩](#user-content-fnref-6)\n7. One advanced maneuver is to pit all your other companies against each other and raise up their initial offers BEFORE talking to Meta. We’ll likely write about how to run this play in a future post. [↩](#user-content-fnref-7)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/how-to-negotiate-with-meta",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Hamtips, or why I still run the Technical Phone Screen as the Hiring Manager",
      "content": "*Hey, Aline (founder of [interviewing.io](https://interviewing.io/)) here. This is the fourth post in our Guest Author series.*\n\n*In this post, our Guest Author, Alexey Komissarouk, talks about how advantageous it is to have hiring managers (rather than individual members of the team) conduct the first technical screen, effectively combining the technical interview and the hiring manager sell call. At [interviewing.io](https://interviewing.io/), where the first interaction an engineering candidate will have with a company is always with another engineer (and often with a hiring manager), we’re strong advocates of this approach. Why? Because selling matters at every stage of the process — in this market, interviews aren’t just for vetting anymore — and hiring managers can sell in a way that a peer cannot. And because hiring managers are going to be among the best-calibrated interviewers, which means that they’ll save the rest of the team time downstream.*\n\n![Author avatar](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ff3f50_alexey_scaled_e1606329022499_edef8e39a7.webp&w=384&q=75 \"Alexey Komissarouk\")\n\nAlexey Komissarouk\n\nAlexey Komissarouk is a growth engineering leader. He is currently Head of Growth Engineering at MasterClass. Before that, he spent 2016-2020 at Opendoor, first as an early engineer, then as an Engineering Manager. Between 2013 and 2016, he built out a product engineering consulting company, helping clients such as Dropbox, Pebble, Boomerang, and Binti grow and expand lines of business through a combination of product management and engineering. In his other lives, Alexey co-founded a boutique work+travel company, Hacker Paradise. Since the company’s inception in 2015, they’ve run trips to over a dozen locations and been joined by more than 800 alumni.\n\n*Caveat: Yes, yes, almost everything about the interviewing / recruiting process is broken. Sometimes though, you just have to play the hand you’re dealt and settle for minor improvements.*\n\nThe 75-minute HMTPS is my proposed minor improvement.\n\n![Cartoon: where do you see yourself in five years?](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Foatmeal_interviewing_1_97fd522520.webp&w=1080&q=75 \"A serious question\")\n\nHat tip to The Oatmeal\n\nWhat is the HMTPS?\n------------------\n\nIt stands for “Hiring Manager Technical Phone Screen.” Since you asked, I’ve been pronouncing it “ham-tips.” It’s the call a candidate will have after their RPS (Recruiter Phone Screen), but before their onsite.\n\nThis combines two calls: the Technical Phone Screen (TPS), which is a coding exercise that usually happens before the onsite, and the HMS call, which is a call with the Hiring Manager (your would-be manager) that can be done before an onsite, or after, or not at all.\n\nInstead of leaving the HMS call to languish, I combine them into one – the HMTPS. It takes 75 minutes.\n\nWhy combine the two interviews?\n-------------------------------\n\nAn ideal interview loop has as few steps as possible, and gets to a decision ASAP. By combining these two steps you shorten the intro-to-offer by ~1 week and reduce candidate dropoff by 5-10%.\n\nIt’s also a lot less work for recruiters playing scheduling battleship.[1](#user-content-fn-1)\n\nFinally, Hiring Managers will, on average, be better at selling working at the company – it’s kind of their job.\n\nWhy 75 minutes?\n---------------\n\nWe’re combining a 30-minute call and a 60-minute call, and combining the 15-minute Q&A at the end of each into one, like so:\n\n![Chart: Consolidating pre-onsite interviews to improve hiring velocity](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FConsolidating_pre_onsite_interviews_to_improve_hiring_velocity_3_3fd8cff5d1.webp&w=1920&q=75 \"Improving hiring velocity\")\n\nI’m also more comfortable shortening the ~50 minute technical question into 30 minutes because (a) I’m pretty calibrated on my question, having run it 200+ times at this point, and so can get most of the signal I’m looking for within the first 30 minutes.\n\nI’ve tried doing this call in 60 minutes and it ends up feeling pretty rushed; not to say somebody else couldn’t pull that off, but I’ve appreciated the bit of space. Also, since most candidates don’t schedule in 15-minute increments, we can always go a little long (up to the 90 minute mark) if we need to.\n\nWhy is this good for the hiring manager?\n----------------------------------------\n\nFirst, it’s easier to schedule (usually towards the end of the day). Second, it usually gives me enough time with the candidate so that I end up being pretty confident about how they’ll do both at the job and on the onsite. I haven’t quantified this yet, but anecdotally I have been surprised by onsite interviewer feedback much more rarely when I do this.\n\nWhy is this good for the candidate?\n-----------------------------------\n\nIt’s one fewer hoop to jump through. Also, whether or not they get along with me as their future manager – both technically and interpersonally – can, and should, be a pretty strong determinant as to whether they should continue with the process. This gives a more accurate and realistic signal, since we are both coding together and talking about work.\n\nWhen is this a bad idea?\n------------------------\n\nThis makes the Hiring Manager a bit of a bottleneck in interviewing; once a company gets to the point where you are interviewing for titles like “Senior Software Engineer, Team TBD” you have to round robin TPS-es to the rest of your [Phone Screen Team](https://interviewing.io/blog/technical-phone-screen-superforecasters).\n\nAlso, as the HM I likely have some unreasonable biases (Golang engineers, I’m looking at you), and making me the bottleneck in interviewing exacerbates those. That said, the HM’s bias is going to be applied sooner or later in the interview process, and my take is that between the more effective selling and the time savings (both for the HMTPS and downstream), the benefits outlined are worth it.\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. Tuesday at 4? You sunk my Grooming Session!\" [↩](#user-content-fnref-1)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/hamtips-or-why-i-still-run-the-technical-phone-screen-as-the-hiring-manager",
      "author": "",
      "user_id": ""
    },
    {
      "title": "The 3 things that diversity hiring initiatives get wrong",
      "content": "I’ve been hiring engineers in some capacity for the past decade. Five years ago I founded interviewing.io, a technical recruiting marketplace that provides engineers with anonymous [mock interviews](https://interviewing.io/mocks) and then fast-tracks top performers—regardless of who they are or how they look on paper—at top companies.\n\nWe’ve hosted close to 100K technical interviews on our platform and have helped thousands of engineers find jobs. For the last year or so, we’ve also been running a [Fellowship program](https://interviewing.io/blog/announcing-the-interviewing-io-technical-interview-practice-fellowship) specifically for engineers from underrepresented backgrounds.\n\nThat’s all to say that even though I have developed some strong opinions about “diversity hiring” initiatives, my opinions are based not on anecdotes but on cold, hard data. And the data points to the following 3 problems with these initiatives:\n\n1. **Over-indexing on unconscious bias, which ignores the much bigger problem of discrimination based on pedigree (i.e., conscious bias):** Too many organizations invest in resume blinding and other attempts to mitigate unconscious bias at the top of the funnel. But they continue to ignore the much more problematic, ever-present *conscious* bias against engineers who didn’t attend a top school or work at a top company, which is especially troubling given that there are many more underrepresented engineers who didn’t attend top schools.\n2. **Focusing on bias in the middle of the funnel rather than the top:** The top of the funnel is orders of magnitude larger than the middle. By the time you get to the middle, you’ve weeded out most of your candidates from underrepresented backgrounds, and the damage has been done. Investing in bias training during interviews, therefore, gives you diminishing returns and is more of an exercise in optics than an attempt to fix anything.\n3. **Ignoring the technical interview practice gap:** Technical interviews are a learned skill, but access to practice is not equitably distributed. As a result, marginalized groups often underperform—not because they’re worse at engineering but simply because they haven’t had access to the same level of preparation.\n\nAll three of these problems are somewhat subtle and require a bit of explanation, but until they’re addressed, we’re not going to make meaningful progress toward equitable representation in software engineering.\n\nProblem 1: Over-indexing on unconscious bias at the top of the funnel (while ignoring conscious bias)\n-----------------------------------------------------------------------------------------------------\n\nFirst off, what is the top of the funnel? In the case of hiring, this refers to the stage where we decide whether we want to move a candidate into our process. If a candidate applies online (inbound), this is the step where we look at their resume and decide whether or not to move forward. And if we’re actively trying to bring candidates into our process, this is when we look at their public profile (usually LinkedIn) and decide if we want to reach out to them (outbound).\n\nWhether the candidate is inbound or outbound, we have limited information available to help us make a decision. The information we have available typically consists of a resume (or a resume-like profile) and contains biographical info like name, where they went to school and their graduation date, and their work history. From these few bits of data, we can glean—intentionally or not—a candidate’s gender and sometimes their race, their age, and their pedigree (i.e., did they attend an elite school and/or work at a top company?).\n\nUnconscious vs. conscious bias\n------------------------------\n\nIn this context, unconscious bias refers to making quick judgments, often without being aware of it, against someone based on the biographical info and other traits gleaned from their resume. In the context of engineering hiring, we tend to focus specifically on reducing bias around attributes like gender and race (though of course discrimination based on other traits like sexual orientation, age, beliefs, disability, etc., is also unacceptable).\n\nConscious bias, on the other hand, is the very *intentional* decision not to move forward with candidates who lack pedigree, i.e., those who didn’t attend an elite computer science school or work at a company with a well-established brand. \\*\\*Unlike unconscious bias, organizations don’t typically condemn this, and more often than not, conscious bias is actually codified into the hiring process, with explicit rules for the recruiting team that say something like, “\\*\\****These are the schools we hire from, and these are the companies we look for on a resume. If a candidate doesn’t have either, let’s not move forward.”***\n\nEither way, at the top of the funnel, before we ever interact with a candidate, we must decide if we want to bring them in for an interview based on very limited information, and that’s where things get complicated.\n\nWhy anonymizing resumes isn’t enough\n------------------------------------\n\nAt the top of the funnel, the canonical way to mitigate unconscious bias is to anonymize resumes. The idea is that if we can remove all signals that might help us uncover someone’s race and gender, then we, by definition, can’t be biased, right?\n\nHere’s the problem. Though anonymizing resumes isn’t inherently bad, it’s not nearly enough. In a nutshell, as long as we keep over-indexing on top schools and top companies, it’s not mathematically possible to hit our goals, no matter how much unconscious bias mitigation and resume anonymization we perform.\n\nIt’s a fact: there are far more women and people of color who didn’t attend one of a handful of well-known universities than there are who did. To make matters worse, filtering based on pedigree doesn’t really work! **As a result, the pedigree-based tradeoffs we currently make (in the name of keeping the bar high) exclude people from underrepresented backgrounds… without actually getting us the best candidates.**\n\nOver-indexing on resumes and pedigree\n-------------------------------------\n\nLet’s talk about resumes first. The resume is a demonstrably terrible way to judge whether someone is a good engineer. People often talk about how, in the spirit of fairness, it’s important to consider non-pedigreed candidates, but there’s a tacit agreement that pedigree is useful and that we’re doing something charitable by broadening the pool. As a result, we accept the premise that we’re going to be either pragmatic or fair, but not both. However, everything I’ve learned from being in this business more than a decade has shown me that pitting fairness against pedigree creates a false dichotomy.\n\nIn other words, resumes inherently don’t carry enough signal about a candidate’s engineering ability to be useful. I’ve written extensively (and angrily) on this subject, and below I’ve linked the TL;DRs for 3 studies that all point to resumes being largely useless:\n\n* [The number of typos and grammatical errors on someone’s resume](https://interviewing.io/blog/lessons-from-a-years-worth-of-hiring-data) matters much more than where they worked or went to school (the latter, in fact, didn’t matter at all)\n* [Neither recruiters nor engineers can figure out who the strong candidates are based on their resumes](https://interviewing.io/blog/resumes-suck-heres-the-data), and, more importantly, they all disagree with each other about what a good candidate looks like\n* Data from interviewing.io (based on thousands of interviews) has shown that [where someone went to school has no bearing on their performance](https://interviewing.io/blog/lessons-from-3000-technical-interviews) in technical interviews\n\n**Simply put, even if you don’t care one iota about inclusion and just pragmatically want to hire the best candidates with the fewest amount of interviews, you should STILL stop focusing on resumes and pedigree.**\n\nSo we’re making decisions based on an incorrect heuristic and bad data. That’s not great, but what does it have to do with diversity? Here’s the thing. If you make hiring decisions based on pedigree, the numbers simply aren’t there, and it’s mathematically impossible to reach your hiring goals.\n\nHere is some napkin math. Every year, about 60,000 students graduate in the US with a computer science degree. If you only hire from top schools, your pool narrows to about 6,000. Of those, about 1,500 are women and fewer than 1,000 are people of color. With the U.S. Bureau of Labor Statistics (BLS) expecting nearly 140,000 new engineering jobs over the 2016–26 decade, it’s laughable to believe that any given company will be able to meet its diversity goals by hiring this way.\n\nWhat happens when you mitigate conscious bias\n---------------------------------------------\n\nBut what happens if you look beyond elite schools and start mitigating *conscious* bias? That’s when you start to have a real shot. Below is a model we built to capture the size of the candidate pool (for gender, specifically) that shows what will happen if we keep hiring the way we’re hiring now vs. expanding our pool beyond elite schools and focusing on what people can actually do, instead of how they look on paper.\n\n![Chart showing the change in gender parity gap over time](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fgender_parity_hiring_3653357b54.png&w=1080&q=75 \"Gender Parity in Hiring\")\n\nYou can read more about [how we built this model](https://interviewing.io/blog/we-ran-the-numbers-and-there-really-is-a-pipeline-problem-in-eng-hiring), but we looked at two alternative sources of candidates: MOOCs and bootcamps. In 2015 alone, over 35 million people signed up for at least one MOOC course, and in 2018 MOOCs collectively had over 100M students.\n\nBecause they’re cheaper and much faster than a four-year program, bootcamps seem like a rational choice when compared to the price of attending a top university. Since 2013, bootcamp enrollment has grown 9X, with a total of 20,316 graduates in 2018. **Though these numbers represent enrollment across all genders, and the raw number of grads lags behind CS programs, the portion of women graduating from bootcamps is also on the rise and graduation from online programs has actually reached gender parity (as compared to only 20% in traditional CS programs).**\n\nBut, even if lower-tier schools and alternative programs have their fair share of talent, how do we surface the most qualified candidates in the absence of the signals we’re used to? Good news. In this brave new world, where we have the technology to write code together remotely, and where we can collect data and reason about it, technology has the power to free us from relying on proxies.\n\nAt interviewing.io, we make it possible to move away from proxies by providing engineers with hyperrealistic, rigorous mock interviews and connecting top performers with employers, regardless of how the candidates look on paper.\n\nOr if you don’t use us, there’s a slew of asynchronous coding assessments like CodeSignal, HackerRank, and Triplebyte, that can help you filter down your candidates before you invest precious engineering time into interviewing them.\n\nSo, despite the numbers and the bevy of available solutions, why do we still latch on to mitigating unconscious bias? Sadly, it’s easier. Removing names from a resume is eminently doable. Redefining credentialing in software engineering is really hard (I know, the interviewing.io team and I have been at this for over five years). But unless we can talk openly about this problem and stop pretending that unconscious bias mitigation will solve our problems, we can’t make meaningful progress toward equitable representation.\n\nSo far, we’ve just talked about bias at the top of the funnel. What about the bias we encounter when, instead of looking at a piece of paper, we’re interacting with another human, especially a human who doesn’t look like us?\n\nProblem 2: Focusing on bias in the middle of the funnel rather than the top\n---------------------------------------------------------------------------\n\nThe second problem I’ve observed occurs during the middle of the funnel, once candidates have made it through the door. Unconscious bias training tries to make us better at interacting with (and avoiding making judgments about) people who don’t look like us. It’s a well-intentioned attempt, but [sadly it’s been shown not to work](https://www.mckinsey.com/featured-insights/gender-equality/focusing-on-what-works-for-workplace-diversity), and it can even be responsible for people digging their heels in on the prejudices they hold.\n\nMost importantly, and this is perhaps the more subtle point, by the time a candidate gets to an interviewer, numerically speaking, it’s too late—you’ve already filtered out most of the candidates who would have moved your numbers, and any effort you put in at this step in the funnel gives you diminishing returns.\n\n![Diagram showing where bias exists vs where most training is focussed](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fdiversity_hiring_funnel_fde8c49b99.png&w=1080&q=75 \"Diversity in Hiring\")\n\nPut another way, there are often 100X the number of candidates at the top of the funnel than at the next stage. Most get cut before they ever talk to a human, simply based on their resumes. Despite that, employers are investing in training interviewers, which unfortunately is only relevant at a stage that most of the non-traditional talent won’t reach. Any gains we can make in the middle of the funnel are mathematically negligible compared to the gains we can make at the top (where, as you saw in the previous section, we persist in our conscious bias and call it a best practice).  \nEspecially when you have limited resources to mitigate bias in your process, and as more and more of your team fall victim to [diversity fatigue](https://www.forbes.com/sites/janicegassam/2020/05/27/has-diversity-become-a-dirty-word/?sh=9881c585a9bd), investing so much effort into a part of the funnel that has a very real cap on its ROI makes no sense—even less so given that the thing you’re doing doesn’t even work.\n\nProblem 3: Ignoring the technical interview practice gap\n--------------------------------------------------------\n\nOf all the problems with diversity hiring, this one is the most subtle and perhaps the most damaging. Because it’s such an important (and tricky) topic that requires going in depth to fully understand, I encourage you to read the [detailed post that I wrote last week](https://interviewing.io/blog/technical-interview-practice-gap) focusing just on this topic. .\n\nAttending an elite computer science institution (like MIT, where I went) provides students with a number of benefits, but perhaps the most significant is boundless access to technical interview practice.\n\nMIT offered a multi-week course dedicated to passing technical interviews, and I got support from my peers who were interviewing at FAANG for internships and new grad positions. This allowed us to practice with each other, share our successes and failures, and recognize just how much of technical interviewing is a numbers game. **And we learned that bombing a [Google interview](https://interviewing.io/guides/hiring-process/google#google) did not mean that you weren’t meant to be an engineer. It just meant that you needed to work some more problems, do more mock interviews, and try again at Facebook.**\n\nBut let’s put the anecdotal experience aside and examine the data that we at interviewing.io have collected. We’ve hosted close to 100K interviews on our platform, which has taught us two things about the technical interview: 1) like standardized testing, it’s a learned skill, and 2) unlike standardized testing, interview results are not consistent or repeatable—the same candidate can ace one interview and fail another one the same day.\n\nThe importance of interview practice\n------------------------------------\n\nIn a recent study, [we looked at how people who got jobs at FAANG performed](https://interviewing.io/blog/how-know-ready-interview-faang) in practice vs. those who did not. We discovered that technical ability did not obviously associate with interview success, but the number of practice interviews people completed (either on interviewing.io or elsewhere) did. Surprisingly, no other factors we included in our model (seniority, gender, degree, etc.) mattered at all.\n\n**Secondly, technical interview performance from interview to interview is fairly inconsistent, even among strong candidates.** (Notably, consistency appears to have nothing to do with seniority, pedigree, or anything else.) **In fact,** [**only about 20% of interviewees perform consistently**](https://interviewing.io/blog/after-a-lot-more-data-technical-interview-performance-really-is-kind-of-arbitrary) **from interview to interview. Why does this matter? Once you’ve done a few traditional technical interviews, you learn to account for and accept the volatility and lack of determinism in the process.** If you happen to have the benefit of speaking with friends who’ve also been through it, it only gets easier. But what if you don’t?\n\nHow the interview practice gap hurts underrepresented groups the most\n---------------------------------------------------------------------\n\nIn an earlier post, we wrote about how [women quit interview practice 7 times more often than men](https://interviewing.io/blog/voice-modulation-gender-technical-interviews) after just one bad interview. Unfortunately, this is likely affecting other underrepresented and underserved groups as well.\n\n**This is a broken process for everyone, but the flaws within the system hit these groups the hardest—and simply because they haven’t had the opportunity to internalize exactly how much of technical interviewing is a game.**\n\nSo what does this have to do with the practice gap? The key takeaway from our research has shown that there is **a meaningful bump in performance** (almost 2X as likely to pass!) **for candidates who have completed at least 5 practice interviews.** This isn’t a lot of interviews for someone who’s actively practicing and knows how the interview prep game works. But it’s a far cry from what companies who are looking to boost their diversity numbers offer their candidates, and it’s equally far from what bootcamps, an increasingly important source of candidates from underrepresented backgrounds, provide their students.\n\nWhy are bootcamps, universities, and employers all falling short and exacerbating the practice gap? With employers and bootcamps, the teams responsible for facilitating interview info sessions or mock interviews typically have non-technical backgrounds and lack a good grasp of the significant differences between preparing for behavioral interviews and technical ones.\n\nPreparing for technical interviews (and other analytical, domain-specific interviews) is more similar to studying for a math test than rehearsing how to present yourself and tell your story. But until you’ve done it, you won’t really get it.\n\nThe worst effect of the practice gap occurs when companies simply walk away from their expanded recruiting efforts, with the mistaken belief that they should return to recruiting exclusively from elite institutions. Or worse, they sometimes have their unconscious biases about race and/or gender confirmed, when the actual problem isn’t the candidates but their lack of access to practice.\n\nIn fact, a few years ago, we ran a study where we looked at interview performance by school tier. For students who were actively and regularly practicing, [there was no difference between elite schools and non-elite schools](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt)!\n\n![Charts showing new grad interview performance by school tier](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FNew_Grad_Interview_Performance_by_School_Tier_b85571f044.png&w=1080&q=75 \"New Grad Interview Performance by School Tier\")\n\nHow to actually increase representation\n---------------------------------------\n\nSo far I’ve outlined my three main problems with diversity hiring initiatives—but what’s the solution? A few things:\n\n### Stop over-indexing on resumes\n\nIn this brave new world, where we have the technology to write code together remotely, and where we can collect data and reason about it, technology has the power to free us from relying on proxies, so that we can look at each individual as an indicative, unique bundle of performance-based data points. At interviewing.io, we make it possible to move away from proxies by looking at each interviewee as a collection of data points that tell a story, rather than a largely signal-less document a recruiter looks at for 10 seconds and then makes a largely arbitrary decision before moving on to the next candidate.\n\nOf course, this post lives on our blog, so I’ll take a moment to plug what we do. In a world where there’s a growing credentialing gap and where it’s really hard to figure out how to separate a mediocre non-traditional candidate from a stellar one, we can help. interviewing.io helps companies find and hire engineers based on ability, not pedigree. We give out free mock interviews to engineers, and we use the data from these interviews to identify top performers, independently of how they look on paper. Those top performers then get to interview anonymously with employers on our platform (we’ve hired for Lyft, Uber, Dropbox, Quora, and many other great, high-bar companies). And this system works. Not only are our candidates’ conversion rates 3X the industry standard (about 70% of our candidates ace their phone screens, as compared to 20-25% in a typical, high-performing funnel), about 40% of the hires made by top companies on our platform have come from non-traditional backgrounds. Because of our completely anonymous, skills-first approach, we’ve seen an interesting phenomenon happen time and time again: when an engineer unmasks at the end of a successful interview, the company in question realizes that the student who just aced their phone screen was one whose resume was sitting at the bottom of the pile all along (we recently had someone get hired after having been rejected by that same company 3 times based on his resume!).\n\nFrankly, think of how much time and money you’re wasting competing for only a small pool of superficially qualified candidates when you could be hiring overlooked talent that’s actually qualified. Your CFO will be happier, and so will your engineers. Look, whether you use us or something else, there’s a slew of tech-enabled solutions that are redefining credentialing in engineering, from asynchronous coding assessments like CodeSignal or HackerRank to solutions that help you vet your inbound candidate pool, like Karat.\n\nAnd using these new tools isn’t just paying lip service to a long-suffering D&I initiative. It gets you the candidates that everyone in the world isn’t chasing without compromising on quality, helps you make more hires faster, and just makes hiring fairer across the board. And, yes, it will also help you meet your diversity goals.\n\n### Stop paying for unconscious bias training, and spend that budget on interview practice for your candidates\n\nAs you saw above, interview practice is absolutely key to success in technical interviews, but access to practice is not equitably distributed. I contend that tech giants, universities, and bootcamps could close the practice gap, level the playing field in software engineering, and hit our representation goals by simply providing candidates who need them with five professional mock interviews each. Hell, we would do it ourselves if we had the means. Yes, this will cost a few hundred dollars per candidate, but compared to the cost of sourcing and interviewing them, it’s a small change.\n\nMoreover, if you’re spending a significant amount of money on initiatives like unconscious bias training, [which has been shown to be ineffective](https://www.mckinsey.com/featured-insights/gender-equality/focusing-on-what-works-for-workplace-diversity), you could make a much bigger impact by shifting that budget toward interview practice for your candidates.\n\nIf you’re an employer, [reach out to us](mailto:hello@interviewing.io) so we can help you make this a reality.\n\nWhether you only do some of these things, and whether you do them with us or with other tools and products, let’s stop simply talking about how we need to fix representation in tech and instead do something that actually makes a difference. We can fix the diversity hiring initiatives that have failed us and ensure that the best people actually get hired.",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/diversity-hiring-initiatives-wrong",
      "author": "",
      "user_id": ""
    },
    {
      "title": "The technical interview practice gap, and how it keeps underrepresented groups out of software engineering",
      "content": "I’ve been hiring engineers in some capacity for the past decade, and five years ago I founded interviewing.io, a technical recruiting marketplace that provides engineers with anonymous mock interviews and then fast-tracks top performers—regardless of who they are or how they look on paper—at top companies.\n\nWe’ve hosted close to 100K technical interviews on our platform and have helped thousands of engineers find jobs. Since last year, we’ve also been running a [Fellowship program specifically for engineers from underrepresented backgrounds](https://interviewing.io/blog/announcing-the-interviewing-io-technical-interview-practice-fellowship).\n\nAll that is to say that even though I have strong opinions about “diversity hiring” initiatives, I’ve acquired them the honest way, through laboratory experience.\n\nThough I find it problematic to label any type of hiring as “diversity hiring” —primarily because it tacitly implies to candidates and everyone else involved that they are getting the job not because of their merits but because of their demographics or to fill some quota—nomenclature is not my main critique of these initiatives. Though there are a number of big issues with how employers approach hiring, in this post I’ll talk about the one that’s at once the most subtle and the most damaging: **systematically ignoring the technical interview practice gap between traditional and non-traditional candidates**. I’ll explain exactly what the interview practice gap is and why it’s so bad in a moment, but first, allow me to tell you a little story.\n\nThe benefit of attending an elite CS university\n-----------------------------------------------\n\nI was fortunate enough to attend MIT for undergrad. Probably the greatest gift that MIT gave me was a big stamp on my forehead that, to this day, makes strangers think I’m smart. But there was another, more relevant gift that gave me a serious advantage over students who did not attend an elite computer science institution: boundless access to technical interview practice.\n\nNot only was there a multi-week course during the month-long break between Fall and Spring semesters that was dedicated exclusively to passing technical interviews, but all of my peers were going through exactly the same thing at the same time. Everyone was interviewing at FAANG for internships and new grad positions, which meant that we could all practice with each other, share our successes and failures, and, over time, internalize just how much of technical interviewing is a numbers game. **We learned that bombing a Google interview did not mean that you weren’t meant to be an engineer. It just meant that you needed to work some more problems, do more mock interviews, and try again at Facebook.**\n\nPractice, practice, practice\n----------------------------\n\nMy anecdotal experience aside, we at interviewing.io have actual data. As I mentioned earlier, we’ve hosted close to 100K interviews on our platform. It has taught us two very important things. This interview style is controversial, in part because it’s not entirely similar to the work software engineers do every day but also because 1) like standardized testing, it’s a learned skill, and 2) unlike standardized testing, interview results are not consistent or repeatable—the same candidate can do well in one interview and fail another one the same day.\n\n**First, regardless of how strong you are technically, practice really matters.** In a recent study, [we looked at how people who got jobs at FAANG performed](https://interviewing.io/blog/how-know-ready-interview-faang) in practice vs. those who did not. Below is a graph of average technical score in interviews and the portion of people with each score who passed Facebook’s interview process.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_16_at_19_21_52_2x_592c564655.png)\n\nYou can see that technical ability did not obviously associate with interview success. So what did? It turned out that the number of practice interviews people completed (either on interviewing.io or off) had a much bigger bearing, as you can see below. Surprisingly, no other factors we included in our model (seniority, gender, degree, etc.) mattered at all.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_16_at_19_22_05_2x_1974c0c5ce.png)\n\n**Secondly, technical interview performance from interview to interview is fairly inconsistent, even among strong candidates.** Notably, as above, consistency appears to have nothing to do with seniority, pedigree, or anything else. **In fact, only about 20% of interviewees perform consistently from interview to interview. Why does this matter? Once you’ve done a few traditional technical interviews, the volatility and lack of determinism in the process is something you figure out anecdotally and kind of accept.** And if you have the benefit of having friends who’ve also been through it, it only gets easier. But what if you don’t?\n\nIn a previous post, we talked about how [women quit interview practice 7 times more often than men](https://interviewing.io/blog/voice-modulation-gender-technical-interviews) after just one bad interview. It’s not too much of a leap to say that this is probably happening to any number of groups who are underrepresented/underserved by the current system. **In other words, though it’s a broken process for everyone, the flaws within the system hit these groups the hardest—just because they haven’t had the chance to internalize exactly how much of technical interviewing is a game.** Sadly, since we’ve started running our Fellowship program for engineers from underrepresented backgrounds, we’ve seen this effect firsthand.\n\nThe technical interview practice gap\n------------------------------------\n\nSo what does this have to do with the practice gap? As you can see in the graph above, **there was a pretty meaningful bump in performance** (almost 2X as likely to pass!) **after at least five practice interviews.** Five interviews is not a lot for someone who’s actively practicing and knows how the interview prep game works. **But it’s a far cry from what companies who are looking to boost their diversity numbers offer their candidates, and it’s equally far from what bootcamps, an increasingly important source of candidates from underrepresented backgrounds, provide their students.**\n\n### What employers are doing wrong\n\nI won’t name any specific companies here, but tech giants (many with good intentions and with marching orders to boost their diversity numbers) often do a considerable amount of outreach and pre-interview engagement with candidates from underrepresented backgrounds. This outreach is usually an info session where one engineer speaks to a virtual room of candidates from underrepresented backgrounds. The engineer tells them what to expect in technical interviews, encourages them to learn how to articulate their thought process out loud while solving a problem, and recommends resources like *Cracking the Coding Interview.* Some companies even go so far as to offer their underrepresented candidates a mock interview or two.\n\nUnfortunately, for candidates who are unfamiliar with the process, neither of these interventions is nearly enough.\n\n### What bootcamps are doing wrong\n\nBecause they’re cheaper and much faster than a four-year program, bootcamps seem like a rational choice when compared to the price of attending a top university. Since 2013, bootcamp enrollment has grown 9X, with a total of 20,316 graduates in 2018. Though these numbers represent enrollment across all genders and the raw number of grads lags behind CS programs, the portion of women graduating from bootcamps is also on the rise and graduation from online programs has actually reached gender parity (as compared to only 20% in traditional CS programs).\n\nDespite these encouraging numbers, things start to go sideways when it’s time for students to start interviewing for jobs, and disappointingly, what bootcamps provide their students isn’t much better than the tech giants with good intentions.\n\nAgain, without naming names, I’ve interacted in some capacity with almost every major bootcamp over the last few years, trying to sell them on providing interview practice for their students, and I’ve also witnessed how recent bootcamp grads perform in mock interviews on interviewing.io. Based on these observations, bootcamps seem to have two big limitations: not nearly enough time spent on data structures and algorithms in the curriculum and lack of interview prep support both during and after the program.\n\nDiscussing bootcamp curriculum is out of the scope of this post, but I will say a few words about how bootcamps handle interview preparation. Bootcamps, by and large, have pretty thin margins and can’t invest much in paying for interview prep or hiring engineers to do it. As such, if they do offer mock interviews with professionals, just like tech companies, they tend to cap them at one or two, which we know isn’t nearly enough.\n\nIf they do not offer mock interviews with professionals (and most do NOT), they instead tend to focus on peer to peer practice (students interviewing other students), or they solicit volunteers from their pool of recent alums. While peer interviews are not entirely useless, they’re a far cry from working with someone who knows what the interview process is like. For the price tag that bootcamps charge their students, it’s appalling that they’re unable to invest resources into the last mile: actually getting their students ready for interviews.\n\nThat’s not to say that bootcamp grads are not viable employees. If that were true, I wouldn’t be advocating bootcamps as a means of increasing diversity in software engineering. What’s heartbreaking is that bootcamp students have all the makings of good engineers—they just need to spend an extra 3-6 months after graduation drilling by themselves on the skills their programs should have offered them.\n\nWhile I have less direct experience with curriculum and career centers at non-elite universities (another contributor to the practice gap), from speaking with students who attend these schools, it’s clear that access to consistent interview prep is hard to come by, especially when compared to what’s available at their more elite counterparts.\n\nThe cost of falling short\n-------------------------\n\nWhy are bootcamps, universities, and employers all falling short and exacerbating the practice gap? With both employers and bootcamps, the teams responsible for facilitating interview info sessions or mock interviews typically consist of career coaches who don’t come from a technical background and therefore don’t have a good grasp of the difference between preparing for behavioral interviews and technical ones … and the big gulf between the two.\n\nBehavioral interviews aren’t easy, sure, but they don’t require you to get used to a completely different mode of communication while simultaneously solving hard problems and turning them into code as someone breathes down your neck. Nor do behavioral interviews require hours and hours of grinding and drilling.\n\nPreparing for technical interviews (and other analytical, domain-specific interviews… see the Tweet about management consulting below) is much more like studying for a math test than it is about practicing presenting yourself and telling your story. But until you’ve done it, you won’t really get it.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_16_at_19_22_19_2x_5d1b0fcf7f.png)\n\nThe most heartbreaking effect of the practice gap is this. Companies, under fire for hiring exclusively from top schools, expand their recruiting efforts and start targeting a broader range of schools and programs. One of the boldest programs of this type interviewed every CS student at each HBCU to develop a pipeline of African American candidates. Sadly, because of the interview practice gap, good-intentioned programs like this are destined to fail. A 2016 piece in Bloomberg (“[Why Doesn’t Silicon Valley Hire Black Coders?](https://www.bloomberg.com/tosv2.html?vid=&uuid=324fb7d1-d308-11ed-a129-7179624d4b55&url=L2ZlYXR1cmVzLzIwMTYtaG93YXJkLXVuaXZlcnNpdHktY29kZXJzLw==)”) covered this problem extensively:\n\n> *When they started interviewing seniors, companies found—as Pratt did at Howard—that many were underprepared. They hadn’t been exposed to programming before college and had gaps in their college classes. The companies were coming into the process too late. So many of them have created programs geared toward freshmen and sophomores.*\n\n**Then, when the initiative inevitably fails, companies walk away with their conscious biases about the “superiority” of top schools confirmed. Or even worse, this confirms their unconscious biases about race and/or gender, when the real problem isn’t the students but their lack of access to interview practice.**\n\nHow you can (actually!) help\n----------------------------\n\nA few years ago, we ran a study that examined interview performance by school tier. Among students who were actively and regularly practicing technical interviewing, [there was no difference in performance between elite schools and non-elite schools](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt)!\n\n![Chart showing new grad interview performance by school tier](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fnew_grad_interview_performance_008804d4ed.png&w=1920&q=75 \"New Grad Interview Performance by School Tier\")\n\nAll that is to say that **with at least 5 professional mock interviews per candidate, we could close the practice gap, level the playing field in software engineering, and hit our representation goals.** This is a bold statement, but in my decade plus of operating in this space, I have not seen anything as effective at closing the gap in tech as getting everyone technical interview practice. If you’re an employer, I urge you to consider the following:\n\n* **Create a program where you sponsor interview practice for your candidates**, especially those who aren’t socialized in technical interviews. I don’t care if it’s through interviewing.io or through someone else. Just do it.\n* One mock interview is not enough. An info session is not enough. **You need at least five interviews to move the needle**, and they need to happen systematically and on a schedule. But you know what? It’s a few hundred dollars per candidate. Compared to the cost of sourcing and interviewing them, it’s peanuts.\n* You may want to think about routing some of your D&I budget toward this purpose. **If you’re spending any appreciable amount of money on initiatives like unconscious bias training**, [which has been shown over and over not to work](https://www.mckinsey.com/featured-insights/gender-equality/focusing-on-what-works-for-workplace-diversity), **you could make a much bigger dent by relocating that budget toward interview practice for your candidates**.\n\nThere is a lot of loud conversation in Silicon Valley about fixing representation in tech. **It’s time to stop talking and instead do something that actually moves the needle. If interviewing.io had the means to give five mock interviews to every student in the US who doesn’t have the ability to pay for them, we’d do it right now.**\n\nMy years of experience have shown me that there are countless members of underrepresented groups who are ready and able to be outstanding engineers, but who have been unable to pass their technical interview. We at interviewing.io are determined to make a difference, and we want to give people the same gift that MIT once gave me: the knowledge that interview practice is everything and that failing an interview doesn’t mean you’re not cut out for this career.\n\nIf you want to help us, [get in touch](mailto:hello@interviewing.io).",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/technical-interview-practice-gap",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Announcing our Pay Later Program: Don’t pay for mock interviews until you get a job",
      "content": "I started interviewing.io 6 years ago with the mission of making eng hiring efficient and fair. In my mind, as long as employers were obsessed with where people had gone to school and worked previously, instead of focusing on what people could actually do, hiring would stay broken.\n\nSo, we set out to build a better system. On our platform, you get mock interviews with senior engineers who’ve been involved in hiring decisions at top companies. If you do well in practice, you get to bypass recruiter calls and resume screens and instantly book real technical interviews at top companies, as early as tomorrow, regardless of how you look on paper. *Real interviews are also* completely anonymous, and because we use interview data, not resumes, our candidates end up getting hired consistently by companies like Facebook, Uber, Amazon, Lyft, Dropbox, and many others, and 40% of the hires we’ve made to date have been candidates from non-traditional backgrounds (many were literally rejected from the same company when they went through the front door and someone saw their resume… only when they got to interview anonymously did they get the chance to show what they could do).\n\nHere’s a new video we made explaining what we do. People don’t usually believe us when we explain it in words (“*Wait, what, you can book a real interview at Amazon tomorrow? What’s the catch?”*), so here are some nice animations. It’s real, we promise, and it works.\n\nFor most of our lifetime, practicing on our platform was completely free for engineers — you could get paired up with a professional interviewer (a senior engineer from a FAANG or FAANG-adjacent company) and do 3 mock interviews completely on us. This model worked pretty well because we monetized employers who wanted to hire our top-performing users, and the proceeds went to paying our awesome professional interviewers.\n\nBut it wasn’t perfect. To keep our costs manageable, we had to cap people at 3 mock interviews, and we had to waitlist a lot of people because either they weren’t experienced enough (despite our best efforts, we couldn’t get companies to pay us for access to junior engineers) or because they weren’t looking for jobs in our target locations.\n\nThen COVID-19 happened and with it, a deluge of layoffs, hiring slowdowns, and freezes. We found ourselves down from 7-figure revenue to nothing. Companies didn’t really want or need to pay for hiring anymore.\n\nIf we wanted to keep offering practice in this climate, while still being able to pay our interviewers, we needed to find another revenue stream. After the quarantine began, we made the very hard call to start charging people for practice. But, charging felt anathema to our mission, so we also made a [public promise back in June of 2020](https://interviewing.io/blog/interviewing-io-is-out-of-beta-anonymous-technical-interview-practice-for-all) to launch a program in the future where engineers could defer paying for practice til they found a job.\n\nToday, we’re making good on that promise by launching our Pay Later Program.\n\nWhat is the Pay Later Program?\n------------------------------\n\nHere’s how it works:\n\n* **You get $512 or $1024 to spend on mock interviews.** Other than being cute powers of 2, we chose these numbers because our data shows that [after 5 mock interviews, your chances of passing real interviews will double](https://interviewing.io/blog/how-know-ready-interview-faang), so we wanted to get people enough practice to make a difference in their outcomes.\n* You put down a credit card and promise to pay us when you find a new job. There’s no interest and no gotchas, and enrolling is really as simple as putting down a card and promising not to screw us over.\n* We check in with you in 4 months (most of our users find jobs within 4 months of starting to practice). **If you’ve found a new job, great, we charge you, and any unused credits are yours to keep. If you have not, no worries, we grant you an extension** (and keep granting you extensions til you’ve found a job).\n* Here’s the best part. **If you find a job through interviewing.io with one of our employer partners, then you don’t have to pay us anything!** (If you’re not part of the Pay Later program and you find a job through us, we *still* refund all the $$ you spent on practice.)\n\nWho’s it good for?\n------------------\n\nThe Pay Later Program is available to all of our users who are authorized to work in the US, Canada, the UK, and Australia. We’ll be rolling out in other countries as we go.\n\nBeyond that one criterion, this program is for any engineer or aspiring engineer who will be interviewing in the near to medium-term future. We know how much practicing for technical interviews sucks. If you’re a junior engineer, it’s scary because you haven’t done it before. If you haven’t come from a traditional computer science background, and especially if you’re coming out of a bootcamp, it’s scary because, chances are, you haven’t been taught this material properly (bootcamps aren’t good at this, sorry).\n\nAnd if you’re an experienced engineer, interviews are scary because they’re not the work that you do every day, and ironically, the more experienced you are, the worse you do in these interviews, even as the bar keeps getting higher and higher.\n\nBelow you can see the seniority of users currently enrolled in our program:\n\n![Chart showing seniority of users enrolled in interviewing.io's Pay Later Program](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FSeniority_of_users_enrolled_in_interviewing_ios_Pay_Later_Program_ee3a6ea4ab.png&w=750&q=75 \"Pay Later Program participant seniority\")\n\nIf you need interview practice, whether you’re junior or experienced, whether you look good on paper or not, you should sign up, and you’ll get value out of it. Whether you don’t have the $$ now or whether you’re just not sure if the investment is worth it, this program is for you.\n\nDoes it really work?\n--------------------\n\nYes! The majority (over 80% of our users find jobs in 4 months or less), and by the 6 month mark, 94% of our users are employed. Here are some testimonials from our users. It was hard to pick which ones to share. We have a lot.\n\n> *I love your platform, and you can confirm I use it all the time! I push using your platform as the #1 advice when anyone I mentor asks me for ‘prep advice’. I always tell them: “When you have that job at Google and all that stock, won’t you think the paid prep was worth it?”*\n\n> *Your program has definitely changed my life. I was able to use some of my signing bonus to buy a home in the DC area, so this work is really starting to pay off.*\n\n> *I can’t speak highly enough of the [Pay Later] program. As an engineer looking to make a jump, the mock interviews are an invaluable practice resource. Being able to utilize the mock interviews even before getting that really well-paying job, is life-changing. It gave me access to a resource which literally catapulted my career in ways I could only vaguely imagine a year ago. I really hope they keep the program around to help others who aren’t yet flush with cash.*\n\n> *My experience with interviewing.io was awesome. I used the Pay Later program to schedule six practice interviews, which led to offers from my top targets: Google, Pinterest, Airtable and Stripe. It is hard to simulate an actual interview while using Leetcode, and I definitely feel the practice interviews were a key factor in the success I had during technical interviews throughout my job search.*\n\n> *I signed up for the [Pay Later] program as I worked on transitioning from teaching high school to software engineering. The program seemed like a no-brainer. I could get all of the excellent practice opportunities I needed and I would only have to pay for the service after I landed a new job. A job which would nearly double my previous salary.*\n\n> *interviewing.io’s [Pay Later] program was a godsend for me. I was not in the best place financially (as a job seeker does) and I needed the practice to improve my interviewing skills. Of all the platforms that I have tried, the most engaging and the most helpful is interviewing.io. They are so patient with the deferral process and I even received my current job through the platform!*\n\n> *My mock interviews on interviewing.io were instrumental in my preparation for my latest job search. I also really appreciated the flexible payment options they were offering during the pandemic. Every mock left with me specific, actionable feedback that I could apply to my next phone screen, and it didn’t take long to see serious improvements in my performance, and the number of onsites I was booking. Eventually, I accepted an offer from Facebook, even with no degree and limited field experience. Sincerely, I can’t thank your team enough for helping make this dream of mine come true!*\n\n> *Prior to using interviewing.io, I had been very discouraged and frustrated when I was constantly rejected after on-site interviews and not knowing what I did wrong or where to improve. To give it a perspective, I found myself clearing 90% of the phone screens but receiving a rejection after the onsite interviews. Even though there are lots of interview preparation resources out there such as Leetcode, AlgoExpert, Growking the Coding Interview, none of them truly bridges the gap between my technical knowledge and the interview expectation. The feedback provided by the professional interviewers during the mock interviews were extremely helpful as they provided points of improvement on areas I have never thought of. On top of all, the deferral program really provided me a risk free and worry free opportunity to try out the platform.*\n\n> *I just cleared Amazon and Google interviews. I have a learning disability and social phobia. Prior to using interviewing.io I had never gotten past the technical phone screens. Getting in the habit of doing back to back interviews every weekend helped with both anxiety and getting better at timing myself to solve all problems within 45 min. $600 owed in a couple of months… will have been worth it 100x over 😊*\n\n> *I think the practice at interviewing.io really helped me land a job at Google. The interviewers were professional and really improved my communication skills during the interview. Since each mock interview is structured just like a real interview, I got more familiarized with the environment and the process. The feedback helped me strengthen my foundation and make up for parts that I missed. Especially with the [Pay Later] program, I could afford the charges as a poor student… it felt like interviewing.io had trust in me and this really boosted my confidence.*\n\n> *After I got laid off from my job, I greatly appreciated the opportunity to be a part of the [Pay Later] program. (I found something within the 4 month window!) The interview practice prepared me for my actual technical interviews and helped me approach my technical interviews with confidence. I got a lot of good feedback about how I could improve my presentation during a living coding session. Being able to listen to live recordings of my coding sessions was painful, but enlightening. My interviewers were helpful and asked tailored questions and gave useful feedback. I’ve recommended this to all of my friends who are looking for software engineering jobs.*\n\n> *I absolutely loved the program. It allowed me to focus my energy on preparing myself for my technical interviews, instead of worrying about the cost of my practice. The mock interviews allowed me to get comfortable in a real interview environment, which helped me pass the interviews at Facebook, Google, Amazon, and more!*\n\n> *Ended up getting a few offers but went with Google! I’m from a non-CS/non-SWE background and without a doubt, your platform helped tremendously with landing these offers. Additionally, the deferral program is a fantastic idea… I really appreciate what you are trying to do with interviewing.io and also with shaping the broader interview process. Keep up the great work!!*\n\n> *My experience with the [Pay Later] program was great. I felt a bit anxious spending money initially on the site without the guarantee of receiving a job offer, [but the] flexibility the program offered to me both made me feel comfortable opting into the program as well as gave me the chance to get quality interviews that helped me receive a job. [The program offered] both offered a vital resource to me while also giving me comfort knowing that I wouldn’t need to worry about the cost until I achieved the goal of the program. If I were ever in the need of more mock interviews, interviewing.io would… be my first choice of sites to visit.*\n\n> *I am happy to share with you that I was offered the software engineering position at Facebook that I had been preparing for. Your assistance through the [Pay Later] payment program and the mock interviews I utilized (both general and company-specific) were an integral part of my outcome as I received feedback on my interview performance that I had only ever guessed at. I am so grateful for the opportunity to have been able to do the deferred payment arrangement with Interviewing.io since I would’ve foregone paid mock interviews otherwise!*",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/announcing-our-pay-later-program-dont-pay-for-mock-interviews-until-you-get-a-job",
      "author": "",
      "user_id": ""
    },
    {
      "title": "There is a real connection between technical interview performance and salary. Here’s the data.",
      "content": "At the end of the day, money is a huge driver for the decisions we make about what jobs to go after. In the past, we’ve written about [how to negotiate your salary](https://interviewing.io/blog/negotiate-salary-recruiter), and there are a lot of [labor statistics](https://www.bls.gov/opub/btn/volume-7/high-tech-industries-an-analysis-of-employment-wages-and-output.htm) and reports out there looking at salaries in the tech industry as a whole. But as with many things in eng hiring, there’s very little concrete data on whether technical interview performance plays a role in compensation offers.\n\nSo we set out to gather the data and asked our users who had gone on to successfully get jobs after using our platform to share their salary info. **With our unique dataset of real coding interviews, we could ask questions like:**\n\n* **Does interview performance matter when it comes to compensation packages?**\n* **Do engineers who prioritize other parts of a role over compensation (e.g. values alignment) end up with lower salaries?**\n* **What else seems to matter in getting a higher salary?**\n\nTo be clear, this is an exploration of past average interview performance and its connection with current salary, versus looking at how someone did in an interview and then what salary they got when they took that specific job. In other words, we haven’t paired job interviews with the salary for that same job. We believe that looking at these more general measures is more informative than trying to match single interviews and job offers, given [how volatile individual interview performance can be](https://interviewing.io/blog/after-a-lot-more-data-technical-interview-performance-really-is-kind-of-arbitrary). But our interviewing platform allowed us to look at performance across multiple interviews for respondents, which gave us more stability and more data.\n\nThe setup\n---------\n\nOn the interviewing.io platform, people can practice technical interviews online and anonymously, with real engineers on the other side.\n\nWhen an interviewer and an interviewee match on our platform, they meet in a collaborative coding environment with voice, text chat, and a whiteboard and jump right into a technical question. Check out our [recordings](https://interviewing.io/mocks) page to see this process in action.\n\nInterview questions on the platform tend to fall into the category of what you’d encounter at a phone screen for a back-end software engineering role, and interviewers typically come from top companies like Google, Facebook, Dropbox, Airbnb, and more.\n\nAfter every interview, interviewers rate interviewees on a few different dimensions: technical skills, communication skills, and problem solving skills. These each get rated on a scale of 1 to 4, where 1 is “poor” and 4 is “amazing!”. On our platform, a score of 3 or above has generally meant that the person was good enough to move forward. You can see what our feedback form looks like below:\n\n![Screenshot showing Interviewing.io interview feedback form](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F6f86b_screenshot_2017_11_29_09_10_46_6a2be62174.webp&w=1920&q=75 \"Interviewing.io interview feedback form\")\n\nWith this in mind, we surveyed interviewing.io users about their current roles, including salary, bonuses, and how satisfied they felt in their job and then tied their comp back to how they did in interviews on our platform. We ended up with responses from 494 engineers[1](#user-content-fn-1), and because compensation packages are so complex and vary from company to company, we analyzed the data in several different ways, looking at annual salary numbers, bonuses, and equity. **Then we tied compensation data to performance in technical interviews to see whether it matters, and if so, how much.**\n\nThe results\n-----------\n\nWe looked at the relationships between interview performance (technical skills, communication ability, and problem solving ability) and the following: base salary, bonuses, and equity. In all cases, we corrected for location (being in the Bay Area means a higher salary) and experience (senior engineers make senior salaries), and where we could, we corrected for company size (bigger companies can generally pay bigger salaries).\n\nThe mean yearly salary for all survey participants was around $130k, and 57% of them reported a yearly bonus. For that group, the average yearly bonus was $20k. For people who reported a dollar amount for equity, the average was $54k. Below is the distribution of experience level/seniority of survey respondents.\n\nHere’s what we found.\n\n### Better technical skills correlate with higher compensation\n\nAs probably comes as no surprise, **people who score higher on technical skills during interviews do make more money**. First, let’s look at base salary.[2](#user-content-fn-2)\n\nBonuses, too, correlate with technical skills, with an additional point in performance potentially worth about 10k[3](#user-content-fn-3):\n\n### The relationship between compensation and other interviewing skills\n\nWe also looked at the two other ratings that interviewers give after interviews: communication and problem solving. **Better communication scores had a small but statistically significant correlation with salaries** (r = .15, p < .01), but we found no significant relationship for problem solving scores in isolation:\n\nWe also didn’t see a relationship between either communication ability or problem solving ability when it came to bonuses.\n\nThe non-relationships didn’t surprise us too much, to be honest, because with a relatively small sample size it’s notoriously difficult to get subcomponents of ratings to show a relationship to something distal and complicated like salaries. It’s very possible these relationships do exist, and with many determiners besides actual interview performance, like seniority and market salary norms, we’d like to repeat this survey at a bigger scale to inform this question.\n\nWhat else?\n----------\n\nWe asked engineers whether they felt satisfied with their role, and found that engineers who felt satisfied earned an average $14k more than engineers who felt dissatisfied.[4](#user-content-fn-4)\n\nWe also looked at people’s perceptions of their own performance. [In a previous post](https://interviewing.io/blog/impostor-syndrome-strikes-men-just-as-hard-as-women), we explored how people rated their own technical performance after an interview compared to how the interviewer rated them and found that even experienced engineers aren’t great at guessing how they did. **For this project, we were curious about whether overconfident engineers might net higher salaries (perhaps they negotiate harder!). So we also looked at people who rated their performance higher than their actual interview score — but found no difference in their compensation packages.**\n\nAnother thing we were curious about was whether people who valued money over other factors while making a job decision would have higher salaries. So we asked people to rank the most important variables in their job decisions. 32% of respondents said that a compensation package was the most important part of their decision; the next highest response was “matches my interests and values”. But these question didn’t have any predictive value for the actual salary amount: **people who said money matters the most didn’t have significantly different salaries from people who say money matters the least**. It’s possible that with salaries being impacted by so many outside factors, like location and role type, candidates don’t truly have a lot of negotiating power over that salary number.\n\nWe looked at equity as well, and the average reported equity package size was 54k. We did not find any significant association between interview performance and reported equity packages. That said, enormous amounts of research have documented various salary gaps based on gender, race, and other important sociocultural and demographic factors, and we hope to repeat this analysis when we have more data.\n\nWhat do these findings mean for you?\n------------------------------------\n\nInterview performance doesn’t just get you in the door or not: it can have a demonstrable connection to your eventual compensation. For instance, **doing just a point better in your technical interview could be worth 10k or more, and with bonus, it could add 20k to your annual comp**.\n\nGiven how much technical interview performance matters, we’d be remiss if we didn’t suggest [signing up for free, anonymous mock interviews](https://interviewing.io/signup) on our platform. So, please go do that.\n\nAnd, if you’re curious about what [our salary survey](https://interviewingio.wufoo.com/forms/salary-survey) looked like or want to participate and contribute to v2 of this post, please do so too!\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. Our salary survey ended up with 494 respondents, but because some people filled out our survey but had not yet done an interview on our platform, only a subset of folks had both salary data and interview data: N = 234, or 47% of our salary sample. Therefore in all the analyses where we compare salaries to interview performance, it’s only for this subgroup. [↩](#user-content-fnref-1)\n2. We ran both a correlation between these factors and a regression to correct for confounding factors like seniority and location. For the correlation, r = .22 and p < 0.001. For the regression, F = 16.06 and p < .001. [↩](#user-content-fnref-2)\n3. As with base salary above, we ran both a correlation between these factors and a regression to correct for confounding factors like seniority and location. For the correlation, r = .17 and p < 0.05. For the regression, F = 1.63 and p < .05. [↩](#user-content-fnref-3)\n4. Looking at it as a binary, satisfied engineers earned significantly more than non-satisfied engineers in a predictive test, F = 5.2398, p < 0.02, and satisfaction and salary amount are also positively correlated. [↩](#user-content-fnref-4)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/there-is-a-real-connection-between-technical-interview-performance-and-salary-heres-the-data",
      "author": "",
      "user_id": ""
    },
    {
      "title": "We’ve raised our Series A!",
      "content": "I’m really excited to announce that we raised a $10M Series A, led by the fine people at [M13](https://m13.co/portfolio). This round of funding is a long time coming. I don’t know what the average company age is when successfully raising an A, but it took us 6 years to get here. It was a long and windy path, and for a little while, we almost died (more on that below), but we’re still here, and we’re so grateful and excited to finally have the resources to do what we’ve always wanted to do: fix hiring, for real.\n\nWhat does “fixing hiring” mean? We believe that the only way to really effect change, is to make hiring more efficient—meaning that the right people are talking to the right companies at the right time. *Make hiring efficient* is actually our company mission. “Efficient? But, that’s so uninspiring,” one might say. And to that, we say false. For us, fairness follows from efficiency… whereas the reverse isn’t necessarily true.\n\nHow do you make hiring efficient? Here’s our todo list:\n\n1. Kill resumes\n2. Democratize access to interview practice: make it so everyone who needs practice can get it and can enter the technical interview process on a level playing field\n3. Democratize access to jobs and put power back in the hands of individual engineers. Make it so people who are good, regardless of who they are or how they look on paper, can talk directly to hiring managers at any company they want, whenever they want\n\nKilling resumes\n---------------\n\nMany years ago, when I was working as a recruiter, I wrote a program that took hundreds of candidate resumes who had previously interviewed at the company, and looked at a bunch of different traits, including GPA, years of experience, university, highest degree earned, previous employer, and so on.\n\nI found that, by far and away, the thing that mattered most was [how many typos and grammatical errors a resume ha](https://blog.alinelerner.com/lessons-from-a-years-worth-of-hiring-data/)d. Where people went to school didn’t matter at all, and neither did any of the other traits above (except their previous employer, which mattered a bit, but much less than the typos).\n\nI also did an experiment where I showed anonymized resumes to recruiters and hiring managers, asking them if they’d interview that candidate. Not only could they not tell who the good candidates were, they all [disagreed with each other about what a good candidate looked like](https://blog.alinelerner.com/resumes-suck-heres-the-data/) in the first place.\n\nIt became abundantly clear to me that resumes sucked, but if we’re honest, they’re not the only problem. Once you’re in the door, you have to pass the technical interview.\n\nDemocratizing access to interview practice\n------------------------------------------\n\nTechnical interviews are hard for all engineers, regardless of background or seniority, but they’re not going away for a while, and, perhaps worst of all, access to practice isn’t equitably distributed. At schools like MIT, where I went for undergrad, students have boundless access to interview practice. When I was there, not only was there a multi-week course during the month-long break between Fall and Spring semesters that was dedicated exclusively to passing technical interviews, but there was a social component to it as well —everyone at these schools is interviewing at FAANG for internships and new grad positions, which means that students can practice with each other, share their successes and failures, and, over time, internalize just how much of technical interviewing is a numbers game. Once you “get it”, you know that bombing a Google interview doesn’t mean that you weren’t meant to be an engineer. It just means that you need to work some more problems, do more mock interviews, and try again at Facebook.\n\nStudents not coming from these schools, or any engineers without an existing network of people with whom they could practice, would miss out not only on the interview prep, but also on the more subtle change in perception where failure is a temporary state, not a death sentence.\n\nAnd so, interviewing.io was born. The idea was simple. Any software engineer could use the platform to prepare for technical interviewing via *anonymous* mock interviews with senior engineers from top companies like Google, Facebook and others. Junior engineers could get a taste of what real interviews were like and grok that these interviews are a learned skill. Senior engineers could de-rust and warm back up without anyone knowing who they were.\n\nInterviews suck for everyone. **Many engineers, regardless of seniority or pedigree, perform poorly in their first mock interview, but after about 5 practice interviews, their odds of passing a real interview double.** **In fact, after getting the requisite amount of practice interviews in and warming up, there is NO difference in interview performance among engineers who graduated from a top-tier CS school and those who did not.**\n\nDemocratizing access to jobs and putting engineers in the driver’s seat of their job search\n-------------------------------------------------------------------------------------------\n\nIf you do well in practice interviews, no matter who you are or how you look on paper, you unlock our jobs portal. With 2 clicks, you can book real interviews with engineers at top companies, bypassing online applications, resume screens, and recruiters.\n\nIn other words, you don’t have to have a friend refer you in or wait for recruiters to contact you and then spend weeks scheduling calls. You’re in control, and you can book or not book whenever you like.\n\nThese interviews are also anonymous, just like practice, which means that companies don’t know who you are until *after* the interview, so you’re judged on what you can do, not how you look on paper.\n\nFor employers who hire through us, this is a win. End-to-end, from first touch to hire, a top-tier tech company will hire about 0.5% of its candidates. With interviewing.io, that number is closer to 10%, and companies who use it save an average of 220 hours per hire, split between engineering and recruiting. **Close to half of our hires are candidates the company would not have considered if they had seen just their resume, and many of these engineers had previously been rejected by literally the same company where they later got hired via interviewing.io.** Over our lifetime, we’ve made hires for companies like Facebook, Amazon, Lyft, Uber, Snap, Dropbox, and dozens of others.\n\n**The other, subtle value-add is that while, to the rest of the world, interviewing.io users look like passive candidates who aren’t on the market — they don’t update their LinkedIn or start blogging or tweeting — interviewing.io knows they’re about to look because they’ve started practicing. In other words, we know the 2 things that actually matter: who’s good and who’s actually looking right now. No one else, not even LinkedIn, has that information.**\n\nAnd for those engineers, we roll out the red carpet.\n\nA brief interlude: The bit where we almost died but then didn’t\n---------------------------------------------------------------\n\nThis all sounds pretty good, right? It kind of was, but then things got bad. You see, for most of our existence, we made all of our money from employers, which meant that we could offer interview practice for free. When hiring ground to a halt in the wake of COVID, we lost almost all of our revenue and started charging individual engineers for interview practice to survive. You can see the [blog post where we announced our pivot](https://interviewing.io/blog/interviewing-io-is-out-of-beta-anonymous-technical-interview-practice-for-all), and while the post’s title is kind of peppy, it was a very hard post to write and an even harder decision to make. If you want to hear the whole sordid tale of woe, you can listen to my 2020 interview on Indie Hackers:\n\nAfter we made the switch, we promised our users that we would find a way to make it so people who couldn’t afford to pay us could still practice on our platform—if we didn’t do that, we’d be in gross violation of our goal to democratize access to practice. That promise is a large part of why we raised this round. More on that in a moment.\n\nHere’s the shape of our revenue before and after COVID (things started to go south a few months before quarantine… there’s always a seasonal dip in our business, but this time it never recovered).\n\n![It looks cool now, but it wasn’t fun to live through.](https://strapi-iio.s3.us-west-2.amazonaws.com/Screen_Shot_2021_10_17_at_2_25_22_PM_bebee560fb.webp)\n\nIt looks cool now, but it wasn’t fun to live through.\n\nThe hockey-stick growth we had after our switch is what enabled us to raise this A. And that hockey-stick growth is due to the herculean efforts of the interviewing.io team to build out and test a brand new product in a little over a month, while the future loomed more and more uncertain. A special thank you here to the longest-standing members of the team who were here before COVID and through it: Liz Graves, Sam Jordan, Eamonn MacConville, and Dawid Kluszczynski.\n\n![This is our awesome team. Thanks guys.](https://strapi-iio.s3.us-west-2.amazonaws.com/team_photo_2021_final_46a24a9101.webp)\n\nThis is our awesome team. Thanks guys.\n\nIt’s due to our intrepid interviewer community, who stuck by us and continued to give hours of their time to helping people practice and get better, even when they were unsure if we’d be around tomorrow.\n\nIt’s due to the short list customers who stayed on even when hiring slowed down, those who came back when the economy started to recover, and the new ones who’ve taken a bet on us since.\n\nAnd it’s due to our users who could have rightfully rage quit when we started charging them for something that used to be free… but instead chose to stay, answered our myriad user research emails about pricing and offerings, and were willing not only to to pay us, but pay us through a series of rather suspicious-looking PayPal links (before we actually integrated payments into our platform).\n\nToday, an engineer signs up for interviewing.io every 8 minutes, we have over a third of Bay Area engineers in our ecosystem, and **over our lifetime, interviewing.io has hosted close to 100K technical interviews. In the process, we’ve observed, over and over, that where someone went to school has no bearing on how they’ll perform in an interview.**\n\nWhy M13 (including the bit where I was pitching while 7 months pregnant)?\n-------------------------------------------------------------------------\n\nWe chose M13 to lead our Series A because both Brent Murri and Matt Hoffman immediately understood our vision for what good hiring looks like and both cared deeply about fixing it for engineers in particular. Perhaps most importantly, they not only understood, but encouraged me to invest time and resources in the power of our community. Our one core value as a company is to put candidate (read: engineer) experience first, regardless of where the money is coming from. We want to be the place where smart people hang out and come back to. If we have that right, everything else will come. Building for engineers, smart driven creative engineers, is a bit of a double-edged sword (because they, too, are brashly opinionated), but it’s also a gift. Not everyone gets to love their users, and most companies don’t get to tangibly change people’s lives and give people stuff they can’t get anywhere else. We’re able to do that, and we’re fortunate to have found partners who believe in this value as fervently as we do.\n\nAnd, look, I have no idea if this is the right place to put this, but there’s one other reason we went with M13. Most people don’t know this (though I guess they will now), but I started pitching our Series A when I was 7 months pregnant. I had been hoping to start earlier, but there was a lot of deck work and pitch practice that preceded it, and I’m a solo founder, so all the pitch practice and deck work had to be slotted in between continuing to run the company. (As an aside, my team is awesome. I didn’t realize exactly how awesome until I had to give birth 3 weeks early and ended up being on unplanned leave for several weeks afterwards with no plans in place yet… in retrospect, I could have stepped back and focused on pitching much earlier!)\n\nSo there I am, on a bunch of Zoom calls with investors, where they see me from the neck up, while fielding all manners of conflicting advice about when to reveal that I’m in the family way. Of course, in an ideal world, you reveal it immediately, and it doesn’t change anything. And there are those who would advise leading with the pregnancy because if investors aren’t immediately supportive, fuck them, right? After hearing all sorts of horror stories, I took the middle road and decided to wait until prospective investors had at least a few meetings with me and had the chance to get what interviewing.io was before dropping the news. Not only were the folks at M13 completely gracious about the big reveal, but the way they handled the rest of the process was exemplary, even when I went dark at the most fraught part of the deal—right after I got their term sheet—because, well, I went into the hospital for a routine checkup in between investor calls and came out with a baby. (Don’t worry, everything worked out alright, and we have a beautiful baby girl.)\n\nI hope every founder who’s pregnant or about to be or has small children has the kind of experience that I had with M13. Full stop. In that world, worrying about when to break the news that we’re pregnant will be an unfortunate, anachronistic blip.\n\nWhat’s next? (Spoiler: We’re going to make it so anyone can practice now and pay later, and we’ll get *all* the companies you want onto our platform)\n-----------------------------------------------------------------------------------------------------------------------------------------------------\n\nNow that we have the resources, we want to make good on the promises we made to our users back when things got real hard. It’s not possible to democratize access to interview prep if you’re charging for it, so that we’d create a Fellowship program where engineers from traditionally underrepresented backgrounds can practice for free and that we’d create a program where people who needed practice now could defer payment until they got a job.\n\nWe’ve already rolled out the first two things (and will be running a 3rd Fellowship cohort soon). Our deferred payment program has been in pilot mode for the last few months, and so far we’ve seen an amazing 94% payback rate. **With our Series A funding, we’ll be able to take this program out of pilot mode and roll it out to every engineer on our platform— don’t pay us a dime until you get a job and are in a position to pay.**\n\nThe other big thing we’re going to focus on is getting more employers on interviewing.io. Hiring is back, post-COVID, in a big way, and as we regrow the employer side of our business, we’ll be able to make our platform more affordable, even without deferred payments. We also want to fulfill our promise of putting power back in the hands of engineers. Hiring is bad. Resumes don’t tell you much, scheduling is painful, getting in front of companies takes weeks of emailing if you’re lucky and infinite time if you’re not. We’re rethinking hiring from the ground up, automating away the cruft, and removing friction for the part that matters: *smart people talking to each other about the actual work.* **So, if you’re a top performer on our platform, no matter who you are, our goal is to make it so you can speak to a hiring manager at any company you want, anonymously, as early as the next day, and get fast-tracked through the rest of the process.**\n\n*This post wouldn’t be complete without a thank you to all of the new investors who joined in this round in addition to M13. A special thank you to our existing investors, many of whom invested in multiple rounds. Of those, I’d like to mention the following people individually: Leo Polovets at Susa Ventures, Freada Kapor Klein at Kapor Capital (who’s been with us since the very, very beginning… thank you)*, *David Waxman at TenOneTen Ventures, and Peter Livingston at Unpopular Ventures. Even when things got really bad and we were sitting at the very nadir in the graph above, they were with us every step of the way and continued to believe in our vision for better hiring.*",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/weve-raised-our-series-a",
      "author": "",
      "user_id": ""
    },
    {
      "title": "After a lot more data, technical interview performance really is kind of arbitrary.",
      "content": "[interviewing.io](https://interviewing.io/) is a platform where people can practice technical interviewing anonymously, and if things go well, get jobs at top companies in the process. We started it because [resumes suck](https://interviewing.io/blog/resumes-suck-heres-the-data) and because we believe that anyone, regardless of how they look on paper, should have the opportunity to prove their mettle.\n\nIn February of 2016, we published a [post about how people’s technical interview performance, from interview to interview, seemed quite volatile](https://interviewing.io/blog/technical-interview-performance-is-kind-of-arbitrary-heres-the-data/). At the time, we just had a few hundred interviews to draw on, so as you can imagine, we were quite eager to rerun the numbers with the advent of more data. **After drawing on over a thousand interviews, the numbers hold up. In other words, technical interview outcomes do really seem to be kind of arbitrary.**\n\nThe setup\n---------\n\nWhen an interviewer and an interviewee match on interviewing.io, they meet in a collaborative coding environment with voice, text chat, and a whiteboard and jump right into a technical question. After each interview, people leave one another feedback, and each party can see what the other person said about them once they both submit their reviews.\n\nAfter every interview, interviewers rate interviewees on a few different dimensions, including technical ability. Technical ability gets rated on a scale of 1 to 4, where 1 is “poor” and 4 is “amazing!” ([you can see the feedback form here](https://strapi-iio.s3.us-west-2.amazonaws.com/9fdaa_new_interviewer_feedback_circled_2d4ba08149.png)). On our platform, a score of 3 or above has generally meant that the person was good enough to move forward.\n\nAt this point, you might say, that’s nice and all, but what’s the big deal? Lots of companies collect this kind of data in the context of their own pipelines. Here’s the thing that makes our data special: **the same interviewee can do multiple interviews, each of which is with a different interviewer and/or different company, and this opens the door for some pretty interesting and somewhat controlled comparative analysis**.\n\nPerformance from interview to interview really is arbitrary\n-----------------------------------------------------------\n\nIf you’ve read our [first post](https://interviewing.io/blog/technical-interview-performance-is-kind-of-arbitrary-heres-the-data) on this subject, you’ll recognize the visualization below. For the as yet uninitiated, every tiny human icon represents the mean technical score for an individual interviewee who has done 2 or more interviews on the platform. The y-axis is standard deviation of performance, so the higher up you go, the more volatile interview performance becomes. If you hover over each , you can drill down and see how that person did in each of their interviews. Anytime you see bolded text with a dotted underline, you can hover over it to see relevant data viz. Try it now to expand everyone’s performance. You can also hover over the labels along the x-axis to drill into the performance of people whose means fall into those buckets.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_22_at_11_55_47_2x_1ec095b1a8.png)\n\nAs you can see, roughly 20% of interviewees are consistent in their performance (down from 25% the last time we did this analysis), and the rest are all over the place. If you look at the graph above, despite the noise, you can probably make some guesses about which people you’d want to interview. **However, keep in mind that each represents a *mean*. Let’s pretend that, instead, you had to make a decision based on just one data point. That’s where things get dicey**.[1](#user-content-fn-1) For instance:\n\n* Many people who scored at least one 4 also scored at least one 2.\n* And as you saw above, a good amount of people who scored at least one 4 also scored at least one 1.\n* If we look at high performers (mean of 3.3 or higher), we still see a fair amount of variation.\n* Things get really murky when we consider “average” performers (mean between 2.6 and 3.3).\n\nWhat do the most volatile interviewees have in common?\n------------------------------------------------------\n\nIn the plot below, you can see interview performance over time for interviewees with the highest standard deviations on the platform (the cutoff we used was a standard dev of 1 or more, and this accounted for roughly 12% of our users). Note that the mix of dashed and dotted lines is purely visual — this way it’s easier to follow each person’s performance path.\n\n**So, what do the most highly volatile performers have in common? The answer appears to be, well, nothing.** About half were working at top companies while interviewing, and half weren’t. Breakdown of top school was roughly 60/40. And years of experience didn’t have much to do with it either — a plurality of interviewees having between 2 and 6 years of experience, with the rest all over the board (varying between 1 and 20 years).\n\n**So, all in all, the factors that go into performance volatility are likely a lot more nuanced than the traditional cues we often use to make value judgments about candidates.**\n\nWhy does volatility matter?\n---------------------------\n\nI discussed the implications of these findings for technical hiring at length in the last post, but briefly, a noisy, non-deterministic interview process does no favors to either candidates or companies. Both end up expending a lot more effort to get a lot less signal than they ought, and in a climate where software engineers are at such a premium, noisy interviews only serve to exacerbate the problem.\n\nBut beyond micro and macro inefficiencies, I suspect there’s something even more insidious and unfortunate going on here. Once you’ve done a few traditional technical interviews, the volatility and lack of determinism in the process is something you figure out anecdotally and kind of accept. And if you have the benefit of having friends who’ve also been through it, it only gets easier. What if you don’t, however?\n\nIn a previous post, we talked about how [women quit interview practice 7 times more often](https://interviewing.io/blog/voice-modulation-gender-technical-interviews) than men after just one bad interview. It’s not too much of a leap to say that this is probably happening to any number of groups who are underrepresented/underserved by the current system. In other words, though it’s a broken process for everyone, the flaws within the system hit these groups the hardest… because they haven’t had the chance to internalize just how much of technical interviewing is a game. More on this subject in our next post!\n\nWhat can we do about it?\n------------------------\n\nSo, yes, the state of technical hiring isn’t great right now, but here’s what we can say. **If you’re looking for a job, the best piece of advice we can give you is to really internalize that interviewing is a numbers game.** Between the kind of volatility we discussed in this post, [impostor syndrome](https://interviewing.io/blog/own-interview-performance), [poor evaluation techniques](https://interviewing.io/blog/resumes-suck-heres-the-data), and how hard it can be to get meaningful, realistic practice, it takes a lot of interviews to find a great job.\n\nAnd if you’re hiring people, in the absence of a radical shift in how we vet technical ability, we’ve learned that drawing on aggregate performance is much more meaningful than a making such an important decision based on one single, arbitrary interview. Not only can aggregative performance help correct for an uncharacteristically poor performance, but it can also weed out people who eventually do well in an interview by chance or those who, over time, simply up and memorize *Cracking the Coding Interview*. At interviewing.io, **even after just a handful of interviews, we have a much better picture of what someone is capable of and where they stack up than a single company would after a single interview, and aggregate data tells a much more compelling, repeatable story than one, arbitrary data point**.\n\n*Huge thanks to [Ian Johnson](https://twitter.com/enjalot), creator of [d3 Building Blocks](http://blockbuilder.org/), who made the graph entitled \"Standard Dev vs. Mean of Interviewee Performance\" (the one with the icons) as well as all the visualizations that go with it.*\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. At this point you might say that it’s erroneous and naive to compare raw technical scores to one another for any number of reasons, not the least of which is that one interviewer’s 4 is another interviewer’s 2. For a comprehensive justification of using raw scores comparatively, please check out the [appendix](https://interviewing.io/blog/technical-interview-performance-is-kind-of-arbitrary-heres-the-data) to our previous post on this subject. Just to make sure the numbers hold up, I reran them, and this time, our R-squared is even higher than before (0.41 vs. 0.39 last time). [↩](#user-content-fnref-1)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/after-a-lot-more-data-technical-interview-performance-really-is-kind-of-arbitrary",
      "author": "",
      "user_id": ""
    },
    {
      "title": "3 exercises to craft the kind of employer brand that actually makes engineers want to work for you",
      "content": "If I’m honest, I’ve wanted to write something about employer brand for a long time. One of the things that really gets my goat is when companies build employer brand by over-indexing on banalities (“look we have a ping pong table!”, “look we’re a startup so you’ll have a huge impact”, etc.) instead of focusing on the narratives that make them special.\n\nHiring engineers is really hard. It’s hard for tech giants, and it’s hard for small companies… but it’s especially hard for small companies people haven’t quite heard of, and they can use all the help they can get because talking about impact and ping pong tables just doesn’t cut it anymore.\n\nAt interviewing.io, making small companies shine is core to our business, and I’ll share some of what we’ve learned about branding as a result. I’ll also walk you through 3 simple exercises you can do to help you craft and distill your employer brand… in a way that highlights what’s actually special about you and will make great engineers excited to learn more.\n\nIf I have my druthers, this will be one of three posts on brand. This first one will focus on how to craft your story, the second will show you how to use that story to write great job descriptions, and the final one will focus on how to take the story you’ve created and get it out into the world in front of the kinds of people you’d want to hire.\n\nSo, onto the first post!\n\nWhat is employer brand, and why does it matter?\n-----------------------------------------------\n\nCompanies like Google have built a great brand, and this brand is largely what makes it possible for them to hire thousands of good engineers every year — when you think of Google (or, more recently, Microsoft!), you might think of an army of competent, first-principles thinkers competently building products people love… or maybe a niche group of engineers working on moonshots that will change the world.\n\nThis is employer brand. Put simply, it’s what comes to mind when prospective candidates think about your company. Employer brand encompasses every aspect of your narrative: whether people use/like your product, the problem you’re trying to solve, your founding story, your mission, your culture, and what generally what it’s like to work for you. Put another way, all of these attributes (and others besides!) coalesce into the visceral feeling people get when they imagine working for you.\n\nBrand is the single most important thing for getting candidates in the door — even if you have a stellar personal network, in most cases, that’ll usually only last you until your first 30 hires or so — after that, your networks begin to sort of converge on one another. Even so, despite how important brand is for hiring, building it is one of the psychologically hardest things to do at the beginning of your journey because the opportunity cost of spending your time on ANYTHING is staggering, and it’s really hard to justify writing blog posts and hosting events and speaking at conferences when you have to build product and make individual hires and do 50 kabillion other things.\n\n**But, until you build a brand and get it out in the world, you’re going to be hacking through the jungle with a proverbial machete, making hires one by one, trying to charm each one by telling them your story. And once you have a brand, all of a sudden, sourcing is going to feel really, really different (just like it feels when you’ve found product market fit!).**\n\nOver time, if you continue to tell your own story, you, too, will see how much easier sourcing and hiring can be. So, let’s talk about how to craft the right narrative and then proudly shout it from the rooftops.\n\nWhy interviewing.io knows about branding\n----------------------------------------\n\nI mentioned earlier that a lot of what we do at interviewing.io is help our customers put their best foot forward and present themselves to engineers in a way that’s authentic and compelling. I’ll show you some examples of good copy in a moment, but here’s a bit of our flow to put it in context.  \nWhen engineers do really well in practice, they unlock our jobs portal, where they can see a list of companies and roles, like so:\n\n![d988c-ff3-compressed.webp](https://strapi-iio.s3.us-west-2.amazonaws.com/d988c_ff3_compressed_38c9b5f5ba.webp)\n\nAs you can see, companies simply describe who they are and what they do, and top-performing engineers just book real interviews with one click. Because our goal is to remove obstacles from engineers talking to other engineers, we don’t have talent managers or talent advocates or, as they’re often called, recruiters, on staff to talk to our candidates and try to convince them to interview at a certain company. As a result, we often find ourselves coaching companies on how to present themselves, given limited time and space. We do work with quite a few companies whose brands are household names, but a good chunk of our customers are smaller, fast-growing companies. **What’s interesting is that while, on our platform, a household name can have 7X the engagement of a company no one’s heard of, companies no one’s heard of that have exceptional brand narratives aren’t far behind burgeoning unicorns!** (We define engagement as the frequency with which candidates who look at our jobs portal then choose to visit that employer’s page.)\n\nAnd that’s why having a brand story matters… and why we’re equipped to talk about it at some depth.\n\nWhat constitutes brand\n----------------------\n\nBelow are some attributes that can make up a brand story. As you look at the list, think about what each of these corresponds to in your company, and then, think about which of these are the most unique to you. For instance, every early-stage startup can say they have a culture characterized by autonomy and the potential for huge impact. It’s become a trope that doesn’t differentiate anyone in any way anymore and is therefore probably not worth emphasizing. On the other hand, if you are solving a problem that a lot of your candidates happen to have or if you use a really cool tech stack that attracts some niche community, that’s really special and worth emphasizing.\n\n* Your product and whether people have heard of it/like it\n* Your growth numbers if they’re impressive\n* Your tech stack and how flashy and cool it is\n* Your founding story and mission… are you working on a problem that people care about personally? If not, are you disrupting some outdated, inefficient way to do things?\n* How hard are the problems you’re solving? Both technically and otherwise?\n* How elite is your team?\n* What is it like to work for you, both with regard to overall culture and then eng culture specifically?[1](#user-content-fn-1)\n  + Overall culture:\n    - Are you known for kindness/work-life balance? Or grueling hours? (Either can be good depending on whom you want to attract.)\n    - What portion of your employees have gone on to found startups?\n    - Do you have a lot of organization/structure or are you chaos pirates?\n  + Eng culture:\n    - Are you more pragmatic/hacker-y vs. academic?\n    - Do you subscribe to or actively reject any particular development methodologies (e.g. agile)?\n    - Do you tend to roll your own solutions in house or do you try to use 3rd party tools wherever possible?\n    - Do you open source parts of your code? Or regularly contribute to open source?\n\nHow to craft your story in 3 easy exercises!\n--------------------------------------------\n\nThere isn’t a single good formula for what to focus on or highlight when crafting this story, but there are a few exercises that we’ve seen be effective. You can do them in the order below, and by the end, you should have a concise, authentic, pithy narrative that you can shout proudly to the world.\n\nWe’ve found that it makes sense to do these exercises in the order below. **First, you’re going to go with your gut and craft a high-level story. Then you’ll embellish it with details that make you unique and with anecdotes from your employees. And finally, you’ll edit it down into something crisp.** As you work, you can use the list from the “What constitutes brand” section above as a reference. Note that, in general, your story plus details shouldn’t have more than 3 distinct ideas total or it’ll start to feel a bit all over the place.\n\n### Exercise 1 – The story itself\n\nImagine you have a smart, technical friend you respect but who doesn’t know anything about your company’s space. Quick, how would you describe what your company does and why it matters to them? Write it down (and target 5-6 sentences… but don’t worry too much about editing it yet… we’ll do that later).\n\nIf you’re feeling a bit stuck, here are some questions to get you started — think about how you might answer if your friend were asking each of these:\n\n* Why does the company exist/what does your product do, and why does that matter?\n* Why are you going to succeed where others have failed?\n* Why does the company matter to you personally?\n* What do you know that no one else does about your space?\n* What is your company doing that no one else is doing, and why does that matter?\n\nAs you do this exercise, note that when talking to your friend, you dispense with flowery language and explain things succinctly and clearly in simple terms! And that’s the point — the audience you’re selling to is not different than your friend, and your friend probably shares the same cynicism about disingenuous branding that they do!\n\n### Exercise 2 – The unique embellishments\n\nOnce you have the story you came up with above, which will likely be at a pretty high level, it’s time to drill down into the details that make you special. These details will likely be 2nd order, in the sense that they won’t be as broad or all-encompassing as the attributes that came to mind in the first exercise, but they might still be special and unique and worth noting.\n\nSome examples of unique embellishments can be:\n\n* Your tech stack\n  + Do you use any cutting-edge programming languages that one might not often see being used in production? If so, it might be a bit polarizing but attract the community around that language. More on the value of polarization when it comes to unique embellishments below.\n* Unfettered access to some type of user/a specific group you care about that your product impacts\n  + Do you build products for Hollywood? Or for VCs? Or for schools? Some portion of your candidates, depending on their interests outside of work or their future career ambitions, are going to be really excited that they’ll get more direct access to users who operate in these worlds.\n* Unique lifestyle/work culture stuff like working remotely or pair programming\n  + E.g. 37Signals and Pivotal respectively\n* Access to a ton of data/ability to work on massively distributed systems\n  + E.g. even in its early days, Twitch had almost as much ingest as YouTube, and this was a meaningful selling point to candidates who wanted to work at scale but didn’t necessarily want to work at FAANG\n\n#### The surprising value of polarization\n\nToday’s job seeker is in equal parts jaded and savvy, and we’re currently in a labor market climate where talent has the power. The latter makes branding especially important, and by now, engineers have been told all manners of generalities about how much impact they’re going to have if they join a small startup and how whatever you’re working on is going to change the world… to the point where these things have become cliches that shows like Silicon Valley deride with impunity. To avoid cliches like this, think about what TRULY makes you special, and even if it’s a bit polarizing, own it. It’s your story, and the more honest you are about who you are and what you do, the more trust you’ll build with your audience and the more they’ll want to engage.\n\nAnother way to say this is that the most memorable stories might be shrouded in a bit of controversy. That’s not to say that you have to be controversial or contrive it when it isn’t there, but if you do operate in a space or have some aspect to your culture or tech stack that not everyone agrees with, you might find that the resulting self-selection among candidates can work to your advantage. Below are some examples of polarizing narratives.\n\n* **Your work style.** Some companies really value work-life balance, whereas others exalt burning the midnight oil. Some run on chaos and some take a more orderly approach. Some work by the book, and some choose more of a bulldoze your way to success and ask forgiveness rather than permission approach. An example of the latter is Uber — for a long time, their culture was known for a take-no-prisoners approach to getting things done, and this approach has a certain type of appeal for the right people.\n* **Your tech stack.** Certainly choosing your tech stack is, first and foremost, and engineering decision, but this decision has some implications for hiring, and choosing a fringe or niche stack or language can be a great way to attract the entire community around it. The more culty a language, the more fiercely passionate its acolytes will be about working for you (e.g. Rust… though by the time this guide comes out it might be something else!). Note that it doesn’t have to be thaaaat fringe of a language choice as long as there’s a tight-knit community around it, e.g. C#.\n* **Your engineering culture.** Do you subscribe to any particular type of methodology that might be controversial, e.g. are you super into TDD? Are you adamant about rolling all your own everything?\n\nNote that there is no right or wrong here — to loosely paraphase Tolstoy, every startup is broken in its own way, and one saying we’ve heard is that, especially during the early days, The only thing you can do wrong is not own who you are — if you misrepresent how you work or make decisions, you’ll find yourself in one of two regrettable positions: either your hires will leave well before their time or you’ll have a bunch of people marching in different directions or completely paralyzed and unable to choose the right course of action on their own.\n\n### Exercise 3 – Your employees’ unique perspective\n\nAs your team grows beyond you, you will find that your employees’ reasons for working for you are likely different than the answers to the questions above. Talking to them (or, if you don’t want to put them on the spot, having another team member do so) can surface gold for your narrative. In particular, when I was a recruiter, one of the most useful exercises I did was asking my client to introduce me to a very specific handful of engineers. **In particular, I was looking for people who 1) didn’t come to the company through personal referrals and 2) had a lot of good offers during their last job search. Why this mix of people? Because they’re the ones who, despite no personal connection to the company and despite other having other options, actively chose to work for you!** You’d be surprised what stories I heard, and they’re rarely just about the company mission. For instance, one candidate I spoke to was really excited about the chance to closely interact with investors because he wanted to start his own company one day. Another was stoked at the chance to use Go in production.\n\nSometimes you’ll be surprised by what you’ll hear because the people working at your company might be there for very different reasons than you, but these anecdotes help flesh out your narrative and make it feel a bit more personal and real.\n\nOnce you have a few choice tidbits from employees, ask yourself whether each one is somehow charming or unusual and whether it’s a reason that a lot of people would find compelling about your company. If it’s all of these things, it should likely make it in your narrative. If it’s not particularly original (e.g. short commute) it may not be worth calling out in your primary narrative, but it’s well worth repeating and telling once you actually interact with candidates.\n\nThe finished product\n--------------------\n\nSo, what should the finished product look like? At a minimum, it’ll be some concise, compelling copy that you can use in your job descriptions. Hopefully, though, it’s more than that. Hopefully it becomes a consistent refrain you and your team use during calls, interviews, maybe investor pitches… a way to highlight all the things you’re most proud of about your company and the things that make you special… without having to reinvent the wheel every time.\n\nIs brand the be-all and end-all of hiring? Not quite.\n-----------------------------------------------------\n\nIn closing, I’d like to leave you with a word or two of encouragement. Sure, as you saw in this post, brand matters. Having a great story will you get somewhere, but it won’t get you everywhere with candidates, and the truth is that the more established you are, the more candidates will come to you. But… there’s one piece of data we found in our interviewing and hiring adventures that flies in the face of brand completely proscribing your hiring destiny.\n\nWhen we looked at how often candidates wanted to work at companies after interviewing there as a function of brand strength, its impact was not statistically significant. In other words, we found that brand strength didn’t matter at all when it came to either whether the candidate wanted to move forward or how excited the candidate was to work at the company. This was a bit surprising, so I decided to dig deeper. Maybe brand strength doesn’t matter overall but matters when the interviewer or the questions they asked aren’t highly rated? In other words, can brand buttress less-than-stellar interviewers? Not so, according to our data. Brand didn’t matter even when you corrected for interviewer quality. In fact, of the top 10 best-rated companies on our platform, half have no brand to speak of, 3 are mid-sized YC companies that command respect in Bay Area circles but are definitely not universally recognizable, and only 2 have anything approaching household name status.\n\nSo, what’s the takeaway here? **Maybe the most realistic thing we can say is that while brand likely matters a lot for getting candidates in the door, once they’re in, no matter how well-branded you are, they’re yours to lose.**\n\nSo, take heart.\n\n*Portions of this post will also appear in part in an upcoming, comprehensive [Guide to Technical Recruiting and Hiring](https://www.holloway.com/g/technical-recruiting-hiring/about) published by Holloway (where you can sign up if you’d like to read, review, or contribute to it).*\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. The [2019 Stack Overflow Developer Survey](https://insights.stackoverflow.com/survey/2019#work) recently came out, and it turns out that in the US the most important thing for engineers is office/company culture… which realistically refers to the eng team culture because that’s engineers will spend most of their time. Anything you can do to call yours out (assuming, well, that it’s good) is going to be a win. [↩](#user-content-fnref-1)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/3-exercises-to-create-the-kind-of-employer-brand-that-actually-makes-engineers-want-to-work-for-you",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Lessons from a year’s worth of hiring data",
      "content": "*Note: This post is syndicated from [Aline Lerner’s personal blog](http://blog.alinelerner.com). Aline is the CEO and co-founder of interviewing.io, and results like these are what inspired her to start this company.*\n\nI ran technical recruiting at [TrialPay](http://www.trialpay.com) for a year before going off to start my own agency. Because I used to be an engineer, one part of my job was conducting first-round technical interviews, and between January 2012 and January 2013, I interviewed roughly 300 people for our back-end/full-stack engineer position.\n\nTrialPay was awesome and gave me a lot of freedom, so I was able to use my intuition about whom to interview. As a result, candidates ranged from self-taught college dropouts or associate’s degree holders to PhD holders, ACM winners, MIT/Harvard/Stanford/Caltech students, and Microsoft, Amazon, Facebook, and Google interns and employees with a lot of people in between.\n\n**While interviewing such a wide cross section of people, I realized that I had a golden opportunity to test some of the prevalent folk wisdom about hiring.** The results were pretty surprising, so I thought it would be cool to share them. Here’s what I found:\n\n* typos and grammatical errors matter more than anything else\n* having attended a top computer science school doesn’t matter\n* listing side projects on your resume isn’t as advantageous as expected\n* GPA doesn’t seem to matter\n\nAnd the least surprising thing that I was able to confirm was that:\n\n* having worked at a top company matters\n\nOf course, a data set of size 300 is a pittance, and I’m a far cry from a data scientist. Most of the statistics here is done with the help of [Statwing](http://www.statwing.com) and with Wikipedia as a crutch. With the advent of more data and more rigorous analysis, perhaps these conclusions will be proven untrue. But, you gotta start somewhere.\n\nWhy any of this matters\n-----------------------\n\nIn the status quo, most companies don’t run exhaustive analyses of hiring data, and the ones that do keep it closely guarded and only share [vague generalities](http://www.businessinsider.com/big-data-in-the-workplace-2013-5) with the public. As a result, a certain mysticism persists in hiring, and great engineers who don’t fit in “the mold” end up getting cut before another engineer has the chance to see their work.\n\nWhy has a pedigree become such a big deal in an industry that’s supposed to be a meritocracy? At the heart of the matter is scarcity of resources. When a company gets to be a certain size, hiring managers don’t have the bandwidth to look over every resume and treat every applicant like a unique and beautiful snowflake. As a result, the people doing initial resume filtering are not engineers. Engineers are expensive and have better things to do than read resumes all day. Enter recruiters or HR people. As soon as you get someone who’s never been an engineer making hiring decisions, you need to set up proxies for aptitude. Because these proxies need to be easily detectable, things like a CS degree from a top school become paramount.\n\n**Bemoaning that non-technical people are the first to filter resumes is silly because it’s not going to change. What can change, however, is how they do the filtering.** We need to start thinking analytically about these things, and I hope that publishing this data is a step in the right direction.\n\nMethod\n------\n\nTo sort facts from folk wisdom, I isolated some features that were universal among resumes and would be easy to spot by technical and non-technical people alike and then ran statistical significance tests on them. My goal was to determine which features were the strongest signals of success, which I defined as getting an offer. I ran this analysis on people whom we decided to interview rather than on every applicant; roughly out 9 out of 10 applicants were screened out before the first round. The motivation there was to gain some insight into what separates decent candidates from great ones, which is a much harder question than what separates poor candidates from great ones.\n\nCertainly there will be some sampling bias at play here, as I only looked at people who chose to apply to TrialPay specifically, but I’m hoping that TrialPay’s experience could be a stand-in for any number of startups that enjoy some renown in their specific fields but are not known globally. It also bears mentioning that this is a study into what resume attributes are significant when it comes to getting hired rather than when it comes to on-the-job performance.\n\nHere are the features I chose to focus on (in no particular order):\n\n* BS in Computer Science from a top school (as determined by U.S. News and World Report)\n* Number of grammatical errors, spelling errors, and syntactic inconsistencies\n* Frequency of buzzwords (programming languages, frameworks, OSes, software packages, etc.)\n* How easy it is to tell what someone did at each of their jobs\n* Highest degree earned\n* Resume length\n* Presence of personal projects\n* Work experience in a top company\n* Undergraduate GPA\n\nTrialPay’s hiring bar and interview process\n-------------------------------------------\n\nBefore I share the actual results, a quick word about context is in order. TrialPay’s hiring standards are quite high. We ended up interviewing roughly 1 in 10 people that applied. Of those, after several rounds of interviewing (generally a phone screen followed by a live coding round followed by onsite), we extended offers to roughly 1 in 50, for an ultimate offer rate of 1 in 500. The interview process is pretty standard, though the company shies away from asking puzzle questions that depend on some amount of luck/clicking to get the correct answer. Instead, they prefer problems that gradually build on themselves and open-ended design and architecture questions. For a bit more about what TrialPay’s interview process (used to) look like, check out [Interviewing at TrialPay 101](https://web.archive.org/web/20150201092525/http://enginerds.trialpay.com/2013/03/08/trialpay-engineering-interviews/).\n\nThe results\n-----------\n\nNow, here’s what I discovered. The bar height represents [effect size](http://en.wikipedia.org/wiki/Effect_size). Every feature with a bar was statistically significant, and if you mouse over each bar, you can also see the [p-value](http://en.wikipedia.org/wiki/P-value). These results were quite surprising, and I will try to explain and provide more info about some of the more interesting stuff I found.\n\n![Chart showing the effect size of various resume features](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F47643_effect_size_of_resume_features_313437aa69.png&w=1920&q=75 \"Effect Size of Resume Features\")\n\n---\n\nThe most significant feature by far was the presence of typos, grammatical errors, or syntactic inconsistencies.\n\nErrors I counted included everything from classic transgressions like mixing up “its” and “it’s” to typos and bad comma usage. In the figure below, I’ve created a fictional resume snippet to highlight some of the more common errors.\n\n![Sample resume fragment highlighting common errors](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fissues_5ce006ca10.png&w=1920&q=75)\n\nThis particular result was especially encouraging because it’s something that can be spotted by HR people as well as engineers. When I surveyed 30 hiring managers about which resume attributes they thought were most important, however, no one ranked number of errors highest. Presumably, hiring managers don’t think that this attribute is that important for a couple of reasons: (1) resumes that are rife with mistakes get screened out before even getting to them and (2) people almost expect engineers to be a bit careless with stuff like spelling and grammar. With respect to the first point, keep in mind that the resumes in this analysis were only of people whom we decided to interview. With respect to the 2nd point, namely that engineers shouldn’t be held to the same writing standards as people in more humanities-oriented fields, I give you my next chart. Below is a breakdown of how resumes that ultimately led to an offer stacked up against those that didn’t. (Here, I’m showing the absolute number of errors, but when I ran the numbers against number of errors adjusted for resume length, the results were virtually identical.)\n\nIf you want to play with these histograms, just click on the image, and an interactive version will pop up in a separate window.\n\n![Charts showing error frequencies among candidates with and without offers](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ferrors_histogram_f9a2291694.png&w=1920&q=75 \"Error frequencies among candidates\")\n\n**As you can see, the distributions look quite different between the group of people who got offers and those that didn’t. Moreover, about 87% of people who got offers made 2 or fewer mistakes**.\n\nIn startup situations, not only are good written communication skills extremely important (a lot of heavy lifting and decision making happens over email), but I have anecdotally found that being able to write well tends to correlate very strongly with whether a candidate is good at more analytical tasks. Not submitting a resume rife with errors is a sign that the candidate has strong attention to detail which is an invaluable skill when it comes to coding, where there are often all manners of funky edge cases and where you’re regularly being called upon to review others’ code and help them find obscure errors that they can’t seem to locate because they’ve been staring at the same 10 lines of code for the last 2 hours.\n\nIt’s also important to note that a resume isn’t something you write on the spot. Rather, it’s a document that you have every opportunity to improve. You should have at least 2 people proofread your resume before submitting it. When you do submit, you’re essentially saying, “This is everything I have done. This is what I’m proud of. This is the best I can do.” So make sure that that is actually true, and don’t look stupid by accident.\n\nTop company\n-----------\n\nNo surprises here. The only surprise is that this attribute wasn’t more significant. Though I’m generally not too excited by judging someone on pedigree, having been able to hold down a demanding job at a competitive employer shows that you can actually, you know, hold down a demanding job at a competitive employer.\n\nOf all the companies that our applicants had on their resumes, I classified the following as elite: Amazon, Apple, Evernote, Facebook, Google, LinkedIn, Microsoft, Oracle, any Y Combinator startup, Yelp, and Zynga.\n\nUndergraduate GPA\n-----------------\n\nAfter I ran the numbers to try to figure out whether GPA mattered, the outcome was a bit surprising: GPA appeared to not matter at all. Take a look at the GPA distribution for candidates who got offers versus candidates that didn’t (click to get a bigger, more interactive version).\n\n![Charts showing GPAs for candidates with and without offers](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fgpa_f345e799bf.png&w=1920&q=75 \"GPAs for candidates with/without offers\")\n\nAs a caveat, it’s worth mentioning that roughly half of our applicants didn’t list their GPAs on their resumes, so not only is the data set smaller, but there are probably some biases at play. I did some experiments with filling in the missing data and separating out new grads, and I will discuss those results in a future post.\n\nIs it easy to tell what the candidate actually did?\n---------------------------------------------------\n\nTake a look at this role description:\n\n![A fragment from a good role description](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Frole_desc_1521a16212.png&w=1920&q=75 \"Role description fragment #1\")\n\nNow take a look at this one:\n\n![A fragment from a bad role description](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fbad_example_ac35863614.png&w=1920&q=75 \"Role description fragment #2\")\n\nIn which of these is it easier to tell what the candidate did? I would argue that the first snippet is infinitely more clear than the second. In the first, you get a very clear idea of what the product is, what the candidate’s contribution was in the context of the product, and why that contribution matters. In the second, the candidate is using some standard industry lingo as a crutch — what he said could easily be applied to pretty much any software engineering position.\n\nJudging each resume along these lines certainly wasn’t an exact science, and not every example was as cut-and-dry as the one above. Moreover, while I did my best to avoid confirmation bias while deciding whether I could tell what someone did, I’m sure that the system wasn’t perfect. All this said, however, I do find this result quite encouraging. **People who are passionate about and good at what they do tend to also be pretty good at cutting to the chase.** I remember the feeling of having to write my resume when I was looking for my first coding job, and I distinctly remember how easily words flowed when I was excited about a project versus when I knew inside that whatever I had been working on was some bullshit crap. In the latter case is when words like “software development life cycle” and a bunch of acronyms reared their ugly heads… a pitiful attempt to divert the reader from lack of substance by waving a bunch of impressive sounding terms in his face.\n\nThis impression is further confirmed by a word cloud generated from candidate resumes that received an offer versus those that didn’t. For these clouds, I took words that appeared very frequently in one data set relative to how often they appeared in the other one.\n\n![Word cloud showing frequent words in successful resumes](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fcloud_1_00190b3201.png&w=1920&q=75 \"Frequent words in successful resumes\")\n\nOffer\n\n![Word cloud showing frequent words in unsuccessful resumes](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fcloud_2_15eace8bc4.png&w=1920&q=75 \"Frequent words in unsuccessful resumes\")\n\nNo offer\n\nAs you can see, “good” resumes focused much more on action words/doing stuff (“manage”, “ship”, “team”, “create”, and so on) versus “bad” resumes which, in turn, focused much more on details/technologies used/techniques.\n\nHighest degree earned\n---------------------\n\n![A quote](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fquote_3_1a25aba753.png&w=1080&q=75 \"A quote\")\n\nThough highest degree earned didn’t appear to be significant in this particular data set, there was a definite trend that caught my attention. Take a look at the graph of offers extended as a function of degree.\n\n![Chart showing offer likelihood as a function of highest degree earned by candidates](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Foffer_4_e24fb96866.png&w=1920&q=75 \"Offer Likelihood as a Function of Highest Degree Earned\")\n\nAs you can see, the higher the degree, the lower the offer rate. I’m confident that with the advent of more data (especially more people without degrees and with master’s degrees), this relationship will become more clear. **I believe that self-motivated college dropouts are some of the best candidates around because going out of your way to learn new things on your own time, in a non-deterministic way, while juggling the rest of your life is, in some ways, much more impressive than just doing homework for 4 years.** I’ve already ranted quite a bit about [how worthless I find most MS degrees to be](http://blog.alinelerner.com/how-different-is-a-b-s-in-computer-science-from-a-m-s-in-computer-science-when-it-comes-to-recruiting/), so I won’t belabor the point here.[1](#user-content-fn-1)\n\nBS in Computer Science from a top school\n----------------------------------------\n\n*But wait*, you say, *even if highest degree earned doesn’t matter, not all BS degrees are created equal! And, having a BS in Computer Science from a top school must be important because it’s in every fucking job ad I’ve ever seen!*\n\nAnd to you I say, *Tough shit, buddy.* Then I feel a bit uncomfortable using such strong language, in light of the fact that n ~= 300. **However, roughly half of the candidates** (122, to be exact) **in the data set were sporting some fancy pieces of paper. And yet, our hire rate was not too different among people who had said fancy pieces of paper and those that didn’t. In fact, in 2012, half of the offers we made at TrialPay were to people without a BS in CS from a top school.** This doesn’t mean that every dropout or student from a 3rd rate school is an unsung genius — there were plenty that I cut before interviewing because they hadn’t done anything to offset their lack of pedigree. However, I do hope that this finding gives you a bit of pause before taking the importance of a degree in CS from a top school at face value.\n\n![Product placement: Pedigree brand](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fpedigree_5_b98917bcd8.png&w=1080&q=75 \"Pedigree brand\")\n\nIn a nutshell, when you see someone who doesn’t have a pedigree but looks really smart (has no errors/typos, very clearly explains what they worked on, shows passion, and so forth), do yourself a favor and interview them.\n\nPersonal projects\n-----------------\n\nOf late, it’s become accepted that one should have some kind of side projects in addition to whatever it is you’re doing at work, and this advice becomes especially important for people who don’t have a nice pedigree on paper. Sounds reasonable, right? Here’s what ends up happening. To game the system, applicants start linking to virtually empty GitHub accounts that are full of forked repos where they, at best, fixed some silly whitespace issue. In other words, it’s like 10,000 forks when all you need is a glimmer of original thought.\n\n![A pile of forks](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffork_6_a18c62785b.png&w=1200&q=75 \"Many forks\")\n\nYay forks.\n\nOutside of that, there’s the fact that not all side projects are created equal. I can find some silly tutorial for some flashy UI thing, copy the code from it verbatim, swap in something that makes it a bit personal, and then call that a side project on my resume. Or I can create a new, actually useful JavaScript framework. Or I can spend a year bootstrapping a startup in my off hours and get it up to tens of thousands of users. Or I can arbitrarily call myself CTO of something I spaghetti-coded in a weekend with a friend.\n\nTelling the difference between these kinds of projects is somewhat time-consuming for someone with a technical background and almost impossible for someone who’s never coded before. **Therefore, while awesome side projects are a HUGE indicator of competence, if the people reading resumes can’t (either because of lack of domain-specific knowledge or because of time considerations) tell the difference between awesome and underwhelming, the signal gets lost in the noise.**\n\nConclusion\n----------\n\nWhen I started this project, it was my hope that I’d be able to debunk some myths about hiring or at least start a conversation that would make people think twice before taking folk wisdom as gospel. I also hoped that I’d be able to help non-technical HR people get better at filtering resumes so that fewer smart people would fall through the cracks. Some of my findings were quite encouraging in this regard because things like typos/grammatical errors, clarity of explanation, and whether someone worked at an elite company are all attributes that a non-technical person can parse. I was also especially encouraged by undergraduate pedigree not necessarily being a signal of success. At the end of the day, spotting top talent is extremely hard, and much more work is needed. I’m optimistic, however. As more data becomes available and more companies embrace the spirit of transparency, proxies for aptitude that don’t stand up under scrutiny will be eliminated, better criteria will take their place, and smart, driven people will have more opportunities to do awesome things with their careers than ever before.\n\nAcknowledgements\n----------------\n\nA huge thank you to:\n\n* [TrialPay](http://www.trialpay.com), for letting me play with their data and for supporting my ideas, no matter how silly they sounded.\n* [Statwing](http://www.statwing.com), for making statistical analysis civilized and for saving me from the horrors of R (or worse, Excel).\n* Everyone who suggested features, helped annotate resumes, or proofread this monstrosity.\n\nLastly, see [Hacker News](https://news.ycombinator.com/item?id=6326477 \"Hacker News\") for some good discussion.\n\nLooking for a job yourself? Work with a recruiter who’s a former engineer and can actually understand what you’re looking for. Drop me a line at [aline@alinelerner.com](mailto:aline@alinelerner.com).\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. It is worth mentioning that my statement about MS degrees potentially being a predictor of poor interview performance does not contradict this data — when factoring in other roles I interviewed for, especially more senior ones like Director of Engineering, the (negative) relationship is much stronger. [↩](#user-content-fnref-1)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/lessons-from-a-years-worth-of-hiring-data",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Building interviewing.io's collaborative & replayable whiteboard, or making systems design interviews not suck",
      "content": "*Read the original article [here](https://medium.com/@shehbaj/building-interviewing-ios-collaborative-replayable-whiteboard-5efda69db35).*\n\n![Screenshot of the new interviewing.io whiteboard](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F6_FAF_87_A6_127_D_4_A71_992_B_6_A94594520_B7_a1af73cd08.webp&w=2048&q=75 \"The new interviewing.io whiteboard\")\n\nDuring the spring of 2022, I went from being a user of interviewing.io to being one of the engineers on the team.\n\nI discovered interviewing.io in 2021 while preparing for my internship interviews, little did I know that I would end up interviewing for interviewing.io via an interview conducted on interviewing.io to receive an internship opportunity at interviewing.io upon passing the said interview. Yes.\n\nDuring my 11 weeks, I solved an important business problem, quadrupled my problem-solving skills, and collaborated with the fantastic folks who built the product made for engineers, by engineers.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/98_DAE_405_8387_4_A56_B342_0_E91_B1_EE_4_EE_4_cdb8f54a4c.webp)\n\nWinning all categories of the interviewing.io code golf competition\n\nVirtual & collaborative whiteboarding is hard\n---------------------------------------------\n\nWhile solutions such as CoderPad have allowed companies including interviewing.io to conduct virtual interviews easily, CoderPad has one significant shortcoming: Drawing Mode.\n\nAnd companies know this too.\n\nText works fine for normal coding questions but it falls flat on its face for systems design interviews which typically involve drawing shapes and lines which might be hard to depict using text in an IDE and CoderPad’s attempt at solving this problem using their Drawing Mode is lackluster. This is why companies have resorted to using Excalidraw for systems design interviews.\n\nExcalidraw is an open-source whiteboard tool and is feature-rich compared to CoderPad’s drawing mode, some of its valuable features include importing user-made shapes, dark mode (yes), support for real-time user collaboration, and adaptability to different screen sizes.\n\nYou can go to excalidraw.com right now and jump into multiplayer mode and start collaborating with your peers.\n\nSo, if the solution to the CoderPad’s drawing mode is just as simple as dropping a link to an Excalidraw board, then what’s left, Shehbaj?\n\nNeither CoderPad’s drawing mode nor Excalidraw’s self-hosted solution is replayable\n-----------------------------------------------------------------------------------\n\nOne of the ways interviewing.io provides the most value to its customers is by allowing them to watch their mock interview replays, and with the rest of the world by sharing these mock interviews.\n\nExample of a systems design interview conducted via text. Imagine how many more high-quality replays we could have gotten if the Drawing Mode worked?\n\nSo by allowing customers to use CoderPad’s Drawing Mode or Excalidraw’s self-hosted solution, not only does interviewing.io rob their customers of a core product feature, but it also prevents interviewing.io from sharing amazing replays of systems design interviews conducted with these tools with the rest of the world.\n\nSo, to deal with all this, we made our own whiteboard! 🚀\n\nSystem overview\n---------------\n\n![Diagram showing how drawing data flows from one participant to the other](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FB9_A3_ACB_7_1_AEC_4951_B89_A_C74015_B02692_000b042a79.webp&w=2048&q=75 \"Whiteboard data flow\")\n\nExcalidraw lets us embed their whiteboard in our React frontend through their npm package. With that, we set up the wiring to connect everything in the full stack together. We use WebSockets to transfer changes between the client and the server and store changes in a MongoDB document.\n\nFeatures\n--------\n\nOur whiteboard supports real-time user collaboration, mobile screens, and most importantly, the ability to watch replays.\n\n### Real-time user collaboration\n\nWasn’t that the whole point of making this thing???\n\n![Screencast animation showing two users collaborating on a drawing](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FColorful_Unpleasant_Iguanodon_size_restricted_41d2eff4f4.gif&w=1920&q=75 \"Real-time user collaboration in action\")\n\n### Mobile friendly\n\nCoding on a mobile screen is hard, but making beautiful drawings isn’t. Now, our users can join from their tablets and other touch screen devices that will allow them to prep for their systems design interviews in style.\n\n![Screencast animation showing the whiteboard on a smaller mobile screen](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FCompassionate_Distant_Arieltoucan_size_restricted_ae0011c06c.gif%3Fupdated_at%3D2022-12-14T13%3A43%3A34.018Z&w=1080&q=75 \"Whiteboard on a smaller mobile screen\")\n\n### Replay support\n\nTo get the replay to work, we store all the elements which are added, updated, or deleted based on the timestamp of the event.\n\n```\nconst whiteboardSchema = new Schema({\n  // ...\n  \n  elements: [{\n    createdAt: { type: Date, default: Date.now, required: true },\n    addedElements: { type: [ExcalidrawElement], required: true }\n  }]\n  \n  // ...\n})\n\n```\n\nPseudocode of a Mongoose Schema for storing an Excalidraw Scene in MongoDB\n\nThen on the frontend, we fetch all the scene data for replay and sync the current scene with the current timestamp in the replay.\n\n```\nconst replayElements = fetch(...); // Fetch all the sceneData from the server\nconst excalidrawCanvas = useRef(...);\n\nconst syncReplay = (timestamp: number) => {\n  \n  // ...\n  \n  const currentSceneElements: Map<string, ExcalidrawElement>() = new Map();\n  \n  replayElements.forEach(replayData => {\n    if (replayData.createdAt <= timestamp) {\n      replayData.changedElements.forEach(element => {\n        currentSceneElements[element.id] = element;\n      }\n    };\n  });\n  \n  excalidrawCanvas.updateScene({ elements : currentSceneElements });\n  \n  // ...\n\n};\n\n```\n\nPseudocode for syncing the Excalidraw scene on the frontend for replay\n\nTo try out the whiteboard in production and test out the replay functionality, I did a mock systems design interview with a Senior Software Engineer (SDE III) from Amazon. We discussed how to build a feed system similar to Facebook. Here are some clips of the replay.\n\n![Screencast animation showing the whiteboard in replay mode and the use of drawing and text tools](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FUnequaled_Teeming_Brontosaurus_size_restricted_66b722e964.gif&w=1200&q=75 \"Replay mode with drawing and text tools\")\n\nScreenshot #1 - Replay mode with drawing and text tools\n\n\n\n![Screencast animation showing the whiteboard in replay mode zoomed out](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FSilver_Wellworn_Agouti_size_restricted_8a367241f7.gif&w=1200&q=75 \"Replay mode zoomed out\")\n\nScreenshot #2 - Replay mode zoomed out\n\nNot bad for a college student doing a systems design interview for the first time 🥱🥱🥱🥱🥱🥱\n\n![Screenshot showing interviewer feedback on an interview where the whiteboard was put to use](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FC8938307_45_BD_4886_B7_FD_C85_B77_B47_E45_6a3d8c204e.webp&w=1080&q=75 \"Interview feedback after using the whiteboard\")\n\nThe whiteboard is now live, so go check it out and let us know if you have any feedback!!!",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/building-interviewing-ios-collaborative-replayable-whiteboard",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Why giving feedback (whether it’s good or bad) will help you hire",
      "content": "*Note: This post originally appeared in [TechCrunch](https://techcrunch.com/2023/02/06/to-improve-close-rates-for-technical-interviews-give-applicants-feedback-good-or-bad/) on February 6, 2023.*\n\nOne of the things that sucks most about technical interviews is that they’re a black box — candidates (usually) get told whether they made it to the next round, but they’re rarely told why they got the outcome that they did.\n\nLack of feedback isn’t just frustrating to candidates. It’s bad for business. We did a [whole study](https://interviewing.io/blog/people-cant-gauge-their-own-interview-performance-and-that-makes-them-harder-to-hire) on this. It turns out that 43% of all candidates consistently underrate their technical interview performance, and 25% of all candidates consistently think they failed when they actually passed.\n\nWhat’s particularly important is that there’s a statistically significant relationship between whether people think they did well in an interview and whether they’d want to work with you. In other words, in every interview cycle, some portion of interviewees are losing interest in joining your company just because they don’t think they did well, even when they actually did.\n\nPractically speaking, giving instant feedback to successful candidates can do wonders for increasing your close rate.\n\nGiving feedback will not only make candidates you want today more likely to join your team, but it’s also crucial to hiring the candidates you might want down the road. Technical interview outcomes are erratic, and according to our data, [only about 25% of candidates perform consistently from interview to interview](https://interviewing.io/blog/after-a-lot-more-data-technical-interview-performance-really-is-kind-of-arbitrary). This means that the same candidate you reject today might be someone you want to hire in 6 months. It’s in your interest to forge a good relationship with them now.\n\nBut won't we get sued?\n----------------------\n\nI surveyed founders, hiring managers, recruiters, and labor lawyers to understand why anyone who’s ever gone through interviewer training has been told in no uncertain terms to not give feedback.\n\nThe main reason: companies are scared of getting sued.\n\nAs it turns out, literally zero companies (at least in the US) have ever been sued by an engineer who received constructive post-interview feedback. As some of my lawyer contacts pointed out, a lot of cases get settled out of court, and that data is much harder to get, but given what we know, the odds of getting sued after giving useful feedback are extremely low.\n\nWhat about candidates getting defensive?\n----------------------------------------\n\nFor every interviewer on our platform, we track two key metrics: the candidate experience score and the interviewer calibration score.\n\nFor our purposes here, all you need to know is that the candidate experience is a measure of how likely candidates are likely to come back after interviewing with a given interviewer, and the interviewer calibration score tell us whether a given interviewer is too strict or too lenient, based on how their candidates do in subsequent, REAL interviews (which we host on our platform as well). If an interviewer continually gives good scores to candidates who fail real interviews, they’re too lenient, and vice versa.\n\nWhen you put the candidate experience score and the interviewer calibration score together, you can reason about the value of delivering honest feedback! To wit, below is a graph of the average candidate experience score as a function of interviewer accuracy, representing data from over 1,000 distinct interviewers (comprising ~100K interviews).\n\nAs you can see, the candidate experience score peaks right at the point where interviewers are neither too strict or too lenient but are, in Goldilocks terms, just right. It drops off pretty dramatically on either side after that.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/The_best_calibrated_interviewers_are_also_the_best_rated_n_1000_cfd3ee6c28.png)\n\nIn short, based on our data, we’re confident that, if you do it right, candidates won’t get defensive and that the benefits of delivering honest feedback greatly outweigh the risks.\n\nThe playbook for how to deliver honest (and sometimes harsh) feedback\n---------------------------------------------------------------------\n\nThe first and most important thing is to NOT focus on the outcome but rather to get specific right away — this will keep your candidate from getting defensive and will set them up to actually hear and internalize your feedback.\n\nIn other words, whether they did well or poorly, don’t tell them right away. Instead, dive into a constructive, detailed assessment of their performance. Reframing feedback in this way takes some practice, but your candidates won’t push you to give them the outcome. Instead, their attention will be redirected to the details, which will make the pass/fail part much more of an afterthought (and, in some cases, entirely moot). After all, why do people get defensive? It’s not because they failed! Rather, it’s because they don’t understand why and feel powerless.\n\nTo help start the conversation, here are some leading questions for you to consider:\n\n* Did they ask enough questions about constraints before getting into coding or before starting to design a system?\n* Go over specific code snippets or portions of their solution, and talk about what they could have done better.\n* Could their solution have been more efficient?\n* Did they discuss and reason about tradeoffs?\n* Did they make mistakes when discussing time or space complexity? What were those specific mistakes?\n* Did they make any mistakes when trying to use their programming language of choice idiomatically (e.g., iterating in Python or JavaScript)?\n* For systems design questions, did they jump to suggesting a specific database, load balancer, tool, etc. without reasoning through why that tool is the right choice for the job?\n\nNote that to answer these questions well and to give specific, constructive feedback, it’s critical to take notes, ideally timestamped ones, during the interview. Then you can always go back to your notes and say, “Hey, you jumped into coding just 5 minutes into the interview. Typically, you’ll want to spend a few minutes asking questions.”\n\nAnd, of course, specific feedback really does mean being specific. One of the kindest, albeit most labor-intensive, things you can do is walk through their code with them, point out places where they went astray, and note what they could have done better.\n\nOne other useful pattern for giving feedback is to share objective benchmarks for a given interview question, both with respect to times and number of hints given. If you’re a great interviewer, you probably do something called layering of complexity, where after a candidate successfully solves a question, you change up the constraints in real time. You may even do this 3-4 times during the interview if a candidate is blowing through your questions quickly.\n\nThis means that you know exactly how many constraint changes you’ll be able to go through with a low-performing candidate vs. a mediocre one vs. one who’s truly exceptional.\n\nYour candidates don’t know this, though! In fact, candidates commonly overestimate their performance in interviews because they don’t realize how many layers of complexity a question has. In this scenario, a candidate will finish, say, the first layer successfully right before time is called. They walk away thinking they did well, when in reality, the interviewer is benchmarking them against people who can complete 3 layers in that amount of time.\n\nHow do you put all of this info to practical use? Let your candidates know what the benchmarks are for a top-performing candidate at the end of the interview. For instance, you could say something like, “In the 45 minutes we spent working this problem, the strongest performers usually complete the brute-force solution in about 20 minutes, optimize it until it runs in linear time (which takes another 10 minutes), and then, in the last 15 minutes, successfully complete an enhancement where, instead of an array, your input is a stream of integers.”\n\nAlso, let them know exactly how many hints are par for the course. Just like with how much time should elapse for different parts of the interview, candidates have no idea what “normal” is when it comes to the number and detail level of hints. For instance, if a candidate needed a hint about which data structure to use, followed by a hint about what time complexity is associated with that data structure, followed by a hint about a common off-by-one error that comes up, you may want to tell them that the strongest performers usually need a hint about one of those things, but not all three.\n\nThe key to communicating these benchmarks constructively is, of course, to be as specific as possible with runtimes or space constraints or whatever success metric you’re using.\n\nOne final technique some of our interviewers employ is to ask their candidate to perform a detailed self-assessment at the end of the interview before giving feedback. This is an advanced maneuver, and if you’re completely new to giving synchronous feedback, I wouldn’t do it in your first few interviews. However, once you get comfortable, this approach can be a great way to zero in on the areas that the candidate needs the most help on immediately.\n\nIf you do end up going the self-assessment route, it’s good to ask your candidate some leading questions. For instance, for algorithmic interviews, you can ask:\n\n* How well do you think you did at solving the problem and arriving at an optimized solution?\n* How clean was your code?\n* Where are some places that you struggled?\n\nWhile the candidate is responding, take notes (perhaps even in your shared editor!), and then go through their points together, and speak to each point in detail. For instance, if a candidate rates themselves well on code quality but poorly on their ability to solve the problem, you can agree or disagree and give them benchmarks (as discussed above) for both.\n\nHere’s the summary playbook:\n\n* Take detailed notes during the interview, ideally with timestamps, that you can refer to later.\n* DON’T lead with whether they passed or failed. Instead, get specific and constructive right away. This will divert the candidate’s attention away from the outcome and put them in the right headspace to receive feedback.\n* As much as possible, give objective benchmarks for performance. For instance, tell candidates that the strongest performers are usually able to finish part 1 within 20 minutes, part 2 within 10 minutes, and part 3 within 15 minutes, with at most 1 hint.\n* Once you get comfortable with giving feedback, you can try asking candidates to do a self-assessment and then use it as a rubric that you can go down, point by point.",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/why-giving-feedback-good-or-bad-will-help-you-hire",
      "author": "",
      "user_id": ""
    },
    {
      "title": "We looked at how a thousand college students performed in technical interviews to see if where they went to school mattered. It didn't.",
      "content": "interviewing.io is a platform where engineers practice technical interviewing anonymously. If things go well, they can unlock the ability to participate in real, still anonymous, interviews with top companies like Twitch, Lyft and more. Earlier this year, we launched an offering specifically for university students, with the intent of helping level the playing field right at the start of people’s careers. The sad truth is that with the state of college recruiting today, if you don’t attend one of very few top schools, your chances of interacting with companies on campus are slim. It’s not fair, and it sucks, but university recruiting is still dominated by career fairs. Companies pragmatically choose to visit the same few schools every year, and despite the career fair being one of the most antiquated, biased forms of recruiting that there is, the format persists, likely due to the fact that there doesn’t seem to be a better way to quickly connect with students at scale. **So, despite the increasingly loud conversation about diversity, campus recruiting marches on, and companies keep doing the same thing expecting different results.**\n\nIn a previous blog post, we explained [why companies should stop courting students from the same five schools](https://interviewing.io/blog/if-you-care-about-diversity-you-should-stop-hiring-from-the-same-five-schools). Regardless of your opinion on how important that idea is (for altruistic reasons, perhaps), you may have been left skeptical about the value and practicality of broadening the college recruiting effort, and you probably concede that it’s rational to visit top schools, given limited resources — while society is often willing to agree that there are perfectly qualified students coming out of non-top colleges, they maintain that they’re relatively rare. We’re here to show you, with some nifty data from our university platform, that this not true.\n\nTo be fair, this isn’t the first time we’ve looked at whether where you went to school matters. In a previous post, we found that [taking Udacity and Coursera programming classes mattered way more than where you went to school](https://interviewing.io/blog/lessons-from-3000-technical-interviews). And way back when, one of our founders figured out that where you went to school didn’t matter at all but that [the number of typos and grammatical errors on your resume did](https://blog.alinelerner.com/lessons-from-a-years-worth-of-hiring-data/). So, what’s different this time? The big, exciting thing is that these prior analyses were focused mostly on engineers who had been working for at least a few years already, making it possible to argue that a few years of work experience smoothes out any performance disparity that comes from having attended (or not attended a top school). In fact, the good people at Google found that while GPA didn’t really matter after a few years of work, [it did matter for college students](https://www.businessinsider.com/how-google-hires-people-2013-6). So, we wanted to face this question head-on and look specifically at college juniors and seniors while they’re still in school. **Even more pragmatically, we wanted to see if companies limiting their hiring efforts to just top schools means they’re going to get a higher caliber of candidate.**\n\nBefore delving into the numbers, here’s a quick rundown of how our university platform works and the data we collect.\n\nThe setup\n---------\n\nFor students who want to practice on interviewing.io, the first step is a brief (~15-minute) coding assessment on [Qualified](https://www.qualified.io/) to test basic programming competency. Students who pass this assessment, i.e. those who are ready to code while another human being breathes down their neck, get to start booking practice interviews.\n\nWhen an interviewer and an interviewee match on our platform, they meet in a collaborative coding environment with voice, text chat, and a whiteboard and jump right into a technical question. Check out our [recordings](https://interviewing.io/mocks) page to see this process in action.\n\nInterview questions on the platform tend to fall into the category of what you’d encounter at a phone screen for a back-end software engineering role, and interviewers typically come from top companies like Google, Facebook, Dropbox, Airbnb, and more.\n\nAfter every interview, interviewers rate interviewees on a few different dimensions, including technical ability. Technical ability gets rated on a scale of 1 to 4, where 1 is “poor” and 4 is “amazing!”. On our platform, a score of 3 or above has generally meant that the person was good enough to move forward. You can see what our feedback form looks like below:\n\n![Screenshot of the Interviewing.io interview feedback form highlighting the question: How were their technical skills?](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F9fdaa_new_interviewer_feedback_circled_3de7112d80.webp%3Fupdated_at%3D2022-12-08T01%3A21%3A24.672Z&w=1200&q=75 \"How were their technical skills?\")\n\n**On our platform, we’re fortunate to have thousands of students from all over the U.S., spanning over 200 universities. We thought this presented a unique opportunity to look at the relationship between school tier and interview performance for both juniors (interns) and seniors (new grads).** To study this relationship, we first split schools into the following four tiers, based on rankings from U.S. News & World Report:\n\n* **“Elite”** schools (e.g. MIT, Stanford, Carnegie Mellon, UC-Berkeley)\n* **Top 15 schools** (not including top tier, e.g. University of Wisconsin, Cornell, Columbia)\n* **Top 50 schools** (not including top 15, e.g. Ohio State University, NYU, Arizona State University)\n* **The rest** (e.g. Michigan State, Vanderbilt University, Northeastern University, UC-Santa Barbara)\n\nThen, we ran some statistical significance testing on interview scores vs. school tier to see if school tier mattered, for both interns (college juniors) and new grads (college seniors), comprising a set of roughly 1000 students.\n\nDoes school have anything to do with interview performance?\n-----------------------------------------------------------\n\nIn the graphs below, you can see technical score distributions for interviews with students in each of the four school tiers (see legend). As you recall from above, each interview is scored on a scale of 1 to 4, where 1 is the worst and 4 is the best.\n\nFirst, the college juniors…\n\nAnd then, the seniors…\n\n**What’s pretty startling is that the shape of these distributions, for both juniors and seniors, is remarkably similar. Indeed, statistical significance testing revealed no difference between students of any tier when it came to interview performance.[1](#user-content-fn-1) What this means is that top-tier students are achieving the same results as those in no-name schools.** So the question becomes: if the students are comparable in skill, why are companies spending egregious amounts of money attracting only a subset of them?\n\nOkay, so what are companies missing?\n------------------------------------\n\nBesides missing out on great, cheaper-to-acquire future employees, companies are missing out on an opportunity to save time and money. Right now a ridiculous amount of money is being spent on university recruiting. We’ve previously cited the [$18k price tag just for entry to the MIT career fair](https://interviewing.io/blog/if-you-care-about-diversity-you-should-stop-hiring-from-the-same-five-schools). In a [study done by Lauren Rivera through the Harvard Business Review](https://hbr.org/2015/10/firms-are-wasting-millions-recruiting-on-only-a-few-campuses), she reveals that one firm budgeted nearly $1m just for social recruiting events on a single campus.\n\nThe higher price tag of these events also means it makes even less sense for smaller companies or startups to try and compete with high-profile, high-profit tech giants. Most of the top schools that are being heavily pursued already have enough recruiters vying for their students. Unwittingly, this pursuit seems to run contrary to most companies desires for high diversity and long-term sustainable growth.\n\nEven when companies do believe talent is evenly distributed across school tiers, there are still reasons for why companies might recruit at top schools. There are other factors that help elevate certain schools in a recruiter’s mind. There are long-standing company-school relationships (for example, the number of alumni who work at the company currently). There are signaling effects too — companies get Silicon Valley bonus points by saying their eng team is comprised of a bunch of ex-Stanford, ex-MIT, ex- etc. etc. students.\n\nA quick word about selection bias\n---------------------------------\n\nSince this post appeared on Hacker News, there’s been some loud, legitimate discussion about how the pool of students on interviewing.io may not be representative of the population at large because we have a self-selected pool of students who decided to practice interviewing. Certainly, all the blog posts we publish are subject to this (very valid) line of criticism, and for this post in particular. As such, selection bias in our user pool might mean that 1) we’re getting only the worst students from top schools (because, presumably, the best ones don’t need the practice), or 2) we’re getting only the best/most motivated students for non-top schools, or both. Any subset of these is entirely possible, but we have a few reasons why we believe what we’ve published here might hold true regardless.\n\nFirst off, in our experience, regardless of their background or pedigree, everyone is scared of technical interviewing. Case in point… before we started working on interviewing.io, we didn’t really have a product yet. So before investing a lot of time and heartache into this questionable undertaking, we wanted to test the waters to see if interview practice was something engineers really wanted, and more so, who these engineers that wanted practice were. So, we put up a pretty mediocre landing page on Hacker News… and got something like 7,000 signups the first day. Of these 7,000 signups, roughly 25% were senior (4+ years of experience) engineers from companies like Google and Facebook (this isn’t to say that they’re necessarily the best engineers out there… but just that the engineers the market seems to value the most still need our service).\n\nAnother data point comes from one of our founders. Every year, Aline does a guest lecture on job search preparedness for a technical communication course at MIT. This course is one way to fulfill the computer science major communication requirement, so enrollment tends to span the gamut of computer science students. Before every lecture, she sends out a survey asking students what their biggest pain points are in preparing for their job search. Every year, trepidation about technical interviewing is either at the top of the list of 2nd from the top.\n\nAnd though this doesn’t directly address the issue of whether we’re only getting the best of the worst or the worst of the best (I hope the above has convinced you there’s more to it than that), here’s the distribution of school tiers among our users, which I expect mirrors the kinds of distributions companies see in their student applicant pool:\n\nSo what can companies do?\n-------------------------\n\nAs such, companies may never stop recruiting at top-tier schools entirely, but they ought to at least include schools outside of that very small circle in the search for future employees. The end result of the data is the same: for good engineers, school means a lot less than we think. The time and money that companies put in to compete for candidates within the same select few schools would be better spent creating opportunities that include everyone, as well as developing tools to vet students more fairly and efficiently.\n\nAs you saw above, we used a 15-minute coding assessment to cull our inbound student flow, and just a short challenge leveled the playing field between students from all walks of life. At the very least, we’d recommend employers do the same thing in their process. But, of course, we’d be remiss if we didn’t suggest one other thing.\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. Of course, this hinges on everyone completing a quick 15-minute coding challenge first, to ensure they’re ready for synchronous technical interviews. We’re excited about this because companies can replicate this step in their process as well! [↩](#user-content-fnref-1)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Does communication matter in technical interviewing? We looked at 100K interviews to find out.",
      "content": "*Hey, Aline (founder of interviewing.io) here. This is the 5th post in our Guest Author series.*\n\n*One of the things I’m most excited about with the Guest Author series is the diversity of opinions it’s bringing to our blog. Technical interviewing and hiring is fraught with controversy, and not everything these posts contain will be in line with my personal opinions or the official opinions of interviewing.io. But that’s what’s great about it. After over a decade in this business, I still don’t think there’s a right way to conduct interviews, and I think hiring is always going to be a bit of a mess because it’s a fundamentally human process. Even if we don’t always agree, I do promise that the content we put forth will be curated, high quality, and written by smart people who are passionate about this space.*\n\n*If you have strong opinions about interviewing or hiring that you’ve been itching to write about, we’d love to hear from you. Please email me at [aline@interviewing.io](mailto:aline@interviewing.io).*\n\n![Author avatar](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FDima_Korolev_61fbf691c1.png&w=384&q=75 \"Dima Korolev\")\n\nDima Korolev\n\nDima Korolev finds joy in helping established companies on a large-scale architecture level, previously at PokerStars, now at Miro. His career includes roles at Google (from 2007–2011), along with other large companies and several successful startups. While in school, Dima once earned Coder of the Month and was consistently in the top 100 globally on Topcoder. He particularly enjoys mentoring engineers and finds it emotionally rewarding to help others develop stronger skills and expand their horizons. His love of helping others inspired Dima to become one of the first interviewers on interviewing.io, completing more than 100 interviews to date. You can reach him on [*LinkedIn*](http://www.linkedin.com/in/dimakorolev)*,* [*GitHub*](https://github.com/dkorolev)*,* [*Twitter*](https://twitter.com/UniqueDima)*,* and his [*website*](http://dima.ai/).\n\nThe interviewing.io platform has hosted and collected feedback from over 100K technical interviews, split between mock interviews and real ones. It’s generally accepted that to pass a technical interview, you have to not only come up with a solution to the problem (or at least make good headway), but you also have to do a good job of articulating your thoughts, explaining to your interviewer what you’re doing as you’re doing it, and coherently discussing tradeoffs and concepts like time and space complexity.\n\nBut how important is communication in technical interviews, really? We looked at the data, and it turns out that *talk is cheap*. Read on to find out how and why.\n\nThe setup\n---------\n\nOn interviewing.io, engineers can practice technical interviewing anonymously. When an interviewer and an interviewee match on interviewing.io, they join a collaborative coding environment with voice, text chat, and a whiteboard, and jump right into a technical interview. After each interview, both parties leave feedback, and once they’ve both submitted, each one can see what the other person said and how they were rated.\n\nHere’s the feedback form that interviewers fill out:\n\n![Sample interview feedback form](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F5a595_screenshot_2017_11_29_09_13_30_1ca42f5eec.png&w=1920&q=75 \"Interview feedback form\")\n\nAs you can see above, there’s one yes/no question: *Would you advance this person to the next round?* There are also three other questions, each graded on a scale from 1 to 4 (4 is best):\n\n* Technical ability, which we’ll call ***Code***\n* Problem-solving ability, which we’ll call ***Solve***\n* Communication skill, which we’ll call ***Communicate***\n\nDo the numeric scores predict whether a candidate gets advanced to the next round? You bet! Here are the top buckets, ranked by how often the interviewers say “yes”:\n\n![Success-Rate.png](https://strapi-iio.s3.us-west-2.amazonaws.com/Success_Rate_f6934e5141.png)\n\nRight away, three pieces of anecdotal evidence suggest that talk is cheap:\n\n1. The only “2” that made it to the list is communication score (see 4-4-2 above).\n2. The least valuable “4” coupled with two “3”-s is also the communication score (see 3-3-4 above), with a major drop in hire rate.\n3. And, ironically, 4-3-3, a scenario where the candidate got 3s in both communication and problem solving, has a [negligibly] higher success rate than 4-3-4, a scenario where the candidate got a 4 in communication and a 3 in problem solving!\n\nIn fact, the 4-4-2 bucket is the third best one can get, losing only to 4-4-4 and 4-4-3. Interviewers are happy to advance 4-4-2 candidates to the onsite round a whopping 96% of the time. Notably, a candidate is 3X more likely to get rejected with 3-3-4 than with 4-4-2.\n\nSo it seems that talk is cheap. But to learn exactly *how* cheap, we have to go deeper, and this is where data science helps.\n\nInto the data\n-------------\n\nIt’s data science time! Let’s do a thought experiment. Imagine a candidate walks into an interview, impaired by the moral equivalent of 1 point, on either their ability to Code, to Solve, or to Communicate.[1](#user-content-fn-1)\n\nTo minimize damage, from which ability should our imaginary candidate have that 1 point deducted?\n\nTo answer this question, let’s take a look at how much worse this rejection rate would be with a 1-point deduction.\n\n![Rejection-with-minus.png](https://strapi-iio.s3.us-west-2.amazonaws.com/Rejection_with_minus_804e06854a.png)\n\nWhoa!\n\nFor a good third of all interviews, **dropping 1 point from the *Code* or *Solve* scores would lead to the rejection rate skyrocketing 6X**! Clearly, that last point in *Code* or *Solve* is extremely valuable.\n\nAt the same time, in all categories, the negative impact of dropping 1 point from the *Communicate* category is by far the smallest.\n\nThe opposite direction is interesting as well. Say our imaginary candidate is fond of Heroes of Might and Magic and was up all night playing. *This is not recommended either, but imaginary experiments really help with science.* Just before the interview, this candidate meditates, their inner interviewee hero visits a Temple, and their Morale improves by 1. They now have a +1 score point. How should our hero best use it? The rejection rate would now decrease with the score increasing. Hence it’s green in the table.\n\n![Rejection-with-plus.png](https://strapi-iio.s3.us-west-2.amazonaws.com/Rejection_with_plus_30cde165c3.png)\n\nThe TL;DR is that +1 to *Code* and +1 to *Solve* have comparable effects, and their effect is always significantly higher than the effect of +1 to *Communicate*.\n\nThe “top 25%” row is not very representative, as the 4s can not improve, and plenty of scores there already are the 4s.\n\nFor other rows, boosting *Communicate* by 1 turns out to barely affect the rejection rate. Boosting *Code* and/or *Solve*, on the other hand, can help flip up to 4 of 10 negative outcomes!\n\nPragmatically speaking\n----------------------\n\nUnfortunately, in real life we don’t have magic potions that +1 your interview performance. But we do have considerable control over selecting which score to improve. Of course, 4-4-4 is the holy grail, but during an interview it is often up to you to decide which score to focus on.\n\nHere is a more realistic example. What if you could trade one score for another?\n\nAgain, the table shows the difference in rejection rates, the numbers in red are bad, and the numbers in green are good. The top row is removed because, as mentioned earlier, it is not representative.\n\n![Rejection-with-swap.png](https://strapi-iio.s3.us-west-2.amazonaws.com/Rejection_with_swap_398980dc53.png)\n\nNothing could be more clear. When in doubt, **trade *Communicate* for *Solve* or *Code*.** Period.\n\nConclusions, and why communication still matters\n------------------------------------------------\n\nThe bottom line: If your objective is to get a “yes,” it is far less damaging to score 1 rating point lower on *Communicate*, and it is much more beneficial to score 1 rating point higher on *Code* and/or on *Solve*. Talk is indeed cheap, and indeed, coding and problem solving is what you need to show in a coding interview.\n\nKeep in mind that, obviously, these insights are valid for coding interviews. In behavioral interviews, the technical skill signal is the least valuable, while problem solving is king. And in systems design interviews, extra points on communication skills, while still the least valuable of three, are not worth exchanging for additional points in tech and problem-solving skills.\n\nIt is worth noting that practice coding interviews, while highly useful, are not 100% representative of real-life interviews. A real-life interview, even if explicitly focused on coding, is a mixture of everything.\n\n**The more senior the position, the more vital systems design and behavioral skills become. This statement is harder to prove with data, but every interviewer I spoke with agrees—at least within the standard L3 to L6 range, from a junior to staff.**\n\nThus, we can conclude that:\n\n* For L3 and L4, you are better off ignoring the communication score entirely, *as long as you are consistently scoring 2+ on it*.\n* For L5+ interviews, where communication skills are far more important, standard coding practice sessions are not the best way to improve on them. Systems design practice, as well as the behavioral interviews, are there to help.\n\nGood luck, and have fun on interviewing.io!\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. Of course, it’s best to postpone the interview in such a situation—or even if you’re just feeling unprepared. Seriously, official advice endorsed by the interviewing.io team here: Do not take your interviews when you’re not in the best shape. Those companies can wait, and in our experience, recruiters are very supportive of you taking the time you need to prepare and put your best foot forward. [↩](#user-content-fnref-1)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/does-communication-matter-in-technical-interviewing-we-looked-at-100k-interviews-to-find-out",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Refuting Bloomberg's analysis: ChatGPT isn't racist. But it is bad at recruiting.",
      "content": "*Note: We’ve reached out to Bloomberg, asking them to share their data and clarify their findings, ahead of publishing this piece. If it turns out that they used a different method for statistical significance testing or if we missed something, we’ll gladly retract the part of this post that’s about their results.*\n\nRecently, Bloomberg published an article called “[OpenAI’s GPT is a recruiter’s dream tool. Tests show there’s racial bias](https://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/).” In this piece, the Bloomberg team ran a clever test where they had ChatGPT review nearly identical resumes with just the names changed to include typically Black, White, Asian, and Hispanic names. Their analysis uncovered racial bias.\n\nBloomberg had published their numbers on GitHub, so we were able to check their work. When we re-ran the numbers, we saw that they hadn’t done statistical significance testing and that there was, in fact, no evidence of racial bias in Bloomberg's data set. However, when we ran our own tests, we discovered that ChatGPT is indeed bad at judging resumes. It’s not bad because it’s racist. It’s bad because it’s prone to a different kind of bias, the same kind of bias as human recruiters — over-indexing on candidates’ pedigrees: whether they’ve worked at a top company and/or whether they attended a top school. Pedigree can be somewhat predictive (especially where people worked), but ChatGPT is significantly overestimating its importance and doing a disservice to candidates from non-traditional backgrounds as a result.\n\nThe Bloomberg study\n-------------------\n\nHere’s what the team at Bloomberg did (taken verbatim from their piece):\n\n> We used demographically-distinct names as proxies for race and gender, a common practice used to audit algorithms… Altogether we produced 800 demographically-distinct names: 100 names each for males and females who are either Black, White, Hispanic or Asian…\n>\n> To test for name-based discrimination, Bloomberg prompted OpenAI’s GPT-3.5 and GPT-4 to rank resumes for a real job description for four different roles from Fortune 500 companies: HR specialist, software engineer, retail manager and financial analyst.\n>\n> For each role, we generated eight nearly-identical resumes using GPT-4. The resumes were edited to have the same educational background, years of experience, and last job title. We removed years of education, as well as any objectives or personal statements.\n>\n> We then randomly assigned a distinct name from each of the eight demographic groups [Black, White, Hispanic, Asian, and men and women for each] to each of the resumes.\n>\n> Next, we shuffled the order of resumes, to account for order effects, and asked GPT to rank the candidates.\n\nThe authors reported that ChatGPT shows racial bias across all groups, except for retail managers ranked by GPT-4.\n\nMore specifically:\n\n> [We] found that resumes labeled with names distinct to Black Americans were the least likely to be ranked as the top candidates for financial analyst and software engineer roles. Those with names distinct to Black women were top-ranked for a software engineering role only 11% of the time by GPT — 36% less frequently than the best-performing group.\n>\n> The analysis also found that GPT’s gender and racial preferences differed depending on the particular job that a candidate was evaluated for. GPT does not consistently disfavor any one group, but will pick winners and losers depending on the context. For example, GPT seldom ranked names associated with men as the top candidate for HR and retail positions, two professions historically dominated by women. GPT was nearly twice as likely to rank names distinct to Hispanic women as the top candidate for an HR role compared to each set of resumes with names distinct to men. Bloomberg also found clear preferences when running tests with the less-widely used GPT-4 — OpenAI’s newer model that the company has promoted as less biased.\n\nThe team also, commendably, [published their results on GitHub](https://github.com/BloombergGraphics/2024-openai-gpt-hiring-racial-discrimination), so we tried our hand at reproducing them. What we found was starkly different from what Bloomberg reported.\n\nBefore we get into what we found, here’s what we did.\n\nIn their results, Bloomberg published the rates at which both GPT-3.5 and GPT-4 chose each demographic as the top candidate. The Bloomberg analysts ran a lot of trials for each job: ChatGPT was asked 1,000 times to rank 8 resumes for the HR specialist job, for example. And if ChatGPT had gender or racial bias, each group should in general be the top pick 125 times (or 12.5% of the time, given that there were 1000 data points).\n\nWhere are we going with this? Stats nerds might have noticed a glaring omission from the Bloomberg piece: statistical significance testing and p-values. Why is that important? Even with 1,000 trials, a perfectly unbiased resume sorter would not give you exactly equal proportions, picking each group precisely 125 times. Instead, purely random variation could mean that one group is picked 112 times and another is picked 128 times, without there actually being any bias. Therefore, you need to run some tests to see if the results you got were by chance or because there truly is a pattern of some kind. Once you run the test, the p-value tells you the probability that a certain set of selection rates are consistent with chance, and in this case, consistent with a random (and, therefore, unbiased) sorting of resumes.\n\nWe calculated the p-values for each group[1](#user-content-fn-1). What we found was starkly different from what Bloomberg reported.\n\nWhere the Bloomberg study went wrong\n------------------------------------\n\nGiven the nature of our business, we looked at software engineers first. Here are the results of Bloomberg having run software engineering resumes through GPT-4 for all 8 groups (the column titled “obsfreq”) as well as our calculated p-value.\n\n![](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fbloomberg_chi2_gpt4_software_78ea5edfd6.png&w=1200&q=75)\n\nA\\_M = Asian man, A\\_W = Asian woman, etc. 12.5% would be the expected rate at which a candidate from each group would be the top choice (“expperc”). Out of 1000 candidates, that’s 125 each (“expfreq”). Finally, “obsfreq” is the observed frequency of the top candidate being from each group, taken from Bloomberg’s results.\n\nIt’s convention that you want your p-value to be less than 0.05 to declare something statistically significant – in this case, that would mean less than 5% chance that the results were due to randomness. This p-value of 0.2442 is way higher than that. As it happened, we couldn’t reproduce statistical significance for software engineers when using GPT-3.5 either. **Using Bloomberg’s numbers, ChatGPT does NOT appear to have a racial bias when it comes to judging software engineers’ resumes.**[2](#user-content-fn-2) **The results appear to be more noise than signal.**\n\nWe then re-ran the numbers for the eight race/gender combinations, using the same method as above. In the table below, you can see the results. TRUE means that there is a racial bias. FALSE obviously means that there isn't. We also shared our calculated p-values. **The TL;DR is that GPT-3.5 DOES show racial bias for both HR specialists and financial analysts but NOT for software engineers or retail managers. Most importantly, GPT-4 does not show racial bias for ANY of the race/gender combinations.**[3](#user-content-fn-3)\n\n| Occupation | GPT 3.5 (Statistically significant? ‖ p-value) | GPT 4 (Statistically significant? ‖ p-value) |\n| --- | --- | --- |\n| Financial analyst | TRUE ‖ 0.0000 | FALSE ‖ 0.2034 |\n| Software engineer | FALSE ‖ 0.4736 | FALSE ‖ 0.1658 |\n| HR specialist | TRUE ‖ 0.0000 | FALSE (but it’s close) ‖ 0.0617 |\n| Retail manager | FALSE ‖ 0.2229 | FALSE ‖ 0.6654 |\n\nThat’s great, right? Well, not so fast. Before we deemed ChatGPT as competent at judging resumes, we wanted to run a test of our own, specifically for software engineers (because, again, that’s our area of expertise). The results of this test were not encouraging.\n\nHow we tested ChatGPT\n---------------------\n\ninterviewing.io is an anonymous mock interview platform, where our users get paired with senior/staff/principal-level FAANG engineers for interview practice. We also connect top performers with top-tier companies, regardless of how they look on paper. In our lifetime, we’ve hosted >100k technical interviews, split between the aforementioned mock interviews and real ones. In other words, we have a bunch of useful, indicative historical performance data about software engineers.[4](#user-content-fn-4) So we decided to use that data as a sanity check.\n\n### The setup\n\nWe asked ChatGPT (GPT4, specifically *gpt-4-0125-preview*) to grade several thousand LinkedIn profiles belonging to people who have practiced on interviewing.io before. For each profile, we asked ChatGPT to give the person a coding score between 1 and 10, where someone with a 10 would be a top 10% coder. In order to improve the quality of the response, we asked it to first give its reasoning followed by the coding score.\n\nWe want to be very clear here that we did not share any performance data with ChatGPT or share with ChatGPT any information about our users — we just asked it to make value judgments about publicly available LinkedIn profiles. Then we compared those value judgments with our data on our end.\n\nHow ChatGPT performed\n---------------------\n\nThere is a correlation between what ChatGPT says and how the coder performed on a real technical screen. The tool performs better than a random guess… but not by much. **To put these results in perspective**, overall, 47% of coders pass the screen. The ChatGPT score can split them into two groups: one that has a 45% chance and one with a 50% chance. So it gives you a bit more information on whether someone will succeed, but not much.\n\nBelow are two more granular ways of looking at ChatGPT’s performance. The first is a modified calibration plot, and the second is a ROC curve.\n\n### Calibration plot\n\nIn this plot, we take each predicted probability from ChatGPT (e.g., 0.4112) and assign it to one of 10 equally-spaced deciles. Decile 1 is the 10% of profiles with the lowest probability. Decile 10 is the 10% of people with the highest probability.\n\nThen, for each decile, we plot the actual probability of those candidates performing well in interviews (i.e., what portion of them actually passed interviews on interviewing.io). As you can see, the plot is something of a mess — for all deciles that ChatGPT came up with, those candidates actually passed about half the time. The ideal plot (“An excellent model”) would have a much steeper slope, with way fewer passing in the bottom decile than the top decile.\n\n![How ChatGPT predictions compare to actual performance on interviewing.io](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fchatgpt_decile_calibration_overall_added_lines_674eab2b9f.png&w=1200&q=75)\n\nWe asked GPT-4 to judge several thousand LinkedIn profiles belonging to people who have practiced on interviewing.io before. Then we split up its predictions into deciles — 10% buckets and compared to how those users actually performed. A great model would have bad performance in the first decile and then improve sharply and steadily.\n\n### ROC curve\n\nAnother way to judge ChatGPT’s performance at this task is to look at a ROC curve. This curve graphs the true positive rate of a model against the false positive rate. It’s a standard way of judging how accurate an ML model is because it lets the viewer see how it performs at different acceptable false positive rates — for cancer diagnostics, you may be OK with a very high false positive rate, for instance. For eng recruiting, you likely will not be!\n\nRelated to ROC curves is the AUC, or the area under the curve. A perfect model would have a 100% true positive rate for every possible false positive rate, and so the area under the curve would be 1. A model that’s basically the same as guessing would have a true positive rate that equals the false positive rate (AUC = 0.5). With that in mind, here’s the ROC curve and the AUC for ChatGPT judging resumes — with an overall AUC of about 0.55, it’s only barely better than random guesses.\n\n![When asked to judge resumes, ChatGPT doesn't perform better than chance](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fchatgpt_oneshot_roc_cca5b11e77.png&w=1200&q=75)\n\nWe asked GPT-4 to judge several thousand LinkedIn profiles belonging to people who have practiced on interviewing.io before. It performed barely better than guessing.\n\nSo, no matter how you measure it, while ChatGPT does not appear to have racial bias when judging engineers’ profiles, it’s not particularly good at the task either.\n\nChatGPT is biased against non-traditional candidates\n----------------------------------------------------\n\nWhy did ChatGPT perform poorly on this task? Perhaps it’s because there may not be that much signal in a resume in the first place. But there’s another possible explanation as well.\n\nYears ago, I ran an experiment where I [anonymized a bunch of resumes and had recruiters try to guess which candidates were the good ones](https://interviewing.io/blog/resumes-suck-heres-the-data). They did terribly at this task, about as well as random guessing. Not surprisingly, they tended to over-index on resumes that had top companies or prestigious schools on them. In my data set of candidates, I happened to have a lot of non-traditional, good candidates — people who were strong engineers but didn’t attend a highly ranked school or work at a top company. That threw recruiters for a loop.\n\nIt looks like the same thing happened to ChatGPT, at least in part. We went back and looked at how ChatGPT treated candidates with top-tier schools on their LinkedIn profiles vs. those without. It turns out that ChatGPT consistently overestimates the passing rate of engineers with top schools and top companies on their resumes. We also saw that ChatGPT consistently underestimates the performance of candidates without those elite “credentials” on their resumes. Both of these differences are statistically significant. In the graph below, you can see exactly how much ChatGPT overestimates and underestimates in each case.\n\n![GPT-4's bias against non-traditional engineers](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fchatgpt_school_and_company_bias_818bd5dfc7.png&w=1200&q=75)\n\nTo ChatGPT’s credit, we did not find the same bias when it came to top companies, which is funny because, in our experience, having worked at a top company carries some predictive signal, whereas where someone went to school does not carry much.\n\nChatGPT likely isn't racist, but its biases still make it bad at recruiting\n---------------------------------------------------------------------------\n\nIn recruiting, we often talk about unconscious bias. Though it’s no longer en vogue, companies have historically spent tens of thousands of dollars on unconscious bias trainings designed to stop recruiters from making decisions based on candidates’ gender and race. At the same time, recruiters were trained to exhibit a different, conscious bias: to actively select candidates from elite schools and top companies.\n\nThe same conscious bias against candidates who didn’t attend a top-tier school appears to be codified in ChatGPT.\n\nThat decision is rational — in the absence of a better signal, you have to use proxies, and those proxies seem as good as any. Unfortunately, as you can see from these results (and from other studies we’ve done in the past; see the footnote for the full list[5](#user-content-fn-5)), it’s not particularly accurate… and it’s definitely not accurate enough to codify into our AI tools.\n\nIn a market where [recruiter jobs are tenuous](https://interviewing.io/blog/when-is-hiring-coming-back-predictions-for-2024), where the [dwindling number of recruiters is dealing with more applicants than before](https://www.ashbyhq.com/talent-trends-report/reports/2023-recruiter-productivity-trends-report) and are pressured, more than ever, to make the aforementioned rapid decisions, and where companies are embracing AI as a tempting and productive cost-cutting measure[6](#user-content-fn-6), we’re in rather dangerous territory.\n\nA few months ago, we published a long piece called, “[Why AI can’t do hiring](https://interviewing.io/blog/why-ai-cant-do-hiring)”. The main two points of the piece were that 1) it’s hard to extract signal from a resume because there’s not much there in the first place, and 2) even if you could, you’d need proprietary performance data to train an AI — without that data, you’re doing glorified keyword matching.\n\nUnfortunately, most, if not all, of the AI tools and systems that claim to help recruiters make better decisions do not have this kind of data and are 1) either built on top of GPT (or one of its analogs) without fine-tuning or 2) are glorified keyword matchers masquerading as AI, or both.\n\nThough human recruiters aren’t particularly good at judging resumes, and though we, as a society, don’t yet have a great solution to the problem of effective candidate filtering, it’s clear that off-the-shelf AI solutions are not the magic pill we’re looking for — they’re flawed in the same ways as humans. They just do the wrong thing faster and at scale.\n\nFootnotes:\n\nFootnotes\n---------\n\n1. We used a Chi-squared goodness of fit test, a type of statistical significance test that you use for discrete data, like yes or no votes on resumes. [↩](#user-content-fnref-1)\n2. Another way to see this is to simulate the same process with a perfectly unbiased resume sorter, i.e., a bot that picks from the 8 resumes at random. If you run 1,000 imaginary versions of the Bloomberg experiment, it’s pretty common for a particular group to have a round where they’re the top pick just 11% of the time. This distribution is in the histogram below. This is another way of saying what the p-values are saying: the disparities are consistent with random chance. [INSERT HISTOGRAM] [↩](#user-content-fnref-2)\n3. A caveat is that these tests might show evidence of bias if the sample size were increased to, say, 10,000 rather than 1,000. That is, with a larger sample size, the p-value might show that ChatGPT is indeed more biased than random chance. The thing is, we just don’t know from their analysis, and it certainly rules out extreme bias. In fact, the [most recent large-scale resume audit study](https://academic.oup.com/qje/article/137/4/1963/6605934) found that resumes with distinctively Black names were 2.1 percentage points less likely to get callbacks from human recruiters. Based on Bloomberg’s data, ChatGPT’s was less biased against resumes from Black candidates than human recruiters — according to our calculations, in the Bloomberg data set, there was 1.5 percentage point drop. [↩](#user-content-fnref-3)\n4. Mock interview performance, especially in aggregate, is very predictive of real interview performance. We have data from both mock and real interviews, spanning ~6 years, and the candidates who’ve done well in mock interviews on our platform have consistently been 3X more likely to pass real interviews. [↩](#user-content-fnref-4)\n5. Here is a list of our past studies that show how top schools are not particularly predictive and top companies are only somewhat predictive:\n\n   * [Lessons from a year’s worth of hiring data](https://interviewing.io/blog/lessons-from-a-years-worth-of-hiring-data)\n   * [We looked at how a thousand college students performed in technical interviews to see if where they went to school mattered. It didn't.](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt)\n   * [Lessons from 3,000 technical interviews… or how what you do after graduation matters way more than where you went to school](https://interviewing.io/blog/lessons-from-3000-technical-interviews)[↩](#user-content-fnref-5)\n6. A significant portion of employers, ranging from 35-55% depending on the source (Zippia, Forbes, USC), are currently using AI to screen job candidates. The adoption of AI in hiring appears to be particularly high among large enterprises and recruiting firms. Given that large enterprises see a disproportionately high volume of candidates, the % of candidates who are screened by AI is likely much higher than the 35-55% number. [↩](#user-content-fnref-6)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/refuting-bloombergs-analysis-chatgpt-isnt-racist",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Does posting Open To Work on LinkedIn help or hurt? A tale of two labor markets.",
      "content": "*Note: This post originally appeared in [TechCrunch on April 3, 2023](https://techcrunch.com/2023/04/03/should-you-post-that-youre-opentowork-a-tale-of-two-labor-markets/).*\n\nLinkedIn members face a dilemma when choosing whether to mark themselves as #OpenToWork. On the one hand, it sends a clear and useful signal to recruiters and hiring managers that you’re open to new job opportunities. On the other hand, the badge can lump you with a crowd of people who are more likely to be unemployed or unhappy with their jobs, which could be seen as a negative sign – you know, the whole Groucho Marx thing:\\* I don’t want to belong to any club that would accept me as one of its members.\\*\n\nIf you’re job hunting, should you list yourself as Open To Work? Does doing so carry a negative signal? And with the recent deluge of layoffs at tech companies, has the meaning of #OpenToWork changed? We decided to find out!\n\nThe correlation between LinkedIn #OpenToWork and interviewing.io interviews\n---------------------------------------------------------------------------\n\ninterviewing.io is an interview practice platform and recruiting marketplace for engineers. Engineers use us for mock interviews. Companies use us to hire top performers. In our lifetime, we’ve hosted over 100k technical interviews, split between mocks and real ones. To test whether listing yourself as #OpenToWork on the profile section of your LinkedIn is a good thing to do, we aggregated pass/fail rates in the interviews our users did and cross-referenced them with whether users marked themselves as #OpenToWork on their LinkedIn profiles. We also made sure to check their LinkedIns twice: once in early 2021, when there were practically [no tech layoffs](https://layoffs.fyi/), and again in early 2023, in the wake of the worst round of tech layoffs since 2001.\n\nWhy did we check twice? Economic theory suggests that the people laid off or searching for a job in boom times are different from those laid off in a recession. If you are let go or cannot find a job when companies are flush, it might be that you were laid off for performance reasons or can never get past a screening call. On the other hand, someone who suffered from an across-the-board layoff in a time of economic crisis could be perfectly-capable: just a casualty of macroeconomic forces.\n\nWorried about layoffs, or just want to be prepared? Sign up for anonymous mock interviews with engineers from top companies.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nWe found that being Open To Work is far more common now. Among the over ten thousand people that we had LinkedIn data for, **only 1.4% had the badge in 2021 compared to 4.2% in the first quarter of 2023**.\n\nIs posting #OpenToWork on LinkedIn good or bad?\n-----------------------------------------------\n\nWe found that being Open To Work was a *negative* signal for those who had it up in 2021, a boom time for tech hiring. The chart below shows the percentage of people who passed their interviews—our summary measure of candidate performance. On average, about 51 percent of candidates pass their interviews. In contrast, those with #OpenToWork badges in 2021 were fully 7 percentage points below that, at 44%.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/percent_hirable_and_open_to_work_f863aa6a5b.png)\n\nThe finding confirms that there can be negative selection among job seekers. Being upfront about looking for a job—at least in 2021—was indeed a bad sign.\n\n**But strikingly, this result flips in present times. Engineers who are currently Open To Work are actually positively selected relative to everyone else. Using the LinkedIn Open To Work feature is a good sign in these rough times: 56% of engineers tagged as OpenToWork passed their interviews, 5 percentage points more than average. The difference in these effects is highly statistically significant.** And we get the same results when we leave out anyone who has worked at a FAANG company—in case the recent [engineering layoffs](https://interviewing.io/blog/2022-layoffs-engineers-vs-other-departments) simply flooded the market with engineers from those top-tier companies.\n\nBefore using the Open To Work feature, consider the job market\n--------------------------------------------------------------\n\nAnecdotally, we’ve heard that layoffs in 2022 and 2023, especially at large companies, were not performance-based. In many cases, cuts were based on an employee's start date—those who hired last were the first on the chopping block. In some cases, entire teams were cut – if a team’s product wasn’t close enough to revenue, they ended up on the chopping block, independently of how strong the engineers on that team actually were.\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/tweet_google_layoffs_81e3f27bb4.png)\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/blind_layoffs_aren_t_about_performance_d0b2a7db8e.png)\n\nWe were excited to see that this anecdotal perception held up in the data.\n\nThat said, people who are OpenToWork have not necessarily been laid off. In uncertain times, it’s more common for workers to be openly searching for jobs. The macroeconomic forces could normalize a more aggressive on-the job search, which might also pull more talented—but still employed—people into checking OpenToWork.\n\nRegardless of layoff composition, in boom times, openly searching for a job can be a negative signal. But during downturns, the rules change, and openly looking for work becomes much more “normal”.\n\nThe signal to hiring managers is clear: because of the rocky economic times, good workers are out there looking for jobs, and displaying #OpenToWork on your profile photo is no longer a negative signal. While hiring has slowed across many tech companies, those with the budget will find a better pool to choose from, and those who were laid off in late 2022/early 2023 might be among the most impressive candidates.\n\nIf you use LinkedIn and are looking for a new job, we wouldn’t go as far as recommending that you list yourself as Open To Work because of the potential residual bias associated with that tag. We would, however, encourage you to feel no shame about having been impacted by layoffs.\n\nThe data is clear, after all—the pool of engineering job seekers has never been more talented.",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/whos-open-to-work-a-tale-of-two-labor-markets",
      "author": "",
      "user_id": ""
    },
    {
      "title": "I love meritocracy, but all the recent anti-DEI rhetoric is bad",
      "content": "I’m the founder of interviewing.io, an anonymous technical recruiting marketplace. In some ways, I’m the meritocracy hipster who was [writing about how eng hiring should be meritocratic](https://blog.alinelerner.com/silicon-valley-hiring-is-not-a-meritocracy/) and about [how quotas are bad](https://blog.alinelerner.com/diversity-quotas-suck-heres-why/), way before saying either was cool. At interviewing.io, my team and I have been trying to make hiring meritocratic for the last decade. Briefly, we do anonymous mock interviews. If people perform well in those interviews, they get introduced directly to decision-makers at top-tier companies, regardless of how they look on paper. 40% of the thousands of people we’ve helped were excellent engineers who did not look good on paper and would NOT have gotten in the door through traditional, “meritocratic” channels. Many of those engineers were rejected based on their resumes by the very same companies where they were later hired through us.\n\nRecently, there’s been a lot of pro-meritocracy, anti-DEI rhetoric. The most salient example is [Alexandr Wang’s (CEO of Scale AI) tweet about how their hiring process has to be meritocratic](https://twitter.com/alexandr_wang/status/1801331034916851995) (including a catchy new acronym, “MEI”).\n\n> Today we’ve formalized an important hiring policy at Scale. We hire for MEI: merit, excellence, and intelligence.  \n>   \n> This is the email I’ve shared with our [@scale\\_AI](https://twitter.com/scale_AI?ref_src=twsrc%5Etfw) team.  \n>   \n> ———————————————————  \n>   \n> MERITOCRACY AT SCALE  \n>   \n> In the wake of our fundraise, I’ve been getting a lot of questions…\n>\n> — Alexandr Wang (@alexandr\\_wang) [June 13, 2024](https://twitter.com/alexandr_wang/status/1801331034916851995?ref_src=twsrc%5Etfw)\n\nThe post got a resounding “Great!” from Elon Musk a half hour later, followed by a wall of accolades from the Twitterverse. Since then, a [“meritocracy board”](https://www.meritocracy.com/) has sprung up as well.\n\nIf you read Wang’s post carefully, you’ll see that he provides no specific examples of how Scale AI makes hiring meritocratic and doesn’t share any details about their current hiring process. I don’t know anyone from the Scale AI team personally, but after doing eng hiring in some form or another for over 15 years, I have questions. Does Scale AI’s hiring process differ substantially from other companies’? Or are they doing the same thing as everyone else: recruiters look at resumes, pick people who have top brands on their resume, and interview them?\n\n**If their process is indeed like everyone else’s, no matter what they say, they’re no more meritocratic than the companies who tout DEI hiring practices… and are just virtue signaling on Twitter.**\n\nI’ll be the first to admit that DEI is ideologically flawed because of its emphasis on race and gender-based outcomes and its insistence on equality of those outcomes. In the last decade, we’ve seen some pretty bad DEI practices, the most egregious being a company looking up candidate photos on LinkedIn and rejecting qualified white, male candidates. (I talk more about the worst-offending hiring practices I’ve seen over the last decade in the section called *The dark side of diversity… and two stories of diversity initiatives gone wrong* below. If you just want the juicy bits, read that part.)\n\n**However, yelling “Meritocracy!” as if it’s a fait accompli is just as harmful as the worst parts of DEI.** In the last decade, we’ve seen countless companies claim to be meritocratic but refuse to talk to candidates unless they had a CS degree from one of a select few schools. There is nothing meritocratic about that. After seeing the pendulum swing back and forth a bunch in this space, **I’d even go so far to say that, ironically, the DEI movement has done more for meritocracy than the loud pro-meritocracy movement is doing right now**.\n\n**I’m delighted that “meritocracy” is no longer a dirty word. But, just saying it isn’t enough. We have to change our hiring practices.** We need to stop using meritocracy as a shield to preserve the status quo. **If we could instead put into practice the best part of DEI – openness to hiring candidates from non-traditional backgrounds while eliminating the toxic hyperfocus on race and gender and the insistence on equality of outcomes, then we could create a real meritocracy, which is what most reasonable people actually want.**[1](#user-content-fn-1)\n\n*A quick disclaimer before we go further. To the right, DEI has come to mean mediocrity, and as such, it’s pitted, apples to apples, against meritocracy. That is not the intent here. When I talk about DEI, I’m not talking about the political side of it or how it’s often co-opted by the left as a gateway to Marxism. Similarly, meritocracy has been co-opted by the right to justify racism, eugenics, and god knows what other horrid things. Both extremes are bad. I’m trying to shed all political associations from either word and to just talk about them purely as hiring ideologies.*\n\nDEI’s outcomes problem\n----------------------\n\nOn its face, increasing diversity sounds great. Some groups are underrepresented in tech, likely because of inequality of opportunity. Talent is distributed uniformly, opportunity is not. Let’s fix it!\n\nTwitter threads like this one (from an engineering leader at Google) are hard to argue with. You should read the whole thing — it’s about a (white) lumberjack’s son who ended up as one of the founding employees at SpaceX.\n\n> Everyone loves SpaceX, and thinks of Elon as the genius founder that invents new types of rockets that are cheaper, faster, more efficient.  \n>   \n> It's fun to think of it as SpaceX versus NASA, or Silicon Valley vs Aerospace.  \n>   \n> But let's talk about D&I, and logs. Logs as in timber. 🌲\n>\n> — Mekka 💉x7 @mekkaokereke@hachyderm.io (@mekkaokerekebye) [January 5, 2019](https://twitter.com/mekkaokerekebye/status/1081619342377156608?ref_src=twsrc%5Etfw)\n\nAnd indeed, ostensibly, DEI is hard to argue against because it speaks to our innate desire for fairness and equal access to opportunity. Many DEI leaders honestly believe this. However, despite the good intentions, in practice, DEI tends to laser focus on race and gender *outcomes*, and that is hard to argue *for*.\n\nOver the years, I’ve seen claims that diverse teams perform better, as well as claims that one must have a diverse workforce if one has a diverse customer base. Though it’s often stated as fact, the former is inconclusive — there are studies with clear results for AND clear results against.[2](#user-content-fn-2) To the best of my knowledge, the latter point is unsubstantiated as well — isn’t the hallmark of a good designer that they be able to design for customers who are different than they are?[3](#user-content-fn-3)\n\nThe arguments for diversity are inconclusive, and as such, the ultimate measure of success for diversity isn’t about the performance of an organization or about customer satisfaction. Those are packaged up as obvious side benefits. The way we measure success for diversity is tautological: success is measured by the diversity of our workforce.\n\nWhat does that mean? **In practice, recruiting orgs usually define success by looking at some demographic and its representation in the general population.** So, in the case of women in tech, women make up half the U.S. population, so 50% of engineers in an organization should be women. Similarly, 12% of the U.S. population is Black, so for hiring to be equitable, 12% of the engineers in an organization should be Black. Likewise, 19% of the U.S. population is Hispanic, so 19% of engineers should be Hispanic, and so on.\n\nWhat’s the problem with this approach? It does not account for inputs. The most basic input is: How many female engineers are there in the US? And how many Black or Hispanic engineers are there in the US?\n\nThe answer: not enough. Only 20% of CS graduates in the US are women. And there are also not enough engineers of color to get to race parity either. Only 6% of CS graduates in the US are Black, and only 7% are Hispanic.[4](#user-content-fn-4)\n\nThose numbers get even more grim when you pare them down to how companies usually hire: from top-ranked schools. We’ll talk more about this pedigree-based approach to hiring when we discuss the pitfalls of meritocracy. For now, suffice it to say that a few years ago, [we ran the numbers to show that getting to gender parity in software engineering is mathematically impossible](https://interviewing.io/blog/we-ran-the-numbers-and-there-really-is-a-pipeline-problem-in-eng-hiring) given companies’ focus on pedigree; though it was unfashionable to admit it, we called out that there really is a pipeline problem.\n\nAnd then there’s this issue: What portion of those candidates are even applying to your company in the first place? And what portion of those applicants are actually qualified to do the work? The ONLY way to really take race and gender bias off the table is to do blind as much of the hiring process as possible and then to accept that you may not get the numbers you want but that your outcomes will *actually* be fair.\n\nThe dark side of diversity (and two stories of diversity initiatives gone wrong)\n--------------------------------------------------------------------------------\n\nIn addition to mock interviews, interviewing.io also helps companies source engineering candidates. We know how people perform in mock interviews, and that lets us reliably predict who’ll do well in real interviews. We identify the top performers from practice and introduce them to companies. We’ve been doing it for a while, and our top performers have consistently outperformed candidates from other sources by about 3X.\n\n**I promised in the beginning of this post that I’d spill some juicy tidbits. Here goes.**\n\nYears ago, we pitched Facebook’s university recruiting team on using us to hire for their intern class. The pitch was that [we had thousands of college students from all over the U.S.](https://interviewing.io/blog/if-you-care-about-diversity-you-should-stop-hiring-from-the-same-five-schools) who had done a bunch of mock interviews, and that we knew who the top performers were. Many of our students did NOT come from the handful of top-tier schools that Meta typically recruited from. If they were to recruit through us, they’d have to do way fewer interviews (we had already taken care of technical vetting), and they’d get a much more diverse slice of the population.\n\nOur only process request was that they conduct interviews with students anonymously, on our platform, so they wouldn’t be biased against top-performing students who didn’t go to top schools.\n\n**We didn’t get the gig. The main bit of pushback from Facebook was that anonymity violated guidelines set by the OFCCP.** The [OFCCP](https://en.wikipedia.org/wiki/Office_of_Federal_Contract_Compliance_Programs) (Office of Federal Contract Compliance Programs) is part of the U.S. Department of Labor and is “responsible for ensuring that employers doing business with the Federal government comply with the laws and regulations requiring nondiscrimination.” One of the many things that the OFCCP requires you to track, if you do business with the federal government, is the race and gender of your applicants. We couldn’t agree to this. While the requirement makes sense on the surface — as they say, you can’t fix what you can’t measure — in this case, it was a Kafkaesque roadblock to achieving the very thing that the OFCCP is fighting for: reducing discrimination.[5](#user-content-fn-5)\n\nMore broadly, you can’t take an outcomes-based approach unless your inputs are homogenous and the playing field is level. The biggest advocates of DEI will argue, correctly, that the playing field is not level. Given that it’s not level, focusing exclusively on outcomes creates all manners of perverse incentives — the dark side of diversity is the logical conclusion of an outcomes-based approach: incentivizing the selection of candidates based on race and gender and ultimately discriminating against non-URM candidates.\n\nWe’ve worked with companies of all sizes, from seed stage startups to FAANG, and at one point or another, we’ve worked with most FAANGs and FAANG-adjacent companies. We’ve seen it all. In 2022, at the height of diversity fever, one well-known FAANG-adjacent customer approached us with a specific request. Let’s call them AcmeCorp (name obviously changed; they’re a household name, but I don’t want to rake them over the coals publicly because they were a great partner to us until this thing happened).\n\n**AcmeCorp’s recruiting team wanted us to do some pre-filtering on the candidates we introduced to them.**\n\nWe already do some pre-filtering: location, years of experience, visa status, and obviously historical performance in mock interviews. Only the top few percent of our candidates get to talk to employers.\n\nBut on our platform, everything is candidate driven. We don’t have a searchable candidate database, and we don’t share candidate data with companies. Rather, we list the companies who hire through us, and our top-performing users can connect with them.\n\nOver our lifetime, plenty of companies have approached us asking if they could get access to *just* top-performing women and people of color on our platform. It makes sense. Recruiters are given marching orders to find more “diverse” candidates, and this is the result. And it’s a convenient way to pass on liability. Now, instead of their sourcers having to filter out candidates who aren’t “diverse”, we have to do it.\n\nOf course, we’ve always denied these requests. We’re not a “diversity” platform, and we can’t imagine a world where we’d block what jobs and employers our users could see based on their race and gender (information we don’t collect systematically in the first place).[6](#user-content-fn-6)\n\nEven though, on their face, these requests weren’t really OK, we got so many of them that, over time, we got desensitized and would joke internally about how yet another company wanted a [binder full of women](https://en.wikipedia.org/wiki/Binders_full_of_women).\n\nHowever, AcmeCorp’s request was more egregious than the rest because it gave us visibility into how many companies were behaving internally when faced with diversity goals. It was common knowledge that many companies were doing diversity-specific sourcing, so we weren’t shocked when we were asked to help with that. What wasn’t common knowledge is that companies were blatantly rejecting qualified applicants who didn’t meet their diversity criteria.\n\nAcmeCorp had a fairly complex candidate filtering process in place, and they wanted us to run that same process on any of our top performers who expressed interest in working there.\n\nHere’s how their process worked. Note that AcmeCorp, like many companies, pays differently depending on where you live.\n\n* List a remote job that’s hiring all over the United States.\n* When candidates apply from high cost of living areas (e.g., the SF Bay Area, NYC), only consider women and people of color. Reject the rest.\n* When candidates apply from lower cost of living areas (e.g., small towns in the Midwest), consider everyone.\n\n**In other words, a white man from San Francisco would have no shot at getting an interview at this company — he would be auto-rejected and left to wonder what was wrong with his resume.**\n\nWhy did this company take this approach? They were willing to pay top dollar for women and people of color but not for other types of engineers, and they hid behind geography to do it. Because of the geographical element, it’s not as blatant as outright rejecting people based on race and gender, but for all intents and purposes, it’s the same.\n\nOutside of this practice being questionably legal at best, it’s also unethical. You can argue that companies should be able to do outreach to any demographic groups that they want. It’s much harder to argue that it’s ok to reject applicants based on their race and gender.\n\nWe terminated the relationship.[7](#user-content-fn-7)\n\nUnfortunately, when you tie the success of your recruiting program to gender and race outcomes, these are the behaviors that inevitably arise. For all its flaws, though, the DEI movement, coupled with increasing demand for engineers, propelled companies to make deep changes to their hiring processes. For every DEI horror story, there is an equal and opposing story about a Head of Talent or investor engineering leader who persuaded their eng hiring managers to stop looking just at students from MIT and Stanford, to change their interview processes, to blind resumes, and to do a bunch of other useful things that benefitted every non-traditional candidate.\n\nBut, back to what’s happening today. You don’t just get to say “meritocracy” and be done with it. In practice, meritocratic hiring doesn’t really exist, and what companies call meritocracy is anything but.\n\nThe false promise of meritocracy\n--------------------------------\n\nFor most sane people, the concept of meritocracy is hard to argue against. Shouldn’t the most qualified person get the job?\n\n**Unfortunately, because the definition of “qualified” is murky, meritocracy often becomes a justification for over-indexing on pedigree: where people went to school or where they worked previously.** “We just hire the best” often means “we hire people from FAANG, MIT, and Stanford.” Unfortunately, those are proxies for ability, not actual measures of it. Our research has consistently shown that where people go to school isn’t very predictive of what they can do. Where they’ve worked is *somewhat* predictive, but it’s not the most important thing.[8](#user-content-fn-8)\n\nDespite that, those are the main criteria that companies use when they decide whom to interview, and because that’s the first step in a hiring funnel, it’s the one that gets applied to the most candidates. Any attempts at making the process meritocratic after the resume review (e.g., training interviewers, doing anonymous interviews) are bound to be less impactful because they affect 10X-100X fewer candidates.\n\nFortunately, for all their flaws, at least technical interviews do focus on ability — once you get in the door, it’s not about how you look on paper but about how you perform. As a result, all other things being equal, how you decide which candidates to let into your process is the litmus test for whether your process is truly meritocratic or not.\n\nUnfortunately, the pedigree-based approach isn’t particularly meritocratic. In our 9 years, we’ve diligently tracked the backgrounds of our candidates, and as I mentioned in the intro to this post, about 40% of our top performers don’t look good on paper (but do as well as or outperform their pedigreed counterparts in interviews).\n\nOne of our users got rejected from a top-tier social network company three times… THREE TIMES… based on his resume before he got hired there through us, after doing very well in an anonymous interview. I’ve shared a few diversity horror stories, but the sad reality is that (faux) meritocracy horror stories like this one happen every day. I wish I had a real meritocracy horror story to share, but as far as I know, eng hiring has never been truly meritocratic. If you know otherwise, please do share.\n\nOur data also shows that pedigree has very little bearing on interview performance. Where people went to school has no bearing on their interview performance, and though where people have worked does carry some signal, it’s not nearly as important as other traits — in past research, we’ve found that not having typos/grammatical errors on your resume is a much stronger signal than whether they’ve worked at a top company, as is whether they’ve done a lot of autodidactic work.[8](#user-content-fn-8)\n\nMoreover, in two separate studies completed a decade apart, where recruiters had to judge resumes and try to pick out the strong candidates, we consistently saw that recruiters are only as accurate as a coin flip and largely disagree with each other about what a good candidate looks like.[9](#user-content-fn-9)\n\n**That’s why posts like the one from Scale AI get my hackles up. You don’t get to say that you’re meritocratic if you’re just scanning resumes for top brands. That’s not meritocracy. It’s co-opting a hot-button word for clout.**\n\nAnd it’s not just Scale AI. This is how tech companies define being meritocratic and hiring the best. It’s just that not all of them are so self-congratulatory about it.\n\nSo how do you ensure that your hiring is *actually* meritocratic?\n\nHow to actually “do” meritocracy, if you mean it\n------------------------------------------------\n\nIn a [recent study](https://interviewing.io/blog/are-recruiters-better-than-a-coin-flip-at-judging-resumes), we looked at how recruiters read resumes and how good they are at finding talent. As you saw above, we learned that recruiters are barely better than a coin flip. Another thing we looked at in the same study was what made them pick certain resumes over others.\n\n**The two traits that were the most predictive of whether a recruiter would pick you? First, whether you had top brands on your resume, and second, whether you were Black or Hispanic. This is how recruiters work today. If you don’t intervene and make changes, today’s competing approaches will *both* be implemented by your team simultaneously, resulting in a farcical chimera of fake meritocracy and outcomes-based diversity goals.**\n\nSo what can you actually do, if you, in good faith, want to run a meritocratic hiring process? (By the way, if you believe that talent is distributed uniformly, by definition, this approach will entail being open to talent from traditionally underrepresented backgrounds.)\n\n**First, you have to move away from identity politics and expand the definition of “underrepresented.”** You have to believe, in your heart of hearts, that great talent can come from anywhere and must stop focusing arbitrarily on one marginalized group at the expense of another. Basically, you have to be open to any candidate who’s good, regardless of how they look on paper, without prioritizing race and gender. This certainly includes race and gender, but it also includes socioeconomic status, educational background (or lack thereof), and any number of other traits that have nothing to do with someone’s ability to do the job. Hell, why not just stop worrying about candidate backgrounds and have a process that welcomes all and surfaces the best? Following this path will logically require moving away from race and gender outcomes-based goals.\n\n**Then, you have to accept and internalize that your current method of deciding who gets to interview, which is very likely focused on brands (where people have worked or where they’ve gone to school), is not only NOT meritocratic but also ineffective.** We talked above about how pedigree is very limited in its ability to predict performance.\n\n**If you accept both of these premises — expanding the definition of “underrepresented” and moving away from focusing on brands — the hard work begins.** Companies have used resumes (and brands by extension) since time immemorial because they’re easy, and as you saw in our data above, they do carry *some* signal. But even though they carry a little signal, recruiters are not very good at extracting it.\n\n**Here’s what you should do to pragmatically and realistically revamp your hiring process to be more meritocratic.** I challenge Scale AI and all the leaders on the “meritocracy board” to publicly commit to at least two of these — or to name the specific, actionable approaches they plan to take.\n\n* Change how you read resumes.[10](#user-content-fn-10)\n* Give candidates the option of submitting some writing about a past project they’re proud of.\n* Give candidates the option of doing a take-home or an asynchronous assessment. Put enough work into those assignments such that you’ll trust their outcome enough to not have to look at a resume.\n* [EXTRA CREDIT] Invest what you can in closing the gaps.\n\n### Change how you read resumes\n\nFirst, SLOW DOWN. In [the study I mentioned above](https://interviewing.io/blog/are-recruiters-better-than-a-coin-flip-at-judging-resumes), we saw that recruiters take a median of 31 seconds to judge a resume, but spending just 15 extra seconds reading a resume could improve your accuracy by 34%.\n\nOur second piece of advice is this. More than 20 years ago, Freada Kapor Klein from Kapor Capital coined the term “distance traveled,” referring to what someone accomplished, in the context of where they started. For instance, Kapor Klein recommends that, in their admissions processes, universities should consider not just the number of AP tests a candidate has passed but the number of AP tests taken divided by the total number offered at their high school. For example, if an applicant took 5 AP tests and their school offered 27, that paints a very different picture from another applicant who also took 5 AP tests when that’s the total number offered at their school. Kapor Capital uses distance traveled as one of their metrics for determining which entrepreneurs to fund. One can easily apply this concept to hiring as well.\n\nThe data shows that slowing down is important, and as part of slowing down, when you read a resume, try to evaluate candidates’ achievements, not in a vacuum, but in the context of where they came from. Think about the denominator. But don’t think for a moment that we recommend that you lower the bar — absolutely not. On interviewing.io, we regularly see nontraditional candidates objectively outperforming their FAANG counterparts.\n\n### Give candidates the option of submitting some writing about a past project they’re proud of\n\nMy friends at KeepSafe and I [ran an experiment about a decade ago where we tried replacing resumes with a writing sample about a past project](https://blog.alinelerner.com/what-happens-when-you-stop-relying-on-resumes/). It was a huge success.\n\nEven today, when we hire at interviewing.io, we use this approach. We mostly hire off of our own platform (we just list our own open positions alongside others). However, not all of our users have done enough mock interviews to have a rating, and for those users, we have a different flow where we ask them to write about a past project. Boy, are the results telling.\n\nHere’s what our application form looks like. Steal it if you want.\n\n![typeform application paragraph about project](https://strapi-iio.s3.us-west-2.amazonaws.com/typeform_application_paragraph_about_project_8777b4bcfc.png)\n  \n\n### Give candidates the option of doing a take-home or an asynchronous assessment\n\nTake-homes and asynchronous assessments are not well-loved by candidates, primarily because of value asymmetry. They ask a lot of the candidate but nothing of the company, and it’s not uncommon for a candidate to have to do hours of work and then never hear anything back.\n\nTo be clear, this is NOT the setup we’re advocating. Here’s what we’d advise instead:\n\nGive candidates the option of doing a take-home/assessment that takes no more than 1 hour, *instead of submitting their resume*. When we say option, we mean that the candidate can decide whether they want to do the take-home or not. If they choose not to, then you’ll read their resume, hopefully using our suggestions above. If they choose to complete the take-home, then you forgo their resume and make your go/no-go decision based entirely on the results of the take-home.\n\nIf you choose this route, it’s critical to [come up with an assessment whose results you trust](https://interviewing.io/blog/why-engineers-dont-like-take-homes-and-how-companies-can-fix-them). Many companies use a take-home in addition to getting the resume and will still not move forward with candidates who look good on paper. That’s not meritocratic. Take the time you need to come up with a question that’s hard to cheat on and that gets you the signal you need. Yes, coming up with a good assessment takes work. But no one said that making your hiring process meritocratic was easy.\n\n### [EXTRA CREDIT] Invest what you can in closing the gaps\n\nThis advice probably applies more to big companies than smaller ones, because bigger ones have more resources to effect change. Regardless, if you believe in meritocracy, then you understand that a true meritocracy is not possible without a level playing field for your candidates. One of the best things about the DEI movement is that it’s made us aware how unlevel the playing field really is. Whether you subscribe to DEI or not, this is probably not a controversial statement, and if you want to see true, meritocratic hiring, you have some obligation to help promote equality of *opportunity*.\n\nWhere to begin?\n\nAlthough I expect that it’s not level in many places, and there are plenty of opportunities to effect change, starting with elementary education[11](#user-content-fn-11), I'll talk about the inequality I’ve observed firsthand repeatedly over the last decade: [the technical interview practice gap](https://interviewing.io/blog/technical-interview-practice-gap). How much you practice is the biggest predictive factor of interview performance — not seniority, not gender, and not a CS degree. And so is socialization. After all, if you’re around people going through the same thing, like at a top-tier CS school, rather than beating yourself up after a disappointing interview, you’ll start to internalize that technical interviewing is flawed and that the outcomes are sometimes unpredictable. Fortunately, there are interventions one can make to close these gaps, and the simplest is to provide practice and community for people who don’t have access to them. Reach out to us about this, find a non-profit that helps people practice, donate to your favorite university if they have a good practice program, or any number of other things.\n\nUltimately, which gap you choose to help close and how you choose to do it is up to you. But if your company has the means, it’s your responsibility to invest in gap-closing measures. You don’t have to donate money. You can offer mock interviews to your candidates before their real interviews. You can start an apprenticeship program. You can encourage your engineers to do some tutoring. However you approach it, though, you can’t talk about meritocracy with a straight face and not do *something* to level the playing field.\n\nFootnotes:\n\nFootnotes\n---------\n\n1. In fairness, the Scale AI post positioned them as symbiotic. I believe that as well. [↩](#user-content-fnref-1)\n2. There are many sources arguing for and against diversity leading to better-performing teams. Here are some examples:\n     \n   For: <https://www.mckinsey.com/featured-insights/diversity-and-inclusion/diversity-wins-how-inclusion-matters>  \n   Against: <https://medium.com/the-liberators/in-depth-the-double-edged-sword-of-diversity-in-teams-765ff72a55da> (except for “age diversity”) and <https://corpgov.law.harvard.edu/2021/05/25/diversity-and-performance-in-entrepreneurial-teams/> [↩](#user-content-fnref-2)\n3. One of the most insulting examples of the “we need a diverse workforce to serve our diverse customer base” argument occurred when I was pitching Amazon on using interviewing.io to hire. This was years ago, and back then, out of curiosity, I’d always ask the organizations we were pitching why they valued diversity. I don’t think I ever got a good answer, but this one was especially bad. One of the recruiters we met with went on a long diatribe about how Amazon sells lots of shoes and you need women on the eng team because women understand shoes better than men. [↩](#user-content-fnref-3)\n4. Getting more women and people of color to study computer science is a worthy cause. Hell, getting anyone who’s historically been marginalized to study computer science is worthwhile. It’s great for our economy, and it’s currently one of the best levers for upward social mobility available. But, while we hope more companies do these things, it is not reasonable to expect that companies can be responsible for educational interventions that often need to start at the elementary school level. Of course, companies should do what they can. But expecting them to pull off mathematical impossibilities is irrational, and the DEI movement’s stalwart refusal to acknowledge the pipeline problem undermines the movement as a whole. [↩](#user-content-fnref-4)\n5. I was actually able to get in touch with a former OFCCP higher-up who admitted that rejecting anonymity in hiring was against the spirit of OFCCP requirements. But they sadly wouldn’t go on the record. [↩](#user-content-fnref-5)\n6. The closest we’ve ever come to doing this is our Fellowship program, where we gave free practice to engineers from traditionally underrepresented backgrounds. It was a great program, but what made it great was that our interviewers were eager to help these candidates. We were able to do free practice because our interviewers graciously agreed not to charge. That said, if I were to run this program again, I’d probably focus more on socioeconomic status and non-traditional credentials rather than just race and gender. [↩](#user-content-fnref-6)\n7. Here’s the email we ended the relationship with. I’m including it because it was hard to write and even harder to hit send on, but I think we did the right thing, and maybe someone else will need to write something like this in the future… in which case, please steal our copy. ![interviewing.io termination letter to partner related to dei](https://strapi-iio.s3.us-west-2.amazonaws.com/Termination_letter_ef5d2c81ed.png) [↩](#user-content-fnref-7)\n8. Research that shows that having attended a top school isn’t very predictive and that, while experience at a top company is somewhat predictive, it’s not the most important thing:\n\n   * [Lessons from 3,000 technical interviews… or how what you do after graduation matters way more than where you went to school](https://interviewing.io/blog/lessons-from-3000-technical-interviews)\n   * [We looked at how a thousand college students performed in technical interviews to see if where they went to school mattered. It didn't.](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt)\n   * [Lessons from a year’s worth of hiring data](https://interviewing.io/blog/lessons-from-a-years-worth-of-hiring-data)[↩](#user-content-fnref-8)\n   [↩2](#user-content-fnref-8-2)\n9. Anyone who’s read my writing for a long time will pause here and wonder why I’m OK with resumes and recommending anything about reading them. Until recently, I was stalwartly against resumes and convinced that they carried no signal whatsoever. Then, as part of [the recruiter study I mentioned](https://interviewing.io/blog/are-recruiters-better-than-a-coin-flip-at-judging-resumes), we built some simple ML models to judge resumes and compared their performance to human recruiters. They all outperformed recruiters, and that surprising result made me reverse my stance. [↩](#user-content-fnref-9)\n10. First study (2014): <https://interviewing.io/blog/resumes-suck-heres-the-data>   \n    Second study (2024): <https://interviewing.io/blog/are-recruiters-better-than-a-coin-flip-at-judging-resumes> [↩](#user-content-fnref-10)\n11. There are other gaps that start way before someone gets to college. Enumerating the is out of scope of this piece, but [this writeup by the National Math and Science Initiative](https://www.nms.org/Resources/Newsroom/Blog/2023/October/Math-Science-Education-Gap-Underserved-Communities.aspx#:~:text=The%20Scope%20of%20the%20Math%20and%20Science%20Gap%20Problem&text=Similar%20gaps%20exist%20in%20science,pursue%20STEM%20majors%20and%20careers) is a good place to start. [↩](#user-content-fnref-11)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/i-love-meritocracy-but-all-the-recent-anti-dei-rhetoric-is-bad",
      "author": "",
      "user_id": ""
    },
    {
      "title": "The definitive list of companies who are hiring engineers right now",
      "content": "A little over a month ago, we conducted a survey among our users to get to the bottom of what was happening with hiring freezes at Google and Facebook and [published the results](https://interviewing.io/blog/google-facebook-hiring-freeze). The post went live on August 1st, 3 days before Google’s two-week freeze was due to come to an end, and I even called out that the post might soon be rendered moot. It looks like that did not come to pass – now, a month later, the giants still appear to be largely frozen (with some specific exceptions… see the post about hiring freezes for details).\n\ninterviewing.io is both a mock interview platform and an eng hiring marketplace (engineers use us for technical interview practice, and top performers get fast-tracked at companies), so we have some unique insights into how these freezes have affected engineers’ behavior. In the wake of most FAANGs freezing hiring, we’ve seen a not entirely unexpected uptick in our users’ appetites for interviewing at non-FAANG companies. What *was* surprising, however, was engineers’ appetite for technical interview practice – though we’ve seen a marked decrease in purchases of Google and Facebook-specific mock interviews, people are practicing for technical interviews *in general* at about the same clip as before the freezes.\n\nArmed with these indicators, and in the spirit of being useful during a hard and uncertain time, we thought it’d be interesting to survey our users once again to figure out 1) whether engineers are battening down the hatches and laying low for a while or if, despite the looming recession, they’re still actively considering new jobs and 2) which companies are actually hiring engineers right now.\n\n**TL;DR** There are lots of engineers actively looking. There are also lots of companies who are actively hiring. Scroll down to see a massive list.\n\nWhat we asked\n-------------\n\nWe were curious about 2 things:\n\n1. Are you looking right now, and if so, where are you interviewing?\n2. Is your current employer actively hiring engineers?\n\nBelow are the actual survey questions:\n\n![Screenshot of the survey question: which companies are you interviewing right now](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fwhere_interviewing_b5ef2cf127.png&w=1920&q=75 \"Survey question #1\")\n\n![Screenshot of the survey question: is the company you currently work at actively hiring engineers?](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fwhere_hiring_d0172e4456.png&w=1920&q=75 \"Survey question #2\")\n\nAre engineers actively looking for work, and are companies actively hiring engineers?\n-------------------------------------------------------------------------------------\n\nBy all accounts, yes, it looks like they are, on both counts. Of survey respondents, 49% said they were in the middle of an active job search, and 51% said that their employer was actively hiring.\n\nBig list of companies who are hiring engineers right now\n--------------------------------------------------------\n\nWe collated responses to both questions above and came up with a list of employers who are hiring. Because we are based in the U.S. and most of our users are here, we confined this list to either companies who are based in the U.S. or are actively hiring here. Note that the logos, descriptions, locations, and sizes below are enriched via Clearbit. We spot-checked the list, and made some corrections/additions here and there, but it’s possible we missed something. If your company is listed here and something is incorrect, let us know by emailing [hello@interviewing.io](mailto:hello@interviewing.io).\n\nHow interviewing.io can help you\n--------------------------------\n\nIf you’re a company who’s hiring engineers and want access to the best engineers in the world, *we have them*. Our candidates convert about 3X better than other sources (including internal referrals) because we use technical interview data, instead of resumes, to identify top performers. Our candidates are the two things you want: they’re really good, and they’re looking right now. Sign up, and [start talking to actually great candidates](https://interviewing.io/employers/). We’ve hired for FAANGs and startups alike and have helped thousands of engineers find great jobs.",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/companies-hiring-engineers-2022",
      "author": "",
      "user_id": ""
    },
    {
      "title": "If you care about diversity, don't just hire from the same five schools",
      "content": "**EDIT:** Our university hiring platform is now on [Product Hunt](https://www.producthunt.com/products/interviewing-io#university-hiring-by-interviewing-io)!\n\nIf you’re a software engineer, you probably believe that, despite some glitches here and there, folks who have the technical chops can get hired as software engineers. We regularly hear stories about college dropouts, who, through hard work and sheer determination, bootstrapped themselves into millionaires. These stories appeal to our sense of wonder and our desire for fairness in the world, but the reality is very different. For many students looking for their first job, the odds of breaking into a top company are slim because they will likely never even have the chance to show their skills in an interview. For these students (typically ones without a top school on their resume), their first job is often a watershed moment where success or failure can determine which opportunities will be open to them from that point forward and ultimately define the course of their entire career. In other words, having the right skills as a student is nowhere near enough to get you a job at a top-tier tech company.\n\nTo make this point concrete, **consider three** (fictitious, yet indicative) **student personas, similar in smarts and skills but attending vastly different colleges. All are seeking jobs as software engineers at top companies upon graduation**.\n\nMason goes to Harvard. He has a mediocre GPA but knows that doesn’t matter to tech companies, where some of his friends already work. Come September, recent graduates and alums fly back to campus on their company’s dime in order to recruit him. While enjoying a nice free meal in Harvard Square, he has the opportunity to ask these successful engineers questions about their current work. If he likes the company, all he has to do is accept the company’s standing invitation to interview on campus the next morning.\n\n[Emily](https://blog.andrewhoang.me/how-to-be-emily/) is a computer science student at a mid-sized school ranked in the top 30 for computer science. She has solid coursework in algorithms under her belt, a good GPA, and experience as an engineering intern at a local bank. On the day of her campus’s career fair, she works up the courage to approach companies – this will be her only chance to interact with companies where she dreams of working. Despite the tech industry being casual, the attire of this career fair is business formal with a tinge of sweaty. So after awkwardly putting together an outfit she would never wear again[1](#user-content-fn-1), she locates an ancient printer on the far side of campus and prints 50 copies of her resume. After pushing through the lines in order to line up at the booths of tech companies, she gives her resume to every single tech company at the fair over the course of several hours. She won’t find out for two more weeks if she got any interviews.\n\nAnthony goes to a state school near the town where he grew up. He is top in his class, as well as a self-taught programmer, having gone above and beyond his coursework to hack together some apps. His school’s career fair has a bunch of local, non-tech employers. He has no means of connecting with tech companies face-to-face and doesn’t know anyone who works in tech. So, he applies to nearly a hundred tech companies indiscriminately through their website online, uploading his resume and carefully crafted cover letter. He will probably never hear from them.\n\nCareer fair mania\n-----------------\n\nThe status quo in university recruiting revolves around career fairs and in-person campus recruiting, which have serious limitations. For one, they are extremely expensive, especially at elite schools. Prime real estate at the MIT career fair will run you a steep $18,000, for entry alone. That’s not counting the price of swag (which gets more exorbitant each year), travel, and, most importantly, the opportunity cost of attending engineers’ time. **While college students command the lowest salaries, it’s not uncommon for tech companies to spend 50% more on recruiting a student than a senior engineer.**\n\nAt elite schools, the lengths to which companies go to differentiate themselves is becoming more exorbitant with each passing year. In fact, students at elite colleges suffer from company overload because every major tech company, big and small, is trying to recruit them. All of this, while students at non-elite colleges are scrambling to get their foot in the door without any recruiters, let alone VPs of high-profile companies, visiting their campus.\n\nOf course, due to this cost, companies are limited in their ability to visit colleges in person, and even large companies can visit around 15 or 20 colleges at most. This strategy overlooks top students at solid CS programs that are out of physical reach.\n\nIn an effort to overcome this, companies are attending conferences and hackathons out of desperation to reach students at other colleges. The sponsorship tier for the Grace Hopper Conference, the premier gathering for women in tech, tops out at $100,000, with the sponsorship tier to get a single interview booth starting at $30,000. Additionally, larger companies send representatives (usually engineers) to large hackathons in an effort to recruit students in the midst of a 48-hour all-nighter. However, the nature of in-person career fairs and events are that not all students will be present. Grace Hopper is famously expensive to attend as a student, especially when factoring in airfare and hotel.\n\nThis cost is inefficient at best, and prohibitive at worst, especially for small startups with low budget and brand. Career fairs serve a tiny portion of companies and a tiny portion of students, and the rest are caught in the pecuniary crossfire. **Demand for talented engineers out of college who bring a different lived experience to tech has never been higher, yet companies are passing on precisely these students via traditional methods.** Confounding the issue even further is the fundamental question of [whether having attended a top school has much bearing on candidate quality](https://interviewing.io/blog/lessons-from-3000-technical-interviews) in the first place (more on that in the section on technical screening below).\n\n![Chart showing how career fairs suck](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F1896f_threshold_4_f34775ee84.webp%3Fupdated_at%3D2022-11-21T19%3A55%3A16.652Z&w=1920&q=75 \"Career fairs suck\")\n\nHomogeneity of hires\n--------------------\n\nThe focus of companies on elite schools has notable, negative implications for the diversity of their applicants. In particular, many schools that companies traditionally visit are notably lacking in diversity, especially when it comes to race and socioeconomic status. According to a [survey of computer science students at Stanford](https://medium.com/@jcueto/race-and-gender-among-computer-science-majors-at-stanford-3824c4062e3a), there were just fifteen Hispanic female and fifteen black female computer science majors in the 2015 graduating class *total*. In this analysis, the Stanford 2015 CS major was 9% Hispanic and 6% black. According to a [2015 analysis](https://medium.com/@winniewu/race-and-gender-among-computer-science-concentrators-at-harvard-1c1943a20457), the Harvard CS major was just 3% black and 5 percent Hispanic. Companies that are diversity-forward and constrained to recruiting at the same few schools end up competing over this small pool of diverse students. Meanwhile, there is an entire ocean of qualified, racially diverse students from less traditional backgrounds whom companies are overlooking.\n\nThe focus on elite schools also has meaningful implications on socioeconomic diversity. According to a detailed [New York Times infographic](https://www.nytimes.com/interactive/2017/01/18/upshot/some-colleges-have-more-students-from-the-top-1-percent-than-the-bottom-60.html?mcubz=0&_r=0), “four in 10 students from the top 0.1 percent attend an Ivy League or elite university, roughly equivalent to the share of students from poor families who attend any two- or four-year college.” The infographic highlights the rigid segmentation of students by class background in college matriculation.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/Where_Todays_25_year_olds_went_to_college_b8f73e7688.webp?updated_at=2022-11-21T20:14:07.383Z)\n\n\nSource: [New York Times](https://www.nytimes.com/interactive/2017/01/18/upshot/some-colleges-have-more-students-from-the-top-1-percent-than-the-bottom-60.html)\n\n\n\nThe article finds that the few lower-income students who end up at elite colleges do about as well as their more affluent classmates but that attending an elite versus non-elite college makes a huge difference in future income.\n\nThe focus of tech companies on elite schools lends credence to this statistic, codifying the rigidity with which students at elite college are catapulted into the 1 percent, while others are left behind. Career-wise, it’s that first job or internship you get while you’re still in school that can determine what opportunities you have access to in the future. And yet, students at non-elite colleges have trouble accessing these very internships and jobs, or even getting a meager first round interview, contributing to the lack of social mobility in our society not for lack of skills but for lack of connections. This sucks. A lot.\n\nThe technical screen\n--------------------\n\nLet’s return to our three students. Let’s say that Emily, the student who attended her college’s career fair, gets called back by one or two companies for a first round interview if her resume meets the criteria that companies are looking for. Not having an internship at a top tech company already — quite the catch-22 — puts her at a disadvantage. Anthony has little to no chance of hearing back from employers via his applications online, but let’s say that by some miracle lands a phone screen with one of the tech giants (his best shot, as there are more recruiters to look through the resume dump on the other end).\n\nWhat are their experiences when it comes to prepping for upcoming technical interviews?\n\nMason, the Harvard student, [attends an event](https://www.facebook.com/events/138642833412845/) on campus with Facebook engineers teaching him how to pass the technical interview. He also accepts a few interviews at companies he’s less excited with for practice, and just in case. While he of course needs be sharp and prepare in order to get good at these sorts of algorithmic problems, he has all of the resources he could ask for and more at his disposal. Unsurprisingly, his Facebook interview goes well.\n\nEmily’s school has an informal, undergraduate computer science club in which they are collectively reading technical interviewing guides and trying to figure out what tech companies want from them. She has a couple interviews lined up, but all of which are for jobs she’s desperate to get. They trade tips after interviews but ultimately have a shaky understanding of they did right and wrong in the absence of post-interview feedback from companies. Only a couple of alumni from their school have made it to top tech companies in the past, and so they lack the kinds of information that Mason has on what companies are looking for. (E.g. Don’t be afraid to take hints, make sure to explain your thought process, what the heck is this CoderPad thing anyway…)\n\nAnthony doesn’t know anyone who has a tech job like the one he’s interviewing for, and only one of his friends is also interviewing. He doesn’t know where to start when it comes to getting ready for his upcoming interview at GoogFaceSoft. He only has one shot at it with no practice interviews lined up. He prepares by googling “tech interview questions” and stumbles upon a bunch of unrealistic interview questions, many of them behavioral or outdated. He might be offered the interview and be fit for the job, but he sure doesn’t know how to pass the interview.\n\nFor students who may be unfamiliar with the art of the technical interview, algorithmic interviews can be mystifying, [leading to an imbalance of information on how to succeed](https://interviewing.io/blog/you-cant-fix-diversity-in-tech-without-fixing-the-technical-interview). Given that [technical interviewing is a game](https://interviewing.io/blog/after-a-lot-more-data-technical-interview-performance-really-is-kind-of-arbitrary), it is important that everyone knows the rules, spoken and unspoken. There are many practice resources available, but no amount of reading and re-reading *Cracking the Coding Interview* can prepare you for that moment when you are suddenly in a live, technical phone screen with another human.\n\nWe built a better way to hire\n-----------------------------\n\nUltimately, as long as university hiring relies on a campus-by-campus approach, the status quo will continue to be fundamentally inefficient and unmeritocratic. No company, not even the tech giants, can cover every school or every resume submitted online. And, in the absence of any meaningful information on a student’s resume, companies default to their university as the only proxy. This approach is inefficient at best and, at worst, it’s the first in a series of watershed moments that derail the promise of social mobility for the non-elite, taking with them any hope of promoting diversity among computer science students.\n\nBecause this level of inequity, placed for maximum damage right at the start of people’s careers, really pissed us off, we decided to do something about it. interviewing.io’s answer to the unfortunate status quo is a university-specific hiring platform. If you’re already familiar with how core interviewing.io works, you’ll see that the premise is exactly the same. **We give out free practice to students, and use their performance in practice to identify top performers, completely independently of their pedigree. Those top performers then get to interview with companies like Lyft and Quora on our platform. In other words, we’re excited to provide students with pathways into tech that don’t involve going to an elite school or knowing someone on the inside.** So far, we’ve been very pleased with the results. You can see our student demographics and where they’re coming from below. Students from all walks of life, whether they’re from MIT or a school you’d never visit, are flocking to the platform, and we couldn’t be prouder.\n\ninterviewing.io evaluates students based on their coding skills, not their resume. We are open to students regardless of their university affiliation, college major, and pretty much anything else (we ask for your class year to make sure you’re available when companies want you and that’s about it). Unlike traditional campus recruiting, we attract students organically (getting free practice with engineers from top companies is a pretty big draw) from schools big and small from across the country.\n\nWe’re also proud that **almost 40 percent of our university candidates come from backgrounds that are underrepresented in tech**.\n\n![Chart showing underrepresented status of students who are using interviewing.io](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FUnderrepresented_status_of_students_on_interviewing_io_a00a8396af.webp%3Fupdated_at%3D2022-11-21T19%3A57%3A53.399Z&w=1920&q=75 \"Underrepresented status of students on interviewing.io\")\n\nBecause of our completely blind, skills-first approach, we’ve seen an interesting phenomenon happen time and time again: when a student unmasks at the end of a successful interview, the company in question realizes that the student who just aced their technical phone screen was one whose resume was sitting at the bottom of the pile all along.\n\nIn addition to identifying top students who bring a different lived experience to tech, we’re excited about the economics of our model. **With interviewing.io, a mid-sized startup can staff their entire intern class for the same cost as attending 1-2 career fairs at top schools… with a good chunk of those interns coming from underrepresented backgrounds.**\n\n*Meena runs interviewing.io’s university hiring program. We help companies hire college students from all over the US, with a focus on diversity. Prior to joining interviewing.io, Meena was a software engineer at Clever, and before that, Meena was in college on the other side of the engineer interviewing equation.*\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. At least her school didn’t send out [this](https://twitter.com/kaybeezee/status/907291100238532609). [↩](#user-content-fnref-1)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/if-you-care-about-diversity-you-should-stop-hiring-from-the-same-five-schools",
      "author": "",
      "user_id": ""
    },
    {
      "title": "A founder’s guide to making your first recruiting hire",
      "content": "Recently, a number of founder friends have asked me about how to approach their first recruiting hire, and I’ve found myself repeating the same stuff over and over again. Below are some of my most salient thoughts on the subject. Note that I’ll be talking a lot about engineering hiring because that’s what I know, but I expect a lot of this applies to other fields as well, especially ones where the demand for labor outstrips supply.\n\nDon’t get caught up by flashy employment history; hustle trumps brands\n----------------------------------------------------------------------\n\nAt first glance, hiring someone who’s done recruiting for highly successful tech giants seems like a no-brainer. Google and Facebook are good at hiring great engineers, right? So why not bring in someone who’s had a hand in that success?\n\nThere are a couple of reasons why hiring straight out of the Googles and Facebooks of the world isn’t necessarily the best idea. First off, if you look at a typical recruiter’s employment history, you’re going to see a lot of short stints. Very likely this means that they were working as a contractor. While there’s nothing wrong with contract recruiting, per se, large companies often hire contract recruiters in batches, convert the best performers to full-time hires, and ditch the rest.[1](#user-content-fn-1) That said, some of the best recruiters I know started out at Google. But I am inclined to believe they are exceptions.\n\nThe second and much more important reason not to blindly hire out of tech giants is the importance of scrappiness and hustle in this hire. **If you work as a recruiter at Google, you’re basically plugged into the matrix. You have a readymade suite of tools that make it much easier to be successful. You have a database of candidates who have previously interviewed that spans a huge portion of the engineering population. Email discovery is easier. Reaching out to people is easier because you have templates that have been proven to work to rely on. And you can lean on the Google brand as a crutch.** Who hasn’t been, at one point in their career, flattered by an email from a Google recruiter? As a result, if you’re sending these emails, you don’t have to go out of your way to write personal messages or to convince people that your company is cool and interesting and worth their time. You get that trust for free.\n\nContrast this setup with being *the very first person* in the recruiting org. You have no tools. You have no templates. You probably have no brand. You probably have, well, jack shit. You need someone who’s going to think critically about tooling and balance the need for tooling with a shoestring budget, especially in a space where most tooling has a price tag of at least $1K per month. You’re going to need someone whose methods are right for your particular situation rather than someone who does things because that’s just how they’ve always been done. You probably want someone who realizes that paying for a LinkedIn Recruiter seat is a huge fucking waste of money and that sourcing on LinkedIn, in general, is a black hole-level time suck. You want someone who is good at engaging with candidates independently of brand sparkle, which likely means someone who understands the value of personalization in their sourcing efforts. You want someone who compensates for your relatively unknown status with great candidate experience during your interview process. You want someone who won’t just blindly pay tens of thousands of dollars for career fair real estate because that’s just what you do, even though the only companies who get ROI on career fair attendance are ones with preexisting brands. And, apropos, you want someone who can start building a sparkly brand for you from day one because building a brand takes time. (More on brand-building in the last two sections on marketing chops and evangelism.)\n\nSales chops are hugely important, and you can test for those\n------------------------------------------------------------\n\nPeople often ask me if having an engineering background is important for technical recruiters. My answer to that is always, “Yes, but.” Yes, it’s useful, but the main reason it’s useful is that it helps build credibility and rapport with candidates. A good salesperson can do that without all the trappings of engineering experience. **To put it another way, at the end of the day, this is a sales job. Great engineers who are shitty salespeople will not do well at recruiting. Great salespeople with no engineering background will likely do well.**\n\nSo, how can you test for sales aptitude? If the candidate is currently an in-house recruiter somewhere, I ask them to pitch me on the company’s product. If they’re an agency recruiter, I ask them to pitch me on one of their clients’ products. Most recruiters do a decent job of pitching the company as a good place to work, but unfortunately, many don’t have a very good understanding of what their company actually does. Given that they’re the first point of contact for candidates, it’s really important to be able to answer basic questions about product-market fit, challenges (both product and engineering), how the company makes money, how much traction there is, what the competition looks like, and so on. Moreover, a lack of interest in something this basic points to a lack of intellectual curiosity in general, and in a technical recruiter, this is a very poor sign because such a huge portion of the job is picking up new concepts and being able to talk about them intelligently to very smart people.\n\nYou want someone who can write\n------------------------------\n\nI was on the fence about whether to include this section because it sounds kind of obvious, but writing well is important in this role for two reasons. **First off, your recruiter is likely going to be the first point of contact with candidates. And if you’re an early-ish company without much brand, correspondence with the recruiter will likely be the first time a candidate ever hears of you.** So, you probably want that interaction to shine. And the other reason you want someone who cares about narrative, spelling, and grammar is that they will be the arbiter of these abilities in future recruiting hires. Enough said.\n\nOne exercise I like to have candidates for this role go through is writing mock sourcing emails to people at your company, as if they were still at their previous position. This portion of the interview process is probably the best lens into what it’s actually like to work with the candidate. In particular, because candidates are not likely to have a clear idea of what they’re pitching yet, I try to make this part of the process iterative and emphasize that I welcome any number of questions about anything, whether it’s the company’s offering, what companies my firm works with, what certain parts of the engineers’ profiles mean, or anything in between. What questions people ask, how they ask them, and how they deal with the ambiguity inherent in this assignment is part of the evaluation, as is the caliber of the research they did on each mock email recipient.\n\nYou want someone with marketing chops\n-------------------------------------\n\nI talked a bit earlier about how you probably have no brand to speak of at this point. I can’t stress enough how much easier having a brand makes hiring. Until you have one, especially in this climate, you’re going to be fighting so fucking hard for every one-off hire. If you can, you ought to put effort into branding such that you end up in the enviable position of smart people coming to you.\n\nSo why don’t early-ish companies do this across the board? **Brand building is a pain in the ass, it takes time, and not all of your outbound efforts are going to be measurable, which can make it harder to get others in your org to buy in.** If you can find someone who’s had even a little bit of marketing experience, they’ll be able to identify channels to get the word out, use their preexisting network to help with outsource-able tasks, and get the ball rolling on things like hosting events, which, if you’ve never done before, can be quite intimidating.\n\nAnd because recruiting doesn’t live in a vacuum and needs help from other teams to send something high-signal and genuine into the world, someone with some marketing experience will likely have an easier time getting other teams to buy in and put time and resources into this endeavor, which brings me to my next point.\n\nYou want someone who can fearlessly evangelize the importance of recruiting… and get you to take an active role even when you don’t feel like it\n------------------------------------------------------------------------------------------------------------------------------------------------\n\nThe harsh reality is that the primary reason companies hire their first recruiter is so that hiring can be taken off the plate of the founders. It’s tempting to have the “set it and forget it” mentality in a founder’s shoes — recruiters aren’t cheap, so presumably if you pay them enough, they’ll just deal with this pesky hiring thing, and then you can get back to work. I get it. Hiring isn’t that fun, and as a founder, despite having been a recruiter myself, there are definitely days when I just want to pay someone to, for the love of god, take this bullshit off my hands so I can get back to talking to users and selling and figuring out what to build next and all sorts of other things.\n\nBut it doesn’t work that way. If you’re a founder, no one can sell your vision as well as you. And all that experience you’ve built up that makes you a subject matter expert probably also makes you pretty good at filtering candidates. You might take a lot of what’s in your head for granted, but transferring that over into someone else’s brain is going to take time and iteration. And you can never really dissociate from hiring entirely because the moment you do, the culture of “hiring is just the domain of recruiting” is going to trickle down into your culture, and over time, it will cost you the best people.\n\nIn my recruiting days, at a high level, I saw two types of hiring cultures. One had the hiring managers and teams taking an active role, participating in sourcing, tweaking interview questions to make them engaging and reflective of the work, and taking time to hang out with candidates, even if they weren’t interviewing yet. The other type had the recruiting org be largely disjoint from the teams it was hiring for. In this type of setup, team members would view recruiting as a hassle/necessary evil that took them away from their regular job, and most of the remaining trappings of the hiring process would be left in the hands of recruiters alone.\n\n**You can guess which type of company ends up with an enviable interview process, a sweet blog, cool, hiring-themed easter eggs in their code, and a wistful, pervading, nose-pressed-against-the-glass refrain of “I wish I could work there”. And you can, in turn, guess which company demands a CS degree and 10 years of [insert recent language name here] experience in their job descriptions.**\n\nDespite these realities, founders and hiring managers often forget how critical their role in hiring is because they have a ton of everyday tasks on their plates. This is why having your recruiter be a fearless evangelist is so important. This person needs to cheerfully yet insistently remind the team and especially founders (who are, after all, the ones who dictate culture) that time spent on hiring is part of their jobs. This person needs to be able to march into the CEO’s office and demand that they go and give a talk somewhere or consistently block off time on their calendar every week to send some sourcing emails. Or that they need to write some stuff somewhere on the internet such that people start to realize that their company is a thing. Marching into a CEO’s office and making demands is tough. You need a person who will do this without trepidation and who will be able to convince you, even when the sky is falling, that a few hours a week spent on hiring are a good use of your time.\n\nIn addition to these points, all the usual thought points about hiring someone who’s going to be growing a team apply here. Is this person already a strong leader? If not, can they grow into one? Are they going to be able to attract other talent to their team? Are they someone you want around, fighting alongside you in the dark, for a long time to come? And, though in an ideal world I’d choose someone with experience who also meets the criteria I’ve outlined in this guide, if ultimately faced with a choice between experience and someone green with hustle, charisma, writing ability, and smarts, I’ll choose the latter every time.\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. As an aside, this process is an unfortunate side effect of employment law meant to protect contractors from being exploited. The thinking is that by capping the length of time that someone can work as a contractor, you can exert pressure on the company to turn them into full-time hires who have to be given benefits. But as with many well-intentioned, regulatory pieces of legislation, that’s not really what happens in practice. The practical takeaway, though, is that if someone is great at recruiting, they’re probably not going to have a bunch of short contracting stints. [↩](#user-content-fnref-1)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/a-founders-guide-to-making-your-first-recruiting-hire",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Why engineers don’t like take-homes – and how companies can fix them",
      "content": "> *[My experiences with take-homes] drive home the idea that this employer doesn't care if you are a carbon-based life form, as long as code comes out of one or more of your orifices.*\n\nTake-home assignments *could*, in theory, be great for both companies and candidates. What better, fairer way to evaluate someone’s ability to do the work… than to have them do the work?\n\nUnfortunately, in practice, take-homes typically suck for engineers.\n\nWe surveyed almost 700 of our users about their experiences with take-homes and interviewed a handful more for deeper insights. We learned a lot—mostly about candidates' poor experiences and negative feelings toward take-homes. They take a lot of time. They don’t respect candidates’ time. Candidates often get no feedback. And candidates are almost never compensated.\n\nThe good news? Turns out there are some pretty simple things companies can do to vastly improve their take-home assignments. But before we dive into that…\n\nWhy do companies use take-home assignments?\n-------------------------------------------\n\nTake-homes vary a ton by role and company in terms of the types of questions, subject matter, length, and intensity. At their simplest, take-homes can be the same questions as in an algorithmic interview, except done asynchronously. The other extreme is asking candidates to build an entire app and deploy it.\n\nWe were surprised to see how often companies use take-homes. About 85% of our users got one at some point in their career, independent of their experience level. Of the users who encountered them, they tended to see them as part of the process about 20% of the time, again, independent of their experience level[1](#user-content-fn-1).\n\nWhy are take-homes relatively popular among employers? They mostly use them to save time in the hiring process. There are, however, some more noble reasons a company might use a take-home assignment:\n\n* Get better signal during the interview process, as a take-home can be more indicative of actual work\n* Get a candidate's best work out of them in a lower-stress environment than a live algorithmic interview and/or attract candidates who don’t like algorithmic interviews (of which there are [plenty](https://hn.algolia.com/?q=coding+interviews))\n* Broaden the candidate pool by offering a way in for candidates from non-traditional backgrounds—in lieu of a resume screen, which they’d likely fail, candidates can do choose to do an assignment\n\nAccordingly, here’s the relevant part of a great conversation between Vincent Woo of CoderPad and Patrick McKenzie (known to Hacker News folks as patio11) of Stripe, formerly of [Starfighter](https://www.kalzumeus.com/2015/03/09/announcing-starfighter/).\n\nVincent: *What general sort of high level change do you think that recruiters at tech\ncompanies that are roughly Stripe’s size or bigger ought to make?*\n\nPatrick: *If I could wave a magic wand and sell the world on one concept, it would be selling the world on the desirability of work sample testing… where the way to tell if someone is good at something is to have them do the something.*\n\nDespite enthusiasm for the theory of take-homes and some very well-intentioned reasons, candidates overwhelmingly don’t like take-homes. Here’s why.\n\nWhy don’t candidates like take-homes? It’s about value asymmetry.\n-----------------------------------------------------------------\n\n**Though users expressed a lot of frustration with take-homes, we were surprised to see very few take a hard-line stance and refuse to do them. Only 6% outright refuse, and 66% of people complete take-homes all or most of the time.** Surprisingly, these stats didn’t really change when we looked only at senior engineers. I was expecting that experienced engineers would do them almost never, if at all, but that’s not what the data shows. It’s possible that seniors are just louder in their disapproval.\n\n![How often people do take-homes.png](https://strapi-iio.s3.us-west-2.amazonaws.com/How_often_people_do_take_homes_e85d712b9b.png)\n\nNevertheless, the more desirable a company, the more likely candidates will do the take-home and feel OK about it—70% told us they completed them because they “Really wanted to work at the company and were willing to do what it took.”\n\n> *I found Weedmaps [to be] a very interesting company. They were the first marijuana related company to IPO. So you'd be on the frontier working for them. I found that exciting. So I applied and they had [a] take-home. I was like, sure, of course I’ll do this.*\n\nOther reasons our users gave for completion included: “Because the take-home would be discussed at the onsite” (38%) and “Interesting/cool assignment” (37%). However, many of those who did finish them had such a poor experience that they said they’d never apply to certain companies ever again. We’ll talk about what makes the experience poor in a little bit.\n\nOf the people who refused to do at least one take-home at some point in their career, here were their reasons.\n\n![](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FWhy_people_don_t_do_take_homes_f138c44568.png&w=1200&q=75)\n\nNumbers don’t add up to 100 because it was a multi-select poll. In addition, we obviously don’t know how often take-homes are boring or unclear in the wild (probably often!), nor do we know how often companies pay people to do them (our guess, based on some napkin math, is >10% of the time). So we don’t have the true denominator for these stats. Nevertheless, we found these results insightful.\n\n**The common thread among all of these reasons is value asymmetry.** The worst take-homes feel unrewarding to candidates. Even exploitative. Take-home assignments ask a lot of candidates: a significant investment of their time, with an often unclear scope, no guarantee of progressing to the next round, often without feedback, and almost always without compensation. Meanwhile, the company has basically invested nothing, except to send the task. We heard this a lot.\n\n> *When I'm interviewing, I look for things that are proxies for valuing team members… or not. If they want me to do a take-home test, and they haven't even spent 30 minutes on a phone screen, I begin to sense an asymmetry in our relationship, with their time and resources being very valuable, and mine not being valuable at all.*\n\n> *A divergence between how much effort they want me to put in, and how much they want to put in themselves. It signals that they are more worried about their time than mine, their costs than mine. It also means they underestimate how much effort it takes to write code, so that if I go to work for them, I am likely to face demands to work uncompensated overtime to meet their optimistic estimates.*\n\n> *Spending five hours on their one-hour test for nothing leaves hard feelings. And if they ghost me after the test, I will happily tell every developer who asks what cheapskates they are.*\n\nIs there any relationship between who candidates are and their willingness to do take-homes?\n--------------------------------------------------------------------------------------------\n\nPerhaps surprisingly, our data says no. We ran a regression to compare our survey respondents’ interview performance on our platform to how likely they were to do take-homes. The relationship was so weak as to be negligible.\n\nSimilarly, we ran a regression to see if people who *look* good on paper are more or less likely to do take-homes. The relationship there was negligible as well.\n\nIn other words, contrary to some popular opinions, you’re not necessarily weeding out your best candidates by doing take-homes, whether you define “best” in terms of how their resume looks or how they perform in interviews.\n\nHow companies can make take-homes better (and why they probably should)\n-----------------------------------------------------------------------\n\nAfter reading the stats above, you might think that, despite their grumblings, candidates generally do take-homes, and the best candidates won’t be weeded out. So, if you’re one of the many companies that uses them, it may not make sense to invest your limited time into making them better. It’s not that simple. For some companies, like the FAANGs, who have extremely strong brands and are known to pay well, changes are probably not worth it, especially in this market, where junior and senior engineers are willing to jump through more hoops than ever before.\n\nIf you’re not a FAANG, though, listen up. Here are some questions you can ask yourself to figure out if you should indeed make some changes to your take-homes.\n\n**First, take an honest look in the mirror and ask yourself about your brand strength. Are you a household name? Does having you on candidates’ resumes give them automatic prestige? Are you known to pay above market?** If the answer to *all* of these questions isn’t a resounding yes, your brand strength is probably not strong enough to make people jump through hoops.\n\nHere’s a sketch to drive that point home. Unless you have a ton of brand strength, candidates’ willingness to jump through hoops drops off sharply.\n\n![](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fbrand_strength_vs_value_asymmetry_4bc96b83d4.png&w=1200&q=75)\n\nIn fairness, I drew this sketch in a boom market. Maybe it’s a bit less steep now, but honestly, I’m not sure.\n\nIf you aren’t maxing out on brand strength, there are two questions you should ask yourself:\n\n1. What is my take-home completion rate?\n2. What is my offer acceptance rate?\n\n**If your completion rate is below, say, 60% (our data shows that candidates complete take-homes around 62% of the time), then it’s DEFINITELY time to make a change.** Honestly, regardless of what our data says, if people aren’t completing them at least 85% of the time, it’s *probably* time to make a change—losing more than 15% of your candidates to attrition at any given stage in the funnel is bad.\n\n**What about your offer acceptance rate? If it’s less than 50% and you’re using a take-home already, there’s an opportunity to make some improvements.** You might be wondering what this has to do with take-homes in the first place. To answer that, let’s change how we think about different parts of the interview process. At face value, every part of the process is there to vet candidates, to determine if they’re the right fit for your organization. However, when used correctly, every part of your process should become a selling vehicle as well. This is *especially* important for companies who do not have a strong, well-known brand. The FAANGs can get away with using their interview processes primarily as vetting exercises because candidates are already sold on the pay or prestige or sometimes on the work and the product. When you don’t have an established brand, the candidates who come to you are, at best, open to learning more, and the interview process is the instrument that teaches them. Wield it accordingly.\n\nThough we strongly advocate coming up with [great, unique interview questions](https://interviewing.io/blog/best-technical-interviews-common)[2](#user-content-fn-2) and making sure you have [great interviewers](https://interviewing.io/blog/we-have-the-best-technical-interviewers-heres-how-we-do-it), if your process does have a take-home component, it is one of the more overlooked parts of the process when it comes to selling. You have the opportunity to have someone *do the actual work that you do*! This is your chance to pick the coolest stuff you’ve worked on and serve it up to someone on a platter and make it stick in their brains and make them imagine what it’d be like to work on these kinds of problems every day! Why wouldn’t you jump at this opportunity?\n\nYou may think you don’t need to sell in this market. But just because employers have all the power right now, it doesn’t mean that will always be the case. And great senior engineers still have a lot of leverage.\n\nIf, after considering your take-home completion rate and your offer acceptance rate, it looks like you *do* need to make some changes, here are some practical tips, based on what we’ve learned from talking to our users (overwhelmingly senior engineers who are targeting top-tier companies—probably the people you want). Let’s start with what we just talked about: using the take-home as a selling vehicle.\n\n### Make them interesting and relevant to the actual work\n\nYou're getting a chance to spend a couple of hours with somebody in a take-home (metaphorically). Why wouldn't you do everything you can to get them excited? **Pick a problem that you've worked on, and get them hooked on it. Pick the kernel of an interesting problem that you've solved, and build something around it that will challenge candidates. Something that gets them thinking, “I could have done that better” or “This is a different or more efficient way to do it.” That's going to be more effective than the standard perks many companies offer.**\n\n> *[Best take-home I’ve seen was an] open-ended system design question on the type of system I would work on, was meant to simulate a team discussion on the system we needed to build, and was a great way for me to start thinking about what I'd be working on there.*\n\nOne way you could do this, is to have your engineering team keep a shared doc of ‘cool’ solutions they've found, or new things they've tried. These can serve as jumping-off points for creating your take-homes.\n\n> *The… challenge was for an internal tooling team that specialized in incident response tools; their challenge was to create a scaled down version of a tool already in use at the company. The focus was more on understanding the domain and customer than wiring up a bunch of complicated stuff, and it was a delight. It being a greenfield also gave you an opportunity to showcase some software design skills. I did not get this job, but enjoyed the experience and still feel connected to the team.*\n\n> *It was conceptually related to the sort of work the team was performing, but it was simplified and stand-alone enough to clearly not be unpaid labor for their product.*\n\nJust be sure that when you come up with a practical problem that you strip out the annoying parts and focus only on the juicy kernel of the problem, the part that’s actually cool and lets the candidate be smart and creative. Don’t make them do grunt work or wrestle with their dev environment!\n\n> *[This] was for a tooling team in Support, where they didn't have a lot of experience creating challenges or interviewing. One of the engineers took a difficult task that he had accomplished recently and just made that the take home challenge. It involved a lot of Ruby version conflict debugging. It was completely demoralizing and felt like hazing.*\n\n### Keep them short\n\nCandidates overwhelmingly favor take-homes that respect their time, i.e., short ones.\n\n> *The best were short and brief, took no more than 2 hours and were directly related to what I would be doing on the job.*\n\n> *Best are realistic and time bound, i.e., low time investment required.*\n\n> *Short and quick take-homes are great.*\n\n**Over 80% of survey respondents said that take-homes should take 4 hours or less, and a plurality thought that they should take 2 hours.**\n\n![](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FHow_long_engineers_think_take_home_assignments_should_take_2a361d8e29.png&w=1200&q=75)\n\nThe outliers are interesting. Unfortunately, we missed an opportunity to ask about those in the survey.\n\nTake-homes being short in theory is one thing… but we got a lot of feedback that take-homes often go far beyond how long companies tell candidates that take-homes should take.\n\n#### Have someone on your team QA the actual length of the assignment\n\nThis disconnect between actual time and expected time is another instance of value asymmetry: companies didn't even bother to have someone on their team do the take-home themselves and time it.\n\n> *I've done a couple of algorithm questions as take-homes that the interviewer said should take an hour. They routinely take longer than that.*\n\n> *Companies will say, ‘don’t spend more than X hours on it,’ but then it actually takes 5X that amount. It's just too much time. And most of the time they didn’t give feedback at all even though I spent so much time on it.*\n\nCompanies should clearly communicate the expected time commitment for a take-home. And they should be realistic about how much time it should take.\n\n> *Best was an interesting problem with a realistic time frame. They estimated 2 hours and it took me exactly that, which was refreshing and made the process seem fair.*\n\n> *I've done a couple of exercises where they were truly respectful of my time. The tasks were small, clearly defined, and they gave me a realistic timeframe to complete them.*\n\n> *For the best take-home I did, the company told me: ‘Tell us when you're going to start, and you'll have 4 hours. We'll be available over email that whole time, and we'll answer your questions within minutes.’ That felt respectful of my time, and was a more reliable signal of my capabilities than a 20-minute algorithms exercise.*\n\n**To get a realistic estimate, companies could simply QA their own take-homes—just by having someone on your team take it, and time them.**\n\nStill, some candidates will spend more time anyway, because they really want to work for a certain company. And it can be easy for candidates to go down a rabbit hole and get most of a take-home done in the first few hours, then spend another day or more, perfecting it to give themselves a better chance.\n\n> *Do you want me to literally only work two hours on something that's clearly going to take 10 to really be able to have a conversation with you about how I think about things? Because two hours of decisions isn't as good as 10 hours of decisions.*\n\nIt’s okay if candidates want to put in extra time, but it should be made clear that that's not the expectation.\n\n### Set a clear scope, related to the role\n\n> *The scope was poorly defined, which made the assignment not only tedious, but also seemed like a waste of time.*\n\n**Having clear, straightforward instructions can significantly enhance the take-home experience for candidates, making them feel purposeful and relevant. The best take-homes are those that directly align with the actual job.** When candidates are asked to solve a simplified version of a real problem your company is facing, it not only tests relevant skills, but also gives a candidate insight into what work at your company would be like, which is really what selling is. (This is, of course, assuming that you believe what you’re working on is cool… If you don’t, how will you ever convince others?)\n\n> *One was a great learning experience because I had to solve a problem similar to what I would face in the job. It was concise and focused.*\n\nBeing able to ask questions before or during the take-home, e.g., having a direct contact point who’s an engineer at the company, can also be a big plus.\n\n### Be thoughtful about where in your process you insert the take-home, and give candidates a choice between a take-home and something else\n\n#### For take-homes early in the process, before speaking to a human\n\nWe expect that many of the companies reading this piece have take-homes as the first step in\ntheir process, before ever talking to a human. This decision isn’t very popular.\n\n> *If a company calls me for a screening interview, it means they've read my resume and cover letter, and not immediately dismissed my application as inappropriate. It also means they're putting a person on a phone call with me for the duration of the screening interview. This is symmetric, my effort matches theirs.*\n\n> *[I] did a take home that was automatically sent to me after applying, before even speaking to a human. The feedback was 'you are too expensive,' no feedback on the coding.*\n\n> *Company needs to build an investment with the candidate first—they shouldn't ask for it before meeting the candidate at least once.*\n\n**That said, take-homes can be a great way to let candidates who don’t look good on paper show what they can do, and those candidates are more likely to complete them.** To prevent candidate unhappiness/attrition, give them a choice. Either submit a resume or do the take-home assignment or both. If you go this route, though, design a take-home that you trust. We’ve seen companies take this approach and then throw out perfect-scoring take-homes when they didn’t like the resume. If you use a take-home, then respect your candidates enough to follow up with the ones who’ve done well, even if they don’t look good on paper.\n\n#### For take-homes later in the process\n\n**Another way to make take-homes purposeful is to give your candidates an explicit choice about whether they’d rather do a technical interview or a take-home.** Engineers are, in fact, split on which they’d prefer. Giving them a choice allows them to showcase their skills in the format they prefer and feel best prepared for. If you have good questions, you should be able to get good signal from either.\n\nOnly 10% of respondents told us that they were given the choice of take-home versus technical interview. So there is an opportunity for more companies to do this. It's a candidate-friendly gesture that shows empathy and can help candidates shine.\n\nMany candidates have spent months preparing for standard technical interviews. So it can feel anticlimactic when they find out that a company they're excited about doesn't do them. Others get so nervous in a live interview that they don’t perform.\n\n> *I rarely don't pass take-home assessments, but I often fail to pass live interviews.*\n\nInterestingly, one user we interviewed told us that they spend far less time on a take-home compared to preparing for a live technical interview. And because of that they prefer take-homes.\n\n> *I prefer take-homes over all other assessments because I find I spend far less time on take-homes then I do preparing for live interviews. It's hard to overestimate the amount of extra time I spend preparing for a technical interview, compared to doing a take-home—for me it’s maybe 10 times as much. I've spent hundreds of hours, maybe 500 hours, over the course of my career preparing for technical interviews. And if I have one pop up, I can't just drop everything and do it right away. I have to spend a lot of extra time just ramping up for a particular interview, in addition to the hundreds of hours that I've done.*\n\n### Give candidates a good, rational reason to do the take-home\n\nClear communication about the purpose of a take-home in the hiring process, as well as why it’s rational to spend time on it, is important for candidates. It’s a way to make sure it feels purposeful and not like a random task.\n\nOne way to make the take-home feel deliberate is to replace some parts of your process with it. A standard process without a take-home has a recruiter call followed by a technical phone screen followed by an onsite (virtual or otherwise). The technical phone screen usually lasts about an hour. The onsite usually lasts 6 hours.\n\nLet’s say your take-home takes 2 hours to do. You can make it replace the phone screen and one of the onsite rounds, which nets out to the same number of hours spent. If you go this route, we recommend doing the math *explicitly* for candidates and showing them that the time they spend on the take-home is equivalent to the time they’d be spending on a process without it.\n\nAnother way to make the take-home feel deliberate is to incorporate it into the onsite, where at least one of the rounds, if not more, will include a code review and/or thoughtful discussion about tradeoffs and choices made. This should be standard practice, but isn’t always. 32% of our users said companies had told them this, and it was the reason they decided to do a take-home.\n\n> *The best take-homes were ones that we discussed in the first rounds of interviews. The worst ones were ones that I submitted and we never talked about them again.*\n\n**Ideally, you do both of these things together, and very clearly explain to candidates both the math and how the take-home informs the content of the onsite.**\n\n### Compensate candidates\n\nProbably the most striking result of our survey was that 58% of candidates think that they deserve compensation for completing take-homes. Yet only 4% reported ever receiving it. Compensation can shift candidate perceptions of the hiring process and of the company:\n\n> *They compensated me for my time, which made the process feel very professional and respectful.*\n\n> *It was a completely open source codebase and so their process was the exact same as someone that was an employee: here's the ticket with the information to do it, set up the environment, download all the code, get everything running. They gave me a few different tasks I could choose from, I could pick two, and if I completed them I would be compensated a fixed rate, which was $100 for each task. Which in terms of the time I spent on it, is still really cheap for them.*\n\nIf candidates know they’re going to be paid for their work on a take-home, they’ll be more likely to complete it as well.\nCompensating candidates is a clear gesture that shows you value their time and effort, that there’s more symmetry in the relationship. It also goes hand-in-hand with time: paying also forces a company to scope the take-home to a reasonable number of hours. So it's a forcing function for good behavior—if you can’t afford much, then don't make the take-home too long!\n\n#### How much to pay?\n\nResponses about how much companies should pay for take-home assignments were split.\n\n1. **Fixed amounts**: Just over half of respondents (52%) suggested specific and reasonable fixed amounts, ranging from $50 to $500.\n2. **Hourly rates**: The other almost half (47%) favored an hourly rate, with suggestions ranging from $50 an hour and upward (average of $217 an hour). Some suggested that the rate should correspond to the salary of the position being applied for, or be comparable to what an employee at that level and company would earn. As one user put it:\n\n   > *Maybe just pay market?*\n\n   One antipattern when it comes to comp is NOT having a set rate, asking the candidate to name their price, and thereby putting the candidate in a position where they feel like they need to negotiate. In this scenario, the candidate has to negotiate twice: once on the take-home and once on their actual comp, with the worry that negotiating too aggressively on the take-home might count them out… or not aggressively enough anchoring the company to lower compensation down the line. No one needs these mind-games in an already stressful process. Just have a set rate, for god’s sake.\n3. **Symbolic compensation**: This is probably not the best option, but a handful of survey respondents (1%) did mention that a minimal symbolic compensation would do. While 1% is small, we found this interesting to include because, when we interviewed people, two of them mentioned this.\n\n   > *I think any compensation at all has symbolic value. A $100 Amazon card would impress me. A $50 Amazon card and a company t-shirt would at least not insult me. It’s kind of a consolation prize to say, ‘No hard feelings.’*\n\n   > *I feel like a couple meal vouchers would do it these days.*\n\n**Of these options, we’d recommend a reasonable fixed amount based on the task, and the actual time it’s supposed to take.** And of course you know how long it takes because you had one of your engineers do it themselves, right? Right??\n\n### Give feedback\n\n**Lack of feedback was the primary reason our survey respondents said their experience with a given company was bad.** Regardless of interview type, we’re always [pro feedback](https://interviewing.io/blog/why-giving-feedback-good-or-bad-will-help-you-hire), but feedback is especially important for take-homes, because in a way they ask more of a candidate. Offering constructive feedback, regardless of the hiring decision, respects the candidate's effort on the take-home.\n\nDespite the time and effort they invested in completing take-homes, many of our survey respondents said they received no feedback at all. This was seen as demoralizing, and it deterred candidates from applying to future opportunities at those companies.\n\n> *Getting rejected without having a chance to discuss the code with anyone is a terrible experience.*\n\n> *It is really discouraging spending a large amount of time to find out you are rejected without explanations.*\n\n> *They provided no feedback after submission, which made the whole effort feel unappreciated and one-sided.*\n\nIncidentally, the main reason companies don’t give feedback is fear of getting sued. As it turns out, [literally ZERO companies (at least in the US) have ever been sued by an engineer](https://interviewing.io/blog/no-engineer-has-ever-sued-a-company-because-of-constructive-post-interview-feedback-so-why-dont-employers-do-it) who received constructive post-interview feedback.\n\n*Thanks to [Dan Fennessy](https://www.linkedin.com/in/danielfennessy/) for all the behind-the-scenes work on this post.*\n\nFootnotes:\n\nFootnotes\n---------\n\n1. Some users told us they’re seeing take-homes more recently, likely a function of worsening market conditions—the less leverage talent has, the more hoops companies can ask them to jump through. [↩](#user-content-fnref-1)\n2. You might have to do this soon anyway, in all your interviews, to [ward off against AI-driven cheating](https://interviewing.io/blog/how-hard-is-it-to-cheat-with-chatgpt-in-technical-interviews). [↩](#user-content-fnref-2)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/why-engineers-dont-like-take-homes-and-how-companies-can-fix-them",
      "author": "",
      "user_id": ""
    },
    {
      "title": "What’s actually going on with Google and Facebook hiring freezes? We surveyed 1000 engineers to find out.",
      "content": "It looks like we’re entering a recession. One of the hardest things about it is the lack of reliable information about whether companies are still hiring and what hiring freezes even mean. Arguably the two most impactful eng hiring freezes were announced by [Facebook (May 4, 2022)](https://www.businessinsider.com/facebook-is-freezing-hiring-heres-why-and-who-it-impacts-2022-5) and then [Google (July 20, 2022)](https://www.theverge.com/2022/7/20/23271634/google-hiring-pause-two-weeks-review-headcount-needs). Facebook’s freeze is allegedly partial, targeting roles below L7 and excluding machine learning engineers. Google’s freeze is allegedly all-encompassing but may only last 2 weeks. But what’s actually going on? To make some sense of a bunch of contradictory information about Google’s and Facebook’s hiring freezes in the press and on Blind, we decided to ask the people who, outside of Google leadership, will probably know best what’s going on – engineers who are *interviewing at these companies right now.* In this post, we’ll share what we learned from surveying 1003 of our users.\n\nWhy do we care about these freezes at interviewing.io? First off, they affect our users – a good chunk of our users are practicing for upcoming [Google](https://interviewing.io/guides/hiring-process/google) and [Facebook](https://interviewing.io/guides/hiring-process/meta-facebook) interviews. Moreover, these freezes materially affect our business. On our platform, you can practice with engineers from a specific company, and many of our users opt to practice with Google engineers before their Google interview and Facebook engineers before they begin the [Facebook interview process](https://interviewing.io/blog/google-facebook-hiring-freeze). To wit, here is a graph of mock interviews on our platform with Facebook and Google engineers, by week. As you can see, we’re definitely incentivized to care about what’s actually going on with these freezes.\n\n![Chart showing Google and Facebook mock interviews on interviewing.io by week](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffacebook_google_mock_interviews_6f9e02e555.png&w=2048&q=75 \"Chart showing Google and Facebook mock interviews\")\n\nNote that though our user base isn’t a perfectly representative sample of engineers interviewing at Google and Facebook, we have reason to believe that it’s reasonably representative. We’ve been around for about 7 years, have hosted over 100K technical interviews on our platform (split between mock interviews and real ones), and thousands of engineers sign up to use interviewing.io every month. Our average user has 7 years of experience, and about 40% of our users either currently work or have worked at a FAANG company.\n\nOur survey\n----------\n\nOur survey went out to users on July 23rd and got 1003 responses (mostly between July 23rd and July 25th). There were a bunch of questions, but here are the 3 most relevant ones:\n\n![Survey question: Are you currently actively interviewing with any of the following companies?](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fsurvey_question_one_1024x789_c325e2022a.png&w=1080&q=75 \"Question #1\")\n\n![Survey question: For each of the companies you mentioned in the previous question, which level are you applying for (e.g. L5, E6, etc?](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fsurvey_question_two_1024x317_9faf480a7a.png&w=1080&q=75 \"Question #2\")\n\n![Survey question: For the comapnies you mentioned in the previous question, are you appying for SWE/SDE roles or something more niche?](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fsurvey_question_three_1024x515_979811f9f2.png&w=1080&q=75 \"Question #3\")\n\nThe results\n-----------\n\nAs mentioned above, we had 1003 people fill out the survey. Here’s what we learned.\n\n* Respondents were primarily based in the US with an average of 6 years of experience (median of 5).\n* 48% were actively interviewing (way more than we expected, given all the recession talk)\n* These are the companies where they were actively interviewing (percents below won’t add up to a 100 because respondents could select as many options as they liked):\n\n![Screenshot of survey results](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fhiring_freeze_059e455d0f.png&w=1080&q=75 \"Hiring Freeze\")\n\nI wasn’t surprised to see Amazon at the top of the list, given that they haven’t announced freezes and are [continuing to aggressively hire engineers](https://www.retaildive.com/news/amazon-to-hire-more-tech-corporate-workers-in-california-hubs/623167/).[1](#user-content-fn-1)\n\nI *was* surprised that, despite their 2-week freeze announcement, Google was right at the top as well. Given that Facebook had supposedly been frozen for a while, I was surprised to see them in 3rd position.\n\nSo, we dug into the data for Google and Facebook specifically.\n\nWhat’s going on with Google and Facebook?\n-----------------------------------------\n\n### Facebook\n\nFacebook first [announced their hiring freeze on May 4th](https://www.businessinsider.com/facebook-is-freezing-hiring-heres-why-and-who-it-impacts-2022-5). Though the announcement made it clear that the engineering org would be affected, it didn’t go into detail. Of course, around the time that the freeze was announced, commentary about the freeze also started popping up on Blind ([like this one](https://www.teamblind.com/browse/Facebook-Hiring-396)). Consistently from thread to thread, it looked like Facebook froze hiring for engineers below E7, with the exception of Machine Learning Engineers, whom Facebook continued to hire, at all levels.\n\nBased on what we learned from the 76 survey respondents who were actively in process with Facebook, it does appear that SWE below E7 is indeed frozen, but there are some eng functions that are still hiring, in addition to Machine Learning.\n\n| Facebook roles frozen | Facebook roles NOT frozen |\n| --- | --- |\n| SWE below E7 | E7+ SWEs |\n|  | Machine Learning Engineers |\n|  | Production Engineers (devops and/or SRE) |\n|  | Enterprise Engineers (work on Facebook’s internal tools & systems; have  their own hiring process distinct from SWE) |\n|  | Managerial tracks, including M1 (some sources had said that hiring was  frozen at M1 but active for M2 and up) |\n\nWe did one other test as a sanity check. What if Facebook was using the hiring freeze as a smoke screen and only continuing with the absolute strongest candidates while keeping the rest on the bench?\n\nOur best insight into candidate ability is how they perform in interviews on our platform (we do both mock interviews and real ones and have hosted over 100K interviews to date). On interviewing.io, every user who’s done at least 2 mock interviews has a score, and this score is a weighted average of coding ability, problem solving ability, and communication on a scale from 0 to 1. We use it internally to figure out who the strongest performers are so we can best match them with companies.\n\nFirst, we grabbed the overall mean score on interviewing.io, across all users. Then we compared that mean to the mean scores of people who were still interviewing at Facebook. There wasn’t a statistically significant difference. In other words, from what we know, Facebook probably isn’t using their hiring freeze as a smoke screen.\n\n*Do you have reliable info that any of the above is incorrect? Email me at [aline@interviewing.io](mailto:aline@interviewing.io).*\n\n### Google\n\nFirst off, Google is supposedly going to announce whether the two-week freeze is going to continue, and if so, on what terms, on August 3rd. So there’s a good chance this post will age poorly!\n\nThat said, I’m a firm believer in marching on with the information we have at the time, so here goes…\n\nAnalyzing Facebook was fairly straightforward because with the exception of a few new pieces of information (e.g. Production Engineering is still hiring), what was in the press and on Blind was consistent with reality. Figuring out what was going on at Google, on the other hand, was a harder task.\n\nGoogle first [announced their hiring freeze on July 20th](https://www.theverge.com/2022/7/20/23271634/google-hiring-pause-two-weeks-review-headcount-needs), following an announcement that hiring would slow for the rest of 2022 but that headcount would remain open in critical roles, including engineering. A Google spokesperson then clarified that over these 2 weeks, Google would be pausing “most offers”.\n\nIn our survey, 210 people were actively in process with Google. That seemed high, so I reached out to all of them to get their take on what was going on and to confirm that they were indeed actively in process and not in nebulously waiting for some next step that may or may not come.\n\nThe results were fascinating. First, here’s what appears to be true across the board:\n\n* Google is indeed not extending offers throughout the duration of the freeze. In other words, there are people in various stages of interviews, but even if they get past the hiring committee, they wouldn’t get an official offer until the freeze is lifted.\n* L3 or “early career” hiring is indeed frozen until next year. Almost everyone who responded to us and had been interviewing for L3 was paused. That said, it looks like Google is willing to continue L3 interviews in some cases with the understanding that those results will be valid for a year, so that when headcount for L3 opens up again, candidates will be able to skip the onsite process.\n\nThe rest is… murky. From what I can tell, different recruiters are telling people different things. Here’s a sample.\n\n> *I asked my Google recruiter the situation with Google hiring, and they confirmed that for at least L3 Early Career SWE, there [is no headcount] left for 2022…. They also said that other positions (L4+) are still actively hiring.*\n\n> *I just received a phone call from the Google recruiter yesterday. She said that they are slowing down hiring for “early career” developers, which are L3 and L4, for the rest of the year, but higher levels are still in full swing.*\n\n> *I just had a conversation with the recruiter and they HAVE a hiring freeze until Aug 3rd, but they didn’t stop the interviewing and team matching process.*\n\n> *I am currently in a freeze mode with them. I was in the team matching process and after several hiring manager’s interviews, they came to me with the news to wait until they reprioritize stuff.*\n\n> *Google is hiring in full swing… Their engineering teams are going to move people around in two weeks to fill some of the open roles but that’ll not affect much in hiring.*\n\n> So, according to their response to me, I “may experience some delays in our hiring process” [but] I was specifically told that the hiring process is not halted with me.\n\n> *I’m in process still, but they said that even if I do well on the interview I [will] not receive an offer until the hiring slowdown ends.*\n\nAs several of these statements blatantly contradict one another, I went back to the data and ran the same sanity check that I did with Facebook above. In other words, what if Google is using the hiring freeze as a smoke screen and only continuing with the strongest candidates, while keeping the rest on the bench?\n\nAs above, our best insight into candidate ability is how they perform in interviews on our platform (see the Facebook section above for an explanation of our scoring system and our approach).\n\nJust like in the Facebook analysis, we grabbed the overall mean score on interviewing.io, across all users. Then, we compared that mean to the mean scores of Google interviewees, this time *at various levels* (L4, L5, and L6). Unlike the Facebook analysis, we did see some statistical significance at L4. Candidates who were still actively interviewing at L4 had significantly higher interviewing.io scores than the average interviewing.io candidate (p < 0.03 with a “medium” effect size of 0.45). We did NOT see any significance for candidates at L5 and above.\n\nNote that we don’t have a nice, clean table in this section where we show what’s frozen and not, like we did for Facebook, precisely because the results were murky. It feels like Facebook made a clear set of rules and then stuck to them and communicated them effectively to candidates. With Google, it feels like decisions are being made on a case-by-case basis. That said, from this running the numbers and interacting with our users, here’s what we *think* is going on.\n\n* L4 is partially frozen, but allowances/exceptions are possibly being made for interview performance – if someone interviews well, then they’re being moved forward, to onsite and to hiring committee. Others are being paused. It’s about half and half for L4 (see the graph below). Note that this is quite presumptive, and I don’t have a lot of conviction around it, but given the data we have, it’s a reasonable conclusion.\n\n![Chart showing that Google appears to be selectively applying their hiring freeze, depending on level](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FGoogle_appears_to_be_selectively_applying_their_hiring_freeze_depending_on_level_811c48f4c3.png&w=1920&q=75 \"Google continue to interview candidates\")\n\n* Unlike L4, L5 and up are actively hiring almost across the board, regardless of performance.\n* Machine Learning, Technical Program Manager, and Engineering Manager roles are actively hiring across the board.\n\n*Do you have reliable info that any of the above is incorrect? Email me at [aline@interviewing.io](mailto:aline@interviewing.io).*\n\nLimitations and conclusion\n--------------------------\n\nAfter running the numbers and talking to our users, it appears that Facebook eng hiring is largely frozen, with some key exceptions. Google, on the other hand, has frozen extending offers for 2 weeks but is still actively interviewing candidates, except for junior ones, sometimes making decisions about whom to interview and whom to pause on a case-by-case basis. We’ll know more about the Google situation in a few days, once the 2 weeks period runs out.\n\nGiven that both companies are clearly prioritizing senior eng roles, we shipped something we hope will help. On interviewing.io, you can now choose to practice specifically with a Staff-level (or above) engineer from Google or Facebook. If you’re still in process, and especially if you’re interviewing for L6/E6 or above, this is a direct line to the best-calibrated, most experienced interviewer you can get.\n\nWhether you use us to practice or not, these are uncertain times, and there’s a lot of bad information out there. With this post, we tried to offset it by reporting on what we’ve learned from primary sources, our users who are/were actively interviewing at both Google and Facebook. While I can count on one hand (maybe both?) the limitations to our approach (e.g. our scoring system being inaccurate, sampling bias, people not being honest in their survey responses, not enough data, us failing to analyze certain attributes of candidates, and more), when things are uncertain and you have limited information, you have to do the best you can with what you’ve got.\n\nThis analysis started as an internal exercise to help us get some predictability around our usage and our revenue. Then as I started working on it, I realized that people outside of interviewing.io might want to see it too. So I hope you, dear reader, will forgive us our data trespasses and know that we did this work with the best intention in mind – combating misinformation and getting our best attempt at the truth out there.\n\n*If you have reliable information that contradicts what you’ve read in this post, please get in touch with me ([aline@interviewing.io](mailto:aline@interviewing.io)). I commit to making edits as I learn new things. Finally, a huge thank you to everyone who filled out our survey and answered my annoying followup questions.*\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. If you’re interviewing at Amazon, you should read our brand-new [Software engineer’s guide to the Amazon behavioral interview](https://interviewing.io/guides/amazon-leadership-principles). 20% or more of people who pass Amazon’s technical bar end up not getting an offer because they do poorly in the behavioral portion. [↩](#user-content-fnref-1)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/google-facebook-hiring-freeze",
      "author": "",
      "user_id": ""
    },
    {
      "title": "You now need to do 15% better in technical interviews than you did at the start of 2022 (and the bar will keep rising).",
      "content": "interviewing.io is a technical mock interview platform and technical recruiting marketplace, so we have a *ton* of useful data around technical interviewing and hiring. One of the most useful pieces of data in the current climate is the ever-changing technical interview bar – throughout 2022, it’s gotten progressively harder to pass technical interviews, and it’s only going to keep getting harder… because employers are gaining more leverage in the market.\n\nOne might say that because we’re a mock interview platform, publishing this post is self-serving. The reality is that we take no joy in this content. Ever since hiring freezes and layoffs have started dominating the news cycle, we’ve lost about half of our interview volume, many of the prominent employers who were hiring through us as recently as Q2 of this year have paused. So, trust us, we’d much rather be writing about something else while living in a hiring boom. And look, whether we want to see it or not or whether we have vested interest in publishing it or not, the data is the data, and the data doesn’t care. Realistically, neither will the employers who are still interviewing and hiring.\n\nWe hope that what we’re about to share will help you enter an increasingly unforgiving labor market, the likes of which engineers haven’t seen since the early 2000s, with your eyes wide open.\n\nThe state of the world\n----------------------\n\nDespite not wanting to believe it (as Upton Sinclair said, “It is difficult to get a man to understand something, when his salary depends on his not understanding it.”), I’m coming around to the idea that engineers are going to lose significant leverage in the market.\n\n![Chart showing the number of jobs is shrinking and the number of unemployed engineers is growing](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FTech_jobs_are_shrinking_and_the_number_of_unemployed_engineers_is_growing_1_50a288a21d.png&w=1920&q=75 \"Chart: Jobs are shrinking, and the number of unemployed engineers is growing\")\n\nIn the graph above, the blue line is the number of open tech jobs, according to [TrueUp](https://trueup.io). TrueUp is a tech job index and layoff tracker, started by [Amit Taylor](https://www.linkedin.com/in/amittaylor/). One of its limitations is that it tracks jobs at somewhat arbitrarily defined “tech” companies and excludes jobs (even if they’re technical) at non-tech companies (e.g. Walmart… their example, not ours). However, it’s the best aggregation of open jobs over time that I’ve been able to find, and I think it’s as good a proxy as any.\n\nThe red line comes from our [own analysis of tech layoffs](https://interviewing.io/blog/2022-layoffs-engineers-vs-other-departments) (it’s on its own axis because the blue line represents the number of open tech jobs across all disciplines, whereas the red line is engineers only). We recently looked at layoff lists on layoffs.fyi, tagged them by function, and did some corrections for how likely people in different functions were to opt in to those lists. We ultimately learned that engineers comprised about 5% of total layoffs overall (though eng departments were hit harder than we expected, and at companies that did layoffs, about 12% of the eng team was let go). As such, we counted up total layoffs in the US by month and took 5% of that.\n\n**TL;DR The number of open jobs is shrinking, and the number of unemployed engineers is growing.**\n\nHow much will this actually affect hiring practices and how high the bar will actually get?\n\nIt’s hard to say, and I’m not an economist, but I do run an eng hiring marketplace, and we have some proprietary data I’d like to share with you all.\n\nThe rising eng bar\n------------------\n\ninterviewing.io is both a mock interview platform and an eng hiring marketplace – engineers use us for [technical interview practice](https://interviewing.io/mocks), and top performers get fast-tracked at companies.\n\nCompanies can actually interview our top performers anonymously, right on our platform, and leave feedback after each interview. If the candidate passes, they unmask and move to the next step (typically an onsite). Feedback is both quantitative and qualitative, and in addition to telling us if the candidate passed, companies also rate them on technical ability, communication ability, and problem solving ability. Technical ability is the most predictive and is therefore weighed the most heavily in our scoring system.\n\nWhile we’re not contractually allowed to list publicly which companies hire through us, it’s historically been a mix of FAANGs, FAANG-adjacent top tier companies (e.g. ride sharing, file sharing), and up and coming startups. That is to say that we’re confident that our data about the eng bar is indeed representative of what’s happening at the top end of the market.\n\nTo figure out where the bar is and how it’s changed, we averaged the technical scores for successful interviews among senior engineers[1](#user-content-fn-1) over the last few quarters and graphed it against trueup.io’s number of open tech jobs (same as the blue line in the graph above). Below, you can see the results. The shape of the 2 curves surprised us in its uncanniness. **Since January of 2022, after a brief rise in Q2, tech jobs have contracted by 40%. At the same time, after a brief dip in Q2, the bar for a successful technical interview has increased by 10 percentile points (or by about 15%).[2](#user-content-fn-2)**\n\n![The engineering bar keeps rising, as the number of open tech jobs shrinks](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FThe_engineering_bar_keeps_rising_as_the_number_of_open_tech_jobs_shrinks_1_01da313f30.png&w=1920&q=75 \"Chart: Engineering bar keeps rising; as the number of open tech jobs shrinks\")\n\nThe interviewing.io technical interview bar index\n-------------------------------------------------\n\nHere’s our quantification of where the “eng bar” is. We’ll call it the **interviewing.io technical interview bar index** (ITIB index). We used historical data to measure the link between the number of open tech jobs and the typical performance of people who cleared a real technical interview on our platform. As the plot above shows, it’s a negative relationship: when the job pool shrinks, candidates have to perform better in technical interviews.\n\n**Our regression estimates show that for every 50K drop in tech jobs, candidates need to perform about 3 percentiles higher in interviews.** Given the almost halving of jobs that has happened so far in 2022, candidates went from needing to beat about two thirds of candidates who had also gotten to the phone screen stage to needing to beat more than three quarters.\n\nOne caveat to this approach is that interviewing.io probably doesn’t represent the whole eng market for 2 reasons: selection bias among employers and selection bias among candidates. As I mentioned earlier, the employers who hire through us tend to have a high bar and conduct a specific kind of interview (typically algorithmic in the phone screen). Moreover, only people who do really well in mock interviews get to the point where they can talk to real companies, so these percentiles are comparing people who are already very strong performers to one another rather than to everyone in the pool. **However, because they’re relative rather than absolute, we feel reasonably confident that these numbers can be generalized to the overall hiring market.**\n\nThe ITIB index captures this in a single number, measuring the current technical bar in percentile terms. If it’s at 40, you need to be better than 40% of engineers to get a Yes. **Right now, it’s at 78, up from 68 in Q1 of 2022.**\n\nWe’ll be tracking this index on a monthly basis on Twitter. Follow us at [@interviewingio](https://twitter.com/interviewingio).\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. We just looked at senior engineers here because our average user is senior (~7 years of experience), and so that’s where we have the most consistent data. [↩](#user-content-fnref-1)\n2. In the graph called, “The eng bar keeps rising…”, we chose to show raw scores rather than percentiles so that our users could benchmark their scores easily. However, because our raw scores likely don’t mean anything to the general public, we recomputed them as percentiles for the technical interview bar index. Also note that if you try to compute the % difference from the raw scores, you’ll get a lower number, but that’s not the right way to approach it because small differences in code scores amount to big differences in engineer ranking. [↩](#user-content-fnref-2)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/you-now-need-to-do-15-percent-better-in-technical-interviews",
      "author": "",
      "user_id": ""
    },
    {
      "title": "How do I know if I’m ready to interview at FAANG?",
      "content": "Recently, someone asked us how you know you’re ready to succeed in a [FAANG interview](https://interviewing.io/guides/hiring-process) (an interview at Facebook/Amazon/Apple/Netflix/Google)..\n\n![Screenshot of a question posted about interviewing at a FAANG: what metric is crucial for tracking preparedness](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fef35d_2020_11_30_faang_question_c_0b58d67526.webp&w=750&q=75 \"Data-driven interview prep\")\n\nIt’s an interesting question, and one I’m sure many of you job seekers out there are wondering.\n\nInternally, we have our own beliefs, but we wanted to see if we could answer this question more objectively. So we set off on a journey to acquire data to try answering it.\n\nMethodology\n-----------\n\ninterviewing.io helps prospective job candidates practice mock interviews with actual interviewers from the major tech companies like the aforementioned FAANG companies, as well as others like Dropbox, Uber, LinkedIn, and Slack.\n\nAfter each mock interview, candidates are measured on a 1-4 scale against three criteria (technical ability, problem solving, and communication) and are also given an overall hire/no hire rating.\n\n![Screenshot of Interviewing.io interview feeback form](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F5a595_screenshot_2017_11_29_09_13_30_70d1167200.webp&w=1080&q=75 \"Interviewing.io interview feedback form\")\n\nFeedback form filled out by interviewers after an interviewing.io mock interview\n\nIn our analysis, we used these scores to estimate a candidate’s overall skill level, which we would expect to be positively associated with “readiness”.\n\nAdditionally, we needed to know whether a job candidate’s skill level and preparation ultimately resulted in the candidate getting the job. Since most real world interviews happen outside of interviewing.io, we surveyed our users to learn how far through the hiring funnel they progressed at three popular tech companies: Google, Facebook, and Amazon. For each of these companies, at least 150 respondents reported participating in the company’s hiring processes. These were the hiring stages we asked for in the survey:\n\n* Applied\n* Recruiter call\n* Technical phone interview\n* Take-home assignment\n* Onsite\n* Offer\n* Hire\n\nWe also needed to consider variables that could affect a person’s chance of progressing through the hiring funnel, but weren’t necessarily related to “readiness”. In addition to the mock interview feedback ratings mentioned above, we grabbed the following data about users that we collect at interviewing.io:\n\n* How many mock interviews completed on interviewing.io\n* Self-reported career experience level (e.g. junior, intermediate, experienced)\n\nFinally, we asked survey respondents to share data that our platform doesn’t collect but could be associated with a higher chance of landing a job. Here were those other attributes:\n\n* Gender\n* Possessed a computer science degree?\n* How many real tech interviews they’ve attended as interviewee (excludes mock interviews)\n* How the person learned to code\n* How long ago did they last go through a job search\n\nRelationship of respondents’ technical scores to interview success\n------------------------------------------------------------------\n\nIntuitively, you would think that better technical ability would be associated with a higher chance of succeeding at interviews. To see if this intuition holds true, let’s look at the frequency of people passing a phone interview, bucketed by a person’s average technical rating on interviewing.io, and see if a positive correlation exists.\n\nBased on 158 respondents who reported to have progressed at least as far as Google’s phone interview phase, we observed a positive-looking relationship, but maybe not as obvious a relationship as one might expect.\n\n![Chart showing the percentage of respondents Who Passed Google Phone Screen](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F2020_10_28_passed_google_phone_tech_e66b4c760b.webp&w=1200&q=75 \"Percentage of respondents Who Passed Google Phone Screen\")\n\nLooking at respondents who went through the Facebook and Amazon hiring processes, the relationship seemed even less obvious:\n\n![Chart showing percentage of respondents who passed Facebook phone screen](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F2020_10_28_facebook_passed_phone_tech_3906569daf.webp&w=1200&q=75 \"Percentage of Respondents Who Passed Facebook Phone Screen\")\n\n![Chart showing percentage of respondents who passed Amazon phone screen](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F2020_10_28_amazon_pass_phone_tech_ad681a5e2e.webp&w=1200&q=75 \"Percentage of Respondents Who Passed Amazon Phone Screen\")\n\nWhile it would be simple to conclude there exists no relationship between technical ability and passing a phone interview, it seems more likely that we were experiencing selection bias. After all, 60-80% of people progressing past Google/Amazon/Facebook phone screens seems a bit high relative to what one might expect.\n\nCompared to the population of all people on interviewing.io, survey respondents with high average technical scores between 3 and 4 were over-represented relative to those who scored between 1.5 and 2.75.[1](#user-content-fn-1) Maybe non-respondents with lower scores and who failed Google/Facebook/Amazon phone interviews happened to be less likely to respond to our survey.\n\nSo that’s pretty limiting. But let’s keep in mind that all models are wrong, yet some are useful, so perhaps it’s still possible to learn other stuff from the data. If the survey respondents’ technical abilities weren’t obviously associated with interview success, what about other factors?\n\nMore experience with real technical interviews is associated with future interview success\n------------------------------------------------------------------------------------------\n\nWhen digging further, the factor that stood out most was how many real technical interviews the candidate had done in the past. Across all three companies, people who had completed 5 or more real technical interviews tended to have higher rates of passing a phone interview than those who had less real interview experience.\n\n![Chart showing probability of passing Amazon phone screen](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F2020_10_30_amazon_phone_familiar_ac8c45e063.webp&w=1080&q=75 \"Probability of Passing Amazon Phone Screen\")\n\n![Chart showing probability of passing Google phone screen](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F2020_10_30_google_phone_familiar_1_f346104f0c.webp&w=1080&q=75 \"Probability of Passing Google Phone Screen\")\n\n![Chart showing probability of passing Facebook phone screen](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F2020_10_30_facebook_phone_familiar_1_18d0557905.webp%3Fupdated_at%3D2022-11-22T13%3A41%3A23.440Z&w=1080&q=75 \"Probability of Passing Facebook Phone Screen\")\n\nIt’s possible this large effect could have been confounded by other factors, so I iterated on the model to account for these factors, to see whether the effect of prior interview experience on interview success would continue to shine through.\n\nHere were the factors considered:\n\n* Average technical rating\n* Average problem solving rating\n* Average communication rating\n* Number of mock interviews completed on interviewing.io\n* Experience level\n* Gender\n* Did the person have a computer science degree?\n\nFor 171 respondents who went through the Amazon hiring process, a statistically significant association continued to exist between prior technical interview experience and succeeding in a phone interview. Below were the predicted probabilities of passing a phone interview for a typical respondent in this survey (males with a computer science degree and 4+ years of experience, with average technical, communication, and problem solving ability), conditional on their prior technical interview experience.\n\n| **Prior Experience** | **Predicted Probability of Passing Amazon Phone Screen** |\n| --- | --- |\n| 1-4 technical interviews | 65% |\n| 5+ technical interviews | 81% |\n\nFor Facebook we observed a similar effect:\n\n| **Prior Experience** | **Predicted Probability of Passing Facebook Phone Screen** |\n| --- | --- |\n| 1-4 technical interviews | 40% |\n| 5+ technical interviews | 71% |\n\nFinally, for Google applicants we observed that the number of interviewing.io mock interviews completed had the clearest association with interview success, not prior technical interview experience. While mock interviews aren’t exactly the same as real-world technical interviews, they are conducted by the same kinds of people asking similar questions and using similar assessment criteria. Because of this, it seems possible that the effects of mock interviews on interview success could be similar to the effect of real technical interview experience.\n\nBelow was the estimated chance of passing the Google phone interview for the typical survey respondent, conditional on the number of interviewing.io practice interviews attended.\n\n| **Mock interviews on interviewing.io** | **Predicted Probability of Passing Google Phone Screen** |\n| --- | --- |\n| 2 | 69% |\n| 3 | 76% |\n\nLooking at the other variables we included in the model, we observed no other statistically significant relationships with success in phone interviews. It’s possible those relationships actually exist, but based on this sample, we did not observe enough evidence to reject the null hypothesis that no relationship exists.\n\nSo even after accounting for other factors, more technical interview experience was still associated with greater success in phone interviews.\n\nWe also wondered whether similar effects might exist when you get further down the hiring funnel. For example, could prior technical interview experience also improve your chances of receiving an offer after an onsite? We performed a similar analysis to predict the chance of receiving an offer conditional upon attending an onsite interview, but we found no noticeable relationships between the predictors listed above and the probability of receiving an offer. Sample sizes are naturally smaller when looking this far down the funnel, and perhaps the differences between candidates are smaller, which could be harder to detect.\n\nBased on an analysis of this particular group of people, it seems we have a pretty clear answer to the original question of “What metric should be used to know you’re prepared to succeed in a FAANG interview?”\n\nThe answer is to **experience five or more real technical interviews**. Simple, right?\n\nWhy more experience with technical interviews helps you succeed in future technical interviews\n----------------------------------------------------------------------------------------------\n\nI don’t know about you, but I find that answer unsatisfying. That conclusion seems *really* questionable and self-serving, especially when the recommendation comes from a company that offers mock technical interviews as a product.\n\nI don’t blame you, and quite frankly, this result wasn’t what we expected either. The metric of technical interviewing experience is probably just a proxy for some other phenomenon that does correlate with interview success in some explainable way. After all, it’s unlikely that just showing up to five interviews will magically bestow upon you new talents.\n\nSo what is it about having technical interview experience that might be associated with success in future technical interviews? To dig even deeper, we followed up with survey respondents who successfully received an offer with Google, Facebook, or Amazon, and asked what factors they felt contributed to their success.\n\nAs you’d expect, nearly all respondents said practicing technical problems was the foundation for their success. So yes, technical competency matters, and no, past interview experience doesn’t appear to be a substitute for it.\n\nBeyond technical ability, respondents shared anecdotes that hint at why such a relationship might exist, as well as some possible underlying factors that could explain what technical interview experience might proxy for.\n\n### Direct feedback from other people helps you improve quicker\n\nIt’s one thing to get a question wrong in a practice environment, and it’s another thing to get a question wrong when you have an interviewer looking over your shoulder. A few respondents shared instances when they performed poorly in an interview, and explained how those instances influenced their future behavior. These negative experiences clearly highlighted areas in their skillset or presentation that companies tended to rate less favorably.\n\n> *I bombed multiple phone interviews with both Google and Facebook where the questions were about graphs or trees, and the questions were actually trivial. I didn’t have a formal CS background, and I knew that I was weak in those areas.*\n\nOnce identified, respondents addressed those weaknesses through study and repetition, helping them allocate their preparation time more effectively.\n\nFor one job hunter, a past [Google interview](https://interviewing.io/guides/hiring-process/google#google) failure not only helped shore up a specific technical weakness, but also helped the person learn how to maintain a more focused mindset in general, which proved valuable in a future interview with Facebook.\n\n> *Because my main weak spot in my Google onsite was being rusty in data structures and algorithms, I studied key data structures in CLRS, such as heaps and red-black trees. The Facebook interviews did not actually ask me to implement any data structures, but studying data structures helped keep my mind “on the game”.*\n\nObviously, there are many other ways to receive feedback other than getting it directly from another person. For example, respondents also made extensive use of LeetCode and Cracking the Coding Interview-style exercises, and I’m sure many of you out there do too.\n\nHowever, receiving direct feedback from another person seems different for some reason. Whether it’s about social acceptance or proving oneself or something else, feedback from another person seems to be internalized more, which can act as an efficient catalyst for personal growth as long as you keep your mind open to suggestions.\n\nRespondents found direct personal feedback to be very useful, not only from feedback received in real interviews, but also from direct feedback in non-interview settings.\n\n> *Whether it’s interviewing.io or friends asking each other questions or any generic peer interview platform, practice is different than practicing yourself. I found interviewers to be a resource rather than just someone evaluating you which is something you don’t get when practicing yourself.*\n\nWhile real interviews give you unambiguous feedback about your overall performance (i.e. you did or didn’t get the job), you don’t always receive specific feedback about problems you answered well or skills that you exhibited effectively. Simulated interview environments can be uniquely beneficial because they allow job seekers to engage in a two-way dialogue with the interviewer, which can yield more information than a simple “hire” or “no hire” decision.\n\n> *For practice interviews, I worked with a friend who was also interviewing for Google and other FAANG companies. I heavily leaned on interviewing.io interviews from FAANG interviewers during the last stage of my preparation, to make sure I was ready for Google, Amazon and FAANG interviews in particular.*\n\n> *Knowing I received solid (and specific) feedback from FAANG interviewers was a huge help and signal to me that I was ready for Google and Amazon interviews.*\n\nSo maybe the reason why prior technical interview experience correlates with interview success is because interviews happen to be the most common avenue for receiving direct, honest feedback from other people about how you and your skills are perceived.\n\n### You learn how to communicate in an interview setting, which is different than how you communicate in everyday work\n\nTechnical interviews can also require different communication patterns than what you might normally use in a typical workplace or academic environment.\n\nFor example, some respondents believe that it’s not enough to solve a problem correctly. Additionally, you are expected to narrate your thought process as you solve it.\n\n> *Another thing study materials remind people is the need to constantly communicate your thought processes during the coding interview. Having done many interviews (and many more pair programming sessions), this is second nature to me, but I know it’s not second nature to everyone and bears repeating.*\n\nThis implicit expectation can catch some off-guard, particularly if their preparation focused solely on programming exercises.\n\n> *Solving Leetcode on your own is quite different from having to explain your thinking process to someone else.*\n\nThese perspectives echo the advice given by our [interviewer Ian Douglas in his guest blog post](https://interviewing.io/blog/ive-conducted-over-600-technical-interviews-on-interviewing-io-here-are-5-common-problem-areas-ive-seen). All five of Ian’s tips help you improve how you communicate with your interviewer while you’re in the middle of the stressful interview environment. At the end of the day, a debugger won’t be making the final assessment on you, a bunch of human beings will, and the things interviewers look for encompass a lot more than the correctness of the code you write.\n\n> *It is okay to get some guidance from the interviewer. You actually can feel that the interviewers are evaluating more than just your problem solving skills (your communication, how you work as a team, will you be a good person to collaborate with, etc).*\n\nBy doing more technical interviews, you gain a better understanding of the unique interpersonal dynamics that exist during interviews. Those dynamics impact how interviewers assess you, and failing to adapt to those dynamics could obfuscate your true abilities. But once you’ve gained that understanding, you are able to hone specialized interviewing skills like the ones Ian suggests and apply them in future interviews.\n\nOne respondent took this concept to an extreme, re-learning a particular programming language from his or her past to be used primarily for interviewing.\n\n> *Python is much more succinct and expressive than [C++](https://interviewing.io/cplusplus-interview-questions), [Java,](https://interviewing.io/java-interview-questions) or [C#](https://interviewing.io/csharp-interview-questions), which I had used earlier in my career. By not having to write all the braces and semicolons, I free my hands and mind to dig deeper into the problem and better engage with the interviewer. I haven’t made my way up Paul Graham’s succinctness = power hierarchy, but in an interview situation, communication is the most important thing, and Python allows me to communicate with the interviewer better than C++ does.[2](#user-content-fn-2)*\n\nThis tactic probably won’t work for everyone. But there probably does exist a tactic that works for you. The more technical interviews you experience, the more chances you’ll have to discover those tactics.\n\nYou learn a lot about how to interview effectively when you communicate directly with other people\n--------------------------------------------------------------------------------------------------\n\nGoing back to the original question, we said that the metric you should track is how many technical interviews you’ve experienced, because that is what the analysis of this particular set of people outputted.\n\nBut that shouldn’t be your main takeaway. The real learning is to acquire another person’s opinion about your interview performance, because you’ll learn a lot of different things from that person’s feedback.\n\nWe can help you accomplish that here at interviewing.io, with options to receive direct feedback from currently-employed Google or Facebook interviewers, and even help assess specific skills like systems design or front-end development. But as our survey respondents mentioned, there are other ways of receiving that direct feedback.\n\nAs I’ve written before, [interviewing isn’t all that objective](https://interviewing.io/blog/the-eng-hiring-bar-what-the-hell-is-it) because [people aren’t always objective](https://interviewing.io/blog/interview-bias-pseudonyms). Given that interviews are still going to be conducted by other people for the foreseeable future, gaining direct feedback from others appears to be an effective tool for succeeding within the existing system, so you might as well take advantage of it.\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. As an incentive for responding to the survey, we gave a reward of either a $30 credit toward a professional interview or a free peer interview on our platform. If there were any bias, we’d have thought people who scored lower would have been comparatively more likely to respond, rather than less. [↩](#user-content-fnref-1)\n2. The idea that Python might be a better programming language for interviews isn’t totally crazy. Empirically, interviewing.io mock interviews conducted in Python have the second-highest success rate among the most popular programming languages. The highest? C++. So if you’re blindly going by the numbers, the respondent should have stuck to C++. However, chances are that this data also suffers from selection bias: people who know C++ might be different in many ways than people who know Python. Key point: use whatever works for you. For this person, Python happened to work better than C++. [↩](#user-content-fnref-2)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/how-know-ready-interview-faang",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Why we’re pausing our Pay Later Program",
      "content": "If you’ve been following the history of interviewing.io, then you’ll know we took a bunch of sharp turns during our journey. I’ll spare you the long version (you can find that in our [announcement that we’re out of beta](https://interviewing.io/blog/interviewing-io-is-out-of-beta-anonymous-technical-interview-practice-for-all), as well as our [Series A announcement](https://interviewing.io/blog/weve-raised-our-series-a), but the TL;DR is that we used to make money from employers, and mock interviews were free for engineers. Then hiring basically froze when COVID-19 happened, and to survive, we started charging engineers. We didn’t feel great about it, but the company would have shut down otherwise. The silver lining was that this allowed us to open up interviewing.io to engineers of all experience levels and in more locations and also meant that we no longer needed to throttle how many mock interviews people could do.\n\nWhen I announced that we were starting to charge our users, I promised 2 things: (1) We would start a Fellowship program for engineers from non-traditional backgrounds, where they could receive mentorship and practice for free, and (2) We would create a program where you didn’t have to pay for practice till you got a job.\n\nWe launched our [Technical Interview Practice Fellowship](https://interviewing.io/blog/announcing-the-interviewing-io-technical-interview-practice-fellowship) in 2020 and have completed 2 successful cohorts so far. We also launched our beta deferred payment program in September 2020 and ran it for a year to see what portion of users would get jobs and pay us back. When that was successful, we followed it in January 2021 with a much bigger, fully productized version called the Pay Later Program.\n\nUnfortunately, we recently made the very difficult decision to pause our Pay Later Program. In this post, we’ll talk about why we made that call and what we’ll be doing instead to ensure that engineers who can’t afford to pay for practice will still be able to get it. We’ll also explain some things we’ve learned along the way about funnel optimization, some mistakes we made while iterating on this program, and what we’ll do differently when we hopefully unpause it in the future.\n\nBut first, some history.\n\nOur beta deferral program\n-------------------------\n\nEver since we started charging, we wanted to give users a way to de-risk buying interview practice. **However, we didn’t want to subject our users to lengthy and invasive loan applications.** I had seen what some Income Share Agreement (ISA) application processes looked like, and they turned my stomach – many demanded bank account info so they could periodically check on your purchases. In one demo we saw, one of these firms bragged that if a student was spending too much money at Starbucks, it could be a sign that it was time to collect.\n\nWe didn’t want any part of that. We also wanted to distance ourselves from ISAs in general – mock interviews don’t cost tens of thousands of dollars, so with us, you wouldn’t have to give us a portion of your income. You’d just defer paying us for some number of mock interviews, which would usually run you somewhere between $500-$1000 depending on the number of interviews. So, we decided we’d ideally backstop this program ourselves, and as a precursor, we ran the beta deferral program for a year to see what kinds of payback rates we’d get.\n\nBelow are the terms of the original deferral program. While this program was in beta, most of our enrollment flow was in Typeform. It looked like this:\n\n![Deferral program application - page 1 - about the program](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fdeferral_program_application_page_1_a107f6db8f.png&w=1920&q=75 \"Deferral program application - page 1\")\n\nThen we’d ask a few questions, and the final step of the form was to sign a contract, like so:\n\n![Deferral program application - page 2 - agreement terms](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fdeferral_program_application_page_2_2222461614.png&w=1920&q=75 \"Deferral program application - page 2\")\n\n**After 4 months, we saw a 70% payback rate, and after 6 months, it was around 90%.** Between September 2020 and September 2021, over 500 beta users enrolled in this program, and feedback was overwhelmingly positive.\n\nWe felt great about this; however, we knew that the execution of the beta was still clunky. Typeform was our whole UI, and our back-end included a bunch of manual work held together with chewing gum and a massive spreadsheet, where we tracked enrollments, statuses, and paybacks.\n\nWhen we saw that the deferral program was clearly working, we decided to go all in, remove the manual work, and productize it properly.\n\nThe shiny new Pay Later Program\n-------------------------------\n\nThe new Pay Later Program would be a fully automated, shiny version of what we had cobbled together in the beta. The terms would be streamlined, enrollment would be easier, check-in emails would be automatic, and everything would just… *work.* As such, we took a critical look at the deferral program flow and all of the operational pain points and decided to streamline them as much as possible.\n\nBecause so many people in the old deferral program reached out asking to upgrade their algorithmic interviews to something more specialized (e.g., “I want to practice machine learning instead,” “I want an interviewer specifically from Google/Amazon/Facebook”), we decided to make the next version easier for everyone by just giving users interviewing.io credits.\n\nWhen you enrolled in the program, you’d start with $512 or $1024 in credits (your choice) that you could spend on any mock interview or mentorship session that you liked.\n\n**We launched our Pay Later Program at the end of December 2021. As we usually do with new features, we A/B tested a few different variants, with the intent of increasing enrollment as much as possible each time.**\n\nOur first iteration of Pay Later was almost identical to its predecessor, the deferral program, except that instead of having to fill out a Typeform, the enrollment flow was in our app. Here’s how it looked:\n\n![Screenshot of the Deferral Program enrollment form - personal details and credit amount](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FScreen_Shot_2022_09_28_at_5_41_04_PM_efb0b88942.png&w=828&q=75 \"Deferral Program enrollment form - personal details and credit amount\")\n\nThe only out-of-app part of the experience was having to sign a contract in HelloSign. We also had a very wordy explanation of the terms. It was a wall of text, much like the first page of the old Typeform.\n\n**Over time, as we continued to A/B test, we pruned more and more steps, tried different UIs, and even changed up the terms. We were ultimately able to more than *double* enrollment in the Pay Later Program.** The 3 biggest and most impactful changes we made were:\n\n* Removing the contract-signing step entirely. Instead, we just asked users to check a box that said, “I agree to the terms of the Pay Later Program,” where clicking into the terms opened them in a new tab (see the screenshot below). *This was by far the most impactful change.*\n* Making our terms more lax. Instead of charging you 4 months later, if you stayed at your job, we amended the terms to make it so you could get as many extensions as you needed, as long as you were still looking for a job (in good faith).\n* Condensing the terms summary from the wordy bullets in the original Typeform to the blue box below:\n\n![Screenshot of the Deferral Program enrollment form - provide a valid credit card](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fpay_later_program_page_1_55f906a475.png&w=1200&q=75 \"Deferral Program enrollment form - provide a valid credit card\")\n\nThe dangers of funnel hacking\n-----------------------------\n\nOur new Pay Later Program was objectively better for users than our old deferral program for 2 reasons. First, it was more flexible – users got credits they could spend however they wished, instead of 5 algorithmic interviews that they’d have to pay to upgrade. Second, the terms were more generous and user-friendly – you didn’t have to pay us back if you stayed at your old job, as long as you promised us you were still looking, whereas in the old deferral program if you stayed at your current job, you’d have to pay back the full amount after 4 months.\n\n**Despite the offering being objectively better, user feedback was objectively worse.**\n\nIn our quest to increase funnel throughput, we made enrollment too frictionless. **By no longer requiring people to sign a contract and by basically creating 2-click enrollment with an abbreviated TL;DR that glossed over the mechanics of payback** (we’ll send you an email, we’ll charge if we don’t hear back from you, you’re on the hook for the full amount and any unused credits are yours to keep in perpetuity), **we failed to sufficiently explain the program to users, many of whom were justifiably shocked when they got billed.**\n\nReading a contract and signing it forced users to think through whether they were willing to be on the hook for a non-trivial amount of money. When we made enrollment as easy as clicking a few buttons, many users (understandably) no longer read the terms and made assumptions about how the program worked. These assumptions varied from “I thought these interviews were free” to “I thought I only had to pay if I got a job specifically at Google” to “I never signed up for this.”\n\nWith that in mind, of course it made sense that some portion of our users were irate and called us a scam when we emailed to ask if they were ready to pay us back. What sucked is that, even though we created this program with the best of intentions and modified the terms to make them more permissive, because of how we packaged and presented it, we experienced a very different response than from the original deferral program.\n\nThough the majority of users still paid us back within the 4 month period, there was now an angry and disappointed vocal minority, and our ops team began to dread the end of the month, when we’d invariably receive a number of irate emails stating that interviewing.io was a scam.\n\n**I don’t feel great that I got caught up in the funnel hacking and in seeing enrollment numbers increase.** If you’ve read my writing over the years, you’ll know that I take pride in transparency, candor, and clarity. You may also know that our one core value as a company is putting our users (i.e., engineers practicing for interviews) first. I didn’t live up to that this time, and I am sorry.\n\n**Practically speaking, I’ve learned that creating some amount of friction is necessary when you’re asking people to promise to pay you ~$1000 in the future. Removing that friction can create short-term wins but may hurt you (and disappoint your users) in the long run.**\n\nNow, you might expect that if we just revert the flow and bring back the friction, everything will be great again. Unfortunately not.\n\nWhy we’re pausing\n-----------------\n\nIn the current economic climate, we’ve had to take a hard look at our business and, like many other companies, cut our burn and tighten our belts. When we launched the program and committed to backstopping it ourselves, it was with the expectation that hiring would continue at a healthy clip. We anticipated a hiring slowdown in the recession, but multiple FAANGs freezing hiring was beyond what we had predicted – all of our models and assumptions had been based on COVID-era economics. This was something new.\n\nThe reality is that because the payback period is at least 4 months, regardless, as long as we’re running it, we are losing 4 months of runway. In this climate that’s significant.\n\nDespite that, we knew that for some portion of our users, the existence of this program would be the difference between being able to practice and not, so we looked into financing it through a third party. Here’s how we did the math:\n\n1. Estimate our enrollment rate in the Pay Later Program if we go back to the old UI/UX and terms.\n2. Estimate our payback rate for the Pay Later Program moving forward\n3. Cross-reference (1) and (2) with the interest rates we’d be paying to finance the program\n\n**Unfortunately, no matter how we ran the numbers, we couldn’t find a way to make this program ROI positive in the near term, especially with the reduction in enrollment we’d expect upon adding friction back into our enrollment process**, which we decided would be a hard requirement for bringing back the program – we learned to be careful with growth hacks through the mistakes we made with this program, and we will not repeat them.\n\nIn summary, the old flow had a lot of friction, and in the current climate, enrollment rates would be too low to justify its existence. The new flow had significantly more enrollment, but it made it too easy for users to enroll without grokking the terms, and this generated enough negative feedback from users to make us want to kill it, especially given our company’s core value of putting our engineering community first. Finally, even if we kept tweaking the old flow to find the perfect balance, payback rates within a reasonable time window would be too low to be ROI positive when we factor in the cost of financing this program by a 3rd party, which we’d need to do to conserve cash.\n\nIt’s possible we’ll bring this program back in the future, but if we do, it’ll look substantially different from what we released in 2022.\n\nWhat about users who are already enrolled in the Pay Later Program?\n-------------------------------------------------------------------\n\nIf you’re already enrolled, you’re good! Your credits will continue to work in perpetuity, just like we promised, and we’ll honor the terms: You get extensions as long as you haven’t found a job and are still actively looking.\n\nWhat we’re doing instead to help those who can’t afford to pay for practice\n---------------------------------------------------------------------------\n\nWe realize that many of you were depending on this program to be able to practice, especially during a recession. Even though we’ve paused our Pay Later Program, until we hopefully bring it back in the future, we have a few other ways that we can make it easier for you to get the practice you need.\n\n* We still have peer-to-peer practice interviews, where you get matched with other interviewing.io users and work problems together. We introduced these right after COVID-19, and they’re not going anywhere. You get 1 interview with a peer when you join the platform, and after that, you can unlock more by conducting interviews yourself. We’ll be looking at this program to see how we can make it more accessible.\n* We’ll be starting the next cohort of our Fellowship for engineers from non-traditional backgrounds soon.\n* We’re experimenting with a few other ideas that may, together, fill the void created by the Pay Later Program. One of the more promising items is implementing Affirm, which will let our users defer payment for practice or pay it in smaller chunks.\n\nThis was a hard post to write, and I welcome feedback. You can always reach me at [aline@interviewing.io](mailto:aline@interviewing.io).",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/why-were-pausing-our-pay-later-program",
      "author": "",
      "user_id": ""
    },
    {
      "title": "We analyzed thousands of technical interviews on everything from language to code style. Here's what we found.",
      "content": "*Note: Though I wrote most of the words in this post, the legendary [Dave Holtz](https://twitter.com/daveholtz) did the heavy lifting on the data side. See more of his work on [his blog](https://www.daveholtz.net/).*\n\nIf you’re reading this post, there’s a decent chance that you’re about to re-enter the crazy and scary world of technical interviewing. Maybe you’re a college student or fresh grad who is going through the interviewing process for the first time. Maybe you’re an experienced software engineer who hasn’t even thought about interviews for a few years. Either way, the first step in the interviewing process is usually to read a bunch of online interview guides (especially if they’re written by companies you’re interested in) and to chat with friends about their experiences with the interviewing process (both as an interviewer and interviewee). More likely than not, what you read and learn in this first, “exploratory” phase of the interview process will inform how you choose to prepare moving forward.\n\nThere are a few issues with this typical approach to interview preparation:\n\n* Most interview guides are written from the perspective of one company. While Company A may really value efficient code, Company B may place more of an emphasis on high-level problem-solving skills. Unless your heart is set on Company A, you probably don’t want to give too much weight to what they value.\n* People lie sometimes, even if they don’t mean to. In writing, companies may say they’re language agnostic, or that it’s worthwhile to explain your thought process, even if the answer isn’t quite right. However, it’s not clear if this is actually how they act! We’re not saying that tech companies are nefarious liars who are trying to mislead their applicant pool. We’re just saying that sometimes implicit biases sneak in and people aren’t even aware of them.\n* A lot of the “folk knowledge” that you hear from friends and acquaintances may not be based in fact at all. A lot of people assume that short interviews spell doom. Similarly, everyone can recall one long interview after which they’ve thought to themselves, “I really hit it off with that interviewer, I’ll definitely get passed onto the next stage.” In the past, [we’ve seen that people are really bad at gauging how they did in interviews](https://interviewing.io/blog/own-interview-performance). This time, we wanted to look directly at indicators like interview length and see if those actually matter.\n\n**Here at interviewing.io, we are uniquely positioned to approach technical interviews and their outcomes in a data-driven way. This time, we’ve opted for a quick (if not dirty) and quantitative analysis. In other words, rather than digging deep into individual interviews, we focused on easily measurable attributes that many interviews share, like duration and language choice.** In upcoming posts, we’ll be delving deeper into the interview content itself. If you’re new to our blog and want to get some context about how interviewing.io works and what interview data we collect, please take a look at the section called “The setup” below. Otherwise, please skip over that and head straight for the results!\n\nThe setup\n---------\n\n[interviewing.io](https://interviewing.io/) is a platform where people can practice technical interviewing anonymously, and if things go well, unlock the ability to interview anonymously, whenever they’d like, with top companies like Uber, Lyft, and Twitch. The cool thing is that both practice interviews and real interviews with companies take place within the interviewing.io ecosystem. As a result, we’re able to collect quite a bit of interview data and analyze it to better understand technical interviews, the signal they carry, what works and what doesn’t, and which aspects of an interview might actually matter for the outcome.\n\nEach interview, whether it’s practice or real, starts with the interviewer and interviewee meeting in a collaborative coding environment with voice, text chat, and a whiteboard, at which point they jump right into a technical question. Interview questions tend to fall into the category of what you’d encounter in a phone screen for a back-end software engineering role. **During these interviews, we collect everything that happens, including audio transcripts, data and metadata describing the code that the interviewee wrote and tried to run, and detailed feedback from both the interviewer and interviewee about how they think the interview went and what they thought of each other.**\n\nIf you’re curious, you can see what the feedback forms for interviewers and interviewees look like below — in addition to one direct yes/no question, we also ask about a few different aspects of interview performance using a 1-4 scale. We also ask interviewees some extra questions that we don’t share with their interviewers, and one of the things we ask is whether an interviewee has previously seen the question they just worked on.\n\n![Screenshot of Interviewing.io interview feedback form for interviewers](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F6ba25_new_interviewer_feedback_8fa7f982c6.webp&w=1200&q=75 \"Feedback form for interviewers\")\n\nFeedback form for interviewers\n\n![Screenshot of Interviewing.io interview feedback form for interviewees](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F6702c_new_interviewee_feedback_3ae44bf112.webp&w=1200&q=75 \"Feedback form for interviewees\")\n\nFeedback form for interviewees\n\nThe results\n-----------\n\nBefore getting into the thick of it, it’s worth noting that the conclusions below are based on observational data, which means we can’t make strong causal claims… but we can still share surprising relationships we’ve observed and explain what we found so you can draw your own conclusions.\n\n### Having seen the interview question before\n\n*“We’re talking about practice!”* -Allen Iverson\n\nFirst thing’s first. It doesn’t take a rocket scientist to suggest that one of the best ways to do better in interviews is to… practice interviewing. There are a number of resources out there to help you practice, ours among them. One of the main benefits of working through practice problems is that you reduce the likelihood of being asked to solve something you’ve never seen before. Balancing that binary search tree will be much less intimidating if you’ve already done it once or twice.\n\nWe looked at a sample of ~3000 interviews and compared the outcome to whether the interviewee had seen the interview question before. You can see the results in the plot below.\n\n**Unsurprisingly, interviewees who had seen the question were 16.6% more likely to be considered hirable by their interviewer.** This difference is statistically significant (p < 0.001).[1](#user-content-fn-1)\n\n### Does it matter what language you code in?\n\n*“Whoever does not love the language of his birth is lower than a beast and a foul smelling fish.”* -Jose Rizal\n\nYou might imagine that different languages lead to better interviews. For instance, maybe the readability of Python gives you a leg up in interviews. Or perhaps the fact that certain languages handle data structures in a particularly clean way makes common interview questions easier. We wanted to see whether or not there were statistically significant differences in interview performance across different interview languages.\n\nTo investigate, we grouped interviews on our platform by interview language and filtered out any languages that were used in fewer than 5 interviews (this only threw out a handful of interviews). After doing this, we were able to look at interview outcome and how it varied as a function of interview language.\n\nThe results of that analysis are in the chart below. Any non-overlapping confidence intervals represent a statistically significant difference in how likely an interviewee is to ‘pass’ an interview, as a function of interview language. Although we don’t do a pairwise comparison for every possible pair of languages, the data below suggest that generally speaking, **there aren’t statistically significant differences between the success rate when interviews are conducted in different languages**.[2](#user-content-fn-2)\n\nThat said, one of the most common mistakes we’ve observed qualitatively is people choosing languages they’re not comfortable in and then messing up basic stuff like array length lookup, iterating over an array, instantiating a hash table, and so on. This is especially mortifying when interviewees purposely pick a fancy-sounding language to impress their interviewer. Trust us, wielding your language of choice comfortably beats out showing off in a fancy-sounding language you don’t know well, every time.\n\n### Even if language doesn’t matter… is it advantageous to code in the company’s language of choice?\n\n*“God help me, I’ve gone native.”* -Margaret Blaine\n\nIt’s all well and good that, in general, interview language doesn’t seem particularly correlated with performance. However, you might imagine that there could be an effect depending on the language that a given company uses. You could imagine a Ruby shop saying “we only hire Ruby developers, if you interview in Python we’re less likely to hire you.” On the flip side, you could imagine that a company that writes all of their code in Python is going to be much more critical of an interviewee in Python – they know the ins and outs of the language, and might judge the candidate for doing all sorts of “non-pythonic” things during their interview.\n\nThe chart below is similar to the chart which showed differences in interview success rate (as measured by interviewers being willing to hire the interviewee) for [C++,](https://interviewing.io/cplusplus-interview-questions) [Java](https://interviewing.io/java-interview-questions), and [Python](https://interviewing.io/python-interview-questions). However, this chart also breaks out performance by whether or not the interview language is in the company’s stack. We restrict this analysis to C++, Java and Python because these are the three languages where we had a good mixture of interviews where the company did and did not use that language. **The results here are mixed. When the interview language is Python or C++, there’s no statistically significant difference between the success rates for interviews where the interview language is or is not a language in the company’s stack. However, interviewers who interviewed in Java were more likely to succeed when interviewing with a Java shop (p=0.037).**\n\nSo, why is it that coding in the company’s language seems to be helpful when it’s Java, but *not* when it’s Python or C++? One possible explanation is that the communities that exist around certain programming languages (such as Java) place a higher premium on previous experience with the language. Along these lines, it’s also possible that interviewers from companies that use Java are more likely to ask questions that favor those with a pre-existing knowledge of Java’s idiosyncrasies.\n\n### What about the relationship between what language you program in and how good of a communicator you’re perceived to be?\n\n*“To handle a language skillfully is to practice a kind of evocative sorcery.”* -Charles Baudelaire\n\nEven if language choice doesn’t matter that much for overall performance (Java-wielding companies notwithstanding), we were curious whether different language choices led to different outcomes in other interview dimensions. For instance, an extremely readable language, like Python, may lead to interview candidates who are assessed to have communicated better. On the other hand, a low-level language like C++ might lead to higher scores for technical ability. Furthermore, very readable or low-level languages might lead to correlations between these two scores (for instance, maybe they’re a C++ interview candidate who can’t explain at all what he or she is doing but who writes very efficient code). The chart below suggests that there isn’t really any observable difference between how candidates’ technical and communication abilities are perceived, across a variety of programming languages.\n\n![Heatmaps showing communication ability vs. coding skill for different programming languages](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fcodistribution_1_72711c2cab.webp&w=1920&q=75 \"Communication ability vs. coding skill for different programming languages\")\n\n**Furthermore, no matter what, poor technical ability seems highly correlated with poor communication ability – regardless of language, it’s relatively rare for candidates to perform well technically but not effectively communicate what they’re doing (or vice versa)**, largely (and fortunately) debunking the myth of the incoherent, fast-talking, awkward engineer.[3](#user-content-fn-3)\n\n### Interview duration\n\n*“It’s fine when you careen off disasters and terrifyingly bad reviews and rejection and all that stuff when you’re young; your resilience is just terrific.”* -Harold Prince\n\nWe’ve all had the experience of leaving an interview and just feeling like it went poorly. Often, that feeling of certain underperformance is motivated by rules of thumb that we’ve either come up with ourselves or heard repeated over and over again. You might find yourself thinking, “the interview didn’t last long? That’s probably a bad sign… ” or “I barely wrote anything in that interview! I’m definitely not going to pass.” Using our data, we wanted to see whether these rules of thumb for evaluating your interview performance had any merit.\n\nFirst, we looked at the length of the interview. Does a shorter interviewer mean you were such a trainwreck that the interviewer just had to stop the interview early? Or was it maybe the case that the interviewer had less time than normal, or had seen in just a short amount of time that you were an awesome candidate? The plot below shows the distributions of interview length (measured in minutes) for both successful and unsuccessful candidates. **A quick look at this chart suggests that there is no difference in the distribution of interview lengths between interviews that go well and interviews that don’t — the average length of interviews where the interviewer wanted to hire the candidate was 51.00 minutes, whereas the average length of interviews where the interviewer did not was 49.95 minutes. This difference is not statistically significant.**[4](#user-content-fn-4)\n\n### Amount of code written\n\n*“Brevity is the soul of wit.”* -William Shakespeare\n\nYou may have experienced an interview where you were totally stumped. The interviewer asks you a question you barely understand, you repeat back to him or her “binary search what?”, and you basically write no code during your interview. You might hope that you could still pass an interview like this through sheer wit, charm, and high-level problem-solving skills. In order to assess whether or not this was true, we looked at the final character length of code written by the interviewee. The plot below shows the distributions of character length for both successful and unsuccessful. A quick look at this chart suggests that there is a difference between the two — interviews that don’t go well tend to have less code. There are two phenomena that may contribute to this. First, unsuccessful interviewers may write less code to begin with. Additionally, they may be more prone to delete large swathes of code they’ve written that either don’t run or don’t return the expected result.\n\n**On average, successful interviews had final interview code that was on average 2045 characters long, whereas unsuccessful ones were, on average, 1760 characters long.** That’s a big difference! This finding is statistically significant and probably not very surprising.\n\n### Code modularity\n\n*“The mark of a mature programmer is willingness to throw out code you spent time on when you realize it’s pointless.”* -Bram Cohen\n\nIn addition to just look at *how much* code you write, we can also think about the type of code you write. Conventional wisdom suggests that good programmers don’t recycle code – they write modular code that can be reused over and over again. We wanted to know if that type of behavior was actually rewarded during the interview process. In order to do so, we looked at interviews conducted in Python[5](#user-content-fn-5) and counted how many function definitions appeared in the final version of the interview. We wanted to know if successful interviewees defined more functions — while having more function handlers is not the definition of modularity, in our experience, it’s a pretty strong signal of it. As always, it’s impossible to make strong causal claims about this – it might be the case that certain interviewers (who are more or less lenient) ask interview questions that lend themselves to more or fewer functions. Nonetheless, it is an interesting trend to investigate!\n\nThe plot below shows the distribution of the number of Python functions defined for both candidates who the interviewer said they would hire and candidates who the interviewer said they would not hire. A quick look at this chart suggests that there *is* a difference in the distribution of function definitions between interviews that go well and interviews that don’t. Successful interviewees seem to define *more* functions.\n\n**On average, successful candidates interviewing in Python define 3.29 functions, whereas unsuccessful candidates define 2.71 functions. This finding is statistically significant. The upshot here is that interviewers really do reward the kind of code they say they want you to write.**\n\n### Does it matter if your code runs?\n\n*“Move fast and break things. Unless you are breaking stuff, you are not moving fast enough.”* -Mark Zuckerberg  \n*“The most effective debugging tool is still careful thought, coupled with judiciously placed print statements.”* -Brian Kernighan\n\nA common refrain in technical interviews is that interviewers don’t actually care if your code runs – what they care about is problem-solving skills. Since we collect data on the code interviewees run and whether or not that code compiles, we wanted to see if there was evidence for this in our data. Is there any difference between the percentage of code that compiles error-free in successful interviews versus unsuccessful interviews? Furthermore, can interviewees actually still get hired, even if they make tons of syntax errors?\n\nIn order to get at this question, we looked at the data. We restricted our dataset to interviews longer than 10 minutes with more than 5 unique instances of code being executed. This helped filter out interviews where interviewers didn’t actually want the interviewee to run code, or where the interview was cut short for some reason. We then measured the percent of code runs that resulted in errors.[6](#user-content-fn-6) Of course, there are some limitations to this approach – for instance, candidates could execute code that does compile but gives a slightly incorrect answer. They could also get the right answer and write it to stderr! Nonetheless, this should give us a directional sense of whether or not there’s a difference.\n\nThe chart below gives a summary of this data. The x-axis shows the percentage of code executions that were error-free in a given interview. So an interview with 3 code executions and 1 error message would count towards the “30%-40%” bucket. The y-axis indicates the percentage of all interviews that fall in that bucket, for both successful and unsuccessful interviews. Just eyeballing the chart below, one gets the sense that on average, successful candidates run more code that goes off without an error. But is this difference statistically significant?\n\nOn average, successful candidates’ code ran successfully (didn’t result in errors) 64% of the time, whereas unsuccessful candidates’ attempts to compile code ran successfully 60% of the time, and this difference was indeed significant. **Again, while we can’t make any causal claims, the main takeaway is that successful candidates do usually write code that runs better, despite what interviewers may tell you at the outset of an interview.**\n\n### Should you wait and gather your thoughts before writing code?\n\n*“Never forget the power of silence, that massively disconcerting pause which goes on and on and may at last induce an opponent to babble and backtrack nervously.”* -Lance Morrow\n\nWe were also curious whether or not successful interviewees tended to take their time in the interview. Interview questions are often complex! After being presented with a question, there might be some benefit to taking a step back and coming up with a plan, rather than jumping right into things. In order to get a sense of whether or not this was true, we measured how far into a given interview candidates first executed code. Below is a histogram showing how far into interviews both successful and unsuccessful interviewees first ran code. Looking quickly at the histogram, you can tell that successful candidates do in fact wait a bit longer to start running code, although the magnitude of the effect isn’t huge.\n\nMore specifically, **on average, candidates with successful interviews first run code 27% of the way through the interview, whereas candidates with unsuccessful interviews first run code 23.9% of the way into the interview, and this difference is significant**. Of course, there are alternate explanations for what’s happening here. For instance, perhaps successful candidates are better at taking the time to sweet-talk their interviewer. Furthermore, the usual caveat that we can’t make causal claims applies – if you just sit in an interview for an extra 5 minutes in complete silence, it won’t help your chances. Nonetheless, there does seem to be a difference between the two cohorts.\n\nConclusions\n-----------\n\nAll in all, this post was our first attempt to understand what does and does not typically lead to an interviewer saying “you know what, I’d really like to hire this person.” Because all of our data are observational, its hard to make causal claims about what we see. While successful interviewees may exhibit certain behaviors, adopting those behaviors doesn’t guarantee success. Nonetheless, it does allow us to support (or call bullshit on) a lot of the advice you’ll read on the internet about how to be a successful interviewee.\n\nThat said, there is much still to be done. This was a first, quantitative pass over our data (which is, in many ways, a treasure trove of interview secrets), but we’re excited to do a deeper, qualitative dive and actually start to categorize different questions to see which carry the most signal as well as really get our head around 2nd order behaviors that you can’t measure easily by running a regex over a code sample or measuring how long an interview took. If you want to help us with this and are excited to listen to a bunch of technical interviews, drop me a line (at [aline@interviewing.io](mailto:aline@interviewing.io))!\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. All error bars in this post represent a 95% confidence interval. [↩](#user-content-fnref-1)\n2. There were more languages than these on our platform, but the more obscure the language, the less data points we have. For instance, all interviews in [Brainfuck](https://en.wikipedia.org/wiki/Brainfuck) were clearly successful. Kidding. [↩](#user-content-fnref-2)\n3. The best engineers I’ve met have also been legendarily good at breaking down complex concepts and explaining them to laypeople. Why the infuriating myth of the socially awkward, incoherent tech nerd continues to exist, I have absolutely no idea. [↩](#user-content-fnref-3)\n4. For every comparison of distributions in this post, we use both a Fisher-Pitman permutation test to compare the difference in the means of the distributions. [↩](#user-content-fnref-4)\n5. We limit this analysis to interviews in Python because it lends itself particularly well to the identification of function definitions with a simple parsing script. [↩](#user-content-fnref-5)\n6. We calculate this by looking at what percentage of the time the interviewee executed code that resulted in either an error or non-error output contained the term “error” or “traceback.” [↩](#user-content-fnref-6)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/we-analyzed-thousands-of-technical-interviews-on-everything-from-language-to-code-style-here-s-what-we-found",
      "author": "",
      "user_id": ""
    },
    {
      "title": "We have the best technical interviewers on the market. Here's how we do it.",
      "content": "interviewing.io is an anonymous mock interview platform and eng hiring marketplace. Engineers use us for mock interviews, and we use the data from those interviews to surface top performers, in a much fairer and more predictive way than a resume. If you’re a top performer on interviewing.io, we fast-track you at the world’s best companies.\n\nWe make money in two ways: engineers pay us for mock interviews, and employers pay us for access to the best performers. This means that we live and die by the quality of our interviewers in a way that no single employer does, no matter how much they say they care about people analytics or interviewer metrics or training. If we don’t have really well-calibrated interviewers, who also create great candidate experience, we don’t get paid.\n\nIn a recent post, we shared how, over time, we came up with two metrics that, together, tell a complete and compelling story about interviewer quality[: the candidate experience metric and the calibration metric](https://interviewing.io/blog/our-business-depends-on-having-the-best-interviewers-so-we-built-an-interviewer-rating-system-and-you-can-too). In this post, we’ll talk about how to apply our learnings about interviewer quality to your own process. We’ve made a bunch of mistakes so you don’t have to!\n\nBut first, a pessimistic word. The more time I spend in this industry, the more I’m convinced that many top companies end up hiring great engineers not because of their process but despite it. The reality is that top companies are generally not incentivized to care about candidate experience or metrics. As long as you have a revolving door of candidates, both can be pretty meh, and if you’re just a little bit better on calibration than a coin flip and not bad enough to outright scare people away, you’ll be fine.\n\nIn the rare instances where I’ve seen companies really care about interviewer quality, it’s because an eng leader there has taken it upon themselves, as a labor of love. Otherwise, if we’re pragmatic about it, hiring is a cost center, and it’s never going to get the attention that a profit center will. At the same time, some of you reading this post will care enough about interviewer quality and candidate experience to make some changes within your organization.\n\nOn the back of that fervent hope, below is a punch list of things you can do to move the needle:\n\n1. Choose the people who are passionate about interviewing to conduct your interviews, and don’t force the others\n2. Track metrics\n3. Make being a good interviewer part of your culture, and reward it explicitly (i.e., create the right incentive structure)\n4. Make a point of delivering constructive feedback after each interview\n\nChoose the people who are passionate about interviewing to conduct your interviews, and don’t force the others\n--------------------------------------------------------------------------------------------------------------\n\nLook, the reality is that conducting interviews is a polarizing task — people either are *passionate* (so passionate that I put it in italics) about it, or they despise it. Speculating on what perfect storm of past experiences, heredity, and political leanings will make an interviewer fall into one camp or the other is outside the scope of this post. However, after personally interviewing hundreds of interviewers, I am 100% convinced that conducting interviews is polarizing.\n\nWhat does this mean for you? Choose the people who are *passionate* about interviewing to conduct your interviews. It’s easy to figure out who they are. Just ask them. Over the years, I’ve listened to a lot of interview replays. You can immediately identify when an interviewer is checked out. It’s painful. You’ll hear them typing. You’ll hear them go silent for a while. You certainly won’t hear them collaborating with their candidate or gently guiding them away from a perilous rabbit hole. Compare that to a good experience [like this one](https://www.youtube.com/watch?v=EhzF81xV1so) [1](#user-content-fn-1). We’ve all been on the receiving end of an interviewer’s callous indifference, but it’s absolutely preventable. *If someone hates conducting interviews, don’t make them.*\n\nIn our experience, a terrible question in the hands of a skilled, engaged interviewer can become great & carry lots of signal. A great question asked by an unskilled, disconnected interviewer will always be bad… like so:\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/resize_b152080cc8.png)\n\nIf you want to be deliberate about choosing your best interviewers, follow the [best practices we've found after sifting through thousands of interview recordings](https://interviewing.io/blog/best-technical-interviews-common). The best interviewers see every interview as a collaborative exercise with the goal of seeing if they can “be smart together\". The data shows that engagement really does matter.\n\nTrack metrics\n-------------\n\nFirst, please take a look at our [post about the 2 metrics we track](https://interviewing.io/blog/our-business-depends-on-having-the-best-interviewers-so-we-built-an-interviewer-rating-system-and-you-can-too) – the candidate experience score and the calibration score – if you haven’t already. Why track metrics in the first place? There’s an old adage that says you can’t fix what you can’t measure. I think this expression is overused — sometimes intuition is good enough.\n\nThat doesn’t work for interviews though — most of the time, an interview is a private interaction between two people, so an independent observer can’t acquire the anecdotal intuition to form opinions. Things are regularly happening, but unless you measure them, they’re likely happening in a vacuum. Yes, you might do some reverse shadows when training a new interviewer, but once they’re on their own, things tend to go sideways.\n\nThe second reason intuition isn’t good enough here is that you’re often not privy to interview outcomes, either because there’s a latency to them or because the candidate isn’t interviewing for your team… or because if you reject someone at the phone screen stage, you don’t get to find out if they were a false negative.\n\nUnfortunately, as I mentioned earlier, no single company can reproduce the kind of data we have, either about candidate experience or about outcomes. For candidate experience, candidates don’t usually give post-interview feedback in the wild, and when they do, because the interviews aren’t anonymous and the candidate wants to work there, they’re not going to say bad things. And for outcomes, though you’ll know if someone passed an onsite, it’s just one data point for that person (e.g., you don’t know how they did in their interviews at other companies, and you typically won’t know what ended up happening to people you rejected).\n\nHere’s what you can do despite these limitations:\n\n1. Send candidates (truly) anonymous surveys AFTER the interview process is over. Ask about question quality, interviewer engagement, whether they’d want to be on a team with their interviewer, and so on.\n2. Make a point of checking the LinkedIn of candidates you reject to see where they ended up. This will give you a decent proxy for what portion of your candidates you wrongfully rejected.\n3. Determine how well calibrated your interviewers are by identifying [superforecasters](https://interviewing.io/blog/technical-phone-screen-superforecasters), i.e., track the onsite pass rate for all the candidates interviewed by a given interviewer.\n\nOnce you do these three things (or even if you do the first one and just one of the other two), you’ll be able to create a “candidate experience score” and an “accuracy score” for each interviewer. Track these scores over time, and use them to inform interview scheduling frequency and assignment.\n\nCreate the right incentive structure\n------------------------------------\n\nWe just talked about metrics. The right incentive structure gives your metrics teeth.\n\nEvery month, we run an onboarding session for all of our new interviewers where we talk about our standards, our business model, the metrics we track, and why they matter.\n\nDuring these onboarding sessions, we hear one thing over and over from new interviewers — how much better the experience of being an interviewer is when they’re paid for their time. If you read that sentence again, you might find it somewhat ironic. After all, these are people who conduct interviews day in and day out at their jobs, where they are literally being paid for their time.\n\nUnfortunately, even though that’s technically true, engineers don’t really feel like they’re being paid to do interviews because it’s so perfunctory. There’s no reward for doing a good job and, usually, no punishment for doing a bad job — it’s just something you have to do. Worse, at most companies, conducting interviews is an unwelcome distraction from doing real work (i.e., shipping product), and the interviewers who are passionate about doing a good job do it despite their incentive structure and not because of it.\n\nThis is simple to fix. Outside of using the two aforementioned metrics to inform who gets booked and how often, you can take it a step further and actually reward being a good interviewer.\n\nIf you use OKRs (Objectives and Key Results), make staying above a certain candidate experience score one of those OKRs and achieving a certain accuracy score another one.\n\nAlternatively (or additionally), institute a cash bonus for conducting interviews while maintaining good ratings.\n\nWhen establishing criteria for promotions, ensure that no one can become a people manager without being a stellar interviewer — I don’t think I have to convince you that those skill sets overlap.\n\nMake a point of delivering constructive feedback after each interview\n---------------------------------------------------------------------\n\nI know this is a big ask, but trust me, it’s the right thing to do.\n\nYour legal department will probably tell you not to do it, but we did the research, and literally [zero companies (at least in the US) have ever been sued by an engineer](https://interviewing.io/blog/no-engineer-has-ever-sued-a-company-because-of-constructive-post-interview-feedback-so-why-dont-employers-do-it) who received post-interview feedback.\n\nIn cases where the candidate performed well, our data shows that [delivering feedback will increase their odds of ultimately accepting your offer](https://interviewing.io/blog/people-cant-gauge-their-own-interview-performance-and-that-makes-them-harder-to-hire). The hard part, of course, is delivering feedback when the interview goes poorly. No one wants to deal with an angry, defensive candidate, even if they’re not worried about the candidate’s litigiousness.\n\nWhile post-interview feedback is baked into interviewing.io, it’s not something that really happens in the wild, so we had to invent our own best practices for how to do it. Below is a slide taken verbatim from our monthly interviewer onboarding sessions. I hope that sharing it encourages a few of you to try this out. The most important takeaway from this slide is to not focus on the outcome but rather to get specific right away — this will keep your candidate from getting defensive and will set them up to actually hear and internalize your feedback.\n\n![](https://strapi-iio.s3.us-west-2.amazonaws.com/delivering_honest_feedback_synchronously_6d5c927a30.png)\n\nIf you do decide to give post-interview feedback, you can also check out our [detailed playbook for exactly how to do it](https://interviewing.io/blog/why-giving-feedback-good-or-bad-will-help-you-hire) (it’s a much more detailed version of the slide pictured above).\n\nWe have no delusions that you’ll do most of these things. But even if you do a few, you will stand out, your candidates will remember you, and you will feel great about having helped someone. Years ago, back when I was head of recruiting at a startup, because I was a rare breed of recruiter who had also been an engineer, I also conducted technical interviews. In some ways, this setup gave me more freedom because I was the one that had to deal with candidates, end to end – when I took risks, I was only creating problems for myself.\n\nAt some point, I started giving candidates verbal feedback after their interviews, especially in cases where it was close and where the candidate would benefit from a nudge in the right direction (“Hey, make sure you get comfortable with manipulating hash tables.”) Then I took it a step further and started sending them a book.\n\nYears later, many of those users who had failed their technical interview with me became interviewing.io’s first customers.\n\nSo, yeah, if you do any of these things, you generate a lot of good will, and accrued good will over time can make all the difference between a stellar employer brand and a mediocre one. It can also help you edge out companies who pay more or have flashier brands but don’t care about making interviewing better.\n\nFootnotes\n---------\n\nFootnotes\n---------\n\n1. We also have replays of shitty interviews. That’s what made us come up with our metric, but I won’t share them to protect the guilty. [↩](#user-content-fnref-1)",
      "content_type": "blog",
      "source_url": "https://interviewing.io/blog/we-have-the-best-technical-interviewers-heres-how-we-do-it",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Figma’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Hiring Manager Call](#step-2)\n  + [Technical Phone Screen](#step-3)\n  + [Onsite](#step-4)\n  + [Executive Screen](#step-5)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Behavioral](#question-behavioral)\n  + [Project Deep Dive](#question-deep-dive)\n  + [Executive Screen](#question-executive-screen)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nFigma’s Interview Process & Questions\n=====================================\n\n*The info below is based on conversations with Figma engineers.*\n\nPublished:\n\nFigma's Interview Process for Software Engineers: 4-5 Steps\n-----------------------------------------------------------\n\nMid to senior-level engineers interviewing at Figma can expect the following process:\n\n* Recruiter call (30 minutes)\n* Hiring manager call (45 mins)\n* Technical phone screen (1 hour)\n* Onsite (4 hours)\n* Additional “Executive Screen” for Staff-level engineers (1 hour)\n\nStaff-level engineers (or even Seniors on the cusp of Staff) can sometimes get an additional round after the onsite. This is an executive screen, which is essentially a behavioral interview with the director of the team you are interviewing for.\n\n![Figma’s interview process: Recruiter call, Hiring manager call, Technical phone screen, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Figma_s_Company_Process_df7aa4a2c0.png)\n\nFigma’s hiring process is a hybrid. Although you apply for a specific team from the get-go, your interviews will likely not be with the people on your team – interviewers are randomly selected from a pool – except for the hiring manager interview and the executive screening (if applicable).\n\nGeneral tips:\n\n* You’ll get [a guide to Figma’s hiring process](https://www.dropbox.com/scl/fi/4tdq51dnc2ykp347vmphy/Public-General-Candidate-Prep-Doc.pdf?rlkey=mfucvg5hiava8x2xpuz24tiwh&e=4&dl=0) that includes a lot of useful information\n* Their interview process is language-agnostic\n* Brush up on general algorithms and data structures, but be prepared for them to use Figma terminology when they pose the questions\n* They say you can only apply to one role at a time, so pick wisely\n* Try to get a referral; it’s hard to get into the process otherwise\n* Learn as much as you can about Figma’s architecture\n* Express why you like the product and why you want to join–they are on the path to IPO and getting a lot of high-quality applicants\n\nThe entire process takes about 3-4 weeks, but we heard they can move faster if needed.\n\n### Step 1: Recruiter Call\n\nFigma’s recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, why you’re interested in Figma, your understanding of Figma’s value proposition, and what you’re looking for moving forward.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Hiring Manager Call\n\nThis is a pretty standard behavioral call with the hiring manager of the team you are interviewing for. Expect some questions about your background and experience but also some information about the team and what they are working on. You’ll get time to ask your own questions too.\n\n### Step 3: Technical Phone Screen\n\nThis interview is scheduled as a block with the hiring manager call above and will be conducted in CoderPad. The questions will be algorithmic, but they will be set in a Figma context. In other words, they might pose a problem they run into at Figma and ask how you’d solve it with code, or ask you a very standard algorithmic question, but layer in some Figma terminology. It’s a good idea to brush up on Figma’s names for the various components they use. [You can find information here](https://help.figma.com/hc/en-us/articles/360038662654-Guide-to-components-in-Figma). Expect about a medium in terms of LeetCode-level.\n\n### Step 4: Onsite\n\nAt this point, candidates split into different loops depending on the role they are interviewing for e.g., ML, frontend, backend etc. Onsite interview loops also vary slightly depending on the role and seniority, but the below is generally what you’ll get:\n\n* **Coding** (1 hour). This interview will be conducted in CoderPad. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/figma-interview-questions#question-coding).\n* **System design** (1 hour). This interview will be conducted in Figma. For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/figma-interview-questions#question-design).\n* **Second system design** (1 hour) This interview will be conducted in Figma. For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/figma-interview-questions#question-design).\n* **Behavioral** (1 hour). For more info about what questions to expect, see the [Behavioral section below](https://interviewing.io/figma-interview-questions#question-behavioral).\n* **Project deep dive** (1 hour) For more info about what questions to expect, see the [Project Deep Dive section below](https://interviewing.io/figma-interview-questions#question-deep-dive).\n\n### [Staff-level or Close to It] Step 5: Executive Screen\n\nIf you’re interviewing for a Staff level role, or quite senior and on the cusp of Staff, they will do one more round after the onsite. It is a behavioral round with a Director. They might tell you this is a casual chat, but one engineer said:\n\n> *The Director clearly used all the feedback given during the onsite and intentionally drilled into areas that were outside of what I’d covered before. This was very intense!*\n\nTypes of Interview Questions to Expect at Figma\n-----------------------------------------------\n\n### Coding\n\nThis will likely be algorithms and data structures-focused but will skew more practical than typical LeetCode questions. Expect about a LeetCode-medium level of difficulty. An engineer we spoke called described them as\n\n> *Pretty standard algorithms and data structures questions, but they are all Figma-flavored.*\n\nSample questions include:\n\n* Implement a Figma doc with layers, properties, class definitions, and ways to update\n* Print out objects on a 2D canvas in a sequential order, left to right and top to bottom\n\nBelow are the technical topics you’re likely to encounter in Figma interviews. To compile this list, we did two things. First, we spoke to some current and former Figma engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Binary Search](/binary-search-interview-questions)\n\n[Questions   \n& tips](/binary-search-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=binary-search)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Stacks](/stacks-interview-questions)\n\n[Questions   \n& tips](/stacks-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=stacks)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Graphs](/graphs-interview-questions)\n\n[Questions   \n& tips](/graphs-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=graphs)\n\n### System Design\n\nYou will get two system design rounds:\n\n1. Relating to [Figma components](https://help.figma.com/hc/en-us/articles/360038662654-Guide-to-components-in-Figma) and how to design a feature around them\n2. Another that is more focused on your specific role\n\nThey will send you a public guide to Figma components to help with the first system design round, so brush up on that. Components are like classes in Figma. The second round will lean more towards the type of work you will be doing, e.g., back-end or front-end.\n\nAs with the coding interviews, expect something practical here. It will very likely be related to an issue that Figma has encountered before. You might be asked to design a poll system within Figma that can handle multiple concurrent users. Expect to be asked about scaling and storage options.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Behavioral\n\nThis interview will be conducted by a hiring manager from within the same organization but not the team you are interviewing for. It’s going to be pretty standard, with questions about:\n\n* How you’ve handled feedback in the past\n* How you approach conflict\n* Past projects you’re proud of\n\n### Project Deep Dive\n\nFor this interview, you’ll be asked to prepare a few slides on a previous technical project you’ve worked on. It’s a pretty interactive session where they’ll dig into:\n\n* The scale and scope of your role on the project\n* The technical details of the project\n* What issues you ran into\n* What tradeoffs were made\n* What you’d do differently\n\nUnlike some project deep dives we’ve come across, this will likely be with only one interviewer.\n\n### Executive Screen\n\nAll we can say is that this is a behavioral round. It’s hard to say what the Director will ask during this session, but they will be looking for any areas left unturned during the onsite and drilling into them.\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/figma-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Salesforce’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#salesforce-interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Onsite](#step-3)\n    - [Coding](#step-3-coding)\n    - [System Design](#step-3-system)\n    - [Behavioral](#step-3-behavioral)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n* [Common Questions](#common-questions)\n* [Interview Replays](#interview-replays)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nSalesforce’s Interview Process & Questions\n==========================================\n\nPublished:\n\nSalesforce’s Interview Process for Software Engineers: 3 Steps\n--------------------------------------------------------------\n\nMid to senior-level engineers interviewing at Salesforce can expect the following process:\n\n* Recruiter call (30 minutes)\n* Technical phone screen (1 hour)\n* Onsite (usually 4 hours but might be more depending on the team)\n\n![Salesforce’s interview process: Recruiter call, Technical phone screen, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Salesforce_s_Company_Process_60d4a716a8.png)\n\nSalesforce’s process is decentralized, which means that you’re applying to either a specific team or, sometimes, a specific org. Each of their orgs is called [*Something* Cloud](https://rainmakercloud.com/blog/2022/01/18/the-15-types-of-clouds-in-salesforce-and-their-features), so if you’re interviewing at the org level, it might be for, say, Marketing Cloud or IoT Cloud, and which team you end up on will be determined after you interview. That said, how each team or org runs their own process is up to them.\n\nIf you end up applying to an org rather than a specific team, you'll do team matching within the org *before* you get your offer numbers.\n\nFinally, our sources tell us that while Salesforce is pretty quick to move for a company of their size once you actually start interviewing. The time from the first interview to offer can take as little as 3 weeks, but they may take a while to get that first interview scheduled.\n\n### Step 1: Recruiter Call\n\nThis is a typical recruiter call, where they’ll ask about previous experience, relevant projects, and why you’re interested in Salesforce and the specific team/org you’re targeting. They’ll also elaborate on the role and confirm that your experience and expectations are a good match.\n\nOne important thing – if you’re primarily a back-end engineer, let your recruiter know as soon as possible so they route you accordingly. If you don’t, and you end up saying you’re full-stack when you’re really not (or when you haven’t touched front-end work in a long time), you might be in for a rude awakening in your coding rounds when you have to traverse the DOM or build progress bars in JavaScript. Even though “full-stack” can mean any number of different things depending on the company, at Salesforce, they really mean it!\n\nFinally, it’s really important, at this stage, not to reveal your salary expectations, your salary history, or where you are in the process with other companies. We wrote a [detailed post about salary negotiation that lays out exactly what to say when recruiters pressure you to name the first number](https://www.google.com/url?q=https://interviewing.io/blog/negotiate-salary-recruiter&sa=D&source=editors&ust=1687335664411636&usg=AOvVaw2y437xxXrTm2-9NHDgDz1y). Just don’t do it – when you give out information this early in the process, you’re painting future you into a corner.\n\n### Step 2: Technical Phone Screen\n\nThe structure and content of the technical phone screen at Salesforce is team-dependent, as is tooling. One tool you’re likely to see, though, is [Quip](https://www.google.com/url?q=https://quip.com/&sa=D&source=editors&ust=1687335664412626&usg=AOvVaw2T-Hsd2TxcJCXmUQdS56j8) (their answer to Google docs).\n\n### Step 3: Onsite\n\nSalesforce’s onsite lasts roughly 4 hours and consists of the following steps:\n\n* Coding (2 hours)\n* System design (1 hour)\n* Behavioral (1 hour)\n\n#### Coding\n\nSalesforce’s onsite usually has 2 coding interviews. As with the technical phone screen, tooling varies, but you might have to use Quip (their answer to Google docs).\n\nThese are the most important parts of the onsite – both the system design and the behavioral rounds don’t carry as much weight.\n\n#### System Design\n\nThis round lasts 1 hour and may also happen in Quip.\n\n#### Behavioral\n\nThe behavioral round is probably the least important out of all the onsite rounds. You’ll get the usual questions about past projects, your contributions, strengths and weaknesses, and so on.\n\nThat said, Salesforce really values [the concept of Ohana (family)](https://www.google.com/url?q=https://www.salesforceben.com/what-is-the-salesforce-ohana/&sa=D&source=editors&ust=1687335664414005&usg=AOvVaw07sLrrAY1tLtQnQilgsbZ6), and you may get questions about the importance of community, supporting your team, the importance of customer relationships, and so on.\n\nTypes of Interview Questions to Expect at Salesforce\n----------------------------------------------------\n\n### Coding\n\nOne unique aspect of this round is more of an emphasis on specific programming language skills (rather than the type of language-agnostic interview you might see at FAANGs and many FAANG-adjacent companies).\n\nMoreover, if you’re applying for a full-stack role, you may get front-end or back-end questions, and which you get is up to your interviewer. One typical question is, “Build a progress bar in JavaScript”. If you’re primarily a back-end engineer who’s done some front-end work in the distant past, questions like this may be really jarring..\n\nIt’s less likely than at other companies that you’ll get LeetCode-style questions, but you might (again, what you get is team-dependent). However, if you do get them, they’re going to be easy to medium (rather than medium to hard, as you might see with other companies we’ve written about), and our sources tell us that if you practice the top 10 medium-difficulty questions on LeetCode, you’ll be in good shape.\n\nBetween the above and cross-referencing it with our own data-set of mock interviews, if you do get back-end, algorithmic questions in your coding interviews, these are the types of questions you’re likely to encounter:\n\n[Binary Trees](/binary-trees-interview-questions)\n\n[Questions   \n& tips](/binary-trees-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=binary-trees)\n\n[Trees](/trees-interview-questions)\n\n[Questions   \n& tips](/trees-interview-questions)\n\n[Watch 7   \n interview replays](/mocks?technical=trees)\n\n[Linked Lists](/linked-lists-interview-questions)\n\n[Questions   \n& tips](/linked-lists-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=linked-lists)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Hash Tables](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 5   \n interview replays](/mocks?technical=hash-tables)\n\n### System Design\n\nThe kinds of questions you’ll get here vary a lot from team to team and may be anything from database-related questions (including writing SQL) to design questions about CRMs (not surprising, given that Salesforce is a CRM company).\n\nYou should also study up on the internal tooling the org/team you’re interviewing for uses, as you are likely to get questions about that (e.g., [technologies they’ve acquired somewhat recently](https://www.google.com/url?q=https://www.salesforceben.com/the-10-biggest-salesforce-acquisitions/&sa=D&source=editors&ust=1687335664415978&usg=AOvVaw2o8Gsi-xA72ccuDkWEvMRj) like Slack, Quip, Tableau, etc.).\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\nCommon Salesforce Interview Questions\n-------------------------------------\n\nBelow are common questions that interviewers from Salesforce ask on our platform. Since our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\nHARD\n\nData Structures and Algorithms\n\n### [Binary Array Partition](/questions/binary-array-partition)\n\n[Given an array Z of 0s and 1s, divide the array into 3 non-empty parts, such that all of these parts represent the same binary value.](/questions/binary-array-partition)\n\nSalesforce Interview Replays\n----------------------------\n\n[![Make change](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FPython_Salesforce_1_be08e38dc2.png&w=3840&q=75)\n\nSalesforce Interviewer\n\nMake change\n\nEpic Cheetah, a Salesforce engineer, interviewed Mighty Lemming in Python](/mocks/salesforce-python-make-change)\n\nWant to know if you're ready to interview at Salesforce? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/salesforce-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Common Samsung Interview Questions",
      "content": "[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nCommon Samsung Interview Questions\n==================================\n\nBelow are common interview questions that interviewers from Samsung ask in mock interviews on our platform. Because our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\n*We'll add details about Samsung's interview process in the future.*\n\nHARD\n\nData Structures and Algorithms\n\n### [Binary Array Partition](/questions/binary-array-partition)\n\n[Given an array Z of 0s and 1s, divide the array into 3 non-empty parts, such that all of these parts represent the same binary value.](/questions/binary-array-partition)\n\nSamsung Interview Replays\n=========================\n\n[![Bipartite graph](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FPython_Samsung_1_34c5115870.png&w=3840&q=75)\n\nSamsung Interviewer\n\nBipartite graph\n\nKind Dragon, a Samsung engineer, interviewed Ghost Armadillo in Python](/mocks/samsung-python-bipartite-graph)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/samsung-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Common J.P. Morgan Interview Questions",
      "content": "[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nCommon J.P. Morgan Interview Questions\n======================================\n\nBelow are common interview questions that interviewers from J.P. Morgan ask in mock interviews on our platform. Because our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\n*We'll add details about J.P. Morgan's interview process in the future.*\n\nHARD\n\nData Structures and Algorithms\n\n### [Binary Array Partition](/questions/binary-array-partition)\n\n[Given an array Z of 0s and 1s, divide the array into 3 non-empty parts, such that all of these parts represent the same binary value.](/questions/binary-array-partition)\n\nJ.P. Morgan Interview Replays\n=============================\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/jpmorgan-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Bloomberg’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Onsite](#step-3)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Behavioral](#question-behavioral)\n* [Hiring Decisions](#hiring)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nBloomberg’s Interview Process & Questions\n=========================================\n\nPublished:\n\nBloomberg’s interview process is completely decentralized. Each team asks its own questions, focuses on different things, and might have slightly different processes – for instance, some teams don’t ask any system design questions.\n\nLike many other decentralized companies (e.g., [Amazon](https://interviewing.io/guides/hiring-process/amazon)), you can apply to and interview with multiple teams simultaneously. However, all teams have access to the same applicant tracking system, so it’s possible that if you’ve done poorly with many teams already, subsequent teams may decline to interview you. From what we’ve heard, however, there’s no real downside in hedging your bets and talking to as many teams as possible.\n\nIf you do decide to interview with multiple teams, you’ll have to repeat the entire process below every time.\n\nGeneral tips for your Bloomberg interviews:\n\n* Do lots of algorithms and data structures practice\n* Be ready to interview in Java or C++\n* Read up on what Bloomberg does before your interviews, and have a good answer for Why Bloomberg?\n\n![Bloomberg’s interview process: Recruiter call, Technical phone screen, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Bloomberg_s_Company_Process_6b61d23f61.png)\n\nBloomberg’s Interview Process for Software Engineers: 3 Steps\n-------------------------------------------------------------\n\nMid to senior-level engineers interviewing at Bloomberg can expect the following process:\n\n* Recruiter call (30 minutes)\n* Technical phone screen (1 hour)\n* Onsite (4 hours)\n\n### Step 1: Recruiter Call\n\nBloomberg's recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, why you’re interested in Bloomberg, your understanding of Bloomberg’s value proposition, and what you’re looking for moving forward. They’ll also review the specific role you’re applying for to make sure you understand the expectations and requirements and go over the hiring process.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Technical Phone Screen\n\nBloomberg’s technical phone screen lasts about an hour and is conducted on Zoom and HackerRank. Algorithms and data structures are typically the areas of focus. Unlike at many other companies, you will not be running your code during the interview, so it’s OK if it’s not syntactically perfect, as long as it’s efficient and largely correct.\n\n### Step 3: Onsite\n\nThe Bloomberg hiring process is all virtual, and they break the onsite up into individual sections – once you get through the first technical phone screen, you are essentially at the onsite phase but will have to pass each interview to get to the next.\n\nDuring the virtual onsite, you will use Zoom and HackerRank for all the sessions. The topics will vary team by team, so you may get system design questions or algorithms & data structures questions during the technical rounds, or a combination of both. It is possible to only get coding questions and not have to do any system design – the details vary from team to team.\n\n* **Technical interview** (1 hour). Just like in the phone screen, you won’t have to run your code. For more detail about the kinds of questions to expect, see the “Types of Interview Questions to Expect at Bloomberg” section [below.](https://interviewing.io/bloomberg-interview-questions#question-types)\n* **Technical interview** (1 hour). As above.\n* **Behavioral interview** (1 hour). This will be a test of your soft skills by the HR team. Cultural fit is very important to them, so have a well-thought-out answer as to why you want to be there.\n* **Final interview with a senior manager** (1 hour). This will be your final interview and it will be conducted by a senior manager, someone who manages at least 50 engineers at Bloomberg. Some of the questions may be technical, others behavioral. Details depend on both the manager and the team.\n\nTypes of Interview Questions to Expect at Bloomberg\n---------------------------------------------------\n\nEach team has its own questions, and there is no central bank. Some teams only ask coding questions, but others will mix in system design questions. Your questions might be in C++ or Python depending on the team. Most teams at Bloomberg use C++. Senior people should know C++ going in, though Java also works. Not knowing either of these languages isn’t a deal-breaker, but it can be a tiebreaker when deciding between two candidates who did well otherwise.\n\n### Coding\n\nAs there is no central bank of questions, each team and each interviewer will pull questions from elsewhere. This could be from LeetCode, previous companies they’ve worked at, or questions they’ve seen before at Bloomberg.\n\nMostly, you should expect standard algorithms and data structures questions in this round. To figure out what technical topics might come up in your Bloomberg interviews, we did 2 things. First, we spoke to some current and former Bloomberg interviewers in our community. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews. Based on all of the above, here are the types of questions you’re likely to encounter.\n\n[Binary Trees](/binary-trees-interview-questions)\n\n[Questions   \n& tips](/binary-trees-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=binary-trees)\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Linked Lists](/linked-lists-interview-questions)\n\n[Questions   \n& tips](/linked-lists-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=linked-lists)\n\n[Binary Search](/binary-search-interview-questions)\n\n[Questions   \n& tips](/binary-search-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=binary-search)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Depth-First Search (DFS)](/depth-first-search-interview-questions)\n\n[Questions   \n& tips](/depth-first-search-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=depth-first-search)\n\n[Breadth-First Search (BFS)](/breadth-first-search-interview-questions)\n\n[Questions   \n& tips](/breadth-first-search-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=breadth-first-search)\n\n[Sorting](/sorting-interview-questions)\n\n[Questions   \n& tips](/sorting-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=sorting)\n\n### System Design\n\nSystem Design questions at Bloomberg tend to skew practical and tend to be finance-related. For instance, you might be asked questions such as: How would you design a stock exchange?. Unlike other companies, you might be asked to write code during this interview.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Behavioral\n\nThis will be a pretty standard behavioral interview conducted by someone from the HR team. Expect questions about your past experience and some situational scenarios, e.g., *How would you handle conflict in the office?*\n\nIt’s also very important to do your homework and read up on Bloomberg and what they do, so you can effectively answer questions about why you want to work there.\n\n### How Bloomberg Makes Hiring Decisions\n\nTheir interviewing team will sync after each stage of the onsite process and do a debrief. This will determine whether or not you pass through. It’s possible to be sent home after any of the onsite rounds.\n\nWant to know if you're ready to interview at Bloomberg? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/bloomberg-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Anduril’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen / Hiring Manager Call](#step-2)\n  + [Onsite](#step-3)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Behavioral](#question-behavioral)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nAnduril’s Interview Process & Questions\n=======================================\n\n*The info below is based on conversations with Anduril engineers.*\n\nPublished:\n\nAnduril's Interview Process for Software Engineers: 3 Steps\n-----------------------------------------------------------\n\nMid to senior-level engineers interviewing at Anduril can expect the following process:\n\n* Recruiter call (30 minutes)\n* Technical phone screen AND/OR Hiring manager call (1 hour)\n* Onsite (4 hours)\n\n![Anduril’s interview process: Recruiter call, Technical phone screen and/or Hiring manager call, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Anduril_interview_process_9764cd993a.png)\n\nAnduril has a hybrid process, which means that you might interview with a specific team or you might do team matching at the end. One engineer we spoke to who recently went through the process told us:\n\n> *They seem to tailor the interviewing process to what you say you’re interested in at the company during the early rounds, so be careful about what you say you want. Stick to what you’re good at.*\n\nFrom our conversations, it seems that in certain cases you do interview for a specific role, but you could end up on multiple teams depending on how the process goes. If you are interviewing for a specific role, we were told by another engineer that you should:\n\n> *Read up about their product and the relationship between the role and the product, they will ask questions very specific to the role and product.*\n\nSome of the engineers we spoke with had a hiring manager call instead of a technical phone screen, and some got both. More on that below.\n\nGeneral tips:\n\n* They might ask you how comfortable you’d be working in the defense industry.\n* LeetCode practice will help, as their coding problems are at a LeetCode medium level, although they like to add practical elements too.\n* If you’re going for a role that involves robotics, you need to know robotics in detail, particularly for L5/L6 level roles. They will press you to discuss the technical aspects of robotics even if your main question is more coding or system design based.\n* They don’t tend to hire for remote roles.\n\nThe entire process takes about 3-4 weeks, and we’ve heard it’s more enjoyable than the typical FAANG style interview process. From one of our users who recently interviewed with Anduril:\n\n> *Their entire process seemed less formal than FAANG, more personable, more conversational.*\n\nThey seem to be OK to move at your speed through the process. We’ve heard it can be done in as little as two weeks but that they don’t have a problem with you slowing things down.\n\n### Step 1: Recruiter Call\n\nAndurils’s recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, why you’re interested in Anduril, and what you’re looking for moving forward. They’ll also sometimes ask you about your qualms, if any, about working in the defence industry.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Technical Phone Screen / Hiring Manager Call\n\nSometimes you’ll get one or both of these. It seems to depend on the role you are interviewing for.\n\nIf you get the standard technical phone screen, you can expect LeetCode medium-level questions. It will be algorithms and data structures based, but they will put it into an Anduril context, e.g., drone management. One engineer told us:\n\n> *The question is not difficult algorithmically, but the follow-up questions in terms of how to speed and scale it up were trickier.*\n\nIf you end up getting the hiring manager call instead, the objective will be to show you are a fit for the role, technically speaking. They’ll ask questions about your resume and experience before throwing in a coding question that is practical for the role in question.\n\n### Step 3: Onsite\n\nThis will vary slightly by role but here’s what you can expect.\n\n* **Coding** (1 hour). This interview will be conducted in CoderPad. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/anduril-interview-questions#question-coding).\n* **Second coding** (1 hour). For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/anduril-interview-questions#question-coding).\n* **System design** (1 hour). This interview will be conducted in CoderPad or on a whiteboard if you go in person. They also sometimes use the whiteboard feature in Zoom. For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/anduril-interview-questions#question-design).\n* **Behavioral** (1 hour). For more info about what questions to expect, see the [Behavioral section below](https://interviewing.io/anduril-interview-questions#question-behavioral).\n\nTypes of Interview Questions to Expect at Anduril\n-------------------------------------------------\n\n### Coding\n\nThis will likely be algorithms and data structures-focused but will skew more practical than typical LeetCode questions. A sample question we heard was about a collection of robots, where they had tasks to perform, and you needed to keep track of the highest priority tasks. Others involved drone management or graph manipulation\n\nYou will have two coding rounds, and they’ll be quite similar.\n\nBelow are the technical topics you’re likely to encounter in Anduril. To compile this list, we did two things. First, we spoke to some current and former Anduril engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Parsing](/parsing-interview-questions)\n\n[Questions   \n& tips](/parsing-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=parsing)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Graphs](/graphs-interview-questions)\n\n[Questions   \n& tips](/graphs-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=graphs)\n\n[Sorting](/sorting-interview-questions)\n\n[Questions   \n& tips](/sorting-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=sorting)\n\n### System Design\n\nYou might get a pretty industry standard question here, or it could be very practical, depending on the role you are interviewing for. Some of the standard questions we’ve heard are:\n\n* Design Tinyurl\n* Design Tetris\n\nIf you get a more practical round, it might be related to managing drones in the field, where you have to get a signal from the drone and then send it back instructions. If you get a practical question, you need to be prepared to discuss the technologies, hardware included at least at a high level. The rounds for L5/L6 roles seem to skew more practical.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Behavioral\n\nThe behavioral interview will be more conversational and less traditional than you might have seen at other companies. That said, it will still include the questions you’d expect in a behavioral interview. They will go over your resume and ask you about projects you’ve worked on in the past. Expect questions about the projects you’ve worked on and areas like:\n\n* Their scale and scope\n* Technical challenges and how you solved them\n* Conflict that arose and how you handled it\n\nYou might also learn more about the type of projects they are working on for the team you are being matched with or interviewing for. If so you’ll have the chance to ask questions, but you’ll also be asked how you’d see yourself fit into the role.\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/anduril-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Anthropic’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Coding Challenge](#step-2)\n  + [Hiring Manager Call](#step-3)\n  + [Onsite](#step-4)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Behavioral](#question-behavioral)\n* [Hiring Decisions](#hiring)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nAnthropic’s Interview Process & Questions\n=========================================\n\n*The info below is based on conversations with Anthropic engineers.*\n\nPublished:\n\nAnthropic's Interview Process for Software Engineers: 4 Steps\n-------------------------------------------------------------\n\nMid to senior-level engineers interviewing at Anthropic can expect the following process:\n\n* Recruiter call (30 minutes)\n* Coding challenge (60-90 mins)\n* Hiring Manager call (1 hour)\n* Onsite (4 hours)\n\n![Anthropic’s interview process: Recruiter call, Asynchronous coding challenge, Hiring manager call, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Anthropic_interview_process_81881a3598.png)\n\nAt Anthropic, you interview for one of their two orgs: Research or Applied. Your interviewers will all come from that org. There is likely some flexibility in terms of which team in that org you end up on.\n\nGeneral tips:\n\n* Brush up on system design\n* Though Anthropic’s coding questions are algorithmic, they tend to be more practical than the verbatim LeetCode questions that many companies use\n* Use a larger monitor for your coding Challenge – time is of the essence, and a laptop has limited real estate to view and write code\n* They don’t give exploding offers and are very patient through the process\n* They use Python, and you’ll need to be comfortable with it\n\nThe entire process takes about 3-4 weeks, and we’ve heard it’s very well thought out. From one of our users who recently interviewed with Anthropic:\n\n> *This felt so easy and thoughtful compared to all the other companies I interviewed with. They have their shit together. Efficient, thoughtful, won’t waste your time. They move faster than expected!*\n\n### Step 1: Recruiter Call\n\nAnthropic’s recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, why you’re interested in Anthropic, your understanding of Anthropic’s value proposition, and what you’re looking for moving forward. They’ll also explain that they are a B Corp and cover what that means.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Coding Challenge\n\nWe’ve heard that this can be done live or asynchronously. Most of the people we spoke to took a 90-minute take-home assessment in CodeSignal, but Anthropic's website says you might get a 60-minute live assessment, so it seems to be role-dependent. It won’t be a LeetCode-style problem, but will still be pretty straightforward. You will be asked to complete a task that gets progressively more complex. One challenge we heard about was to implement a bank with multiple transaction types. You would have to build the core business logic for the application. We’ve heard that candidates tend to run out of time in this round, so manage your time carefully.\n\nOne engineer who took the take-home assessment told us that:\n\n> *The screening was conducted on Code Signal, consisting of a general specification and a black-box evaluator. There were four levels. The spec would get more complicated at each level, and your code had to pass all the tests at one level to get to the next level. All in two hours.*\n>\n> *The verbal spec was straightforward. No special algorithmic knowledge was needed. But the spec interpretation could only be obtained by repeatedly running your code to see what test cases failed.*\n>\n> *Makes me think the folks who go to the next level are probably good at reading ambiguous specs and trying out theories against black-box graders.*\n\n### Step 3: Hiring Manager Call\n\nThis is mostly a technical call and is usually split into two parts. You will be asked to talk about (1) a project you’ve completed and (2) review code examples in different programming languages. You’ll be asked to detect issues and recognize what task the code is used for.\n\n### Step 4: Onsite\n\nThis will vary slightly by role but here’s what you can expect.\n\n* **Coding** (1 hour). This interview will be conducted in CodeSignal. For more detail about the kinds of questions to expect, see the [Coding section](https://interviewing.io/anthropic-interview-questions#question-coding) below.\n* **System design** (1 hour). This interview will be conducted in the drawing tool of your choice. For more detail about the kinds of questions to expect, see the [System Design section](https://interviewing.io/anthropic-interview-questions#question-design) below.\n* **Second coding (role-specific)**(1 hour). For more detail about the kinds of questions to expect, see the [Coding section](https://interviewing.io/anthropic-interview-questions#question-coding) below.\n* **Behavioral** (1 hour). For more info about what questions to expect, see the [Behavioral section](https://interviewing.io/anthropic-interview-questions#question-behavioral) below.\n\nTypes of Interview Questions to Expect at Anthropic\n---------------------------------------------------\n\n### Coding\n\nThis will likely be algorithms and data structures-focused but will skew more practical than typical LeetCode questions. A sample question we heard was about iterating over and debugging a call stack.\n\nMost coding rounds are carried out in a shared Python environment, so you’ll need to be comfortable with the syntax and standard library. Some roles will incorporate Machine Learning elements but only in one of the coding rounds and you can pass on that if you’d like. You do not have to know anything about machine learning to interview with Anthropic as an engineer.\nBelow are the technical topics you’re likely to encounter in Anthropic interviews. To compile this list, we did two things. First, we spoke to some current and former Anthropic engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews.\n\nFirst, here’s a list of more niche technical topics that are, in our experience, specific to Anthropic:\n\n* Data mutation\n* Concurrency\n\nAnd here are technical topics that you’re likely to find at other companies as well (for these we’ve created detailed write-ups of their own):\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Parsing](/parsing-interview-questions)\n\n[Questions   \n& tips](/parsing-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=parsing)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Sorting](/sorting-interview-questions)\n\n[Questions   \n& tips](/sorting-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=sorting)\n\n### System Design\n\nAs with the other interviews, expect something practical here. It will very likely be related to an issue that Anthropic has encountered before. You might be asked to:\n\n* Design a system that enables a GPT to handle multiple questions in a single thread\n* Design a Claude chat service\n* Design a banking app\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Behavioral\n\nThis will be more conversational and less traditional than you might have seen at other companies. Expect questions about AI in areas like ethics, data protection, safety, the job market and knowledge sharing.\n\n### How Anthropic Makes Hiring Decisions\n\nDecisions are reached by consensus (everyone agrees to hire or no-hire), but in cases where consensus isn’t possible, the hiring manager has final say.\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/anthropic-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Stripe’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Second Recruiter Call](#step-3)\n  + [Onsite](#step-4)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Behavioral](#question-behavioral)\n* [Hiring Decisions](#hiring)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nStripe’s Interview Process & Questions\n======================================\n\nPublished:\n\nStripe's Interview Process for Software Engineers: 4 Steps\n----------------------------------------------------------\n\nMid to senior-level engineers interviewing at Stripe can expect the following process:\n\n* Recruiter call (30 minutes)\n* Technical phone screen with any engineer at the company (1 hour)\n* Second recruiter call (30 minutes)\n* Onsite (5 hours)\n\nStaff-level and above engineers interviewing at Stripe will follow the same basic 3 steps but with some variation at the Onsite stage. We’ll get to that later.\n\n![Stripe's interview process: Recruiter call, Technical phone screen, Recruiter Call, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Stripe_s_Company_Process_938ad18527.png)\n\nStripe has a hybrid process. Their eng team is divided into two orgs: Product and Infrastructure. You interview for a specific org, but your interview loop will have engineers involved from across the company, as well as a hiring manager from the specific org you are interviewing for. There is likely some flexibility in terms of which team in that org you end up on. That discussion usually happens post-onsite. You will have the opportunity to discuss team placement during the onsite interview with the hiring manager.\n\nThe entire process takes about 6 weeks but can be completed in as little as 2 weeks, particularly if you are a referral, according to our sources.\n\n### Step 1: Recruiter Call\n\nStripe’s recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, why you’re interested in Stripe, your understanding of Stripe’s value proposition, and what you’re looking for moving forward. They’ll also review the specific role you’re applying for to make sure you understand the expectations and requirements and go over the hiring process.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. [We’ve written a detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Technical Phone Screen\n\nStripe’s technical phone screen lasts about an hour. You can use either your own IDE and share your screen or you can just use CoderPad. They leave it up to you; whatever you’re most comfortable with.\n\nExpect the unexpected here i.e., they won’t ask standard LeetCode-style questions. We will cover what we know of their question style in the section called \"[Types of Interview Questions to Expect at Stripe](https://interviewing.io/stripe-interview-questions#question-types)\" below.\n\nThe hiring managers at Stripe are encouraged to share all good candidates with other teams, so even if you’re not a good fit for their exact team, they may help you find something else.\n\n### Step 3: Second Recruiter Call\n\nThis is an informational call to help prepare you for the onsite.\n\n### Step 4: Onsite\n\nAt this point, candidates split into different loops depending on the role they are interviewing for e.g., ML, frontend, backend etc. Onsite interview loops also vary slightly depending on the role and seniority, but the below is generally what you’ll get:\n\n* **Coding** (1 hour). This interview will be conducted in either your own IDE with screen-share or in CoderPad. Your choice. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/stripe-interview-questions#question-coding).\n* **System design** (1 hour). This interview will be conducted in either your own drawing tool with screen-share or in Whimsical. Your choice. For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/stripe-interview-questions#question-design).\n* **Bug bash** (1 hour). You’ll be given a piece of code and will have to find the bug(s). They usually pick something that they’ve actually seen before, so it will be a generic version of a real Stripe bug. In this round, they’re looking for you to approach the problem thoughtfully and test different approaches, rather than just barreling into something and hitting a wall\n* **[For roles below Staff] Integrations** (1 hour). You’ll be asked to use the Stripe API here, and it will be based on real-world integrations they've seen their merchant customers create. If you interview in the Integrations org, you will not get this round.\n* **[For Staff roles and above] Presentation** (1 hour). You’ll have to write a one pager about a past project and present this to a staff-level engineer, and one other, more junior engineer who will take notes. You’ll be asked to explain it, and the panel will assess your ability to communicate both via written word and to a live technical audience. They want to test your ability to give business context or rationale for what you did, and your ability to justify technical decisions. They may also ask you what you’d do in situations where constraints changed on the fly. They will assess the scope of the project and probe for your use of soft power to get the project done. This interview is particularly important for leveling – if candidates can’t effectively communicate the business impact of their work, for instance, they may be down-leveled.\n* **Behavioral** (1 hour). This interview will be conducted by a hiring manager or a “Leveler” – someone who interviews across a lot of levels and tries to maintain a consistent bar, similar to a Bar Raiser at [Amazon](https://interviewing.io/guides/hiring-process/amazon). For more info about what questions to expect, see the [Behavioral section below](https://interviewing.io/stripe-interview-questions#question-behavioral).\n\nTypes of Interview Questions to Expect at Stripe\n------------------------------------------------\n\nAll of Stripe’s interview questions are created in-house. They run questions through tests internally via mock interviews. They are tested for fairness and fun-ness. Don’t expect industry standard questions here. Be prepared to encounter something unusual but hopefully interesting! Stripe really values proactivity and independent thinking so their questions are aimed at testing you for those skill sets.\n\nStripe’s questions are language-agnostic but might change a little bit depending on the language used. They playtest the questions internally to figure out if they are easier or more difficult in certain languages, and the interviewer will adjust as needed during your interview.\n\n“Stripe is not for everyone” is something we heard often while writing this piece. You get a lot of freedom to do things you think are important, but that comes with the responsibility to deliver. If you thrive in that kind of culture you’ll do well, but if you like to operate in a more rigid structure with very clear instructions and deliverables, you might not enjoy it there.\n\nStripe does not ask LeetCode questions, and grinding on LeetCode may end up being counterproductive. Instead, they tend to be practical questions that originate with the work that Stripe engineers do every day. This means that while computer science fundamentals matter, questions designed to get at them will not be formulated the same way as LeetCode problems.\n\n### Coding\n\nThis interview will be less algorithms and data structures heavy than the LeetCode-style of interview you’re probably used to. Expect questions like:\n\n* How would you build a simple version of Identity Access Management?\n* How would you blur credit card numbers from logs?\n* Design a rate limiter in any programming language\n\nRegardless of what question you get, they’ll ask about decision-making, time and space complexity, etc. They want to know why you would approach a problem in a certain way, and they look for things that might cause issues with client integrations, etc.\n\nBelow are the technical topics you’re likely to encounter in Stripe interviews. To compile this list, we did two things. First, we spoke to some current and former Stripe engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Search](/search-interview-questions)\n\n[Questions   \n& tips](/search-interview-questions)\n\n[Watch 7   \n interview replays](/mocks?technical=search)\n\n[Parsing](/parsing-interview-questions)\n\n[Questions   \n& tips](/parsing-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=parsing)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n### System Design\n\nThis round focuses mostly on large systems and will involve designing an entire service with scalability, reliability, and usability concerns in mind. What technologies would you use and why? What are you optimizing for? Would you use a relational or non-relational database? How would caching work? And so on.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Behavioral\n\nThis varies by role. For IC roles you’ll be asked about your own experiences. For management roles, you will be asked about how you would handle certain scenarios (e.g., underperforming direct reports, conflict in your org, etc.).\n\nLike Amazon, Stripe has some principles they harken back to when evaluating your responses. They’re called [Operating Principles](https://stripe.com/jobs/culture) (the analog to [Amazon’s Leadership Principles](https://interviewing.io/guides/amazon-leadership-principles)). Unlike Amazon, however, you won’t be asked behavioral questions in other rounds, just like this one.\n\n### How Stripe Makes Hiring Decisions\n\nThe entire onsite panel submits written feedback after the interviews are completed and then meets to discuss. Most of the time, decisions are reached by consensus (everyone agrees to hire or no-hire), but in cases where consensus isn’t possible, the hiring manager has final say.\n\nWant to know if you're ready to interview at Stripe? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/stripe-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Capital One’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#cap-one-interview-process)\n  + [Recruiter Call](#step-1)\n  + [Online Assessment](#step-2)\n  + [Onsite](#step-3)\n    - [Coding](#step-3-coding)\n    - [System Design](#step-3-system)\n    - [Behavioral](#step-3-behavioral)\n    - [Case Study](#step-3-study)\n  + [Team Matching](#step-4)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n* [Common Questions](#common-questions)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nCapital One’s Interview Process & Questions\n===========================================\n\nPublished:\n\nCapital One’s Interview Process for Software Engineers: 4 Steps\n---------------------------------------------------------------\n\nMid to senior-level engineers interviewing at Capital One can expect the following process:\n\n* Recruiter call (30 minutes)\n* Online assessment (via [CodeSignal](https://codesignal.com)) (1.5 hours)\n* Onsite, or as they call it, a “Power Day” (3-4 hours)\n* Team matching\n\n![Capital One’s interview process: Recruiter call, Online assessment, Onsite, Team matching](https://strapi-iio.s3.us-west-2.amazonaws.com/Capital_One_s_Company_Process_07d5de20b0.png)\n\nCapital One’s process is fully centralized. This is a recent change – before that, you had to apply separately to each team. Now, everyone enters the same standardized process, and team matching happens at the end.\n\nOnce you apply, a hiring manager will review your information and coding assessment to decide if they want to invite you for the virtual onsite (aka Power Day). If there are multiple hiring managers interested, you’ll have an opportunity to chat with them after your onsite and make a choice of team.\n\n### Step 1: Recruiter Call\n\nThis is a typical recruiter call, where they’ll ask about previous experience, relevant projects, and why you’re interested in Capital One. They’ll also elaborate on the role and confirm that your experience and expectations are a good match.\n\nIt’s really important, at this stage, not to reveal your salary expectations, your salary history, or where you are in the process with other companies. We wrote a [detailed post about salary negotiation that lays out exactly what to say when recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter). Just don’t do it – when you give out information this early in the process, you’re painting future you into a corner.\n\n### Step 2: Online Assessment\n\nCapital One’s online assessment consists of three LeetCode-style algorithms and data structure questions: one easy, one medium, and one hard, administered via CodeSignal. You get 1.5 hours to complete it.\n\n### Step 3: Onsite aka “Power Day”\n\nCapital One’s onsite lasts roughly 4 hours and consists of the following steps:\n\n* Coding (1 hour)\n* System design (1 hour)\n* Behavioral (1 hour)\n* Case study (1 hour)\n\n#### Coding\n\nThis interview takes about an hour and focuses on algorithms and data structures. You’ll share your screen with the interviewer (usually over Zoom) and code in whatever environment you usually work in. Capital one’s preferred languages are JavaScript, Java, TypeScript, Python, and Go.\n\n#### System Design\n\nThis is a one hour interview. The expectation here is that you’ll be able to go in-depth on your design choices and that your design needs to be as functional as possible, given the interview time allotted. One common mistake we heard about from one of our interviewers at this step is that people get hung up on trying to perfect a small piece of their solution instead of getting to an end-to-end solution that works. With that in mind, make sure that you’re able to articulate your assumptions and call out any shortcomings in your design.\n\n#### Behavioral\n\nThis interview can either happen live, in which case it lasts one hour, or in some cases it’ll be an asynchronous online assessment. In both cases, it’s pretty standard behavioral fare – you’ll get questions with the expectation that you’ll answer using the [STAR format](https://capd.mit.edu/resources/the-star-method-for-behavioral-interviews).\n\n#### Case Study\n\nIn this interview, you’ll be presented with a hypothetical situation and asked to develop a solution to the underlying business problem. This interview evaluates your communication skills and business sense, as well as your ability to think logically and quantitatively. It’s important to be aware of [Capital One’s values](https://www.capitalonecareers.com/culture) and incorporate them into your responses.\n\n### Step 4: Team Matching\n\nOnce you finish your onsite, your interviewers will submit their hire/no hire recommendations. If you were matched with only one team, you’ll be extended an offer. If you matched with multiple teams, you’ll have a brief chat with each of the hiring managers, so that you have an opportunity to decide where to go, at which point the offer will be extended officially.\n\nTypes of Interview Questions to Expect at Capital One\n-----------------------------------------------------\n\n### Coding\n\nThis interview usually has two medium-difficulty LeetCode-style questions.\n\nTo figure out what types of questions to expect in your Capital One interviews, we did two things. First, we spoke to some current and former Capital One interviewers in our community. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews. Based on all of the above, here are the types of questions you’re likely to encounter:\n\n[Binary Trees](/binary-trees-interview-questions)\n\n[Questions   \n& tips](/binary-trees-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=binary-trees)\n\n[Linked Lists](/linked-lists-interview-questions)\n\n[Questions   \n& tips](/linked-lists-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=linked-lists)\n\n[Matrices](/matrices-interview-questions)\n\n[Questions   \n& tips](/matrices-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=matrices)\n\n[Parsing](/parsing-interview-questions)\n\n[Questions   \n& tips](/parsing-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=parsing)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Dynamic Programming](/dynamic-programming-interview-questions)\n\n[Questions   \n& tips](/dynamic-programming-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=dynamic-programming)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Hash Tables](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 5   \n interview replays](/mocks?technical=hash-tables)\n\n[Graphs](/graphs-interview-questions)\n\n[Questions   \n& tips](/graphs-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=graphs)\n\n### System Design\n\nThe focus of this interview depends on the interviewer and your seniority. While there’s a general focus on high-level system design, some interviewers will let you choose the focus (OOD, UI, etc.). The most popular question we’ve heard about is “Design a banking system”.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\nCommon Capital One Interview Questions\n--------------------------------------\n\nBelow are common questions that interviewers from Capital One ask on our platform. Since our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\nHARD\n\nData Structures and Algorithms\n\n### [Binary Array Partition](/questions/binary-array-partition)\n\n[Given an array Z of 0s and 1s, divide the array into 3 non-empty parts, such that all of these parts represent the same binary value.](/questions/binary-array-partition)\n\nWant to know if you're ready to interview at Capital One? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/capital-one-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Senior Engineer's Guide To Microsoft's Interview Process and Questions",
      "content": "We helped write the sequel to \"Cracking the Coding Interview\". [Read 9 chapters for free →](https://bctci.co/free-chapters)",
      "content_type": "other",
      "source_url": "https://interviewing.io/guides/hiring-process/microsoft",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Snap’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#snap-interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Onsite](#step-3)\n    - [Coding](#step-3-coding)\n    - [System Design](#step-3-system)\n    - [Q&A](#step-3-qa)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n* [Common Questions](#common-questions)\n* [Hiring Decisions](#hiring-decision)\n* [Interview Replays](#interview-replays)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nSnap’s Interview Process & Questions\n====================================\n\nPublished:\n\nSnap's Interview Process for Software Engineers: 3 Steps\n--------------------------------------------------------\n\nMid to senior-level engineers interviewing at Snap can expect the following process:\n\n* Recruiter call (1 hour)\n* Technical phone screen (1 hour)\n* Onsite (6 hours)\n\n![Snap’s interview process: Recruiter call, Technical phone screen, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Snap_s_Company_Process_f5523e80b8.png)\n\nSnap’s interview process is fully centralized, which means that everyone enters the same standardized process, and team matching happens at the end.\n\nLike [Amazon](https://interviewing.io/guides/hiring-process/amazon), Snap has interviewers who are trained up to be Bar Raisers, except that they’re called Deciders. Instead of Leadership Principles, they evaluate candidates on their adherence to Snap’s values: “We are Kind, “We are Smart”, and “We are Creative”. Note that Snap’s behavioral evaluations are spread across all of your interviews, so, unlike most companies, there isn’t a dedicated behavioral interview. Make sure to keep the Snap values in mind during your rounds. They’re also looking for product knowledge and passion for Snap in every interview, so make sure you’re familiar with the app and ready to talk about it. Review their values and specifically their engineering values before heading into your interviews!\n\nFinally, Snap has a bar that’s been described to us as “unexpectedly high”, and exactly how high can vary from team to team.\n\n### Step 1: Recruiter Call\n\nSnap’s recruiter call is pretty typical, but in addition to the standard recruiter questions, they may ask you product questions or want to hear about your usage of the app and its features. Be sure to spend some time playing around with their product before going in.\n\nIt’s really important, at this stage, not to reveal your salary expectations, your salary history, or where you are in the process with other companies. We wrote a [detailed post about salary negotiation that lays out exactly what to say when recruiters pressure you to name the first number](https://www.google.com/url?q=https://interviewing.io/blog/negotiate-salary-recruiter&sa=D&source=editors&ust=1687337715912604&usg=AOvVaw2DmZ7rz25IR1Q42FqB380A).\n\n### Step 2: Technical Phone Screen\n\nThe technical phone screen at Snap is a data structures/algorithms interview via HackerRank (their interview tool, not the async assessment tool). Depending on the interviewer, there may also be some light behavioral questions in this round (as we mentioned earlier, behavioral questions will be sprinkled throughout the process).\n\n### Step 3: Onsite\n\nThe onsite at Snap consists of 6 interview rounds with the following steps:\n\n* Coding (4 hours)\n* System design (1 hour)\n* Q&A (30 minutes to 1 hour)\n\n#### Coding\n\nThere are generally four interviews focused on algorithms and data structures. The main thing they’re looking for besides a working solution that actually runs is speed.\n\n#### System Design\n\nThe [system design interview](https://interviewing.io/guides/system-design-interview) at Snap will focus on high-level system design questions and will likely expect a tie into a feature/design relevant to their product.\n\n#### Q&A\n\nOnce you’ve wrapped up your technical interviews, you’ll get the chance to chat with a hiring manager and ask any burning questions you may have about Snap. This round isn’t meant to be an evaluation and does not factor into whether you get an offer.\n\nTypes of Interview Questions to Expect at Snap\n----------------------------------------------\n\n### Coding\n\nSnap's coding questions come from a company-wide question bank and are usually LeetCode-style questions ranging from medium to hard difficulty.\n\nTo figure out what types of questions to expect in your Snap interviews, we did 2 things. First, we spoke to some current and former Snap interviewers in our community. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews. Based on all of the above, here are the types of questions you’re likely to encounter:\n\n[Sets](/sets-interview-questions)\n\n[Questions   \n& tips](/sets-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=sets)\n\n[Parsing](/parsing-interview-questions)\n\n[Questions   \n& tips](/parsing-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=parsing)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Hash Tables](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 5   \n interview replays](/mocks?technical=hash-tables)\n\n[Queues](/queue-interview-questions)\n\n[Questions   \n& tips](/queue-interview-questions)\n\n[Watch 5   \n interview replays](/mocks?technical=queue)\n\n### System Design\n\nTypical questions include:\n\n* Design a chat app\n* Design an ad server\n* Design a photo sharing app\n* Design a document management system\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\nCommon Snap Interview Questions\n-------------------------------\n\nBelow are common questions that interviewers from Snap ask on our platform. Since our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\nEASY\n\nData Structures and Algorithms\n\n### [Reverse a Linked List](/questions/reverse-linked-list)\n\n[Given the head of a linked list, reverse the list and return the new head.](/questions/reverse-linked-list)\n\nMEDIUM\n\nData Structures and Algorithms\n\n### [Copy List With Random Pointers](/questions/copy-list-with-random-pointers)\n\n[Given a linked list with nodes that have an additional pointer referring to another node in the list, return a deep copy of the list.](/questions/copy-list-with-random-pointers)\n\nHow Snap Makes Hiring Decisions\n-------------------------------\n\nOnce you’ve wrapped up your onsite, you’ll be given an offer based on the performance in the technical interviews, decided by your interviewers. Once you receive and accept your offer, you’ll begin the team matching process (given that team matching comes after an offer in their process, we didn’t officially include it as part of the interview).\n\nThis process can take several weeks (or longer in some cases), and consists of multiple calls with different interested hiring managers from across Snap.\n\nSnap Interview Replays\n----------------------\n\n[![Deep copy linked list](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FPython_Snap_1_f3fbacca55.png&w=3840&q=75)\n\nSnap Interviewer\n\nDeep copy linked list\n\nDJ Cyclone, a Snap engineer, interviewed Massively Parallel Hedgehog in Python](/mocks/snap-python-deep-copy-linked-list)\n\n[![Copy list with random pointers](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FC_Snap_2_65608ecd94.png&w=3840&q=75)\n\nSnap Interviewer\n\nCopy list with random pointers\n\nDJ Cyclone, a Snap engineer, interviewed Parallel Prism in C++](/mocks/snap-cplus-plus)\n\n[![Print linked list reverse](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FC_Snap_3_0c0955c3f8.png&w=3840&q=75)\n\nSnap Interviewer\n\nPrint linked list reverse\n\nDJ Cyclone, a Snap engineer, interviewed Epic Rainbow in C++](/mocks/cplusplus-print-linked-list-reverse)\n\nWant to know if you're ready to interview at Snap? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/snap-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Common Wurl Interview Questions",
      "content": "[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nCommon Wurl Interview Questions\n===============================\n\nBelow are common interview questions that interviewers from Wurl ask in mock interviews on our platform. Because our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\n*We'll add details about Wurl's interview process in the future.*\n\nHARD\n\nData Structures and Algorithms\n\n### [Binary Array Partition](/questions/binary-array-partition)\n\n[Given an array Z of 0s and 1s, divide the array into 3 non-empty parts, such that all of these parts represent the same binary value.](/questions/binary-array-partition)\n\nWurl Interview Replays\n======================\n\n[![Evaluate Unix path](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FJava_Wurl_1_80bca19883.png&w=3840&q=75)\n\nWurl Interviewer\n\nEvaluate Unix path\n\nIntrepid Hawk, a Wurl engineer, interviewed Spasmodic Pheasant in Java](/mocks/wurl-java-evaluate-unix-path)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/wurlinc-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Common Pivotal Labs Interview Questions",
      "content": "[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nCommon Pivotal Labs Interview Questions\n=======================================\n\nBelow are common interview questions that interviewers from Pivotal Labs ask in mock interviews on our platform. Because our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\n*We'll add details about Pivotal Labs' interview process in the future.*\n\nMEDIUM\n\nData Structures and Algorithms\n\n### [LRU Cache](/questions/lru-cache)\n\n[Implement an LRU Cache\nLRU = Least recently used cache](/questions/lru-cache)\n\nPivotal Labs Interview Replays\n==============================\n\n[![LRU Cache](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FJavascript_Pivotal_Labs_1_5adce15044.png&w=3840&q=75)\n\nPivotal Labs Interviewer\n\nLRU Cache\n\nFearsome Sandwich, a Pivotal Labs engineer, interviewed Special Chameleon in JavaScript](/mocks/pivotal-labs-javascript-lru-cache)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/pivotal-labs-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Rippling’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Hiring Manager Screen](#step-3)\n  + [Onsite](#step-4)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n* [Hiring Decisions](#hiring)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nRippling’s Interview Process & Questions\n========================================\n\n*The info below is based on conversations with Rippling engineers.*\n\nPublished:\n\nRippling's Interview Process for Software Engineers: 4 Steps\n------------------------------------------------------------\n\nMid to senior-level engineers interviewing at Rippling can expect the following process:\n\n* Recruiter call (15-30 minutes)\n* Technical phone screen (1 hour)\n* Hiring manager screen (1 hour)\n* Onsite (3-4 hours)\n\nGeneral Tips:\n\n* Rippling places a big emphasis on testing and running code during interviews\n* Grinding on LeetCode medium and hard questions will help - you will need to move fast!\n* System design round is used for leveling\n\n![Rippling’s interview process: Recruiter call, Technical phone screen, Hiring manager screen, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Rippling_Company_Process_78740d50a1.png)\n\nRippling has a decentralized hiring process, which means that you interview for a specific role and will most likely be interviewed by people from that team, including a hiring manager. They seem to have recently moved to a decentralized process from a centralized one.\n\nThe entire process takes about 4-6 weeks but can be completed in as little as two weeks. Getting a referral might help you skip certain steps like online assessments and initial recruiter screens, but this seems to vary by team.\n\n### Step 1: Recruiter Call\n\nRippling’s recruiter call lasts up to 30 minutes, and, if you’ve gotten referred in, it isn’t an assessment. The recruiter will introduce you to the company and walk you through the interview process. Without a referral, you may get some light behavioral questions.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Technical Phone Screen\n\nRippling’s technical phone screen lasts about an hour and is conducted in CodePair. It will feature LeetCode-style questions. We will cover what we know of their question style in the section called [“Types of Interview Questions to Expect at Rippling” below](https://interviewing.io/rippling-interview-questions#question-types).\n\n### Step 3: Hiring Manager Screen\n\nThis round can vary depending on team and location. Some hiring managers will use this round to discuss team fit, your background, and the scale and scope of projects you’ve worked on in a general sense.\n\nWe’ve heard that other hiring managers will ask you to present a project you’ve worked on and answer questions about it. You will know in advance. If you have to present a project, it’s wise to create slides. You will be asked about workflows and processes, what you were responsible for, what you learned, and what you might do differently. Either way, you will have time to ask questions, so come prepared.\n\n### Step 4: Onsite\n\n* **Coding** (90 mins). This interview will be conducted in either your own IDE with screen-share or in CodePair. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/rippling-interview-questions#question-coding).\n* **System design** (1 hour). Different teams at Rippling use different tooling here. We’ve even heard of one team asking for sketches to be done on paper! For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/rippling-interview-questions#question-design).\n* **Coding** (1 hour). This round will be in CodePair. For more info about what questions to expect, see the [Coding section below](https://interviewing.io/rippling-interview-questions#question-coding).\n\nTypes of Interview Questions to Expect at Rippling\n--------------------------------------------------\n\nRippling does ask LeetCode-style questions. Most will be LeetCode medium-level, but we have heard of some harder questions being asked.\n\n### Coding\n\nThere are two coding interviews during the onsite.\n\n**The 90-minute coding interview** will have two parts. In the first one, you will be asked to build something. In the second, you will discuss what you built, what you could have done differently, how you would scale it, etc. This interview will be fairly practical. You might be asked to prepare a simple HTTP server and then write some simple REST APIs.\n\n**The 1-hour coding interview** will be LeetCode-style, but the questions will build on each other. You might be asked to design a data structure where you can get objects, insert objects, and get the average of all the objects in the data structure in a given time.\n\nFrom one of our users:\n\n> *“You’ll need to get comfortable with LeetCode medium and hard questions – you won’t have time to figure it out on the day. You need to work at a Meta-like pace.”*\n\nBelow are the technical topics you’re likely to encounter in Rippling interviews. To compile this list, we did two things. First, we spoke to some current and former Rippling engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Binary Trees](/binary-trees-interview-questions)\n\n[Questions   \n& tips](/binary-trees-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=binary-trees)\n\n[Trees](/trees-interview-questions)\n\n[Questions   \n& tips](/trees-interview-questions)\n\n[Watch 7   \n interview replays](/mocks?technical=trees)\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Matrices](/matrices-interview-questions)\n\n[Questions   \n& tips](/matrices-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=matrices)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Stacks](/stacks-interview-questions)\n\n[Questions   \n& tips](/stacks-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=stacks)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Recursion](/recursion-interview-questions)\n\n[Questions   \n& tips](/recursion-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=recursion)\n\n### System Design\n\nYou shouldn’t encounter anything too unusual here. You might be asked to design a news recommendation engine, a shopping recommendation engine, or a file-sharing system. The key here is to recognize the tradeoffs between approaches early and articulate them. Do move fast though, as they’ll ask a lot of questions about scaling. They are looking for breadth and depth of system design experience here, and this interview is often used for leveling.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/rippling-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Common interviewing.io Interview Questions",
      "content": "[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nCommon interviewing.io Interview Questions\n==========================================\n\nBelow are common interview questions that interviewers from interviewing.io ask in mock interviews on our platform. Because our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\n*We'll add details about interviewing.io's interview process in the future.*\n\nMEDIUM\n\nData Structures and Algorithms\n\n### [Integer Replacement](/questions/integer-replacement)\n\n[Given an integer as an input, replace all the digits ‘0’ with ‘5’ in the integer.](/questions/integer-replacement)\n\nMEDIUM\n\nData Structures and Algorithms\n\n### [K Largest Elements](/questions/k-largest-elements)\n\n[Write an efficient program for printing k largest elements in an array. Largest elements are returned in order largest to smallest.](/questions/k-largest-elements)\n\nMEDIUM\n\nData Structures and Algorithms\n\n### [Shuffle String](/questions/shuffle-string)\n\n[Write a function that takes a string as an input and returns a shuffled version of that string then write another function to analyze how well it was shuffled.](/questions/shuffle-string)\n\ninterviewing.io Interview Replays\n=================================\n\n[![String shuffle and analysis](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FPython_interviewing_io_1_d9faa97d4a.png&w=3840&q=75)\n\ninterviewing.io Interviewer\n\nString shuffle and analysis\n\nAdequate Lobster, an interviewing.io engineer, interviewed Stateful Armadillo in Python](/mocks/interviewing.io-python-string-shuffle-and-analysis)\n\n[![Print k largest elements](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FPython_interviewingio_1_9c92b365f4.png&w=3840&q=75)\n\ninterviewing.io Interviewer\n\nPrint k largest elements\n\nThe Incredible Croc, an interviewing.io engineer, interviewed Quantum Cheetah in Python](/mocks/interviewing.io-python-print-k-largest-elements)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/interviewingio-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Instacart’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Onsite](#step-3)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Behavioral](#question-behavioral)\n* [Hiring Decisions](#hiring)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nInstacart’s Interview Process & Questions\n=========================================\n\nPublished:\n\nInstacart's Interview Process for Software Engineers: 3 Steps\n-------------------------------------------------------------\n\nMid to senior-level engineers interviewing at Instacart can expect the following process:\n\n* Recruiter call (30 minutes)\n* Technical phone screen (45 mins)\n* Onsite (3-4 hours)\n\n![Instacart’s interview process: Recruiter call, Technical phone screen, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Instacart_s_Company_Process_7854cd3a14.png)\n\nInstacart has a hybrid hiring process: it’s not fully decentralized or centralized. You usually apply or get brought in for a generic job listing, and then they will determine which team you interview for. Your interviewers will come from all across the org, but the hiring manager for the role you are interviewing for will usually be involved from the start. You can only interview once, but if you don’t get an offer from the team you interviewed with, you may still have a chance with another team.\n\nThe entire process takes about 2-4 weeks.\n\nGeneral advice:\n\n* Instacart doesn’t ask questions that require knowledge of academic or obscure data structures and algorithms. Although questions can still skew LeetCode-y, they won’t be questions that require knowledge of niche trees, dynamic programming, etc. Brush up on the basics, and you should be fine.\n* Make sure you practice system design, as that interview is often make or break and is definitely used for leveling\n\n### Step 1: Recruiter Call\n\nInstacart’s recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, and why you’re interested in Instacart. They’ll also talk about the specific role and team you’ve been matched with.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Technical Phone Screen\n\nInstacart’s technical phone screen lasts about an hour. They use CodeSignal.\n\n### Step 3: Onsite\n\n* **Coding** (45 mins). Like the technical phone screen, this round will be conducted in CodeSignal, and you can use any language that CodeSignal supports. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/instacart-interview-questions#question-coding).\n* **Second coding**. As above.\n* **System design** (45 mins). This section is really important and usually where people fail. You can use any tooling you’d like, e.g., Google Draw or Excalidraw. For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/instacart-interview-questions#question-design).\n* **Behavioral** (45 mins). This interview will be conducted by a hiring manager. For more info about what questions to expect, see the [Behavioral section below](https://interviewing.io/instacart-interview-questions#question-behavioral).\n\nTypes of Interview Questions to Expect at Instacart\n---------------------------------------------------\n\nA lot of Instacart’s questions are created in-house, but you may run into LeetCode-style questions as well.\n\n### Coding\n\nYou’ll be tested for pragmatic coding skills here. You might be asked to write a script to convert dollars to euros, get a question about API optimization, be asked about designing key-value stories, edit distance, expression evaluation, combinatorics, file I/O, or any number of other fairly practical questions.\n\nIn recent years, as Instacart has hired more FAANG alums, the question composition has shifted a bit to include more LeetCode-style questions. That said, even if you do get a LeetCode question, it will likely NOT touch on obscure algorithms or data structures.\n\nBelow are the technical topics you’re likely to encounter in Instacart interviews. To compile this list, we did two things. First, we spoke to some current and former Instacart engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Binary Trees](/binary-trees-interview-questions)\n\n[Questions   \n& tips](/binary-trees-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=binary-trees)\n\n[Matrices](/matrices-interview-questions)\n\n[Questions   \n& tips](/matrices-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=matrices)\n\n[Parsing](/parsing-interview-questions)\n\n[Questions   \n& tips](/parsing-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=parsing)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Stacks](/stacks-interview-questions)\n\n[Questions   \n& tips](/stacks-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=stacks)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Hash Tables](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 5   \n interview replays](/mocks?technical=hash-tables)\n\n[Queues](/queue-interview-questions)\n\n[Questions   \n& tips](/queue-interview-questions)\n\n[Watch 5   \n interview replays](/mocks?technical=queue)\n\n### System Design\n\nSystem design interviews at Instacart matter a lot for leveling. The most common failure mode we heard about was passing the coding portion and then failing on architecture.\n\nSpecifically, if you want to hit at least L6, you NEED to do well in this interview.\n\nThe engineers we spoke to said that in the system design portion, it’s really important to manage your time because there’s typically a lot you can cover with your answer, but they are looking for you to be concise. Also be prepared to justify your design decisions.\n\nYou might be asked to:\n\n* Design a database model for an Instacart-like system\n* Design a system for tracking shipments\n* Design a system for communicating with a payment processor\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Behavioral\n\nInstacart’s behavioral interviews are pretty standard fare. You’ll be asked about past projects and your impact in previous roles. This interview is primarily used for leveling.\n\n### How Instacart Makes Hiring Decisions\n\nHiring decisions are made by rough consensus. There’s a debrief the same day or the day after your interview loop. People vote, but ultimately the decision rests with the hiring manager. From one of the engineers we spoke to:\n\n“*If there’s one strong no, that’s usually the end of it, and a strong yes from someone else probably wouldn’t countermand that.*”\n\nWant to know if you're ready to interview at Instacart? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/instacart-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Roblox’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Onsite](#step-3)\n  + [Bar Raiser Interview](#step-4)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Behavioral](#question-behavior)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nRoblox’s Interview Process & Questions\n======================================\n\n*The info below is based on conversations with Roblox engineers.*\n\nPublished:\n\nRoblox's Interview Process for Software Engineers: 4 Steps\n----------------------------------------------------------\n\nMid to senior-level engineers interviewing at Roblox can expect the following hiring process:\n\n* Recruiter call (30 minutes)\n* Technical phone screen (60-90 mins)\n* Onsite (4-7 hours)\n* Bar raiser interview (1 hour)\n\nAt Roblox, the process is decentralized, with a lot of variation in interview steps depending on the role and seniority you are interviewing for. You interview for one specific role, and most of your interviewers, with one exception, will come from the same team.\n\nThe stages in the process vary depending on the seniority and role you are interviewing for, with managers having a lot of scope to customize the process for their open roles.\n\nGeneral tips:\n\n* Referrals will help you get a recruiter call.\n* They look for a good match between a candidate’s skills and the role, so make sure your resume matches the job description you apply to.\n\nThe entire process takes about 6-8 weeks. By default, they tend to schedule interviews quite far apart compared to other companies, but if you push you can sometimes move things along.\n\n### Step 1: Recruiter Call\n\nIf you get a recruiter call, it’s a great sign that they are genuinely interested. Recruiters will often send resumes to hiring managers before they even take a call so you’ve done well to get this far already. The recruiter call itself lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, why you’re interested in Roblox, and what you’re looking for moving forward.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Technical Phone Screen\n\nThere is some variation in the type of screen you might get here, with more senior roles sometimes getting a system design interview. Most mid to senior-level backend and full stack engineers will get a coding round. It will be conducted in CodeSignal. We will cover the types of questions you can get here in the [Coding section](https://interviewing.io/roblox-interview-questions#question-coding) below.\n\n### Step 3: Onsite\n\nSometimes you’ll get a recruiter prep call before the onsite. You can split the onsite interviews and complete them over a week or two. The mix of interviews can vary by team, and the more senior you are, the more rounds you can get, but this is what most candidates can expect.\n\n* **2 x Coding** (1 hour). These interviews will be conducted in CodeSignal. For more detail about the kinds of questions to expect, see the [Coding section](https://interviewing.io/roblox-interview-questions#question-coding) below.\n* **1-2 x System design** (1 hour). For more detail about the kinds of questions to expect, see the [System Design section](https://interviewing.io/roblox-interview-questions#question-design) below.\n* **1-3 Behavioral** (1 hour). For more info about what questions to expect, see the [Behavioral section](https://interviewing.io/roblox-interview-questions#question-behavior) below.\n\n### Step 4: Bar Raiser Interview\n\nIf you pass the onsite, the final round will be a bar raiser interview. The style of this interview is unique to Roblox. They will give you a technical problem, likely one they have faced at Roblox in the past, but it will be designed to take you out of your comfort zone. It’s not a coding exercise but more a test of your creativity, intelligence, values and ability to navigate tricky situations.\n\nAn example of the type of technical problem might be that people are finding ways to bypass the Roblox moderation system and post inappropriate content.\n\nThe problem [in the Bar Raiser interview] will be something uncomfortable for you - something you can’t assign normal CS best practices to. It’ll be a weird situation.\n\n> *The problem [in the Bar Raiser interview] will be something uncomfortable for you - something you can’t assign normal CS best practices to. It’ll be a weird situation.*\n\nTypes of Interview Questions to Expect at Roblox\n------------------------------------------------\n\n### Coding\n\nRoblox’s coding questions will be LeetCode medium in terms of difficulty but won’t necessarily be LeetCode-style. They will try to place problems in a situational context that is relevant to the type of work you will do there, although you will use algorithms and data structures in your solutions. They aren’t trying to trick you with anything too difficult, but they do want to see clean code. For Roblox, it’s not just about solving the problem, you have to write clean code, run test cases, compile the code etc.\n\nBelow are the technical topics you’re likely to encounter in Roblox interviews. To compile this list, we did two things. First, we spoke to some current and former Roblox engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Matrices](/matrices-interview-questions)\n\n[Questions   \n& tips](/matrices-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=matrices)\n\n[Search](/search-interview-questions)\n\n[Questions   \n& tips](/search-interview-questions)\n\n[Watch 7   \n interview replays](/mocks?technical=search)\n\n[Parsing](/parsing-interview-questions)\n\n[Questions   \n& tips](/parsing-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=parsing)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Dynamic Programming](/dynamic-programming-interview-questions)\n\n[Questions   \n& tips](/dynamic-programming-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=dynamic-programming)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Graphs](/graphs-interview-questions)\n\n[Questions   \n& tips](/graphs-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=graphs)\n\n[Sorting](/sorting-interview-questions)\n\n[Questions   \n& tips](/sorting-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=sorting)\n\n### System Design\n\nRoblox’s system design rounds come in two different styles. One is a standard system design round, and the other is a deep dive into a system you have previously designed or been involved in. If you only get one round during the onsite, it will likely be the former.\n\nFor the standard interview, you might be asked to build something like Minesweeper. For the project deep dive, you will be asked to explain how you built something and what the tradeoffs were.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Behavioral\n\nYou might have as many as 3 behavioral rounds at Roblox. They can be with the engineering manager, director of engineering and/or VP of engineering overseeing the role you are interviewing for. The interview with the engineering manager and director are standard behavioral interviews, where you will be posed situational questions, and can use something like the STAR method to answer them.\n\nThe VP interview is different. It can seem more casual in terms of their tone, but it’s a really critical interview. If you get one of these interviews it’s a good sign that other rounds have gone well, but it is also a round many fail due to treating it casually. They will likely start by going through your resume but will find something to drill down into. They want to hire engineers they can be proud of, so if you have something on your resume, be prepared to justify why it is there and why it makes you a good hire. You might get asked about the project you’re most proud of. Don’t just give them a project that had a successful outcome; show them something that makes you stand out.\n\n> *Don’t be just a nice guy, be impressive, think about your resume and your stories.*\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/roblox-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "OpenAI’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Second Technical Phone Screen](#step-3)\n  + [Onsite](#step-4)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Presentation](#question-presentation)\n  + [Behavioral](#question-behavioral)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nOpenAI’s Interview Process & Questions\n======================================\n\n*The info below is based on conversations with OpenAI engineers.*\n\nPublished:\n\nOpenAI's Interview Process for Software Engineers: 3-4 Steps\n------------------------------------------------------------\n\nMid to senior-level engineers interviewing at OpenAI can expect the following process:\n\n* Recruiter call (30 minutes)\n* Technical phone screen (1 hour)\n* Possible 2nd technical screen or assessment, where the format depends on the role (1 hour)\n* Onsite (4-6 hours)\n\nGeneral tips:\n\n* Your recruiter will give you detailed tips on what to prepare for before some interviews. Take the tips seriously!\n* The coding questions you’ll get are more practical than LeetCode. They are algorithms and data structures questions, but they are actual things that you might do at work.\n* Prepare slides for the presentation part of the onsite (even though it’s not required).\n* Be prepared to discuss ethics and safety in AI. Read their blog!\n\nThe hiring process at OpenAI is decentralized, with a lot of variation in interview steps and styles depending on the role and team you are interviewing for. You will get some sort of technical assessment before the onsite, and it can be anything from a technical phone screen, an asynchronous assessment, or a take-home project. It may even be two separate steps. We’ve heard they use HackerRank for asynchronous coding tests but CoderPad for live interviews. This guide will assume that there will be two live technical phone screens, but your mileage may vary depending on team and role – check out [OpenAI’s own interview guide](https://openai.com/interview-guide) for more info.\n\n![OpenAI’s interview process: Recruiter call, Technical phone screen, 2nd Technical Screen, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Open_AI_interview_process_cd24f67a14.png)\n\nAt OpenAI, there is some flexibility in terms of which role and team you might end up on – you might apply to one role but have them suggest others as you move through the process. Your interviewers will probably come from multiple different teams. Prepare for it to feel chaotic.\n\nFrom one of our users who recently interviewed with Open AI:\n\n> *“Everything they did made them seem wildly disorganized. They didn’t stay in touch through the process. A lot of radio silence.\"*\n\nThe entire process can take 6-8 weeks, but if you put pressure on them throughout you can speed things up, especially if you mention that you have other offers.\n\n### Step 1: Recruiter Call\n\nOpenAI’s recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, why you’re interested in OpenAI, your understanding of OpenAI’s value proposition, and what you’re looking for moving forward.\n\nIn this round, your recruiter will also tell you what to expect in the next round, given how much variability there is in the interview process for different roles and teams. Your recruiter will also be in touch throughout the interview process to prep you for what’s coming up next.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Technical Phone Screen\n\nOpenAI’s first technical phone screen lasts about an hour and is conducted in CoderPad. This is an algorithms and data structures style interview, but the questions are more practical than questions you’d find on LeetCode. We will cover what we know of their question style in the section called [“Types of Interview Questions to Expect at OpenAI”](https://interviewing.io/openai-interview-questions#question-types) below.\n\n### Step 3: Second Technical Phone Screen or Assessment\n\nThe format of this round varies by role and will be more domain-specific than the previous round. For instance, you may get an asynchronous exercise or a take-home assignment. Or you may have to do another technical phone screen.\n\nWe expect that for many of our readers, who tend to be senior, back-end focused engineers, this round will be an architecture interview. See the [System Design section](https://interviewing.io/openai-interview-questions#question-design) below for more details on question types.\n\n*If you have more information about this part of the process or see anything that’s incorrect, please fill in [our form](https://iiosurveys.typeform.com/to/LXqccdMk)*\n\n### Step 4: Onsite\n\nOnsite interview loops also vary slightly depending on the role and seniority, but the below is generally what you’ll get:\n\n* **Behavioral interview with a senior manager** (45 mins) This is a phone call with a hiring manager, typically someone quite senior at the company. For more detail about the kinds of questions to expect, see the [Behavioral section](https://interviewing.io/openai-interview-questions#question-behavioral) below.\n* **Presentation** (45 mins) You’ll be asked to prepare this in advance. For more detail, see the [Presentation section](https://interviewing.io/openai-interview-questions#question-presentation) below.\n* **Coding** (1 hour). This interview will be conducted in your own IDE with screen-share or in CoderPad. Your choice. For more detail about the kinds of questions to expect, see the [Coding section](https://interviewing.io/openai-interview-questions#question-coding) below.\n* **System design** (1 hour). You'll use Excalidraw for this round. For more detail about what kind of questions to expect, see the [System Design section](https://interviewing.io/openai-interview-questions#question-design) below.\n* **Behavioral interview focused on working with teams** (30 mins). This and the hiring manager screen above are both behavioral. For more info about what questions to expect, see the [Behavioral section](https://interviewing.io/openai-interview-questions#question-behavioral) below.\n\nTypes of Interview Questions to Expect at OpenAI\n------------------------------------------------\n\nAll of OpenAI’s interview questions have a degree of practicality and are usually language agnostic.\n\n### Coding\n\nOpenAI’s onsite coding interviews will be more practical than many companies’. Although you will still get algorithmic questions, they are going to be about stuff you might actually do at work.\n\n> *“You’re not going to get questions on string manipulation.”*\n\nIn these interviews, they are looking for your ability to write code that is going to be fast enough now but flexible enough to scale and adapt in the future.\n\nYou can choose the language for the coding rounds, and the questions are picked based on whichever language you choose.\n\nBelow are the technical topics you’re likely to encounter in OpenAI interviews. To compile this list, we did two things. First, we spoke to some current and former OpenAI engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\nFirst, here’s a list of more niche technical topics that are, in our experience, specific to OpenAI:\n\n* Time-based data structures\n* Versioned data stores\n* Coroutines in your chosen language (multithreading, concurrency, etc.)\n* object-oriented programming concepts (abstract classes, iterator classes, inheritance, etc.)\n\nAnd here are technical topics that you’re likely to find at other companies as well (for these we’ve created detailed write-ups of their own):\n\n[Trees](/trees-interview-questions)\n\n[Questions   \n& tips](/trees-interview-questions)\n\n[Watch 7   \n interview replays](/mocks?technical=trees)\n\n[Binary Search](/binary-search-interview-questions)\n\n[Questions   \n& tips](/binary-search-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=binary-search)\n\n[Graphs](/graphs-interview-questions)\n\n[Questions   \n& tips](/graphs-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=graphs)\n\n[Depth-First Search (DFS)](/depth-first-search-interview-questions)\n\n[Questions   \n& tips](/depth-first-search-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=depth-first-search)\n\n[Breadth-First Search (BFS)](/breadth-first-search-interview-questions)\n\n[Questions   \n& tips](/breadth-first-search-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=breadth-first-search)\n\n[Recursion](/recursion-interview-questions)\n\n[Questions   \n& tips](/recursion-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=recursion)\n\n### System Design\n\nYou may get two system design rounds during the interview loop, one before the onsite and one during.\n\nIf you get one before the onsite, expect something practical – you might be asked to design Yelp, Foursquare, Twitter, or a notifications system.\n\nIn this round, they probe for depth of knowledge and will ask a bunch of follow-up questions. It’s also best not to name-drop names of tools:\n\n> *“If you call out any specific technologies during this round, be prepared to go into detail about them! It may be best not to bring up specific examples as they seem to like drilling into the pros and cons of your choice.”*\n\nAlthough a more domain-specific interview may pop up for some roles, most engineers will get another system design round during the onsite. You will likely be prepped well for the topic of this interview, so pay attention to any tips they give you beforehand. If you got a system design round before the onsite, the style of question that you get in the onsite round will be very similar, though they may ask you to get into more detail.\n\nWe have heard that OpenAI might ask you to code in this interview. One user told us that they designed a solution to the problem that was posed to them but were then asked to code up a new solution using a different method.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Presentation\n\nYou’ll be asked to present a project you worked on to a senior manager. You won’t specifically be asked to prepare slides, but it’s a very good idea to do so. Be prepared to discuss the technical and business aspects and impact of the project, as well as your level of contribution, what tradeoffs were made, what other team members were involved, and what everyone’s responsibilities were. You may get some behavioral questions about how you worked with the team.\n\n### Behavioral\n\n#### Senior Manager Call\n\nThis is often with someone pretty high up at OpenAI. Our sources tell us it can be an interesting call. Although a lot of the questions will be standard, you may also be asked to delve deeper into something on your resume that catches the eye.\n\nFrom one source who interviewed with OpenAI recently:\n\n> *\"This interview was with John Rizzo, who is a well-known employee at OpenAI. It wasn’t an intense call, and John asked me some great questions about my background. It felt like he might have had veto power, but I enjoyed the call.\"*\n\nIt’s also a good idea to read OpenAI’s blog, particularly any articles that discuss ethics and safety in AI. They want to know that you’ve thought about the topic, so prepare yourself!\n\n#### Working with Teams\n\nThis is another type of behavioral interview you might encounter. Expect questions about times:\n\n* You had to work cross-functionally across teams\n* You’ve experienced conflict between teams or roles\n* You’ve had competing ideas to move a project forward within your own team.\n\nAll of these questions are meant to get at your ability to work collaboratively.\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/openai-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Common Walmart Interview Questions",
      "content": "[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nCommon Walmart Interview Questions\n==================================\n\nBelow are common interview questions that interviewers from Walmart ask in mock interviews on our platform. Because our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\n*We'll add details about Walmart's interview process in the future.*\n\nMEDIUM\n\nData Structures and Algorithms\n\n### [Permutation in String](/questions/permutation-in-string)\n\n[Given two strings s1 and s2, return true if s2 contains a permutation of s1, or false otherwise.](/questions/permutation-in-string)\n\nWalmart Interview Replays\n=========================\n\n[![Permutation in string](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FPython_Walmart_1_218c445c1f.png&w=3840&q=75)\n\nWalmart Interviewer\n\nPermutation in string\n\nMythic Unicorn, a Walmart engineer, interviewed Phantom Storm in Python](/mocks/walmart-python-permutation-in-string)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/walmart-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Airbnb’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#airbnb-interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Onsite](#step-3)\n    - [Coding](#step-3-coding)\n    - [System Design](#step-3-system)\n    - [Behavioral](#step-3-behavioral)\n    - [Culture Fit](#step-3-culturefit)\n  + [Team Matching](#step-4)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n* [Common Questions](#common-questions)\n* [Hiring Decisions](#hiring-decision)\n* [Interview Replays](#interview-replays)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nAirbnb’s Interview Process & Questions\n======================================\n\nPublished:\n\nAirbnb's Interview Process for Software Engineers: 4 Steps\n----------------------------------------------------------\n\nMid to senior-level engineers interviewing at Airbnb can expect the following process\n\n* Recruiter call (30 minutes)\n* Technical phone screen (1 hour)\n* Onsite (5-7 hours)\n* Team matching\n\n![Airbnb’s interview process: Recruiter call, Technical phone screen, Onsite, Team matching](https://strapi-iio.s3.us-west-2.amazonaws.com/Airbnb_s_Company_Process_8e67ab3109.png)\n\nAirbnb’s interview process is fully centralized, which means that everyone enters the same standardized process, and team matching happens at the end.\n\nFor most levels of engineering at Airbnb, the process and questions are the same, but your answers are graded differently depending on your level. There is also a huge emphasis on culture fit, much more so than at most other companies of their size and stage.\n\n### Step 1: Recruiter Call\n\nAirbnb’s recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, why you’re interested in Airbnb, and what you’re looking for moving forward. They’ll also review the specific role you’re applying for to make sure you understand the expectations and requirements.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Technical Phone Screen\n\nGenerally, there’s only one technical phone screen, but if your interviewer didn’t get enough signal in the first one, you may be asked to do one more. Airbnb uses [CoderPad](https://coderpad.io) to conduct technical interviews, and in these interviews you’re expected to write working code and run it – pseudocode is not allowed, and the expectation is that you’re extremely proficient in your language of choice.\n\n### Step 3: Onsite\n\nThe onsite at Airbnb consists of 5-7 interview rounds with the following steps:\n\n* Coding (1-3 hours)\n* System design (1 hour)\n* Behavioral (1 hour)\n* Culture Fit (2 hours)\n\n#### Coding\n\nAs with the technical phone screen, you’ll have to write working code and run it (no pseudocode). The number of rounds is dependent on your level, so more senior engineers will have less coding interviews, but more system design.\n\n#### System Design\n\nThis interview is similar to [Google’s system design interview](https://interviewing.io/guides/hiring-process/google).\n\n#### Behavioral\n\nThis interview will touch on your past projects, contributions, how you work in teams, and so on. The core engineering culture at Airbnb is all about making your mark and owning your impact.\n\n#### Culture Fit\n\nAirbnb calls these “Host interviews”, and they’re conducted by non-engineers. These interviews are very important, even if you do well on the technical portion, fail these and you won't get an offer. They'll be evaluating whether or not you’d be a good culture fit with everyone at Airbnb, and how you embody their values:\n\n* Champion the mission\n* Be a host\n* Embrace the adventure\n* Be a cereal entrepreneur\n\n### Step 4: Team matching\n\nOnce you’ve passed your onsite, you can expect to speak with anywhere from 1 to 4 hiring managers. The hiring managers will try to sell you on joining their team, but will also use the time to decide if you’re ultimately a good fit for their specific org.\n\nTypes of Interview Questions to Expect at Airbnb\n------------------------------------------------\n\n### Coding\n\nAirbnb asks LeetCode-style questions, ranging from medium to hard difficulty.\n\nTo figure out what types of questions to expect in your Airbnb interviews, we did two things. First, we spoke to some current and former Airbnb interviewers in our community. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews. Based on all of the above, here are the types of questions you’re likely to encounter:\n\n[Binary Trees](/binary-trees-interview-questions)\n\n[Questions   \n& tips](/binary-trees-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=binary-trees)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Stacks](/stacks-interview-questions)\n\n[Questions   \n& tips](/stacks-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=stacks)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Queues](/queue-interview-questions)\n\n[Questions   \n& tips](/queue-interview-questions)\n\n[Watch 5   \n interview replays](/mocks?technical=queue)\n\n### System Design\n\nYou may get low-level or high-level questions in this round. Examples include:\n\n* Design a cache system\n* Design a voting system\n* Design Twitter's trending topics\n* Design a load balancer\n* Design Facebook Messenger\n* Design an auth platform\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\nCommon Airbnb Interview Questions\n---------------------------------\n\nBelow are common questions that interviewers from Airbnb ask on our platform. Since our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\nEASY\n\nData Structures and Algorithms\n\n### [Reverse String](/questions/reverse-string)\n\n[Write a program to reverse the given string.](/questions/reverse-string)\n\nEASY\n\nData Structures and Algorithms\n\n### [Two Sum](/questions/two-sum)\n\n[Given an array of integers, return the indices of the two numbers that add up to a given target.](/questions/two-sum)\n\nMEDIUM\n\nData Structures and Algorithms\n\n### [Find the Missing Number in an Array](/questions/find-missing-number-in-array)\n\n[Given an unsorted array of unique integers (size n + 1) and a first array identical to the second array, missing one integer (size n), find and output the missing integer.](/questions/find-missing-number-in-array)\n\nHow Airbnb Makes Hiring Decisions\n---------------------------------\n\nThere is a hiring committee for engineering positions at Airbnb, who will ultimately decide on leveling, but whether or not to extend the offer falls to the hiring manager. You can expect an offer to come within a week or so of successful team matching.\n\nAirbnb Interview Replays\n------------------------\n\n[![Missing item list difference](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FPython_Airbnb_4_4146a1d87b.png&w=3840&q=75)\n\nAirbnb Interviewer\n\nMissing item list difference\n\nThe Legendary Artichoke, an Airbnb engineer, interviewed Mammoth Avenger in Python](/mocks/airbnb-python-missing-item-list-difference)\n\n[![Two sum](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FPython_Airbnb_1_1cc56d4ce3.png&w=3840&q=75)\n\nAirbnb Interviewer\n\nTwo sum\n\nRecursive Beast, an Airbnb engineer, interviewed Adequate Penguin in Python](/mocks/airbnb-python-two-sum)\n\n[![Missing item list difference](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FC_Airbnb_2_47260145b9.png&w=3840&q=75)\n\nAirbnb Interviewer\n\nMissing item list difference\n\nThe Legendary Artichoke, an Airbnb engineer, interviewed Supreme Werewolf in C++](/mocks/airbnb-cplusplus-missing-item-list-difference)\n\nWant to know if you're ready to interview at Airbnb? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/airbnb-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Nvidia’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Hiring Manager Call](#step-3)\n  + [Onsite](#step-4)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [Coding (domain-specific)](#question-domain)\n  + [System Design](#question-design)\n  + [Hiring Manager](#question-manager)\n* [Hiring Decisions](#hiring)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nNvidia’s Interview Process & Questions\n======================================\n\n*The info below is based on conversations with Nvidia engineers in 2024.*\n\nPublished:\n\nNvidia's Interview Process for Software Engineers: 2-4 Steps\n------------------------------------------------------------\n\nMid to senior-level engineers interviewing at Nvidia can expect the following process:\n\n* [Can skip if referred in] Recruiter call (30 minutes)\n* Technical phone screen (1 hour)\n* [Not always] Hiring Manager call (30 minutes)\n* Onsite (5 hours)\n\n![Nvidia’s interview process: Recruiter call, Technical phone screen, Hiring manager call, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Nvidia_company_process_a84381e04f.png)\n\nNvidia has a decentralized process that varies based on the hiring manager's preference, e.g., some people we spoke with had no recruiter screen and some had no hiring manager call before the onsite. One candidate even had their hiring manager call first, before anything else happened! The onsite rounds can also change a lot. We will do our best to sketch out the process, but yours may differ depending on the role you are interviewing for. While there is some flexibility regarding what role you end up interviewing for, you don’t seem to be able to interview for multiple teams. All of your interviewers will be from the team you are interviewing for.\n\nOne engineer we spoke to, who interviewed with Nvidia for two separate teams told us:\n\n> *If you get an early hiring manager call, you can ask them exactly what to expect in the rest of the rounds. If they say the language is the priority, brush up on those skills, if they say it will be more general, then focus on general coding.*\n\nGeneral tips:\n\n* LeetCode practice will help, but they will throw in more practical questions too.\n* They don’t have an internal question bank, so the interview questions are up to the individual interviewer.\n* Nvidia puts a premium on experience and advanced academic degrees.\n* A good (senior) referral can help you skip the recruiter call.\n* Most roles they hire for are software-orientedsoftware oriented, not hardware as you might expect\n* If you fail with one team, you can immediately interview with another one, but you will have to go through the whole process again, though it’s possible you’ll be able to skip the technical phone screen.\n* They will always have at least one interview that is focused on the language you will need for whatever team you are joining.\n\nThe entire process takes about 6-8 weeks.\n\n### Step 1: Recruiter Call\n\nNvidia’s recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, and why you’re interested in Nvidia and do some basic skills assessment.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Technical Phone Screen\n\nYou might start with 15 minutes of general chat in this round but, the guts of the interview will be technical. In the general chat, you might be asked about your background and why you’re interested in Nvidia. The technical portion will be 45 minutes long. One engineer we spoke to was asked only LLM questions here and everything was done in PyTorch, but this is largely dependent on the role you are interviewing for. Nvidia is very focused on hiring engineers with LLM experience right now so, if that’s what you applied to, expect LLM questions rather than general CS skill questions. If not, expect a LeetCode medium-style question, but remember, they don’t have an internal bank from what we’ve heard, so you might get something more practical too.\n\nIf you are asked LLM questions, expect them to focus on concepts such as:\n\n* Mixture-of-experts model\n* Beam search\n* Autoregressive decoding with KV-cache\n* Low-rank adaptation (LoRA)\n* High-level distributed computing\n\nFor example, you might be asked to implement beam search for LLM inference and evaluate the time complexity of each operation.\n\n### Step 3: Hiring Manager Call\n\nThis is usually split into two parts. You will be asked about your background and experience in the first part. The hiring manager will be testing for culture fit. In the second part, they will be selling you on the role and team.\n\n### Step 4: Onsite\n\nThis will vary by role but here’s what you can expect.\n\n* **Coding x 2** (1 hour each). This interview will be conducted in CoderPad. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/nvidia-interview-questions#question-coding).\n* **System design** (1 hour). For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/nvidia-interview-questions#question-design).\n* **Second Coding** (1 hour). For more detail about the kinds of questions to expect, see the [Coding (domain-specific) section below](https://interviewing.io/nvidia-interview-questions#question-domain).\n* **Hiring Manager** (1 hour). For more detail about the kinds of questions to expect, see the [Hiring Manager interview section below](https://interviewing.io/nvidia-interview-questions#question-manager).\n\nTypes of Interview Questions to Expect at Nvidia\n------------------------------------------------\n\n### Coding\n\nOk, things can vary quite a bit here, as with everything in the Nvidia hiring process! One engineer we spoke to had 2 low-level coding rounds that were very practical and domain-aligned for the team he interviewed with. Others got more standard LeetCode-style questions during these two rounds.\n\nWe also heard that there are different formats for the coding rounds for certain teams. You might get a problem to solve in some and have to review existing code in others, similar to a debugging round.\n\nIf the role you have applied to will rely heavily on Cuda, expect to be interviewed in C++. Every role at Nvidia is a bit different though so lot’s of languages are used. It should be clear from the job description if there is a specific language required. If not, ask the hiring manager in advance so you are prepared!\n\nBelow are the technical topics you’re likely to encounter in Nvidia interviews. To compile this list, we did two things. First, we spoke to some current and former Nvidia engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews.\n\nFirst, here’s a list of more niche technical topics that are, in our experience, specific to Nvidia:\n\n* C++ Fundamentals\n* LLMs\n  + See the [technical phone screen section above](https://interviewing.io/nvidia-interview-questions#step-2) for examples of LLM sub-topics\n\nAnd here are technical topics that you’re likely to find at other companies as well (for these we’ve created detailed write-ups of their own):\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Inorder Traversal](/inorder-traversal-interview-questions)\n\n[Questions   \n& tips](/inorder-traversal-interview-questions)\n\n[Watch 2   \n interview replays](/mocks?technical=inorder-traversal)\n\n[Linked Lists](/linked-lists-interview-questions)\n\n[Questions   \n& tips](/linked-lists-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=linked-lists)\n\n[Matrices](/matrices-interview-questions)\n\n[Questions   \n& tips](/matrices-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=matrices)\n\n[Binary Search](/binary-search-interview-questions)\n\n[Questions   \n& tips](/binary-search-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=binary-search)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Graphs](/graphs-interview-questions)\n\n[Questions   \n& tips](/graphs-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=graphs)\n\n### Coding (Domain-Specific)\n\nThis will be a more practical round. You might be asked to reimplement an algorithm from first principles. You might get a mix of technical questions and actual coding to do. Whatever you get will be specific to the role you are interviewing for.\n\n### System Design\n\nThis will more than likely be a pretty standard system design round but, again, things vary from team to team. Brush up on the usual questions like “How would you design Twitter / Uber / a chatbot for a website,” BUT they may ask you something more practical for the role. Learn about CPU/GPU architecture and anything else relevant to the team you are interviewing for!\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Hiring Manager Interview\n\nThis will be more conversational than the other rounds but could have behavioral/experience-based questions, as well as technical questions depending on the hiring manager.\n\n### How Nvidia Makes Hiring Decisions\n\nAfter the onsite, the panel all submit a scorecard. You don’t need to score perfectly (it’s from 1-5) on each scorecard, but it’s unlikely to be hired if you’ve scored poorly on even one round.\n\nWant to know if you're ready to interview at Nvidia? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/nvidia-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "A Senior Engineer’s Guide to Amazon's Interview Process and Questions",
      "content": "We helped write the sequel to \"Cracking the Coding Interview\". [Read 9 chapters for free →](https://bctci.co/free-chapters)",
      "content_type": "other",
      "source_url": "https://interviewing.io/guides/hiring-process/amazon",
      "author": "",
      "user_id": ""
    },
    {
      "title": "VMware’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#VMware-interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Onsite](#step-3)\n    - [Coding](#step-3-coding)\n    - [System Design](#step-3-system)\n    - [Hiring Manager](#step-3-hm)\n    - [Behavioral](#step-3-behavioral)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n* [Common Questions](#common-questions)\n* [Interview Replays](#interview-replays)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nVMware’s Interview Process & Questions\n======================================\n\nPublished:\n\nVMware's Interview Process for Software Engineers: 3 Steps\n----------------------------------------------------------\n\nMid to senior-level engineers interviewing at VMware can (usually) expect the following process:\n\n* Recruiter call (30 minutes)\n* Technical phone screen (1 hour)\n* Onsite (5-6 hours)\n\n![VMware's interview process: Recruiter call, Technical phone screen, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/VM_Ware_s_Company_Process_d97f92854f.png)\n\nThat said, VMware’s interview process is very decentralized. You interview for a specific team from the get-go, and not only the content of the interviews, but the types of interviews themselves, vary significantly from team to team, so take the list above with a grain of salt.\n\nOne other somewhat unique aspect of VMware’s process is their commitment to the Rooney Rule, i.e., [committing to interviewing at least one woman and one minority candidate for every open position](https://www.cnbc.com/2020/06/19/no-job-hire-made-unless-minority-candidate-interviewed-vmware-ceo.html). In practice, sometimes the entire process for a role will get stalled until recruiting can find those candidates, and all candidates who are not from traditionally underrepresented backgrounds may end up in an indefinite holding pattern.\n\nFinally, a word about VMware’s eng culture. VMware wants to find very technical people who are interested in solving hard problems but also want to go home at the end of the day. VMware’s sales cycles are slow – there’s just one big release every year, so they don't need to find people who are willing to work really long hours or people whose job is to make sure that the build tool chain is on the cutting edge (and in fact, most of their customers, who tend to be huge enterprise organizations, don't want cutting edge).\n\n### Step 1: Recruiter Call\n\nThis is a typical recruiter call, where they’ll ask about previous experience, relevant projects, and why you’re interested in VMware. They’ll also elaborate on the role and confirm that your experience and expectations are a good match.\n\nIt’s really important, at this stage, not to reveal your salary expectations, your salary history, or where you are in the process with other companies. We wrote a [detailed post about salary negotiation that lays out exactly what to say when recruiters pressure you to name the first number](https://www.google.com/url?q=https://interviewing.io/blog/negotiate-salary-recruiter&sa=D&source=editors&ust=1687337715912604&usg=AOvVaw2DmZ7rz25IR1Q42FqB380A). Just don’t do it – when you give out information this early in the process, you’re painting future you into a corner.\n\n### Step 2: Technical Phone Screen\n\nThis is a 1-hour long algorithmic interview. You will likely be using [CoderPad](https://coderpad.io) in your technical phone screen, but tools vary from team to team.\n\n### Step 3: Onsite\n\nDepending on the role, you may have a virtual onsite or an in-person onsite. In either event they follow similar formats. The exact composition will vary not only with the team but with your seniority – if you’re a mid-level engineer they will be more focused on coding. If you’re very senior, your interview will be much more about what you’ve done in the past — your work history, your work style, technical decisions you’ve made and so on. At the highest levels, the interview will feel more like an exchange of stories rather than vetting for a specific skill.\n\nSome of your questions will invariably involve VMware-specific products and knowledge, so make sure you do some background research before heading in.‘\n\nThat said, VMware’s onsite lasts between 5 and 6 hours and consists of the following rounds:\n\n* Coding (1 hour)\n* (For niche roles) Domain-specific coding (1 hour)\n* System design (1 hour)\n* Technical communication (1 hour)\n* Behavioral (1 hour)\n\nThe order of these rounds can vary, as well as the number of coding interviews required. Depending on your score for various portions of the onsite, you may be required to complete an extra system design, object oriented design, or algorithms interview. There may be extra rounds for certain teams and roles as well. Given that VMware’s interview process is centralized, you generally won’t be interviewing with engineers or managers from the team you’ll end up on.\n\n#### Coding\n\nAt minimum there will be two algorithms and data structures interviews in your onsite loop. Your interview will happen in CoderPad if it’s remote, but if it’s in person, it’ll be on a physical whiteboard.\n\n#### System Design\n\nThere will be one or two design rounds depending on your specific role and focus.\n\n#### (Potential) Hiring manage round\n\nThe main purpose of this round is to chat about your past projects and work experience, and they’re very focused on your ability to communicate technical details clearly and concisely. Try to come prepared to talk about projects that are relevant to VMware and its values.\n\n#### Behavioral\n\nThis call’s aim is to assess your fit for VMware and will focus heavily on the company’s values and mission.\n\nTypes of Interview Questions to Expect at VMware\n------------------------------------------------\n\n### Coding\n\nCoding questions will focus on algorithms and data structures, and while the questions asked can vary depending on the role, you’ll likely see medium to hard-difficulty LeetCode style problems.\n\nTo figure out what types of questions to expect in your VMWare interviews, we did two things. First, we spoke to some current and former VMWare interviewers in our community. Then we cross-referenced all the anecdotes we heard with our own data-set of mock interviews. Based on all of the above, here are the types of questions you’re likely to encounter:\n\n[Binary Trees](/binary-trees-interview-questions)\n\n[Questions   \n& tips](/binary-trees-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=binary-trees)\n\n[Linked Lists](/linked-lists-interview-questions)\n\n[Questions   \n& tips](/linked-lists-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=linked-lists)\n\n[Matrices](/matrices-interview-questions)\n\n[Questions   \n& tips](/matrices-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=matrices)\n\n[Parsing](/parsing-interview-questions)\n\n[Questions   \n& tips](/parsing-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=parsing)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Dynamic Programming](/dynamic-programming-interview-questions)\n\n[Questions   \n& tips](/dynamic-programming-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=dynamic-programming)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Hash Tables](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 5   \n interview replays](/mocks?technical=hash-tables)\n\n[Graphs](/graphs-interview-questions)\n\n[Questions   \n& tips](/graphs-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=graphs)\n\n### System Design\n\nThese rounds tend to ask you to build or design something team-specific, and they’re really looking at your ability to clearly explain your approach and choices.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\nCommon VMware Interview Questions\n---------------------------------\n\nBelow are common questions that interviewers from VMware ask on our platform. Since our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\nEASY\n\nData Structures and Algorithms\n\n### [Intersection of Linked List](/questions/intersection-of-linked-list)\n\n[Given the heads of two singly linked-lists headA and headB, return the node at which the two lists intersect.](/questions/intersection-of-linked-list)\n\nVMware Interview Replays\n\n[![Linked list intersection](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FPython_VM_Ware_1_9b7447a6be.png&w=3840&q=75)\n\nVMware Interviewer\n\nLinked list intersection\n\nThe Masked Hedgehog, a VMware engineer, interviewed Ghost Armadillo in Python](/mocks/vm-ware-python-linked-list-intersection)\n\nWant to know if you're ready to interview at VMware? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/vmware-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Block/Square’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#Square-interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Onsite](#step-3)\n    - [Coding](#step-3-coding)\n    - [System Design](#step-3-system)\n    - [Leadership](#step-3-leadership)\n    - [Hiring manager](#step-3-hiringmanager)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n* [Common Questions](#common-questions)\n* [Hiring Decisions](#hiring-decision)\n* [Interview Replays](#interview-replays)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nBlock/Square’s Interview Process & Questions\n============================================\n\n*The info below is based on conversations with Block/Square engineers in 2023.*\n\nPublished:\n\nSquare's Interview Process for Software Engineers: 4 Steps\n----------------------------------------------------------\n\nMid to senior-level engineers interviewing at Block/Square (from now on we'll refer to them as Square) can expect the following process:\n\n* Recruiter call (30 minutes)\n* Technical phone screen (1 hour)\n* Onsite (4-5 hours)\n* Team matching\n\n![Square’s interview process: Recruiter call, Technical phone screen, Onsite, Team matching](https://strapi-iio.s3.us-west-2.amazonaws.com/Square_s_Company_Process_fe968457a9.png)\n\nSquare’s hiring process is a hybrid. Although you apply for a specific team from the get-go, your interviews will likely not be with the people on your team – interviewers are randomly selected from a pool – except for the hiring manager interview.\n\nNote: Square is one of the few companies that will actually give you some constructive post-interview feedback!\n\nThe entire process takes about 6 weeks, based on our sources.\n\n### Step 1: Recruiter Call\n\nSquare’s recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, why you’re interested in Square, and what you’re looking for moving forward. They’ll also review the specific role you’re applying for to make sure you understand the expectations and requirements.\n\nIt’s really important to not reveal your salary expectations or where you are in the process with other companies. We wrote a [detailed post about salary negotiation that lays out exactly what to say when recruiters pressure you to name the first number](https://www.google.com/url?q=https://interviewing.io/blog/negotiate-salary-recruiter&sa=D&source=editors&ust=1687337715912604&usg=AOvVaw2DmZ7rz25IR1Q42FqB380A).\n\n### Step 2: Technical Phone Screen\n\nSquare’s technical phone screen lasts about an hour. In rare cases, they will let very senior candidates skip this step. In this interview, you’ll pair with your interviewer on a coding problem in [CoderPad](https://coderpad.io).\n\nAfter this round is over, you’ll get high-level feedback about your performance from your recruiter, regardless of outcome (e.g., you might get feedback about your coding speed, your attention to detail, and so on).\n\n### Step 3: Onsite\n\nThe onsite at Square consists of 4-5 sessions, depending on the role and experience level of the candidate. Most of the interviews, except for the hiring manager interview and the coding rounds, are two-person panels. If you’re a mid to senior-level engineer, you can expect the onsite to look something like this:\n\n* Coding (2 hours)\n* System design (1 hour)\n* (For L6+) Leadership interview (1 hour)\n* Hiring manager interview (30 minutes)\n\n#### Coding\n\nSquare’s onsite includes two separate 1-hour coding interviews, conducted in [CoderPad](https://coderpad.io).\n\n#### System Design\n\nThis round is conducted by a two-person panel.\n\n#### (For L6+) Leadership Interview\n\nThis round is conducted by a panel of two very senior engineers (both will be L6 and above).\n\n#### Hiring Manager Interview\n\nThis is a call with a hiring manager from the team you’d be working on. For more junior engineers, it usually takes the form of a “Teach me something technical”. For more senior engineers, it’s usually a deep-dive into a relevant project you’ve worked on in the past.\n\nTypes of Interview Questions to Expect at Square\n------------------------------------------------\n\n### Coding\n\nCoding questions at Square are practical, e.g., “Build an app to split a bill with friends”, rather than LeetCode-style. Square has a question bank with approved questions that interviewers can pull from, and they can submit their own questions to it as well, though there’s a rigorous review process:\n\n* You propose your question\n* It gets approved by other interviewers\n* You try out your question in the wild, and it gets shadowed as well\n* Once it’s fully approved, it goes into a company-wide question bank\n\nInterviewers are also encouraged to layer complexity in their interviews, and a question that might start out simple can go to unexpected places.\n\nSquare’s questions, while practical, may touch on the following technical topics/concepts, as you’ll need them to build whatever it is your interviewer tasks you with:\n\n[Binary Trees](/binary-trees-interview-questions)\n\n[Questions   \n& tips](/binary-trees-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=binary-trees)\n\n[Sets](/sets-interview-questions)\n\n[Questions   \n& tips](/sets-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=sets)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Hash Tables](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 5   \n interview replays](/mocks?technical=hash-tables)\n\n[Queues](/queue-interview-questions)\n\n[Questions   \n& tips](/queue-interview-questions)\n\n[Watch 5   \n interview replays](/mocks?technical=queue)\n\n### System Design\n\nThese will likely be high-level [system design questions](https://interviewing.io/guides/system-design-interview), and though it’s now retired, one of the more common system design questions at Square was “Design a Hotel booking system”.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\nCommon Square Interview Questions\n---------------------------------\n\nBelow are common questions that interviewers from Square ask on our platform. Since our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\nHARD\n\nData Structures and Algorithms\n\n### [Binary Array Partition](/questions/binary-array-partition)\n\n[Given an array Z of 0s and 1s, divide the array into 3 non-empty parts, such that all of these parts represent the same binary value.](/questions/binary-array-partition)\n\nHow Square Makes Hiring Decisions\n---------------------------------\n\nAfter each round, interviewers leave feedback and makes a hire or no-hire recommendation. Those are then collected and sent to the hiring manager, who will decide whether to “plead your case” in front of the Hiring Bar (Square’s version of a hiring committee).\n\nNote that you can get mixed feedback from your interviewers and still get moved forward. It really depends on what the hiring manager needs and what they can justify.\n\nSquare’s Hiring Bar is composed of 3 people – either eng managers or senior engineers. Your hiring manager takes your resume and interview results and explains why you’d be an asset to their team. Then the Hiring Bar votes, and if you get at least 2 of the 3 votes, you get an offer.\n\nSquare Interview Replays\n------------------------\n\n[![Threaded order of execution](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FPython_Square_1_5015750c0d.png&w=3840&q=75)\n\nBlock/Square Interviewer\n\nThreaded order of execution\n\nEponymous Squirrel, a Block/Square engineer, interviewed Dystopian Sphinx in Python](/mocks/square-python-threaded-order-of-execution)\n\n[![Sleep sort](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FJava_Square_2_08692ecf13.png&w=3840&q=75)\n\nBlock/Square Interviewer\n\nSleep sort\n\nEponymous Squirrel, a Block/Square engineer, interviewed The Phenomenal Lemur in Java](/mocks/square-java-sleep-sort)\n\nWant to know if you're ready to interview at Block/Square? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/block-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Grammarly’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Hiring Manager Screen](#step-3)\n  + [Onsite](#step-4)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Values](#question-values)\n* [Hiring Decisions](#hiring)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nGrammarly’s Interview Process & Questions\n=========================================\n\n*The info below is based on conversations with Grammarly engineers.*\n\nPublished:\n\nGrammarly's Interview Process for Software Engineers: 4 Steps\n-------------------------------------------------------------\n\nMid to senior-level engineers interviewing at Grammarly can expect the following process:\n\n* Recruiter call (30 minutes)\n* Technical phone screen (1 hour)\n* Hiring Manager screen (1 hour)\n* Onsite (5.5 hours)\n\n![Grammarly’s interview process: Recruiter call, Technical phone screen, Hiring Manager screen, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Grammarly_Interview_Process_4de1973fa7.png)\n\nGeneral tips:\n\n* Having a referral guarantees a call-back (not true at many other companies).\n* Grinding on LeetCode will help because they primarily ask questions straight from LeetCode.\n* They can be slow, so don’t be afraid to follow up a lot and keep reminding them you have other offer deadlines.\n* Learn their [EAGER framework of values](https://www.grammarly.com/jobs). They will want you to know these and align with them during your interviews.\n* It's a foreign-founded company, so you may encounter interviewers who aren’t native English speakers and have a different cultural style than you’re used to.\n\nGrammarly has a hybrid process, which means that you will interview for a specific team and role but your interview panel will be a mix of engineers from the team and from elsewhere in the company. It will also include the hiring manager for the role you are interviewing for.\n\nThe entire process takes about 6 weeks, but our sources tell us it shouldn’t have to – by default, Grammarly is slow in moving things forward, but if you’re proactive, you can speed up the process.\n\n### Step 1: Recruiter Call\n\nGrammarly’s recruiter call lasts about 30 minutes, and it’s different from a lot of recruiter screens. Be prepared to get asked questions that require STAR-style responses. One of our sources was surprised by the style of question they got at this stage, so be prepared!\n\nIt’s also really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Technical Phone Screen\n\nGrammarly’s technical phone screen lasts about an hour. It will be conducted in HackerRank, and you should expect medium-difficulty LeetCode-style questions.\n\n### Step 3: Hiring Manager Screen\n\nThis is a presentation and a deep dive into a complex project that you’ve worked on. For more senior roles, you’ll be asked to describe a project you led.\n\nWe’ve heard that you may not necessarily get clear instructions about this round, and you may not be told to prepare a presentation in advance, but we heartily recommend that you do! One of our sources did well in this round *because he already had a presentation prepared for another company and was able to seamlessly jump into it when he realized what the call was about*. Though it may not be critical to have a presentation fully prepared, doing so will give you an edge. Either way, come prepared to discuss the overall architecture (including diagrams as needed), what problem you were trying to solve, and your role in the successful completion of the project.\n\n### Step 4: Onsite\n\nWe’ve heard that Grammarly’s onsite is very long and can be split across multiple days.\n\n* **Product intuition/domain expertise** (1 hour). This interview will be conducted in CodePair (HackerRank’s coding interview tool). For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/grammarly-interview-questions#question-coding).\n* **System design** (1 hour). This will likely be in Miro or HackerRank's whiteboarding tool. For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/grammarly-interview-questions#question-design).\n* **Coding / CS fundamentals** (1 hour). This interview will be conducted in CodePair. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/grammarly-interview-questions#question-coding).\n* **Coffee chat (virtual)** (30 minutes). You meet with two people from your future team and get to ask them questions.\n* **Values call 1** (1 hour) This interview will be with someone from Grammarly who isn’t from the org or team you are applying to. For more info about what questions to expect, see the [Values section below](https://interviewing.io/grammarly-interview-questions#question-values) .\n* **Values call 2** (1 hour) This interview will be conducted by a hiring manager. For more info about what questions to expect, see the [Values section below](https://interviewing.io/grammarly-interview-questions#question-values) .\n\nTypes of Interview Questions to Expect at Grammarly\n---------------------------------------------------\n\nFrom what we’ve heard, Grammarly doesn’t ask practical questions. They do have a standard bank of questions they regularly ask, and LeetCode-style questions are common.\n\n### Product Intuition / Domain Expertise\n\nThis might be a two-part question and varies by role. In the first part, you could be shown an email with the Grammarly tool engaged and certain words or phrases highlighted. Your job will be to improve the product.\n\nThe second part will involve a standard LeetCode-style question.\n\n### Coding\n\nExpect another two-part interview here with standard LeetCode-style questions in the first part.\n\nThe second part might be a lot more academic. One source told us:\n\n> *\"The second problem looked like it came straight out of an algorithms textbook.”*\n\nBelow are the technical topics you’re likely to encounter in Grammarly interviews. To compile this list, we did two things. First, we spoke to some current and former Grammarly engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Binary Trees](/binary-trees-interview-questions)\n\n[Questions   \n& tips](/binary-trees-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=binary-trees)\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Parsing](/parsing-interview-questions)\n\n[Questions   \n& tips](/parsing-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=parsing)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Recursion](/recursion-interview-questions)\n\n[Questions   \n& tips](/recursion-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=recursion)\n\n[Two Pointers](/two-pointers-interview-questions)\n\n[Questions   \n& tips](/two-pointers-interview-questions)\n\n[Watch 3   \n interview replays](/mocks?technical=two-pointers)\n\n[Heaps](/heaps-interview-questions)\n\n[Questions   \n& tips](/heaps-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=heaps)\n\n### System Design\n\nThis will likely be a two-part question where you’re asked to design something at a small scale in the first part and then asked to scale it up in the second. Time management is critical here because they will ask lots of questions during the first part. We’ve heard that this is a tough interview, but that you can do poorly, and potentially get a chance to redo it after the onsite.\n\nAreas to focus on are:\n\n* Client-server communication\n* Separation of concerns\n* Data structures and flows\n* A system’s extensibility and scaling\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Values\n\nYou will have two calls that focus on Values, one with a hiring manager and the other with someone from a completely different part of the company. One of the people we interviewed for this guide was interviewed by someone from the Social Media team.\n\nBoth calls will ask questions about your past work experience. They will use your resume and frame their questions around your past companies. Be prepared to use their [EAGER framework](https://www.grammarly.com/about) of values, and show that you understand and embody the values they are looking for. EAGER stands for Ethical, Adaptable, Gritty, Empathetic, and Remarkable. Look for ways to show that you embody these values as you go through your interviews, even the technical ones.\n\nExpect to go through each company you’ve worked from and answer questions like:\n\n* What would your manager say your strengths were?\n* What would your manager say your weaknesses were?\n* What would you say your manager's strengths were?\n* What would you say your manager's weaknesses were?\n\n### How Grammarly Makes Hiring Decisions\n\nThe entire onsite panel submits written feedback after the interviews are completed and then meets to discuss. Most of the time, decisions are reached by consensus (everyone agrees to hire or no-hire), but in cases where consensus isn’t possible, the hiring manager has the final say.\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/grammarly-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "LinkedIn’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#linkedin-interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Second Recruiter Call](#step-3)\n  + [Onsite](#step-4)\n    - [Coding](#step-4-coding)\n    - [Domain-Specific Coding](#step-4-niche)\n    - [System Design](#step-4-system)\n    - [Technical Communication](#step-4-communication)\n    - [Behavioral](#step-4-behavioral)\n  + [Team Matching](#step-5)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n* [Common Questions](#common-questions)\n* [Hiring Decisions](#hiring-decision)\n* [Interview Replays](#interview-replays)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nLinkedIn’s Interview Process & Questions\n========================================\n\nLinkedIn's Interview Process for Software Engineers: 5 Steps\n------------------------------------------------------------\n\nFor a mid to senior-level software engineer, LinkedIn’s process (usually) looks like this:\n\n* Recruiter call (30 minutes)\n* Technical phone screen (1 hour)\n* Second recruiter call (30 minutes)\n* Onsite (5-6 hours)\n* Team matching\n\n![LinkedIn’s interview process: Recruiter call, Technical phone screen, Recruiter call, Onsite, Team matching ](https://strapi-iio.s3.us-west-2.amazonaws.com/Linked_In_Company_Process_7af8ecb3ab.png)\n\nLinkedIn’s interview process is centralized, which means that you don’t interview with specific teams and do team matching after the fact.\n\n### Step 1: Recruiter Call\n\nThe first recruiter call lasts 30 minutes. Its purpose is to check qualifications, hear about past projects, and find out what you’re looking for from LinkedIn and in general. It also acts as a culture fit check, so they’ll want to see how your communication and personality align with their values. Make sure you’re up to date on LinkedIn features, blog posts and news. Finally, your recruiter will make sure you understand the role you’re applying for and clarify next steps in the process.\n\nIt’s really important, at this stage, not to reveal your salary expectations, your salary history, or where you are in the process with other companies. We wrote a [detailed post about salary negotiation that lays out exactly what to say when recruiters pressure you to name the first number](https://www.google.com/url?q=https://interviewing.io/blog/negotiate-salary-recruiter&sa=D&source=editors&ust=1687337715912604&usg=AOvVaw2DmZ7rz25IR1Q42FqB380A). Just don’t do it – when you give out information this early in the process, you’re painting future you into a corner.\n\n### Step 2: Technical Phone Screen\n\nThe technical phone screen lasts an hour, and interviews at this stage (and beyond) will usually have two interviewers, the primary interviewer and a trainee interviewer who’s shadowing.\n\nYou’ll be given 2-3 algorithms and data structures questions, structured as follows:\n\n* If you’re applying for a niche role (e.g., mobile), you’ll get a 10-minute domain-specific question. This question is meant to quickly  gauge your expertise on your niche subject and screen out generalists.\n* 15-minute small problem\n* 30-minute medium problem\n\n### Step 3: Second Recruiter Call\n\nIf you pass the technical phone screen, a recruiter will reach out again for a 30 minute call. Having a second recruiter call in the middle of the process is unusual, but LinkedIn does it for a good reason: they have historically lost a lot of good candidates to FAANG, and so this call is their way of playing defense to get ahead of attrition. In this call, they’ll re-ask about whom you’re in process with and will try to make sure that they don’t lose you by moving too slowly.\n\nThough our advice about not revealing your hand stands, in this call, if you do have any tight timelines from other companies, it’s good to let them know because, according to our sources, they are indeed able to speed things up and move quickly.\n\n### Step 4: Onsite\n\nLinkedIn’s onsite usually lasts 5-6 hours and includes the following steps:\n\n* Coding (1 hour)\n* (For niche roles) Domain-specific coding (1 hour)\n* System design (1 hour)\n* Technical communication (1 hour)\n* Behavioral (1 hour)\n\nThe order of these rounds can vary, as well as the number of coding interviews required. Depending on your score for various portions of the onsite, you may be required to complete an extra system design, object oriented design, or algorithms interview. There may be extra rounds for certain teams and roles as well. Given that LinkedIn’s interview process is centralized, you generally won’t be interviewing with engineers or managers from the team you’ll end up on.\n\n#### Coding\n\nThis interview will generally be one question or sometimes two, depending on how quickly you work through the first one.\n\n#### (For Niche Roles) Domain-Specific Coding\n\nIf you’re applying for a niche role (e.g., mobile), in addition to a general coding interview, you’ll have to do a domain-specific interview as well.\n\n#### System Design\n\nThe [system design interview](https://interviewing.io/guides/system-design-interview) will cover general system design knowledge and will focus on building large-scale systems. Make sure to communicate clearly and often during this interview and substantiate your design choices as well as your choices of specific technologies.\n\n#### Technical Communication\n\nThis interview will evaluate your ability to communicate and collaborate. You’ll be asked about one of your past projects and then walk your interviewer through it, with an emphasis on the technical aspects of your projects. Make sure you explain what the project was, why it mattered to the business, and what you did specifically. They’re expecting you to be able to dive really deep into the project you choose, so be ready to answer anything and everything about it.\n\n#### Behavioral\n\nThe behavioral interview at LinkedIn is very conversational. They’re looking to assess your culture fit and will ask you ad-hoc questions rather than preset or typical behavioral interview questions. Despite it being conversational in nature, it’s important to find ways to tie your answers to LinkedIn’s values:\n\n* We put members first\n* We trust and care about each other\n* We are open, honest and constructive\n* We act as One LinkedIn\n* We embody diversity, inclusion and belonging\n* We dream big, get things done and know how to have fun\n\n### Step 5: Team matching\n\nAssuming all has gone well to this point, you’ll be contacted by hiring managers for a team matching call. These calls are for the hiring managers to sell you on joining LinkedIn, and to create excitement about their specific orgs. It’s important to use this time to ask any questions you have about LinkedIn, the role or the process so far, and find out as much as possible about your team options.\n\nOnce you’re matched with a team they will extend you an offer. If you are far along in the process with other companies, make sure that you let them know that they should move quickly. This is another stage where LinkedIn loses a lot of candidates to FAANG, so they’re ready and able to speed things up.\n\nIn the event you’re unable to be matched with a team, your onsite results are valid with LinkedIn for 1 year, so they will continue trying to find a team for you.\n\nTypes of Interview Questions to Expect at LinkedIn\n--------------------------------------------------\n\n### Coding\n\nLinkedIn has a company-wide question bank, though interviewers have a bit of wiggle room to change them up. We’ve heard from several sources that the question bank is fairly small (compared to other companies), and that the questions haven’t changed much over the past 7 years or more.\n\nTo figure out what specific types of questions to expect in your LinkedIn interviews, we did two things. First, we spoke to some current and former LinkedIn interviewers in our community. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews. Based on all of the above, here are the types of questions you’re likely to encounter:\n\n[Binary Trees](/binary-trees-interview-questions)\n\n[Questions   \n& tips](/binary-trees-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=binary-trees)\n\n[Matrices](/matrices-interview-questions)\n\n[Questions   \n& tips](/matrices-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=matrices)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Stacks](/stacks-interview-questions)\n\n[Questions   \n& tips](/stacks-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=stacks)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Graphs](/graphs-interview-questions)\n\n[Questions   \n& tips](/graphs-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=graphs)\n\n### System Design\n\nTypical [system design](https://interviewing.io/guides/system-design-interview) questions include:\n\n* Design a system that will determine trending posts\n* Design a system that will log requests\n* Design a system that will show client feeds\n* Design an API for some specific use case\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\nCommon LinkedIn Interview Questions\n-----------------------------------\n\nBelow are common questions that interviewers from LinkedIn ask on our platform. Since our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\nMEDIUM\n\nData Structures and Algorithms\n\n### [Subarray Sum Equals K](/questions/subarray-sum-equals-k)\n\n[Given an unsorted array of integers and an integer k, find the number of subarrays whose sum equals k.](/questions/subarray-sum-equals-k)\n\nEASY\n\nData Structures and Algorithms\n\n### [Two Sum](/questions/two-sum)\n\n[Given an array of integers, return the indices of the two numbers that add up to a given target.](/questions/two-sum)\n\nMEDIUM\n\n### [Find Leaves of a Binary Tree](/questions/find-leaves-of-binary-tree)\n\n[Given a binary tree, extract all the leaves in repeated succession into a list of lists by starting at the bottom and working your way upwards.](/questions/find-leaves-of-binary-tree)\n\nHow LinkedIn Makes Hiring Decisions\n-----------------------------------\n\nLinkedIn grades each onsite round on a 4-point scale, where 3 is passing. If your aggregate score after the onsite is borderline, you may get asked to do another interview.\n\nCandidates who end up over the line get passed to the hiring committee, who make the final decision about both hiring and leveling.\n\nLinkedIn Interview Replays\n--------------------------\n\n[![Reverse word in string](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FJava_Linked_In_1_878141e885.png&w=3840&q=75)\n\nLinkedIn Interviewer\n\nReverse word in string\n\nSpace Dragon, a LinkedIn engineer, interviewed Ice Gyro in Java](/mocks/linked-in-java-reverse-word-in-string)\n\n[![Matching pairs](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FPython_Linked_In_2_d36bb244fa.png&w=3840&q=75)\n\nLinkedIn Interviewer\n\nMatching pairs\n\nAdmiral Velociraptor, a LinkedIn engineer, interviewed Dystopian Pizza in Python](/mocks/linked-in-python-matching-pairs)\n\n[![Falling leaves of a tree](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FGo_Linked_In_4_adbb4c4dcb.png&w=3840&q=75)\n\nLinkedIn Interviewer\n\nFalling leaves of a tree\n\nExistential Crumpet, a LinkedIn engineer, interviewed Neuro Owl in Go](/mocks/linked-in-go-falling-leaves-of-a-tree)\n\n[![Two sum](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FJava_Script_Linked_In_6_2b87fe415b.png&w=3840&q=75)\n\nLinkedIn Interviewer\n\nTwo sum\n\nExistential Crumpet, a LinkedIn engineer, interviewed Chaotic Pizza in JavaScript](/mocks/linked-in-javascript-two-sum)\n\nWant to know if you're ready to interview at LinkedIn? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/linkedin-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Databrick’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Hiring Manager Call](#step-3)\n  + [Onsite](#step-4)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Cross-Functional/Behavioral](#question-behavioral)\n* [Hiring Decisions](#hiring)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nDatabrick’s Interview Process & Questions\n=========================================\n\n*The info below is based on conversations with Databrick engineers*\n\nPublished:\n\nDatabricks's Interview Process for Software Engineers: 4 Steps\n--------------------------------------------------------------\n\nMid to senior-level engineers interviewing at Databricks can expect the following process:\n\n* Recruiter screen (30 minutes)\n* Technical phone screen (1 hour)\n* Hiring manager call (1 hour)\n* Onsite (spread across 4-5 hours)\n\n![Databricks's interview process: Recruiter screen, Technical phone screen, Hiring manager call, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Databricks_interview_process_a0fc569fb6.png)\n\nDatabricks has a hybrid hiring process. Candidates are typically matched to a specific part of the organization early on, and you'll interview for a specific role, but your interviewers will not necessarily be people from your future team. There is also scope to switch roles after the onsite. According to our sources, about a quarter of candidates pivot to another team after completing their onsite interviews.\n\nThe entire process can take up to 8 weeks, making it one of the longer hiring processes in the industry. Recruiters are upfront about this timeline with candidates.\n\nGeneral tips:\n\n* Study LeetCode questions, particularly those tagged for Databricks in the past 3 months, as some questions come directly from this pool. You’ll get LeetCode medium and hard level questions.\n* Brush up on concurrency and multithreading concepts, which are essential for one of the coding rounds.\n* Practice system design on Google Docs, as it is sometimes used for the system design round.\n* Be prepared for graph algorithms and optimization problems, as these appear frequently in the coding rounds.\n* Have impressive references ready—they're weighted heavily in the final decision process at Databricks.\n\n### Step 1: Recruiter Call\n\nDatabricks' recruiter screen lasts about 30 minutes, and it’s pretty standard fare.. They'll ask about your previous experience, your interest in Databricks, and go over the specific role you're applying for. At this stage, your profile (resume, referral information, location preferences) begins to be shared with engineering leads for potential team matching.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Technical Phone Screen\n\nThe technical phone screen lasts about 1 hour with an engineer. You'll use CoderPad or a similar online IDE with runnable code. Expect LeetCode-style questions, typically medium to hard difficulty. One engineer we spoke with, who had just gone through the process, told us:\n\n> *The question was one of the LeetCode questions for Databricks. I was able to find it by searching for the common Databricks questions from the past 3 months.*\n\nQuestions candidates have faced include a weighted paths problem (graph optimization) and the [house robber problem](https://leetcode.com/problems/house-robber/description/), which builds in difficulty as time permits. After the technical screen, Databricks begins preliminary level mapping. For L4/L5 roles, the interview loops remain similar.\n\n### Step3: Hiring Manager Call\n\nMost candidates, particularly for senior roles, will have a 1-hour call with a hiring manager for the role they have been mapped to initially. This interview is primarily behavioral, covering your background and experience, what you've worked on and enjoy working on, and questions you have about Databricks and the specific organization. If a hiring manager is unavailable, you might be interviewed by the Director of Engineering for the organization in question.\n\n### Step 4: Onsite Interviews\n\nAt this point, candidates interview for specific teams based on their background and interests. Onsite interview loops vary slightly depending on role (ML, frontend, backend, etc.) and seniority level, but the below is generally what you'll get:\n\n* **Coding 1: Algorithms** (1 hour). This interview will be conducted in CoderPad or a similar online IDE with runnable code. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/databricks-interview-questions#question-coding).\n* **Coding 2:** (1 hour). For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/databricks-interview-questions#question-coding).\n* **Coding: Concurrency/Multithreading** (1 hour). This round focuses on implementing programs that leverage multithreading for efficiency. For more detail about the kinds of questions to expect, see the [Coding: Concurrency/Multithreading section below](https://interviewing.io/databricks-interview-questions#question-coding).\n* **System Design** (1 hour). The system design interview is often conducted using Google Docs, which some candidates find unusual compared to whiteboarding or specialized diagramming tools. For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/databricks-interview-questions#question-design).\n* **Cross-functional/Behavioral** (1 hour). For more detail about the kinds of questions to expect, see the [Cross-functional/Behavioral section below](https://interviewing.io/databricks-interview-questions#question-behavioral).\n\nTypes of Interview Questions to Expect at Databricks\n----------------------------------------------------\n\nDatabricks' technical questions tend to lean toward the difficult side. For coding interviews, expect LeetCode hard more than medium questions.\n\n### Coding\n\nThere are usually 3 coding rounds. Two will focus on Data Structures and Algorithms, and one might focus on concurrency and multithreading. The coding rounds that include algorithm questions are LeetCode medium or hard in terms of level. An example question we heard involved IAP to CIDR (checking if IP address ranges fit into CIDR notation). Another was a variable-sized tic-tac-toe game implementation question.\n\nCandidates report these coding rounds tend to involve tricky optimizations—you can use a brute force solution to make progress, but you should be thinking of optimizations throughout.\n\nIn the Concurrency and Multithreading coding round, you might be asked to implement an efficient logger that processes messages in a queue. This round is considered particularly challenging by most candidates. One engineer we spoke to, who is familiar with the process, told us:\n\n> *This question wasn’t on LeetCode, but preparing by solving the LeetCode questions on concurrency and multithreading will be a huge help.*\n\nBelow are the technical topics you’re likely to encounter in Databricks interviews. To compile this list, we did two things. First, we spoke to some current and former Databricks engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Binary Search](/binary-search-interview-questions)\n\n[Questions   \n& tips](/binary-search-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=binary-search)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Graphs](/graphs-interview-questions)\n\n[Questions   \n& tips](/graphs-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=graphs)\n\n[Sorting](/sorting-interview-questions)\n\n[Questions   \n& tips](/sorting-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=sorting)\n\n### System Design\n\nTheir system design questions are pretty standard, but they will expand on the initial question to dig deeper into your depth of knowledge. One example we heard involved designing a service that provides customers with the cheapest copy of a book they're searching for, which requires considering integration with different book distributors, search functionality, and purchase flows.\n\nBe prepared to use Google Docs for these interviews rather than whiteboarding tools, which some candidates found unusual. This choice seems to be up to the individual interviewer, so don’t be caught off guard!\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Cross-Functional/Behavioral\n\nThis interview is with the hiring manager and includes standard behavioral questions about your past experiences and projects. The recruiter may brief you on the hiring manager's style beforehand. You’ll be asked about your past projects and explore areas of conflict resolution and teamwork. Expect questions like \"Tell me about a time you had a conflict with a coworker\" and \"Describe a project you're most proud of,\" with deep follow-up questions on specifics.\n\n### How HubSpot Makes Hiring Decisions\n\nAfter the onsite interviews, Databricks has a thorough evaluation process that consists of multiple layers of review.\n\nReference checks are an important part of the evaluation, typically involving 1 manager and 2 more senior team members. Databricks emphasizes having impressive references, as they're weighted heavily in the final decision.\n\nNext comes the Hiring Committee review. Unlike the interview panel, this is a separate committee that reviews all aspects of your candidacy: interview feedback, background, career trajectory, and references. Our sources emphasize that the final decision is holistic, so having every aspect of your application as strong as possible is crucial.\n\nFinally, the VP of Engineering reviews the complete candidate packet and has the final say on hiring decisions. One of our sources reported failing at this step despite positive feedback in earlier rounds, suggesting that the bar remains high throughout the entire process.\n\nIn some cases, candidates might be asked to complete an additional take-home assessment after the onsite. While this is rare, it might be used when the recruiter believes it could strengthen a candidate's application. These assessments can take around 5 hours to complete and may involve specialized problems such as database challenges.\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/databricks-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Common FireEye Interview Questions",
      "content": "[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nCommon FireEye Interview Questions\n==================================\n\nBelow are common interview questions that interviewers from FireEye ask in mock interviews on our platform. Because our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\n*We'll add details about FireEye's interview process in the future.*\n\nMEDIUM\n\nData Structures and Algorithms\n\n### [Three Sum](/questions/three-sum)\n\n[Given an array of integers, return an array of triplets such that i != j != k and nums[i] + nums[j] + nums[k] = 0.](/questions/three-sum)\n\nMEDIUM\n\nMathematics\n\n### [Reverse Integer](/questions/reverse-integer)\n\n[Given a 32-bit signed integer, reverse digits of the integer.](/questions/reverse-integer)\n\nFireEye Interview Replays\n=========================\n\n[![Three sum](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FJava_Fireeye_1_19619057a3.png&w=3840&q=75)\n\nFireEye Interviewer\n\nThree sum\n\nWarp Dromedary, a FireEye engineer, interviewed Samurai Loris in Java](/mocks/fire-eye-java-three-sum)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/fireeye-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Morgan Stanley’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Onsite](#step-2)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [Object-Oriented Programming](#question-object-programming)\n* [Hiring Decisions](#hiring)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nMorgan Stanley’s Interview Process & Questions\n==============================================\n\nPublished:\n\nMorgan Stanley's Interview Process for Software Engineers: 3 Steps\n------------------------------------------------------------------\n\nMid to senior-level engineers interviewing at Morgan Stanley can expect the following process:\n\n* Recruiter call (30 minutes)\n* Online assessment (only for some teams) (45 mins)\n* Onsite (3-4 hours)\n\n![Morgan Stanley’s interview process: Recruiter call, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Morgan_Stanley_s_Company_Process_e08eede811.png)\n\nMorgan Stanley has a decentralized hiring process. Each team interviews in its own style, and there are variations in the numbers and types of interviews you might encounter. A lot of it depends on the region you are interviewing in. You can apply to and interview for multiple roles simultaneously.\n\nYour interviewers will be selected from the team you are interviewing for by the hiring manager, based on their ability to assess you for specific skills, e.g., if the role requires you to be a Python expert, they will use another Python expert to interview you. The order of your interviews for the onsite portion is not predetermined, and the hiring manager will decide the schedule based on the availability of the interviewers deemed necessary.\n\nThe process can be quite slow, and they will not expedite things unless they feel like you are a perfect fit. Expect the process to last more than 6 weeks in most cases.\n\nGeneral advice:\n\n* There is a big focus on collaboration, so don’t be combative in interviews.\n* Communication is important for leveling. They want to know if you can communicate cross-functionally or potentially manage a team in the future.\n* Practice the STAR method of answering behavioral interview questions.\n* Just because you don’t get a job with one team doesn’t mean you’ve failed with Morgan Stanley as a whole. You don’t need to wait a year to apply to other teams, and you might find a more suitable role in the course of that same job search.\n\n### Step 1: Recruiter Call\n\nMorgan Stanley’s recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, and why you’re interested in Morgan Stanley. They’ll look for basic skill set alignment. They’ll also talk about the specific role and team you’ve been matched with.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Onsite\n\nAgain, this will vary in terms of interview types and order depending on the team you are interviewing with, but here is a sample schedule:\n\n* **Asynchronous coding challenge** (45 mins). This challenge will be conducted in HackerRank. For some teams, this may happen before the onsite rather than as part of it. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/morgan-stanley-interview-questions#question-coding).\n* **Language-specific coding** (45 mins). Morgan Stanley is one of the few companies we’ve written about whose process isn’t always language agnostic. For some roles and teams, they may need you to know a specific language, and if that’s the case, they will do one round in that language where they’re specifically vetting you for familiarity with that language and its idiosyncrasies. This round is usually conducted in HackerRank. .\n* **Data structures and algorithms** (45 mins). This round is usually conducted in HackerRank. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/morgan-stanley-interview-questions#question-coding).\n* **Object-oriented programming** (45 mins). This will not be a coding interview. You’ll be asked a series of questions on the topic of object-oriented programming. For more detail about the kinds of questions to expect, see the [Object-Oriented Programming section below](https://interviewing.io/morgan-stanley-interview-questions#question-object-programming).\n* **Behavioral** (45 mins) This is a team fit call with the hiring manager. Morgan Stanley’s behavioral interviews are pretty standard fare. You’ll be asked about past projects and your impact in previous roles.\n\nYou might notice the lack of a system design interview in the schedule above. This is typical for a lot of teams although you might interview for a team that includes one. If you do get a system design interview you might be asked product-focused system design questions.\n\nBe prepared for some repetition here. You should be ready to drive into your own product designs.\n\nTypes of Interview Questions to Expect at Morgan Stanley\n--------------------------------------------------------\n\nManagers tend to come up with their own questions, but for core coding interviews, prepping on LeetCode will be very helpful. A lot of the teams will pull questions from there.\n\n### Coding\n\nAs above, you’ll run into a lot of LeetCode-style questions here. Below are the technical topics you’re likely to encounter in Morgan Stanley interviews. To compile this list, we did two things. First, we spoke to some current and former Morgan Stanley engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Binary Trees](/binary-trees-interview-questions)\n\n[Questions   \n& tips](/binary-trees-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=binary-trees)\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Inorder Traversal](/inorder-traversal-interview-questions)\n\n[Questions   \n& tips](/inorder-traversal-interview-questions)\n\n[Watch 2   \n interview replays](/mocks?technical=inorder-traversal)\n\n[Matrices](/matrices-interview-questions)\n\n[Questions   \n& tips](/matrices-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=matrices)\n\n[Sliding Window](/sliding-window-interview-questions)\n\n[Questions   \n& tips](/sliding-window-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=sliding-window)\n\n[Binary Search](/binary-search-interview-questions)\n\n[Questions   \n& tips](/binary-search-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=binary-search)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Graphs](/graphs-interview-questions)\n\n[Questions   \n& tips](/graphs-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=graphs)\n\n[Sorting](/sorting-interview-questions)\n\n[Questions   \n& tips](/sorting-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=sorting)\n\n[Two Pointers](/two-pointers-interview-questions)\n\n[Questions   \n& tips](/two-pointers-interview-questions)\n\n[Watch 3   \n interview replays](/mocks?technical=two-pointers)\n\n### Object-Oriented Programming\n\nBelow are the types of questions you’ll likely be asked during this interview:\n\n* Explain object-oriented programming. What is inheritance? What are the different types of inheritance?\n* What is polymorphism?\n* Give an example of operator overloading\n* What is the difference between a struct and a class?\n* What is the difference between an abstract class and an interface?\n* What is the difference between object-oriented programming and procedural programming?\n\n### How Morgan Stanley Makes Hiring Decisions\n\nDecisions are at the hiring manager's discretion. Some managers will make the decision themselves, and most will take feedback, but they do overrule negative feedback at times.\n\nWant to know if you're ready to interview at Morgan Stanley? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/morgan-stanley-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "A Senior Engineer’s Guide to Apple's Interview Process And Questions",
      "content": "We helped write the sequel to \"Cracking the Coding Interview\". [Read 9 chapters for free →](https://bctci.co/free-chapters)",
      "content_type": "other",
      "source_url": "https://interviewing.io/guides/hiring-process/apple",
      "author": "",
      "user_id": ""
    },
    {
      "title": "HubSpot’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Asynchronous Coding Challenge](#step-2)\n  + [Onsite](#step-3)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Behavioral](#question-behavioral)\n* [Hiring Decisions](#hiring)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nHubSpot’s Interview Process & Questions\n=======================================\n\n*The info below is based on conversations with HubSpot engineers.*\n\nPublished:\n\nHubSpot's Interview Process for Software Engineers: 2-4 Steps\n-------------------------------------------------------------\n\nMid to senior-level engineers interviewing at HubSpot can expect the following process:\n\n* Recruiter call (30 minutes)\n* Asynchronous coding challenge (3 hour)\n* Onsite (4 hours)\n\n![HubSpot’s interview process: Recruiter call, Asynchronous coding challenge, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Hub_Spot_s_Interview_Process_c6178a59fd.png)\n\nHubspot’s hiring process is a hybrid. Although you apply for a specific team from the get-go, your interviews will likely not be with the people on your team – interviewers are randomly selected from a pool – except for the hiring manager interview.\n\nHubspot is one of the few companies that will actually give you some constructive post-interview feedback!\n\nGeneral tips:\n\n* LeetCode practice is helpful, as their coding questions are all in that style.\n* During the coding rounds, it is important to find edge cases.\n* You might read that they are very focused on the behavioral round and your ability to match up to their values, but in practice we’ve heard their behavioral round is pretty standard. That said, the interviewers will talk about the culture so it’s best to come prepared with questions about their values.\n* They use Java, but the interview process is language-agnostic.\n* You must convince the interviewer you are familiar with the technologies you mention in a system design interview. You need to know more than buzzwords.\n* They know that most of their questions are found online, so they look for you to go deeper.\n* You’ll get detailed feedback from the recruiter after the technical onsite rounds. This can be given verbally or via email and will happen before the behavioral round.\n\nThe hiring process takes about 2-4 weeks, and we’ve heard that their interviewers are pretty friendly and collaborative compared to FAANG companies. They are also competitive with FAANG salaries.\n\n### Step 1: Recruiter Call\n\nHubSpot’s recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, why you’re interested in HubSpot, and what you’re looking for moving forward. They will discuss salary ranges, and we’ve heard they are competitive with Meta and Google.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Asynchronous Coding Challenge\n\nIn this challenge, you’ll get API access. You’ll need to make a call to the API, process data, and POST back a response/solution. If the solution is correct, you’ll get a 200 status code, which means you’ve completed the exercise.\n\nYou can use any language for this assignment, and it starts as soon as you click the link in the email they will send you. You have three hours to complete the assignment, i.e., make the request to the API. After that, you’ll need to submit the code in a zip file, but that doesn’t need to happen within the three hour window, so you’ll have time to clean up your code afterward.\n\nOne engineer we spoke to said:\n\n> *I don’t think they even review the code, you just need to get 200, and that’s good enough to pass.*\n\nAnother engineer we spoke to described the problem as follows:\n\n> *Overall, the pattern is going to be a sorting/aggregating approach to a JSON array, where you have some field like “timestamp”, and some other field that might be an enum, and you will be posting it back to the API provided*\n\n### Step 3: Onsite\n\nThis will vary slightly between Senior SWE 1 and Senior SWE 2 but only during the second system design round. **You have to pass each round before moving on to the next.**\n\n* **Coding** (1 hour). This interview will be conducted in CoderPad, but we have heard you can use your own IDE too. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/hubspot-interview-questions#question-coding).\n* **System design** (1 hour). This interview will be conducted in the drawing tool of your choice. For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/hubspot-interview-questions#question-design).\n* **System design 2 (depending on your level, details are different)** (1 hour). For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/hubspot-interview-questions#question-design).\n* **Behavioral** (1 hour). For more info about what questions to expect, see the [Behavioral section below](https://interviewing.io/hubspot-interview-questions#question-behavioral).\n\nTypes of Interview Questions to Expect at HubSpot\n-------------------------------------------------\n\n### Coding\n\nThis will be algorithms and data structures-focused, with LeetCode-style questions. We’ve heard they usually ask LeetCode easy to mediums. While the challenge might seem easy, HubSpot wants you to catch all the edge cases and run tests.\n\nCommunication and clarifying questions are things they are looking for. They know their questions can be found online. They expect you have already seen and solved them.\n\nSome examples of the question types they ask are:\n\n* Most repeated substring of size k\n* Find the matching string pattern in a given string.\n* Find all commonly repeated strings in the 'engineering' string one. How to create an add comma function in js and add it to the global\n* Merge two sorted lists up to size k\n\nBelow are the technical topics you’re likely to encounter in HubSpot interviews. To compile this list, we did two things. First, we spoke to some current and former HubSpot engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Linked Lists](/linked-lists-interview-questions)\n\n[Questions   \n& tips](/linked-lists-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=linked-lists)\n\n[Sliding Window](/sliding-window-interview-questions)\n\n[Questions   \n& tips](/sliding-window-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=sliding-window)\n\n[Parsing](/parsing-interview-questions)\n\n[Questions   \n& tips](/parsing-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=parsing)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Sorting](/sorting-interview-questions)\n\n[Questions   \n& tips](/sorting-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=sorting)\n\n[Two Pointers](/two-pointers-interview-questions)\n\n[Questions   \n& tips](/two-pointers-interview-questions)\n\n[Watch 3   \n interview replays](/mocks?technical=two-pointers)\n\n### System Design\n\nYour first system design round will be very standard. You’ll be asked to design something like Netflix. The main difference between HubSpot and other companies is that they aren’t as obsessed with you mapping out the most optimized solution. It’s more about capturing a broad answer than going deep into one solution.\n\nYour second system design will depend on which role you are interviewing for.\n\n#### Senior SWE 1 (Equivalent to L4 at other companies)\n\nYou will usually be asked to design something like a weather service. One engineer we spoke to described it as being more like application design. The areas you need to consider are:\n\n* Data storage\n* Reliability\n* Performance\n* Horizontal scaling\n* Trade-offs\n\nMake sure to ask clarifying questions before you do anything. Their interviewers are collaborative. If you bring up any technologies, you need to know them well as they will drill into them.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n#### Senior SWE 2 (Equivalent to L5 at other companies)\n\nThis system design round is split into two parts. First, you will be asked to write a document covering a past, technically challenging project and submit it for review They want you to list everything involved from start to finish, including:\n\n* An overview of business problem\n* The challenges faced\n* The technical implementation\n* Any design tradeoffs\n* The rollout plan\n* The chosen success metrics\n* Any lessons learned\n\nThey will review your write-up, and, if they like it, you will be invited to another live interview where they will ask you questions about the project.\n\n### Behavioral\n\nThis interview will be conducted by the hiring manager of the team you are interviewing for. It’s going to be pretty standard, with situational questions. As above, they may talk about the culture at HubSpot so it’s a good idea to come prepared with questions that delve into their values.\n\n### How HubSpot Makes Hiring Decisions\n\nDecisions are reached based on your performance in each round. You can still get an offer with one “no-hire”, but your other rounds need to be very strong for this to happen. Hiring managers may still reject candidates in the team matching phase if they see one “no-hire”.\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/hubspot-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Shopify’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#shopify-interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Life story interview](#step-3)\n  + [Onsite](#step-4)\n    - [Technical deep dive](#step-4-deep)\n    - [Coding/Pair programming](#step-4-coding)\n    - [System Design](#step-4-system)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n* [Common Questions](#common-questions)\n* [Interview Replays](#interview-replays)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nShopify’s Interview Process & Questions\n=======================================\n\n*The info below is based on conversations with Stripe engineers in 2023.*\n\nPublished:\n\nShopify's Interview Process for Software Engineers: 4 Steps\n-----------------------------------------------------------\n\nFor a mid to senior-level software engineer, Shopify’s process (usually) looks like this:\n\n* Recruiter call (30 minutes)\n* Technical phone screen (40 minutes)\n* \"Life Story\" interview (1 hour)\n* Onsite (3-4 hours)\n\n![Shopify’s interview process: Recruiter call, Technical phone screen, Life Story interview, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Shopify_s_Company_Process_2d50eabf8f.png)\n\nShopify’s process is decentralized, which means that you interview for a specific team out of the gate.\n\n### Step 1: Recruiter Call\n\nThe first recruiter call lasts 30 minutes. Its purpose is to check qualifications, hear about past projects, and find out what you’re looking for from Shopify and in general. It also acts as a culture fit check, so they’ll want to see how your communication and personality align with their values. Make sure you’re up to date on Shopify features, blog posts and news. Finally, your recruiter will make sure you understand the role you’re applying for and clarify next steps in the process.\n\nThis is a typical recruiter call, but shorter. It’s primarily meant to gauge your interest in Shopify and lay out the rest of the process. There will be a more detailed recruiter call, called the “Life Story” interview, later on.\n\nThat said, it’s still really important not to reveal your salary expectations or your salary history. We wrote a [detailed post about salary negotiation that lays out exactly what to say when recruiters pressure you to name the first number](https://www.google.com/url?q=https://interviewing.io/blog/negotiate-salary-recruiter&sa=D&source=editors&ust=1687337715912604&usg=AOvVaw2DmZ7rz25IR1Q42FqB380A). Just don’t do it – when you give out information this early in the process, you’re painting future you into a corner.\n\nNormally, we’d also advise you not to reveal where you are in process with other companies, but we’ve heard from our sources that Shopify is very good at moving quickly if you’re getting close to offer stage with other companies, so if you need them to move, make sure to mention that in this call (you don’t have to go into detail about which companies you’re talking to, just that you’re getting close to offers or have some already).\n\n### Step 2: Technical Phone Screen\n\nThe technical phone screen at Shopify is a data structures/algorithms interview conducted via CoderPad. There will likely be 2 interviewers conducting this round.\n\n### Step 3: \"Life Story\" interview\n\nThis interview is conducted by a recruiter, who will be asking you questions about your past to see what motivates and drives you: is there some common thread that’s been a theme or shown up in multiple places in your story and your career to date? Their goal is to suss out whether you’re a “3D person” who can communicate effectively with others, instead of just someone who’s technically gifted. Because Shopify is fully remote, they place extra importance on having employees who are personable and able to have a chat.\n\nYou may also be asked to sign [Shopify’s code of conduct](https://s27.q4cdn.com/572064924/files/doc_downloads/Governance%20document/2019/Code-of-Conduct-as-amended-May-29-2019.pdf) (we were able to find this copy from 2019; please email us to let us know if it’s no longer valid).\n\n### Step 4: Onsite\n\nShopify’s onsite lasts roughly 4 hours and includes the following steps:\n\n* Pair programming (2 hours)\n* Technical deep dive (1 hour)\n* Possibly system design (1 hour; usually just for senior/staff-level engineers)\n\n#### Pair Programming/Coding\n\nYou’ll have two separate pair programming sessions as part of your onsite.\n\nIn these interviews, you’ll be using your IDE of choice and sharing your screen.\n\nWhen you practice for this interview, get in the habit of talking out loud about what may work even if it's not ideal, and explain to the interviewer what you would do differently, or how you would improve it if you had more time. The goal isn’t just to solve the problem, but to show that you know what you’re doing and are able to consider the long-term effects of your design decisions.\n\n#### Technical Deep Dive\n\nIn this interview, you’ll be doing a deep dive on a project you worked on, why it mattered, your contributions, how you overcame challenges, and so on. Be prepared to discuss technical details in depth.\n\n#### System Design\n\nYou may not get a separate system design interview in your loop – those are reserved for senior (and sometimes staff-level candidates). There will be a system design component in your pair programming sessions, however.\n\nTypes of Interview Questions to Expect at Shopify\n-------------------------------------------------\n\n### Coding/Pair Programming\n\nYou’re more likely to get LeetCode-style questions in the technical phone screen, rather than the onsite.\n\nTo figure out what types of questions to expect in your Shopify interviews, we did two things. First, we spoke to some current and former Shopify interviewers in our community. Then we cross-referenced all the anecdotes we heard with our own data-set of mock interviews. Based on all of the above, here are the types of questions you’re likely to encounter:\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Hash Tables](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 5   \n interview replays](/mocks?technical=hash-tables)\n\nDuring the onsite, you will not get LeetCode-style questions. Rather, you will iteratively build something with your interviewer. You’ll start with the simplest possible implementation and then add features to it, testing as you go. Depending on how the interview is going, your interviewer will likely add on up to three additional layers of complexity, and what started as a simple problem will end up with a system design component as well.\n\nExample questions include:\n\n* Design and implement an LRU cache\n* Given a list of products with a price and popularity rating, order them based on price and use popularity as a tiebreaker\n* Implement a discounting feature for retail\n\n### System Design\n\nIf you do get a [system design round](https://interviewing.io/guides/system-design-interview), be prepared for high-level system design questions that will likely expect a tie into a feature/design relevant to Shopify’s product.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\nCommon Shopify Interview Questions\n----------------------------------\n\nBelow are common questions that interviewers from Shopify ask on our platform. Since our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\nHARD\n\nData Structures and Algorithms\n\n### [Binary Array Partition](/questions/binary-array-partition)\n\n[Given an array Z of 0s and 1s, divide the array into 3 non-empty parts, such that all of these parts represent the same binary value.](/questions/binary-array-partition)\n\nShopify Interview Replays\n\n[![Validate string against dictionary](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FRuby_Shopify_1_d597a7fcf5.png&w=3840&q=75)\n\nShopify Interviewer\n\nValidate string against dictionary\n\nCaptain Hamburger, a Shopify engineer, interviewed Dystopian Corgi in Ruby](/mocks/shopify-ruby-validate-string-against-dictionary)\n\nWant to know if you're ready to interview at Shopify? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/shopify-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "A Senior Engineer’s Guide to Netflix's Interview Process and Questions",
      "content": "We helped write the sequel to \"Cracking the Coding Interview\". [Read 9 chapters for free →](https://bctci.co/free-chapters)",
      "content_type": "other",
      "source_url": "https://interviewing.io/guides/hiring-process/netflix",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Uber’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Onsite](#step-3)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Collaboration and Leadership](#question-collaboration)\n  + [Bar Raiser](#question-bar-raiser)\n* [Hiring Decisions](#hiring)\n* [Interview Replays](#interview-replays)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nUber’s Interview Process & Questions\n====================================\n\nPublished:\n\nUber's Interview Process for Software Engineers: 3 Steps\n--------------------------------------------------------\n\nMid to senior-level engineers interviewing at Uber can expect the following process:\n\n* Recruiter call (30 minutes)\n* Technical Phone Screen (1 hour)\n* Onsite (4-5.5 hours)\n\n![Uber’s interview process: Recruiter call, Technical Phone Screen, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Uber_s_company_process_337ca52852.png)\n\nUber’s interview process was centralized in the past but is in the process of becoming decentralized because they tend to only hire for specific roles in the current market. Right now, it looks to be a hybrid, and practically speaking, that means if you were to apply today, you might be matched with a specific team at the outset or you might do team matching at the end. We’ve seen both, and it appears to be a function of how niche your skills are (e.g., if you’re an ML engineer, you’ll likely be matched with a specific team at the outset).\n\nThat said, your interviewing panel will likely all be from the same org and use a company-wide, internal question bank. Candidates tend to interview for only one role, and though the process is somewhat standardized, the hiring manager will have a say in what the process will look like as well, so expect a little bit of variance.\n\nGeneral tips:\n\n* LeetCode practice will help a lot with the coding portion. The system design interviews are pretty standard as well.\n* They want you to write code that runs during coding rounds.\n* Referrals will help; you may even be able to skip the technical phone screen with one.\n* Be prepared to be grilled on your past experience and projects, particularly during the Collaboration and Leadership and Bar Raiser rounds.\n* If you go through team matching, it may take a while (weeks to months).\n\nThe entire process takes about 4-6 weeks but possibly longer if you end up having to do team matching.\n\n### Step 1: Recruiter Call\n\nUber’s recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll speak about the role and team you are interviewing for, discuss salary, location, and level expectations, and ask about your interest in Uber.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Technical Phone Screen\n\nAlthough we heard from one engineer that they were able to skip this stage with a referral, most senior engineers will get a pretty standard technical screen here. It will be conducted in CodeSignal, and there may be one or two questions. The questions themselves will be algorithmic. They have an internal bank of questions that most interviewers use, some of which are versions of real problems that Uber has had to solve in the past, and some of which are standard algorithmic questions with some Uber-isms papered over the top, but there does seem to be some variance. Either way, expect LeetCode medium in terms of difficulty.\n\nIt’s important to write fully compilable code here, i.e., code that runs. You should also run test cases.\n\nOne engineer, familiar with the process told us:\n\n> *You might get slightly harder questions first, and it’s OK to ask for help. If you do solve it without help, it’s a definite pass, but you can still pass with help assuming your communication skills etc. are good.*\n\n### Step 3: Onsite\n\nThis will vary slightly by role but most engineers can expect something like this:\n\n* **Coding** (1 hour). This interview will be conducted in CodeSignal. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/uber-interview-questions#question-coding).\n* **Second coding (depth in specialization)** (1 hour). This interview will be conducted in CodeSignal. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/uber-interview-questions#question-coding).\n* **System design** (1 hour). This interview will be conducted in CodeSignal. For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/uber-interview-questions#question-design).\n* **Collaboration and Leadership** (75 mins). This is with the hiring manager for the role. For more info about what questions to expect, see the [Collaboration and Leadership section below](https://interviewing.io/uber-interview-questions#question-collaboration).\n* **Bar raiser** (1 hour) This is with a special bar raiser interviewer. For more infor about what questions to expect, the [Bar Raiser section below](https://interviewing.io/uber-interview-questions#question-bar-raiser).\n\nTypes of Interview Questions to Expect at Uber\n----------------------------------------------\n\n### Coding\n\nThis round will be just like the technical phone screen – they have an internal bank of questions that most interviewers use, some of which are versions of real problems that Uber has had to solve in the past, and some of which are standard algorithmic questions with some Uber-isms papered over the top. If you get LeetCode-style questions, they’ll be of medium difficulty or slightly harder. Note that you may get two questions, but if you do, they’ll likely be mediums.\n\nYou should get in the habit of running test cases, if it’s not something you do by default already..\n\nBelow are the technical topics you’re likely to encounter in Uber interviews. To compile this list, we did two things. First, we spoke to some current and former Uber engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Linked Lists](/linked-lists-interview-questions)\n\n[Questions   \n& tips](/linked-lists-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=linked-lists)\n\n[Matrices](/matrices-interview-questions)\n\n[Questions   \n& tips](/matrices-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=matrices)\n\n[Binary Search](/binary-search-interview-questions)\n\n[Questions   \n& tips](/binary-search-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=binary-search)\n\n[Parsing](/parsing-interview-questions)\n\n[Questions   \n& tips](/parsing-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=parsing)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Graphs](/graphs-interview-questions)\n\n[Questions   \n& tips](/graphs-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=graphs)\n\n### Second Coding (Depth of Specialization)\n\nWe’ve heard from multiple engineers that this round is sometimes just another LeetCode-style coding round but sometimes it’s not an algorithmic question at all. This round is meant to be more reflective of the role you are interviewing for and may include some design aspects.\n\nOne engineer we spoke with said:\n\n> *This round is supposed to be more focused on [a mix of code and] designing… where you might have to build a mobile game like Snake, but in reality it’s very similar to the first coding round, and interviewers often use something from the internal question bank.*\n\nNote that your recruiter may not be prepared to set expectations for this round, as there seems to be a lot of variability in how it’s run, possibly at the request of the hiring manager. Ask as much as you can about the round beforehand, but be prepared for a possible curveball.\n\n### System Design\n\nMost of the people we interviewed told us to expect a very standard system design round, so you shouldn’t expect many surprises here. That said, they have an internal bank of questions that are based on old problems they had to solve at Uber so you might be asked to solve one of those. You might be asked to design a map that shows every Uber driver in the world. It won’t be a typical distributed systems question (like design a chat app), and you will need to focus on scale. Think about Uber in the early days and the problems they had to solve.\n\nYou’ll see other, more standard, questions such as:\n\n* Design Dropbox\n* Design Facebook Messenger\n* Design a ride-sharing app (surprise!)\n* Design a card game\n* Design an industrial system\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Collaboration and Leadership\n\nThis interview will be conducted by the hiring manager for the role you are interviewing for, or in some cases, a hiring manager from the same org.\n\nThere seem to be two distinct styles used:\n\n* You will be asked to go through past projects and how they were delivered\n* You will be asked how you *would* deliver potential projects\n\nThe hiring manager can take this in any direction they see fit,but they are looking to test your ability to work on a team, your ability to deal with failures, and your leadership skills. Have your past projects ready to discuss in detail, they might drill down into one specific project if it seems interesting. Be able to quantify both your impact and the impact of the project.\n\nIf the role is a speciality role, they might ask you questions that are relevant to it, e.g., if the role includes some frontend aspects, be prepared for some frontend questions.\n\n### Bar Raiser\n\nOk, this is where things get serious. If you’ve made it this far, it’s a good sign. We only spoke to one Uber engineer who did not get a bar raiser round and got hired. Everyone else told us that you need to pass one to get hired and that they sometimes won’t schedule it if the rest of your onsite hasn’t gone well.\n\nSimilar to [Amazon](https://interviewing.io/guides/hiring-process/amazon#amazon), which is known for this type of interview, the bar raiser interviewer will be a specialty interviewer from across the company. They won’t be part of the team you are interviewing for. They will choose the focus on the interview. We’ve heard it’s usually a reverse system design / project introspection style of interview, but if you weren’t strong in one of the other onsite rounds, it might be a repeat of that.\n\nIf it is a reverse system design / project introspection interview, they will likely ask you to discuss one past project in deep detail. They will ask questions around:\n\n* The technical complexity of the project\n* The project’s impact\n* The high level architecture decisions and the pros and cons associated\n* The time it took to complete the project and/or reach each milestone\n* Any failures\n\nBe prepared to quantify the project’s impact where possible. The project discussed should match the level of seniority you are interviewing for in terms of scale and scope. A staff engineer will need to talk about a project that spans at least 2 or 3 quarters of work for example, but a senior engineer can use something a little less complex. They are looking for your depth of knowledge here and this round is used for leveling. It’s probably the most important round you will have.\n\n### How Uber Makes Hiring Decisions\n\nThe entire interview panel meets, and the decision should be unanimous. If there is a mixed signal, people on the panel can try to convince others until they achieve consensus. Typically, one soft no-hire can be turned around, but one strong no-hire will be enough to reject you. In rare cases, the Bar Raiser interviewer can also overturn a hire decision.\n\nUber Interview Replays\n----------------------\n\n[![K closest elements](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FJava_Uber_2_c1be7b108e.png&w=3840&q=75)\n\nUber Interviewer\n\nK closest elements\n\nBlue Centurion, an Uber engineer, interviewed Mutable Pigeon in Java](/mocks/uber-java-k-closest-elements)\n\nWant to know if you're ready to interview at Uber? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/uber-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Palantir’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Onsite](#step-3)\n  + [Hiring Manager Screen](#step-4)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Re-engineering](#question-reengineering)\n  + [Problem Decomposition](#question-problem-decomposition)\n  + [Hiring Manager Interview](#question-hiring-manager)\n* [Hiring Decisions](#hiring)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nPalantir’s Interview Process & Questions\n========================================\n\n*The info below is based on conversations with Palantir engineers.*\n\nPublished:\n\nPalantir's Interview Process for Software Engineers: 4 Steps\n------------------------------------------------------------\n\nMid to senior-level engineers interviewing at Palantir can expect the following process:\n\n* Recruiter call (30 minutes)\n* Technical phone screen (1 hour)\n* Onsite (3 hour)\n* Hiring manager screen (1 hour)\n\nGeneral tips:\n\n* They put a huge emphasis on cultural fit. Behavioral questions will be asked in every interview. Come prepared!\n* Palantir believes in protecting civil liberties and rights. They look for people who are comfortable discussing these topics.\n* It’s a mix of standard LeetCode-style questions and very non-standard questions. LeetCode prep isn’t going to be enough here.\n\n![Palantir’s interview process: Recruiter call, Technical phone screen, Onsite, Hiring manager interview](https://strapi-iio.s3.us-west-2.amazonaws.com/Palantir_interview_process_af7ed42bbf.png)\n\nAt Palantir, you apply for a catch-all backend or full stack role and team matching will happen later in the process. The process doesn’t vary much between roles although there are some variations for different levels of seniority. The recruiter might adjust the track you are on very slightly through the process but you’re more likely to do team matching after the onsite.\n\nThe entire process takes and 3-4 weeks but it’s possible to expedite things, especially with other offers in hand.\n\n### Step 1: Recruiter Call\n\nPalantir’s recruiter call lasts 30 minutes, and it’s an important step. They filter out more candidates than most companies do at this stage from what we hear. They are looking for your motivations to join the company, so come to this call prepared to discuss why you want to be there in detail. It helps to have a compelling story as to what drew you to Palantir. You might get asked about your favorite, and least favorite past projects, as well as what you want to work on moving forward. They look for any red flags indicating you won’t be there long-term. An engineer who knows their process very well says:\n\n> *“Surface-level motivations to join the company won’t get you far and they will reject strong technical candidates if they don’t seem like a good cultural fit!”*\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Technical Phone Screen\n\nPalantir’s technical phone screen lasts about an hour and happens in CodePair. This interview will feature algorithms and data structures but, the difference is that it won’t be purely technical and the algorithm you are solving will be put in the context of something you are building for an end-user. You’ll get asked to solve a problem in one half of the interview and behavioral questions in the other half. We will cover what we know of their question style in the section called [“Types of Interview Questions to Expect at Palantir” below](https://interviewing.io/palantir-interview-questions#question-types).\n\n### Step 3: Onsite\n\nPalantir has 4 types of interviews they give most engineers during the onsite loop. **You will only get 3 of the 4, but we will describe each here.**\n\nEvery onsite interview will have 20 minutes of behavioral questions!\n\n* **Problem decomposition** (1 hour). This is one of the options that you will almost definitely get. It’s really important and non-standard. For more detail about the kinds of questions to expect, see the [Problem decomposition section below](https://interviewing.io/palantir-interview-questions#question-problem-decomposition).\n* **System design** (1 hour). This will be conducted in CodePair using the whiteboard functionality. For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/palantir-interview-questions#question-design).\n* **Re-engineering** (1 hour). You’ll be given a piece of code in CodePair and asked to review it for bugs. For more detail about the kinds of questions to expect, see the [Re-engineering section below](https://interviewing.io/palantir-interview-questions#question-reengineering).\n* **Coding** (1 hour). This will be similar to the technical screen before the onsite, and is conducted in CodePair. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/palantir-interview-questions#question-coding).\n\n### Step 4: Hiring Manager Interview\n\nThis interview will be a repeat of one of the onsite interviews. They look for red flags during the onsite and like to revisit any area they felt wasn’t up to scratch. It might not even be the technical piece they want to review! Again, the behavioral questions are very important.\n\nTypes of Interview Questions to Expect at Palantir\n--------------------------------------------------\n\nWe can’t stress enough how embedded behavioral questions are at Palantir. As there is no behavioral interview during the onsite, they like to assess your motivations, cultural fit, and ability to work collaboratively all through the process. Come prepared with STAR-style answers, stories about why you want to work for Palantir, and ways you’ve thought about the end-user experience in other roles. If you propose a solution, talk about how it could be improved or modified for the users' benefit. You will also be asked about what projects you’ve enjoyed and not enjoyed. They want to know you’re OK working on all sorts of projects, not just glamorous ones.\n\n### Coding\n\nThe coding problems posed before are during the onsite will reflect the type of work you will be doing at Palantir, i.e., building products for end-users. So, while the core technical question might look like a standard LeetCode-style problem, it will be put in the context of a larger solution for an end-user. You’ll have to solve the problem but also consider and discuss its impact on the whole system, and how it will be used by the user. You’ll need to ask a lot of clarifying questions as it will be under-defined at first. Just keep thinking about the end-user and what trade-offs you will have to make in order to make the user experience positive. Maybe your solution is memory intensive, or maybe it's more runtime complexity intensive - think about and discuss improvements you could make to help the user in future iterations. They value user-centric thinking and empathy in finding solutions over algorithmic complexity. Think about how the user will use the code and how it could be improved, e.g., maybe you could implement caching so the user has a better experience each time they use the solution.\n\nA user who is very familiar with the Palantir interview process had this to say:\n\n> *“What strategies do you consider for enhancing performance beyond just the code you’re currently working on? Consider aspects such as caching and pre-computation to improve efficiency. Initially, your solution may not be the fastest, but implementing these strategies could significantly speed up subsequent operations.*\n>\n> *\"Expect the problem to be intentionally vague. It’s important to anticipate and address any ambiguous scenarios, demonstrate caution regarding potential issues, and safeguard the system from malicious users.”*\n\nRegardless of what question you get, they’ll ask about decision-making, time and space complexity, etc. They want to know why you would approach a problem in a certain way, and they look for things that might cause issues with client integrations, etc.\n\nBelow are the technical topics you’re likely to encounter in Palantir interviews. To compile this list, we did two things. First, we spoke to some current and former Palantir engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Search](/search-interview-questions)\n\n[Questions   \n& tips](/search-interview-questions)\n\n[Watch 7   \n interview replays](/mocks?technical=search)\n\n[Sets](/sets-interview-questions)\n\n[Questions   \n& tips](/sets-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=sets)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Graphs](/graphs-interview-questions)\n\n[Questions   \n& tips](/graphs-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=graphs)\n\n[Queues](/queue-interview-questions)\n\n[Questions   \n& tips](/queue-interview-questions)\n\n[Watch 5   \n interview replays](/mocks?technical=queue)\n\n### System Design\n\nAgain, this will be an industry standard round bar the added behavioral aspects. Our [system design guide](https://interviewing.io/guides/system-design-interview) has been called “excellent preparation” by someone very familiar with Palantir’s interview.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Re-engineering\n\nIn this interview, you will be given a bunch of code in CodePair and asked to review it for bugs. You’ll be told how the system is supposed to work and that the output is wrong when you run the code. It will be about 500-100 lines of code, and it’s really important not to get lost in the first issue you see. Review everything from top to bottom as they will probably put in red herrings to distract you!\n\n### Problem Decomposition\n\nThis is probably the most important technical round you will face. It’s a non-standard interview where you will be asked to solve a high-level, real-world technical problem, with a real focus on thinking about the end-user. You won’t have to write code outside of maybe some pseudocode.\n\nThe problem might be something like:\n\n*\"How would you design a technology to help elderly people with poor vision who want to cook for themselves?\"*\n\nIt will be focused on an individual or an organization. Your goal is to come up with how to use technology to solve this problem. A lot of the focus here is on breaking the problem down into its component parts so you can tackle it. You’ll need to think about the time your solution will take, as well as how the respective parts of the solution might be built by different teams.\n\nThere will be two parts:\n\n* Ideation: Break the problem down into its respective pieces and propose a solution to each. Agree on the solution with your interviewer.\n* Execution: How you would design the solution at a high-level. This is more like system design. You won’t need to go too deep technically, e.g., you won’t need to provide detail on the type of database you would use, but you might discuss the type of data / schema you’d need to solve the problem.\n\nThe important thing here is, as with other rounds, to focus on the end-user. Think about how your solution would be used. Discuss ways to make it better. You’re being asked to display both your ability to problem-solve and your ability to empathize with users/clients.\n\n### Hiring Manager Interview\n\nAfter the onsite, the interview panel meets with a hiring manager to discuss your performance. If you pass, they’ll match you with a team that needs your skillset. From there, you’ll interview again with the hiring manager for that team. This will be your final round and it will repeat one of the above rounds. If they see a red flag around motivation or culture fit, they’ll be looking to test you on it again.\n\n### How Palantir Makes Hiring Decisions\n\nThe final decision rests with the hiring manager you interview with last from what we can tell.\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/palantir-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Jane Street’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Onsite](#step-3)\n* [Question Types](#question-types)\n  + [Coding/System Design](#question-coding)\n  + [Technical Project Deep Dive](#question-project)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nJane Street’s Interview Process & Questions\n===========================================\n\n*The info below is based on conversations with Jane Street engineers.*\n\nPublished:\n\nJane Street's Interview Process for Software Engineers: 3 Steps\n---------------------------------------------------------------\n\nMid to senior-level engineers interviewing at Jane Street can expect the following hiring process:\n\n* Recruiter call (30 minutes)\n* Technical phone screen (1 hour)\n* Onsite (5 hours)\n\n![Jane Street’s interview process: Recruiter call, Technical phone screen, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Jane_Street_s_Interview_Process_a3566cc29a.png)\n\nAt Jane Street, the process is centralized, i.e., you interview with a central team and then get matched to a specific team after you pass the onsite loop.\n\nGeneral tips:\n\n* Work on your coding stamina - the onsite is pretty intense.\n* Practice coding something up from scratch – during the onsite, they’ll ask you to build something from nothing, e.g., a Tetris game\n* Their interviewers want you to be collaborative, so ask lots of questions and communicate well throughout the process. They value collaboration as much as they do good code.\n* Read Jane Street’s [blog post about their interview process](https://blog.janestreet.com/applying-to-jane-street/). Unlike most company pages, it’s detailed, useful, and good, and talks a lot about engineering interviews, specifically.\n* Don’t try to impress them by coding in OCaml (their internal functional language of choice). Use the language you’re most comfortable in.\n\nThe entire process takes about 4 weeks.\n\n### Step 1: Recruiter Call\n\nJane Street’s recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, why you’re interested in Jane Street, your understanding of Jane Street’s value proposition, and what you’re looking for moving forward.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Technical Phone Screen\n\nThis is a pretty standard technical round, with one LeetCode medium-style question that will have two parts. Your interviewer will be more interested in how you think and how you arrive at the solution than an optimal solution. Communicate your assumptions and discuss alternate approaches as you work your way through the problem.\n\n### Step 3: Onsite\n\n* **3 coding / system design hybrid rounds** (75 mins each). These interviews will be conducted in CoderPad. For more detail about the kinds of questions to expect, see the [Coding / System Design section](https://interviewing.io/jane-street-interview-questions#question-coding) below.\n* **Technical project deep dive** (75 mins). For more info about what questions to expect, see the [Technical project deep dive section](https://interviewing.io/jane-street-interview-questions#question-project) below.\n\nTypes of Interview Questions to Expect at Jane Street\n-----------------------------------------------------\n\n### Coding / System Design\n\nWhereas the technical screen before the onsite is LeetCode-style and more academic, the coding rounds during the onsite are a little bit more practical. They also blend coding and system design so you might have to whiteboard out some pieces initially before jumping into code.\n\nYou will get a problem statement that is underspecified. Your job is to ask the right questions before you dive in and while you are working. Get comfortable building things from scratch. You might be asked to:\n\n* Design Tetris\n* Design a video player API\n\nYou’ll have to design it, code it out, and go deep into the logic.\n\nBelow are the technical topics you’re likely to encounter in Jane Street interviews. To compile this list, we did two things. First, we spoke to some current and former Jane Street engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Trees](/trees-interview-questions)\n\n[Questions   \n& tips](/trees-interview-questions)\n\n[Watch 7   \n interview replays](/mocks?technical=trees)\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Maps](/maps-interview-questions)\n\n[Questions   \n& tips](/maps-interview-questions)\n\n[Watch 2   \n interview replays](/mocks?technical=maps)\n\n[Matrices](/matrices-interview-questions)\n\n[Questions   \n& tips](/matrices-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=matrices)\n\n[Search](/search-interview-questions)\n\n[Questions   \n& tips](/search-interview-questions)\n\n[Watch 7   \n interview replays](/mocks?technical=search)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Stacks](/stacks-interview-questions)\n\n[Questions   \n& tips](/stacks-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=stacks)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Memoization](/memoization-interview-questions)\n\n[Questions   \n& tips](/memoization-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=memoization)\n\n### Technical Project Deep Dive\n\nYou will be asked to present an interesting, technically complex project that you’ve worked on, ideally something that you can talk about for an hour. They tell you not to over-prepare for this one, so you don’t necessarily need a robust presentation, but do prepare to answer a lot of questions about the project you’re presenting, the reasoning and logic behind it, and its impact.\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/jane-street-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "A Senior Engineer’s Guide to Meta's Interview Process and Questions",
      "content": "We helped write the sequel to \"Cracking the Coding Interview\". [Read 9 chapters for free →](https://bctci.co/free-chapters)",
      "content_type": "other",
      "source_url": "https://interviewing.io/guides/hiring-process/meta-facebook",
      "author": "",
      "user_id": ""
    },
    {
      "title": "SpaceX’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Hiring Manager Call](#step-2)\n  + [Asynchronous Coding Assessment](#step-3)\n  + [Onsite](#step-4)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Behavioral](#question-behavioral)\n* [Hiring Decisions](#hiring)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nSpaceX’s Interview Process & Questions\n======================================\n\nPublished:\n\nSpaceX's Interview Process for Software Engineers: 4 Steps\n----------------------------------------------------------\n\nMid to senior-level engineers interviewing at SpaceX can expect the following process:\n\n* Recruiter call (15 minutes)\n* Hiring manager call (1 hour)\n* Asynchronous coding assessment (3 hours of work, 2 weeks to complete)\n* Onsite (5 hours)\n\n![SpaceX’s interview process: Recruiter call, Hiring manager call, Asynchronous coding assessment, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Space_X_Company_Process_7639e7f4fb.png)\n\nSpaceX has a hybrid hiring process. You apply to a general job posting, and the recruiting team will decide if they want to have a call. After you speak with them, they will shortlist your profile and show it to different hiring managers. If a hiring manager thinks there is a good fit, you’ll move forward to speak with them. If you make it through that call and an offline coding assessment, you’ll head to the onsite phase where you’ll be interviewed by engineers from the hiring manager’s team.\n\nTo work at SpaceX you need to be a permanent resident and/or citizen of the United States.\n\nFrom what we’ve been told, the entire process can take about four weeks.\n\nGeneral advice:\n\n* Go onsite in person. They put a lot of stock in personal relationships, and visiting the site is worth it!\n* They don’t need you to get to full solutions, focus on asking the right questions and showing them how you think about the problems.\n* LeetCode is helpful for practice, but they are really looking for your ability to solve real-world challenges and pull from your experience.\n* Invest a lot of time in the presentation\n* Know [Elon Musk's 5-step Design Process](https://modelthinkers.com/mental-model/musks-5-step-design-process).\n\n### Step 1: Recruiter Call\n\nSpaceX’s recruiter call lasts 15 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, and why you’re interested in SpaceX\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Hiring Manager Call\n\nThis is usually a hybrid call, with some questions about your experience and some technical questions. As the hiring manager has picked your profile from a shortlist, they will have seen something in your background that they want to delve into. After that, you might get a system design question that relates to your previous work or the team you are interviewing with.\n\n### Step 3: Asynchronous Coding Assessment\n\nThis assessment should take you 3 hours, but you have 2 weeks to complete it. It takes place on Codility. Expect medium-level LeetCode questions that you can find online but with slightly different descriptions.\n\n### Step 4: Onsite\n\nThe onsite can be virtual or on-location. As above, try to actually go onsite if possible. Seeing the rockets alone is worth it but, they also like to meet people in person. You’ll get an additional 45 mins to have lunch with the recruiter. This is a great time to ask good questions and build rapport.\n\n* **Facility tour** (if in person, 25 mins) The recruiter will walk you around the site.\n* **Prepared project/source code presentation** (1 hour). You’ll be asked to suggest five ideas ahead of time, and they’ll pick the one they’d like you to present in person. The presentation will be in front of the entire team. This is a really important step in the process. You’ll be asked to present your project and some of the source code (if possible), and they will ask lots of questions throughout your presentation. Keep Elon Musk’s 5-step Design Process in mind here. They look for simplicity in your approach.\n* **Coding** (1 hour). This will likely be in CodeSignal. There might be some design elements here in addition to coding, and it will usually relate to SpaceX. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/spacex-interview-questions#question-coding).\n* **Lunch** (if onsite, 45 mins) This will be with the recruiter and is a great time for you to ask good questions.\n* **Second Coding** (1 hour). This will likely be in CodeSignal and is usually conducted by two interviewers. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/spacex-interview-questions#question-coding).\n* **System design** (1 hour) For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/spacex-interview-questions#question-design).\n* **Behavioral** (1 hour) This interview is with the hiring manager. For more detail about the kinds of questions to expect, see the [Behavioral section below](https://interviewing.io/spacex-interview-questions#question-behavioral).\n\nTypes of Interview Questions to Expect at SpaceX\n------------------------------------------------\n\n### Coding\n\nDon’t expect LeetCode-style questions at the onsite stage. Instead, you’ll see hybrid questions that have an algorithmic component, a coding component, and a design component. SpaceX likes to ask questions that make you think and draw on previous experience. A lot of their coding questions will bear some relation to real-world SpaceX problems. Talk through your approach, as it’s more important to show how you think than to reach a full solution.\n\nFrom one of our users who recently went through SpaceX interviews:\n\n> *The coding portion was more about normal logic and problem-solving design questions which did not require deep algorithmic knowledge: more about understanding the problem, asking the right questions, and providing SIMPLE and scalable design.*\n\nA sample question might focus on spare parts for rockets and how to keep track of them as they move in and out of refrigeration. You may have to write code that tracks and logs the time each spare part is out, for instance.\n\nBelow are the technical topics you’re likely to encounter in SpaceX interviews. To compile this list, we did two things. First, we spoke to some current and former SpaceX engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Binary Search](/binary-search-interview-questions)\n\n[Questions   \n& tips](/binary-search-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=binary-search)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Stacks](/stacks-interview-questions)\n\n[Questions   \n& tips](/stacks-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=stacks)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Sorting](/sorting-interview-questions)\n\n[Questions   \n& tips](/sorting-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=sorting)\n\nOutside of the topics above, you may see questions about data hazards, memory management, and generally questions that get at your understanding of how programming works under the hood (e.g., compilers).\n\n### System Design\n\nHere you will likely be presented with a relatively straightforward design task but not given many details. For instance, you might be asked to design an inventory management system that\ninvolves multiple locations and items, similar to the example in the coding section above, except that in this interview, you’ll be focusing more on the design and less on the coding parts. In the inventory management system question, you’ll have to:\n\n* Design database tables for managing spare part inventory\n* Think about how to handle movement and transfers of items\n* Discuss how you would do this at scale.\n\nNote that these questions will intentionally sound simple, and your interviewer will not provide a lot of details – they’re looking to see how you deal with ambiguity and how you ask questions to fill in the blanks.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Behavioral\n\nThe behavioral interview is another critical step in the process. It’s possible that you perform well in this interview but poorly in one or more coding sessions yet remain in the process. If that happens’ you’ll likely be asked to do another coding interview.\n\nExpect to encounter similar questions to the [Amazon Leadership Principles](https://interviewing.io/guides/amazon-leadership-principles) type of interview during this round.\n\nOne engineer we spoke to also said they were asked how they felt about working with Product Managers. They told SpaceX it wasn’t always necessary to work with one and the interviewer seemed happy. It’s possible they do not have PMs.\n\n### How SpaceX Makes Hiring Decisions\n\nDecisions are at the hiring manager's discretion but they look for consensus. If you fail a round you might be asked to complete an additional interview. For example, if you fail one of the coding rounds, you might get another to complete.\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/spacex-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Coinbase’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [CodeSignal Assessment](#step-1)\n  + [Recruiter call](#step-2)\n  + [Onsite](#step-3)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Hiring Manager Interview](#question-hiring)\n* [Hiring Decisions](#hiring)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nCoinbase’s Interview Process & Questions\n========================================\n\nPublished:\n\nCoinbase’s process is almost completely decentralized, with each team typically running its own hiring funnel. This means you can interview with multiple teams although they sometimes allow you to skip stages if they feel like they’d be duplicating their efforts. There doesn’t seem to be any downside to interviewing for more than one role at a time. In fact, if you end up interviewing for multiple teams, you may be able to skip steps (e.g. not have to repeat CodeSignal).\n\nA good way to get into the process with Coinbase is to find a role you like and then track down the lead recruiter or hiring manager aligned to it on Linkedin. Recruiters will also share candidates they like with other teams if they don’t have a slot open on their specific team.\n\nGeneral tips for your Coinbase interviews:\n\n* It’s OK to reach out to your recruiter or hiring manager contact to ask questions\n* If you feel like there was some miscommunication during an interview, be proactive and reach out to the recruiter afterward to clear things up\n* Apply for the right level and don’t be too aggressive – it’s better to secure a role at a lower level than to fail out of a higher one.\n\n![Coinbase’’s interview process: CodeSignal assessment, Recruiter call, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/coinbase_process_flowchart_6439ce881a.png)\n\nCoinbase’s Interview Process for Software Engineers: 3 Steps\n------------------------------------------------------------\n\nMid to senior-level engineers interviewing at Bloomberg can expect the following process:\n\n* CodeSignal assessment (70 minutes)\n* Recruiter call (30 minutes)\n* Onsite (3 hours)\n\n### Step 1: CodeSignal Assessment\n\nThis is an asynchronous challenge focused on data structures and algorithms. As of the date of publication, it includes 4 questions for you to work on. The first one is usually a warm-up. The others are LeetCode medium-level.\n\n### Step 2: Recruiter call\n\nCoinbase’s recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, why you’re interested in Coinbase, your understanding of Coinbase’s value proposition, and what you’re looking for moving forward. They’ll also review the specific role you’re applying for to make sure you understand the expectations and requirements and go over the hiring process. Once you pass this stage you’ll enter the team pipeline.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 3: Onsite\n\nThe Coinbase onsite is usually done over two days and includes three, one-hour long interviews. You’ll do two technical interviews and one final call with a hiring manager. For the technical portions, you can use your own IDE with a screen-share, or they can provide tooling.\n\n* **Technical interview** (1 hour). For more detail about the kinds of questions to expect, see the “Types of Interview Questions to Expect at Coinbase” section [below](https://interviewing.io/coinbase-interview-questions#question-types). You can typically use whatever language you want, although a few teams do require you to use a specific language.\n* **System design interview** (1 hour). As above.\n* **Interview with a hiring manager** (1 hour). As above\n\nTypes of Interview Questions to Expect at Coinbase\n--------------------------------------------------\n\nEach team has its own questions, but there are consistencies in the type you will encounter at each stage.\n\n### Coding\n\nThis round is a “pseudo project” – you’ll be asked to build a feature, and you’ll get input data in the form of a mini database. You will have one hour to get as far as you can.\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Inorder Traversal](/inorder-traversal-interview-questions)\n\n[Questions   \n& tips](/inorder-traversal-interview-questions)\n\n[Watch 2   \n interview replays](/mocks?technical=inorder-traversal)\n\n[Maps](/maps-interview-questions)\n\n[Questions   \n& tips](/maps-interview-questions)\n\n[Watch 2   \n interview replays](/mocks?technical=maps)\n\n[Matrices](/matrices-interview-questions)\n\n[Questions   \n& tips](/matrices-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=matrices)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Graphs](/graphs-interview-questions)\n\n[Questions   \n& tips](/graphs-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=graphs)\n\n[Memoization](/memoization-interview-questions)\n\n[Questions   \n& tips](/memoization-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=memoization)\n\n### System Design\n\nSystem design questions at Coinbase tend to skew practical. Many questions involve setting up microservices and then showing the network topology of your chosen architecture. Basically, they will want to test your understanding of microservice architecture but through the lens of building a product.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Hiring Manager Interview\n\nThis will be your final interview, and it will be conducted by a hiring manager. You’ll likely be asked some culture fit/behavioral questions, but at this point, the hiring manager will also be trying to sell you on the role.\n\n### How Coinbase Makes Hiring Decisions\n\nTheir interviewing team will all meet after the onsite and give their feedback to the hiring manager, who will make the decision. A director will then review the decision and has the power to veto, but this very rarely happens.\n\nWant to know if you're ready to interview at Coinbase? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/coinbase-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Atlassian’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Onsite](#step-3)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Values](#question-values)\n  + [Manager Interview](#question-manager)\n* [Hiring Decisions](#hiring)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nAtlassian’s Interview Process & Questions\n=========================================\n\n*The info below is based on conversations with Atlassian engineers.*\n\nPublished:\n\nAtlassian's Interview Process for Software Engineers: 3 Steps\n-------------------------------------------------------------\n\nMid to senior-level engineers interviewing at Atlassian can expect the following hiring process:\n\n* Recruiter call (30 minutes)\n* Technical phone screen (1 hour)\n* Onsite (4-5 hours)\n\n![Atlassian’s interview process: Recruiter call, Technical phone screen, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Atlassian_s_Interview_Process_f7d2fec3b4.png)\n\nAt Atlassian, the process is centralized, meaning you won’t do team matching until after the onsite phase. You’ll be interviewed by people from different teams during the interview loop.\n\nGeneral tips:\n\n* Their coding questions are discoverable online – they have a set bank that they use\n* You can use any language for the coding rounds and your own IDE\n* They have published their own [guide to the engineering hiring process](https://www.atlassian.com/company/careers/resources/interviewing/how-to-nail-your-engineering-interview)\n* They hire remotely, but salaries are location-specific\n\nThe entire process takes about 4-6 weeks.\n\nAn engineer familiar with their hiring process told us that Atlassian’s “[Chaos Score](https://interviewing.io/guides/hiring-process#the-chaos-score) is very low, i.e., there is a high level of consistency in terms of process and outcomes for different candidates.\n\n### Step 1: Recruiter Call\n\nAtlassian’s recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, why you’re interested in Atlassian, and how the teams you’ve worked on before have been structured. They’ll also discuss the possible roles that are available.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Technical Phone Screen via Karat\n\nAtlassian uses a third-party interview service called Karat to conduct the technical screen that comes before the onsite. This seems to be true for most engineering roles but not all. If you haven’t been interviewed by Karat before, you’ll be interviewed by an engineer who does NOT work for Atlassian but who’s getting paid by Karat to conduct interviews on their behalf.\n\nIf you are interviewing for a backend or full stack engineering role, you’ll get a coding question. It will be a LeetCode-style, medium-level question selected from Atlassian’s question bank. You can find examples of the types of questions they ask online.\n\nIf you aren’t interviewing for a backend or full stack engineering role, we’ve heard that this can be a coding round OR a system design round and may be conducted by an Atlassian engineer, rather than an outsourced third party like Karat. If that ends up being the case, you will be able to use your own IDE or tooling, whatever language you’d prefer, and even any developer tools you use during your day-to-day, outside of AI technologies.\n\nIf you get a system design round, you might be asked to design something like a task list or job scheduler. You’ll be able to use any tooling you’d like to build the system. It’s really important to show how you are thinking about the problem by asking good questions, discussing tradeoffs and constraints, and mentioning both technologies and internal partners you might use to achieve your goal.\n\n### Step 3: Onsite\n\nThis will vary slightly by role but here’s what you can expect during the back-end and full-stack hiring loops.\n\n* **Coding x 2** (60 mins each). These interviews will be conducted in your IDE of choice. For more detail about the kinds of questions to expect, see the [Coding section](https://interviewing.io/atlassian-interview-questions#question-coding) below.\n* **System design** (60 mins). This interview will be conducted in the drawing tool of your choice. For more detail about the kinds of questions to expect, see the [System Design section](https://interviewing.io/atlassian-interview-questions#question-design) below.\n* **Values** (45 mins). For more info about what questions to expect, see the [Values section](https://interviewing.io/atlassian-interview-questions#question-values) below.\n* **Manager interview** (60 mins) For more info about what questions to expect, see the [Manager interview section](https://interviewing.io/atlassian-interview-questions#question-manager) below.\n\nTypes of Interview Questions to Expect at Atlassian\n---------------------------------------------------\n\n### Coding\n\nAtlassian has a bank of questions that they use for coding rounds, and you can find examples online. Except something that feels like a LeetCode medium-level question.\n\nThey are trying to test out how you would approach a problem if you encountered it at work, so you’re able to use the language and tooling you are comfortable with. They are assessing you based on 5 key criteria:\n\n1. Code Quality: Clean up your code as you go and act as if it is eventually going into production.\n2. Adaptability: Be prepared to be given additional requirements to incorporate as you go and be open to change when it makes sense.\n3. Conceptual Thinking: Break down the problem in components and discuss it in conceptual terms before you start to solve it.\n4. Decision Making: They put more focus on why you approach a problem from a certain angle than whether you solve it. Discuss the pros and cons of your decisions.\n5. Resourcefulness: They want to see how you use your experience and other resources to solve problems when needed.\n\nBe prepared to discuss how to scale your solution as well!\n\nBelow are the technical topics you’re likely to encounter in Atlassian interviews. To compile this list, we did two things. First, we spoke to some current and former Atlassian engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Priority Queues](/queue-interview-questions)\n\n[Questions   \n& tips](/queue-interview-questions)\n\n[Watch 3   \n interview replays](/mocks?technical=priority-queues)\n\n[Search](/search-interview-questions)\n\n[Questions   \n& tips](/search-interview-questions)\n\n[Watch 7   \n interview replays](/mocks?technical=search)\n\n[Parsing](/parsing-interview-questions)\n\n[Questions   \n& tips](/parsing-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=parsing)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Sorting](/sorting-interview-questions)\n\n[Questions   \n& tips](/sorting-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=sorting)\n\n### System Design\n\nAgain, you can use any tooling you’d like for this round. You might be asked to:\n\n* Design a job scheduler\n* Design a task list\n* Design a tagging system\n\nThis is a pretty standard system design interview. You won’t need to code anything and the focus will be on how you would build the system and the tradeoffs, considerations and optimizations that might be involved.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Values\n\nThis interview can be conducted by anyone at the company. You might talk to someone from Marketing, Sales or any other department. It will feature scenario-based behavioral questions. They prefer the STAR framework for answering questions, so brush up on that methodology. It’s a good idea to review [Atlassian’s values](https://www.atlassian.com/company/values) and incorporate them into your answers. Expect questions about how you’ve handled giving and getting feedback, how you’ve worked on teams, etc.\n\n### Manager Interview\n\nThis is another behavioral interview, this time with an engineering manager, and it will be a deep dive into a past project you’ve worked on with a team, so be sure to prepare. The types of questions that you get here will focus on things you learned while working as part of a team and the scale and scope of your impact. They will ask questions around conflict resolution, the lessons you’ve learned, how you’ve mentored others, and how you were able to drive outcomes. Again, knowing Atlassian’s values, and incorporating them into your answers will help.\n\n### How Atlassian Makes Hiring Decisions\n\nDecisions are made by a hiring committee. We don’t have a lot of information on their process but we do know that they sometimes ask you to complete an additional round if you didn’t quite meet the bar during the onsite.\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/atlassian-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Snowflake’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Hiring Manager Call](#step-2)\n  + [Technical Phone Screen](#step-3)\n  + [Onsite](#step-4)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n* [Common Questions](#common-questions)\n* [Hiring Decisions](#hiring-decision)\n* [Interview Replays](#interview-replays)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nSnowflake’s Interview Process & Questions\n=========================================\n\nPublished:\n\nSnowflake’s Interview Process for Software Engineers: 4 Steps\n-------------------------------------------------------------\n\nMid to senior-level engineers interviewing at Snowflake can expect the following process:\n\n* Recruiter call (30 minutes)\n* (Possible) Hiring manager screen (30 minutes)\n* Technical phone screen (2 hours)\n* Onsite (4 hours)\n\n![Snowflake’s interview process: Recruiter call, possibly a hiring manager screen, technical phone screen, onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Snowflake_s_Company_Process_4813d73a40.png)\n\nSnowflake’s process isn’t centralized, so there can be differences from team to team in terms of exactly how many interviews are required, what type, and what order they’re scheduled in. You’ll also generally be interviewing with members of the team you’ve applied to.\n\n### Step 1: Recruiter Call\n\nSnowflake’s recruiter call is pretty typical. They’ll ask about your background and interests, and how they relate to the role you’re interviewing for. In some cases this call will also replace the hiring manager call, depending on if the recruiter is aware of exactly what the hiring manager of the specific team is looking for.\n\nIt’s really important, at this stage, not to reveal your salary expectations, your salary history, or where you are in the process with other companies. We wrote a [detailed post about salary negotiation that lays out exactly what to say when recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter). Just don’t do it – when you give out information this early in the process, you’re painting future you into a corner.\n\n### Step 2: Hiring Manager Call\n\nWhether you talk to a hiring manager or not depends on which team you’re applying for and the relationship that the hiring manager and the recruiter have – if the hiring manager feels that the recruiter is well-calibrated, they may opt to skip this step.\n\nIn the event that you do speak with a hiring manager, they’ll talk to you about your background and see how well it fits with the role and their specific team. They’re looking to see how well your passions, interests, and previous experience ties into the Snowflake values as well, and how it will translate into your role. Ideally, they’d like to see some overlap in your hobbies and the things you’ll be doing at work.\n\nThe hiring managers at Snowflake are encouraged to share all good candidates with other teams, so even if you’re not a good fit for their exact team, they may help you find something else.\n\n### Step 3: Technical Phone Screen\n\nThe technical phone screen at Snowflake consists of BOTH a data structures/algorithms interview AND A system design interview. That’s why it takes 2 hours. These are conducted via CoderPad and Zoom.\n\n### Step 4: Onsite\n\nIn addition to having a strong back-end focus, Snowflake’s interviews test your ability to perform within a specific team. It’s also important to note that since Snowflake is a database company, a lot of the interview questions will involve having some more database expertise and knowledge than your average company.\n\nSnowflake’s onsite is usually virtual and takes place over Zoom using CoderPad. While the type/amount of rounds may vary from team to team, generally you can expect to see the following depending on your level:\n\n##### **Mid-to-senior (IC3 and 4):**\n\n* **Presentation (30 minutes)**: This interview is usually with multiple engineers from the team you’re applying for and is an opportunity to showcase a past project in a way that highlights your technical ability and foundational knowledge.\n* **Coding (1 hour)**\n* **Expertise (1 hour)**: This is a technical interview that will touch on skills relevant for the specific team you’re interviewing for. It may be coding, system design, or both. Or it could be a deep dive into a specific language or framework (again depending on the team).\n* **Behavioral (1 hour)**: This interview is usually with a project manager but sometimes a hiring manager will do it, depending on your team.\n* **Close (1 hour)**: This is a final call, with a recruiter or a senior leader (depending on the team), to answer any last questions about the role and team, and to potentially wrap up any loose ends from previous rounds.\n\n##### **Principal (IC5 and above):**\n\n* **Presentation (30 minutes)**: Same as mid-level above.\n* **Coding (1 hour)**\n* **Expertise (1 hour)**: Same as mid-level above.\n* **Behavioral (1 hour)**: Same as mid-level above.\n* **Cross-functional (1 hour)**: This will be an interview with members from multiple teams across Snowflake who will come prepared with their own questions as it pertains to their department and your potential team.\n* **Close (1 hour)**: Same as mid-level above.\n\nTypes of Interview Questions to Expect at Snowflake\n---------------------------------------------------\n\n### Coding\n\nFor coding interviews, you can expect algorithmic questions that are around a LeetCode medium, but with a Snowflake-specific twist and a focus on database internals. An example question is: Get a log file, aggregate the logs, and do it in a way that won’t take too long.\n\nEven if the questions aren’t pure LeetCode, they will touch on a variety of technical topics. As such, to figure out what technical topics to expect in your Snowflake interviews, we did 2 things. First, we spoke to some current and former Snowflake interviewers in our community. Then we cross-referenced all the anecdotes with our own data-set of mock interviews. Based on all of the above, here are the types of questions you’re likely to encounter:\n\n[Binary Trees](/binary-trees-interview-questions)\n\n[Questions   \n& tips](/binary-trees-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=binary-trees)\n\n[Binary Search](/binary-search-interview-questions)\n\n[Questions   \n& tips](/binary-search-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=binary-search)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Hash Tables](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 5   \n interview replays](/mocks?technical=hash-tables)\n\n[Memoization](/memoization-interview-questions)\n\n[Questions   \n& tips](/memoization-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=memoization)\n\n### System Design\n\nThis interview will focus more on data systems and database design, and the generic “Design Instagram” types of questions are much less common.\n\nAn example question could be: Take in events that happened at a certain time, keep track of and organize them so that you can retrieve them quickly, and insert new events quickly while keeping track of the tradeoffs.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\nCommon Snowflake Interview Questions\n------------------------------------\n\nBelow are common questions that interviewers from Snowflake ask on our platform. Since our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\nMEDIUM\n\nData Structures and Algorithms\n\n### [Decode String](/questions/decode-string)\n\n[Given an encoded string, return its decoded string.](/questions/decode-string)\n\n### How Snowflake Makes Hiring Decisions\n\nOnce you’ve finished your onsite, the team you’re interviewing with at Snowflake will meet and discuss your performance, review the interviews and make a decision. This usually takes a few days, after which you’ll receive feedback for each round and find out whether or not there’s an offer.\n\nSnowflake Interview Replays\n---------------------------\n\n[![Decode string](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FPython_Snowflake_1_7e14aae6af.png&w=3840&q=75)\n\nSnowflake Interviewer\n\nDecode string\n\nSupreme Kraken, a Snowflake engineer, interviewed Stealthy Hawk in Python](/mocks/snowflake-python-decode-string)\n\n[![ID generator](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FC_Snowflake_2_1e71c2aa2c.png&w=3840&q=75)\n\nSnowflake Interviewer\n\nID generator\n\nWinged Avenger, a Snowflake engineer, interviewed Ghost Koala in C++](/mocks/cplusplus-id-generator)\n\nWant to know if you're ready to interview at Snowflake? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/snowflake-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Common Slack Interview Questions",
      "content": "[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nCommon Slack Interview Questions\n================================\n\nBelow are common interview questions that interviewers from Slack ask in mock interviews on our platform. Because our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\n*We'll add details about Slack's interview process in the future.*\n\nHARD\n\nData Structures and Algorithms\n\n### [Transformation Dictionary](/questions/transformation-dictionary)\n\n[Given a dictionary of words, determine whether it is possible to transform a given word into another with a fixed number of characters.](/questions/transformation-dictionary)\n\nSlack Interview Replays\n=======================\n\n[![Transformation dictionary](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FPython_Slack_1_9b79e7f753.png&w=3840&q=75)\n\nSlack Interviewer\n\nTransformation dictionary\n\nSpasmodic Pizza, a Slack engineer, interviewed Winter Griffin in Python](/mocks/slack-python-transformation-dictionary)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/slack-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Affirm’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Onsite](#step-3)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Behavioral](#question-behavioral)\n* [Hiring Decisions](#hiring)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nAffirm’s Interview Process & Questions\n======================================\n\nPublished:\n\nAffirm's Interview Process for Software Engineers: 3 Steps\n----------------------------------------------------------\n\nMid to senior-level engineers interviewing at Affirm can expect the following process:\n\n* Recruiter call (30 minutes)\n* Technical phone screen (1 hour)\n* Onsite (4 hours)\n\n![Affirm’s interview process: Recruiter call, Technical phone screen, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Affirm_hiring_process_f008cbaa24.png)\n\nAffirm has a hybrid process, which means that you interview for a specific org but will have engineers involved from across the company on your panel, as well as a hiring manager from the specific org you are interviewing for.\n\nThe entire process can be completed within 2 weeks. You’ll be asked to provide 3 different times that you’re available so you can be more, or less, aggressive in how you approach it.\n\n### Step 1: Recruiter Call\n\nAffirm’s recruiter call lasts 30 minutes, and it’s more about prep for the hiring process than anything else. You might be asked about your interest in Affirm, and they will likely talk about the role you are applying for.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Technical Phone Screen\n\nAffirm’s technical phone screen lasts about an hour and is conducted in HackerRank.\n\nWe will cover what we know of their question style in the section called “Types of Interview Questions to Expect at Affirm” [below](https://interviewing.io/affirm-interview-questions#question-types).\n\n### Step 3: Onsite\n\nYou can do the onsite section in one day or over two days.\n\n* **Behavioral** (45 mins). This is with the hiring manager of the team you are interviewing with. For more detail about the kinds of questions to expect, see the [Behavioral section below](https://interviewing.io/affirm-interview-questions#question-behavioral).\n* **Coding** (1 hour). This will be conducted in HackerRank. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/affirm-interview-questions#question-coding).\n* **Second coding** (1 hour). As above.\n* **System design** (1 hour). This will also be conducted in Hackerrank. For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/affirm-interview-questions#question-design).\n\nTypes of Interview Questions to Expect at Affirm\n------------------------------------------------\n\n### Coding\n\nAffirm’s coding questions are pretty straightforward and will be familiar to anyone practicing on LeetCode. You can commonly find them online. That said, their interviewers will try to put their own spin on questions – expect some variations, but if you can solve medium LeetCode problems, you should be fine.\n\nBelow are the technical topics you’re likely to encounter in Affirm interviews. To compile this list, we did two things. First, we spoke to some current and former Affirm engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Binary Trees](/binary-trees-interview-questions)\n\n[Questions   \n& tips](/binary-trees-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=binary-trees)\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Matrices](/matrices-interview-questions)\n\n[Questions   \n& tips](/matrices-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=matrices)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Stacks](/stacks-interview-questions)\n\n[Questions   \n& tips](/stacks-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=stacks)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Recursion](/recursion-interview-questions)\n\n[Questions   \n& tips](/recursion-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=recursion)\n\n### System Design\n\nThis round won’t be a completely generic system design interview. It will usually relate to Affirm’s business. Expect questions about databases, multiple layers of schema, and functional design.\n\nYou might be asked to design a system that enables transactions between two users via their bank accounts but that has certain constraints built-in e.g., they can’t transfer more than what is in their account.\n\nTiming is important here. You won’t have enough time to go into everything in detail. Try to cover everything at a high level, and ask your interviewer which areas would be most interesting to delve into more thoroughly.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Behavioral\n\nExpect the standard behavioral questions here. Questions like:\n\n* Have you ever felt pressure at work? How did you handle it?\n* When have you worked on a good team? Can you describe why it was good?\n* Have you ever received criticism from leadership at work?\n\nThis round is important as it is conducted by your potential manager. You can do well in the coding rounds but still fail out if you don’t perform well in the behavioral interview.\n\n### How Affirm Makes Hiring Decisions\n\nAll the interviewers submit their feedback, and the decision is made by the hiring manager.\n\nWant to know if you're ready to interview at Affirm? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/affirm-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "A Senior Engineer’s Guide to Google’s Interview Process and Questions",
      "content": "We helped write the sequel to \"Cracking the Coding Interview\". [Read 9 chapters for free →](https://bctci.co/free-chapters)",
      "content_type": "other",
      "source_url": "https://interviewing.io/guides/hiring-process/google",
      "author": "",
      "user_id": ""
    },
    {
      "title": "TikTok’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Onsite](#step-3)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Behavioral](#question-behavioral)\n* [Hiring Decisions](#hiring)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nTikTok’s Interview Process & Questions\n======================================\n\nPublished:\n\nTikTok's Interview Process for Software Engineers: 5 Steps\n----------------------------------------------------------\n\nMid to senior-level engineers interviewing at TikTok can expect the following process, although there are variations from team to team:\n\n* Recruiter call (30 minutes)\n* Moral equivalent of an onsite (even though each interview is separate, and you have to pass one to get to the next) consists of:\n  + Technical phone screen (30 minutes)\n  + Second technical phone screen (1 hour)\n  + System design (1 hour)\n  + Behavioral (1 hour)\n\n![TikTok’s interview process: Recruiter call, Multi-step “virtual onsite” (where you have to pass each round to get to the next)](https://strapi-iio.s3.us-west-2.amazonaws.com/Tik_Tok_s_Company_Process_a844d35564.png)\n\nAt TikTok, the process is decentralized, and you can usually interview for more than one team at a time. We have heard that certain teams do not allow this, but for the most part, it’s safe to apply to multiple positions. As each team manages its own headcount and process it is unlikely that you will be able to skip any steps for one team that you’ve already completed for another, i.e., you’ll still have to complete the full process each time. Your interviewers will all come from the team you are interviewing for.\n\nAs a lot of TikTok’s engineers are based in China, particularly their senior managers, you can expect to have interviews that overlap with Chinese business hours (e.g., Sunday evenings, other evenings).\n\nUnlike other companies where your virtual onsite will have all of your interviews scheduled on the same day, at TikTok you are likely to do one interview at a time, having to pass each one to get to the next.\n\nThe entire process takes about 5 weeks, although it can move much faster, especially if you let them know that you need to have your interviews complete by a certain date, e.g., if you have competing offers. TikTok is very amenable to moving quickly if they think there’s a chance they’ll lose out, but you’ll have to actively and repeatedly remind your recruiter to move faster.\n\nGeneral tips for interviewing with TikTok:\n\n* Be prepared to interview during Chinese business hours and even to be asked questions in Mandarin if you’ve added it to your resume.\n* Be prepared for huge variations in the process between teams. TikTok is hiring like crazy, and they don’t have an official process.\n* Practice LeetCode-style questions! They rely heavily on these questions in your first few technical rounds.\n* If you need them to move quickly, they can, but you have to really push and keep pushing. That goes for scheduling, and it goes for getting an offer in a timely fashion as well. If you keep bringing up that you have another offer, they’ll move.\n\n### Step 1: Recruiter Call\n\nTikTok’s recruiter call lasts 30 minutes, and it’s more informational than anything else. You’ll be briefed on the process and prepared for the first interview. You may also be asked some questions about the company and your background.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\nIf you have offer deadlines coming up and need TikTok to move faster, mention that on this call, and keep reminding your recruiter of it. We’d advise not going into details about where you’re interviewing til you’re ready to start negotiating (read more how to manage your negotiations at the start of your job search), but we do advise hammering home the point that you’re in a rush (if you are).\n\n### Step 2: Technical Phone Screen\n\nTikTok’s first technical phone screen is relatively straightforward and should only take about 30 minutes. It’s usually done in HackerRank. If you brush up on LeetCode questions, you should do fine here.\n\n### Step 3: Onsite\n\nAt TikTok, there isn’t a discrete onsite portion of the hiring process. Instead, you will do each interview one by one and have to pass them one at a time to move forward. That said, once you get past the first technical screen, the order will look similar to other company’s onsite interviews. Onsite interview loops also vary depending on the role and seniority, but the below is generally what you’ll get:\n\n* **Coding** (1 hour). This is much more challenging than the technical phone screen and should last about an hour. The interview is usually conducted by someone on the team you are interviewing for and will use HackerRank. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/tiktok-interview-questions#question-coding).\n* **System design** (1 hour). This interview will be in HackerRank and will be conducted by a hiring manager on the team. For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/tiktok-interview-questions#question-design).\n* **Behavioral** (1 hour). This interview will be conducted by a skip-level manager. For more info about what questions to expect, see the [Behavioral section below](https://interviewing.io/tiktok-interview-questions#question-behavioral).\n\nTypes of Interview Questions to Expect at TikTok\n------------------------------------------------\n\nThe types of questions asked at TikTok vary by team, but you can expect a lot of industry-standard LeetCode-style questions.\n\n### Coding\n\nExpect LeetCode-style questions here. Most teams are language agnostic. The first interview will be fairly simple, based on feedback we’ve gotten. The second one is a lot harder. A few of the engineers we spoke to recommended brushing up on synchronized read/write locks for the second interview, in particular.\n\nOutside of concurrency and threading, below are the technical topics you’re likely to encounter in TikTok interviews. To compile this list, we did two things. First, we spoke to some current and former TikTok engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Binary Trees](/binary-trees-interview-questions)\n\n[Questions   \n& tips](/binary-trees-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=binary-trees)\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Matrices](/matrices-interview-questions)\n\n[Questions   \n& tips](/matrices-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=matrices)\n\n[Binary Search](/binary-search-interview-questions)\n\n[Questions   \n& tips](/binary-search-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=binary-search)\n\n[Parsing](/parsing-interview-questions)\n\n[Questions   \n& tips](/parsing-interview-questions)\n\n[Watch 1   \n interview replay](/mocks?technical=parsing)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Dynamic Programming](/dynamic-programming-interview-questions)\n\n[Questions   \n& tips](/dynamic-programming-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=dynamic-programming)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Graphs](/graphs-interview-questions)\n\n[Questions   \n& tips](/graphs-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=graphs)\n\n[Depth-First Search (DFS)](/depth-first-search-interview-questions)\n\n[Questions   \n& tips](/depth-first-search-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=depth-first-search)\n\n[Breadth-First Search (BFS)](/breadth-first-search-interview-questions)\n\n[Questions   \n& tips](/breadth-first-search-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=breadth-first-search)\n\n### System Design\n\nThis round focuses mostly on large systems and will likely involve designing an entire service with scalability, reliability, and usability concerns in mind. Some example questions:\n\n* [Design LeetCode](https://interviewing.io/questions/design-leetcode). What technologies would you use and why? What are you optimizing for? How would logging work for every single user action on such a large scale? And so on.\n* Design a system that can handle logging on a large scale for every single user action. Imagine 100k users are coming in and there are 5 different services. Each service has to do its own logging and send it to one final database. How would you do this?\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Behavioral\n\nThis varies by role, but be prepared to discuss your background, impactful projects you’ve worked on, and times you’ve shown initiative.\n\nCulture fit seems to be very important to TikTok, even as they are growing so fast. Their teams are based mainly in China so that brings its own unique flavor, and you should be prepared for it.\n\n### How TikTok Makes Hiring Decisions\n\nThis varies from team to team, but it’s likely at the discretion of the hiring manager, with input from other team members.\n\nWant to know if you're ready to interview at TikTok? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/tiktok-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Spotify’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#Spotify-interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Onsite](#step-3)\n    - [Coding](#step-3-coding)\n    - [System Design](#step-3-system)\n    - [Case Study](#step-3-case)\n    - [Behavioral/Values](#step-3-behavioral)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n* [Common Questions](#common-questions)\n* [Hiring Decisions](#hiring-decision)\n* [Interview Replays](#interview-replays)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nSpotify’s Interview Process & Questions\n=======================================\n\nPublished:\n\nSpotify's Interview Process for Software Engineers: 3 Steps\n-----------------------------------------------------------\n\nFor a mid to senior-level software engineer, Spotify’s process (usually) looks like this:\n\n* Recruiter call (30 minutes)\n* Technical phone screen (75 minutes)\n* Onsite (4 hours)\n\n![Spotify’s interview process: Recruiter call, Technical phone screen, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Spotify_s_Company_Process_3347d11538.png)\n\nSpotify is notorious for moving slowly, so don’t be surprised if there are a couple of weeks of lull in between each of these steps.\n\nSpotify’s interview process is a hybrid, which means that while you interview for a specific team at the outset, your interviewers won’t all be from that team. Usually, you will meet the hiring manager and one other engineer on the team. The rest of your interviewers will be from other teams. One exception to this rule is high-profile teams, in which case you may meet with 2 different engineers from that team.\n\nBecause you’re interviewing with a specific team from the outset, there is no additional matching round. However, for open-ended roles or in cases where you did well in your interviews, but the original team wasn’t a fit for whatever reason, they’ll pass you to a call with hiring managers where they sell you on joining their teams instead.\n\n### Step 1: Recruiter Call\n\nThis is a typical recruiter call, where they’ll ask about previous experience, relevant projects, and why you’re interested in Spotify. They’ll also elaborate on the role and confirm that your experience and expectations are a good match.\n\nIt’s really important, at this stage, not to reveal your salary expectations, your salary history, or where you are in the process with other companies. We wrote a [detailed post about salary negotiation that lays out exactly what to say when recruiters pressure you to name the first number](https://www.google.com/url?q=https://interviewing.io/blog/negotiate-salary-recruiter&sa=D&source=editors&ust=1687337715912604&usg=AOvVaw2DmZ7rz25IR1Q42FqB380A). Just don’t do it – when you give out information this early in the process, you’re painting future you into a corner.\n\n### Step 2: Technical Phone Screen\n\nThe exact process can vary from team to team. Generally, you’re asked technical trivia questions and some values-based questions, followed by coding.\n\nHow interviews are conducted also varies. The most frequently used tools are Coderpad, HackerRank, and in some cases, you’ll be asked to share your screen and use your IDE while you work through the problems you’re given.\n\nFinally, you may be asked to describe a past relevant project you’ve worked on. Make sure you’re prepared to demo and/or go into detail.\n\n### Step 3: Onsite\n\nSpotify’s onsite usually lasts 4 hours and consists of the following steps:\n\n* Case Study (1 hour)\n* Coding (1 hour)\n* System design (1 hour)\n* Behavioral/Values (1 hour)\n\nThe order of these rounds can vary, as well as the number of coding interviews required. Depending on your score for various portions of the onsite, you may be required to complete an extra system design, object oriented design, or algorithms interview. There may be extra rounds for certain teams and roles as well. Given that Spotify’s interview process is centralized, you generally won’t be interviewing with engineers or managers from the team you’ll end up on.\n\n#### Coding\n\nThe technical interview will consist of domain-specific questions, values-based questions, and a few medium to hard difficulty LeetCode-style questions.\n\nAs with the technical phone screen, tooling is team-dependent.\n\n#### System Design\n\nFor the system design interview, Spotify wants to see a broad and general understanding of the system design process, they'll also mix in domain specific questions. Generally the questions will follow the format of \"Design a system that does (x)\" with an opportunity for follow-up questions on scalability.\n\nSpotify uses [Mural](https://mural.co) for system design interviews.\n\n#### Case Study\n\nThe most important round in Spotify’s onsite is the case study interview. In this interview, you’ll be presented with a very open-ended real-world problem that may require debugging a system to figure out what’s wrong with it or to figure out why some process has stalled.\n\nSome system design diagrams may be shared with you, along with fake terminals, code snippets, and so on.\n\nThis interview requires lots of dialogue and critical thinking, and the expectation is that you’ll ask your interviewer a bunch of questions. Successful candidates will have a broad understanding of the software engineering process itself, as well as a broad understanding of the system design process.\n\n#### Values/Behavioral\n\nSpotify places great importance on their values and on candidates being good culture fits. Their values are as follows:\n\n* Innovative\n* Collaborative\n* Passionate\n* Playful\n* Sincere\n\nIn this interview, you’ll get a series of behavioral questions that try to get at the following (taken verbatim from their careers site):\n\n* Do your values align with our values?\n* Do you take ownership of your work and take pride in what you deliver?\n* Are you adaptable?\n* Are you collaborative?\n* Are you driven?\n\nTypes of Interview Questions to Expect at Spotify\n-------------------------------------------------\n\n### Coding\n\nThis interview usually has a few medium to hard difficulty LeetCode-style questions.\n\nTo figure out what types of questions to expect in your Spotify interviews, we did two things. First, we spoke to some current and former Spotify interviewers in our community. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews. Based on all of the above, here are the types of questions you’re likely to encounter.\n\n[Binary Trees](/binary-trees-interview-questions)\n\n[Questions   \n& tips](/binary-trees-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=binary-trees)\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[MapReduce](/mapreduce-interview-questions)\n\n[Questions   \n& tips](/mapreduce-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=mapreduce)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Depth-First Search (DFS)](/depth-first-search-interview-questions)\n\n[Questions   \n& tips](/depth-first-search-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=depth-first-search)\n\n[Breadth-First Search (BFS)](/breadth-first-search-interview-questions)\n\n[Questions   \n& tips](/breadth-first-search-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=breadth-first-search)\n\n### System Design\n\nFrom what we’ve heard, Spotify’s [system design interview](https://interviewing.io/guides/system-design-interview) follows the format of “Design a system (or feature) that does X” followed by a discussion about scalability, concurrency, and/or load balancing.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\nCommon Spotify Interview Questions\n----------------------------------\n\nBelow are common questions that interviewers from Spotify ask on our platform. Since our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\nEASY\n\nData Structures and Algorithms\n\n### [Palindrome Generator](/questions/palindrome-generator)\n\n[Print out all 8-digit palindromes. Limitation: We can't use string manipulation.](/questions/palindrome-generator)\n\nHow Spotify Makes Hiring Decisions\n----------------------------------\n\nAt Spotify, the hiring manager has final say over who gets hired. They do have a rubric, but there is no specific score or bar that guarantees an offer, and the hiring manager has space to hire someone for potential or because they were impressed with the candidate’s communication style, even if their technical skills were borderline.\n\nSpotify Interview Replays\n-------------------------\n\n[![Even palindrome generator](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FJava_Spotify_1_f56b84ca13.png&w=3840&q=75)\n\nSpotify Interviewer\n\nEven palindrome generator\n\nThe Benevolent Enigma, a Spotify engineer, interviewed Spasmodic Donut in Java](/mocks/spotify-java-even-palindrome-generator)\n\nWant to know if you're ready to interview at Spotify? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/spotify-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Common MathWorks Interview Questions",
      "content": "[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nCommon MathWorks Interview Questions\n====================================\n\nBelow are common interview questions that interviewers from MathWorks ask in mock interviews on our platform. Because our data comes from mock interviews, questions may not be exactly the same as what you'd see in real interviews.\n\n*We'll add details about MathWork's interview process in the future.*\n\nHARD\n\nData Structures and Algorithms\n\n### [Binary Array Partition](/questions/binary-array-partition)\n\n[Given an array Z of 0s and 1s, divide the array into 3 non-empty parts, such that all of these parts represent the same binary value.](/questions/binary-array-partition)\n\nMathWorks Interview Replays\n===========================\n\n[![Verify rotated integer](/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FJava_Math_Works_1_5fb6569727.png&w=3840&q=75)\n\nMathWorks Interviewer\n\nVerify rotated integer\n\nJocular Panther, a MathWorks engineer, interviewed Quantum Tetrahedron in Java](/mocks/mathworks-java-verify-rotated-integer)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/mathworks-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "DoorDash’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#interview-process)\n  + [Recruiter Call](#step-1)\n  + [Hiring Manager Screen](#step-2)\n  + [Technical Phone Screen](#step-3)\n  + [Onsite](#step-4)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n  + [Behavioral](#question-behavioral)\n* [Hiring Decisions](#hiring)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nDoorDash’s Interview Process & Questions\n========================================\n\nPublished:\n\nDoorDash's Interview Process for Software Engineers: 4 Steps\n------------------------------------------------------------\n\nMid to senior-level engineers interviewing at DoorDash can expect the following process:\n\n* Recruiter call (30 minutes)\n* Hiring manager screen (1 hour)\n* Technical phone screen (1 hour)\n* Onsite (4-5 hours)\n\nGeneral tips:\n\n* Have STAR-style behavioral answers prepared because behavioral interviews are used for leveling\n* Polish up on system design. Along with behavioral, it’s what determines your level, and the bar is high.\n\n![Recruiter call, Hiring Manager screen, Technical phone screen, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Door_Dash_Interview_Process_64864553b9.png)\n\nAt DoorDash, the process has recently moved from centralized to decentralized. This means that the interview process will vary from team to team somewhat. You apply and interview for a specific role, and the hiring manager for that role will be part of your loop. You can interview for up to 3 roles at a time and will encounter interviewers from across the organization in most scenarios.\n\nThe entire process takes about 3-4 weeks but can be completed in as little as 2 weeks, according to our sources.\n\n### Step 1: Recruiter Call\n\nDoorDash’s recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, why you’re interested in DoorDash, your understanding of DoorDash’s value proposition, and what you’re looking for moving forward. They’ll also review the specific role you’re applying for to make sure your skill set matches the requirements. After this call, the recruiter will circulate your resume and specific hiring managers will have the opportunity to pick it up and request to interview you.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Hiring Manager Screen\n\nThis interview will be with the hiring manager for the team you applied to. They will ask you to do a deep dive into a previous project.\n\n### Step 3: Technical Phone Screen\n\nThis is a pretty standard technical screen and normally gets conducted in CodePair (HackerRank). Our sources tell us that LeetCode medium questions are pretty typical in this round.\n\n### Step 4: Onsite\n\nAt this point, candidates are split into different loops depending on the role they are interviewing for, e.g., infrastructure, frontend, backend etc. Onsite interview loops vary slightly depending on the role and seniority, but the below is generally what you’ll get:\n\n* **Two coding rounds** (1 hour each). These rounds usually take place in CodePair (HackerRank). For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/doordash-interview-questions#question-coding).\n* **System design** (1 hour). For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/doordash-interview-questions#question-design).\n* **Behavioral** (1 hour). For more info about what questions to expect, see the [Behavioral section below](https://interviewing.io/doordash-interview-questions#question-behavioral).\n\nTypes of Interview Questions to Expect at DoorDash\n--------------------------------------------------\n\nDoorDash has shifted its process from a centralized one to a decentralized one pretty recently, so there is some variation in terms of the style of question you will encounter. That said, the engineers we’ve spoken to say it’s still pretty standard fare across the board, so practicing medium-level LeetCode questions will help.\n\n### Coding\n\nThis interview will be less algorithms and data structures heavy and will look to test your ability to write code fast and refactor quickly. You won’t be asked to use any specific programming language. Most questions should be practical in nature, and LeetCode questions that focus on job scheduling are popular.\n\nFrom one of our users, who was a DoorDash interviewer:\n\n> *Some questions are very practical, but it depends on the interviewer. Some love very specific LeetCode-style graph questions.*\n\nBelow are the technical topics you’re likely to encounter in DoorDash interviews. To compile this list, we did two things. First, we spoke to some current and former DoorDash engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Trees](/trees-interview-questions)\n\n[Questions   \n& tips](/trees-interview-questions)\n\n[Watch 7   \n interview replays](/mocks?technical=trees)\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Linked Lists](/linked-lists-interview-questions)\n\n[Questions   \n& tips](/linked-lists-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=linked-lists)\n\n[Matrices](/matrices-interview-questions)\n\n[Questions   \n& tips](/matrices-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=matrices)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Graphs](/graphs-interview-questions)\n\n[Questions   \n& tips](/graphs-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=graphs)\n\n### System Design\n\nThis round focuses mostly on large systems and will involve designing an entire service or app with scalability, reliability, and usability concerns in mind. Expect a vague problem with ill-defined boundaries. Your focus should be on not getting lost in one area. Ask your interviewer what area you should hone in on.\n\nThey use this round for leveling, and senior candidates should be able to call out all requirements and considerations. They aren’t looking for a full solution here, so don’t get lost in the details. Cover all bases and concerns.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Behavioral\n\nThis round varies by role but is usually with the hiring manager for your team. It is a behavioral interview, but the hiring manager will also try to close you. You could pass all the other rounds and not do well here but still get an offer. Expect to answer questions about past experiences.\n\n### How DoorDash Makes Hiring Decisions\n\nThe entire onsite panel submits written feedback after the interviews are completed, and then they have a roundtable meeting to discuss both hire/no-hire and leveling. Most of the time, decisions are reached by consensus (everyone agrees to hire or no-hire), but in cases where consensus isn’t possible, the hiring manager has final say. Our sources say that it is rare for a hiring manager to veto a hire.\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/doordash-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Robinhood’s Interview Process & Questions",
      "content": "Table of Contents\n\n* [Interview Process](#robinhood-interview-process)\n  + [Recruiter Call](#step-1)\n  + [Technical Phone Screen](#step-2)\n  + [Recruiter Prep Call](#step-3)\n  + [Onsite](#step-4)\n    - [Coding](#step-4-coding)\n    - [System Design](#step-4-system)\n    - [Past Project Review](#step-4-pastproject)\n    - [Hiring Manager Call](#step-4-hiringmanager)\n* [Question Types](#question-types)\n  + [Coding](#question-coding)\n  + [System Design](#question-design)\n* [Hiring Decisions](#hiring-decision)\n[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nRobinhood’s Interview Process & Questions\n=========================================\n\nRobinhood's Interview Process for Software Engineers: 5 Steps\n-------------------------------------------------------------\n\nMid to senior-level engineers interviewing at Robinhood can expect the following process:\n\n* Recruiter call (30 minutes)\n* Technical phone screen (1 hour)\n* Recruiter prep call (30 minutes)\n* Onsite (5 hours)\n* Team matching\n\n![Robinhood’s interview process: Recruiter call, Technical phone screen, Additional recruiter call, Onsite, Team matching](https://strapi-iio.s3.us-west-2.amazonaws.com/Robinhood_s_Company_Process_61bcb29aaf.png)\n\nRobinhood’s process is centralized, meaning that everyone follows the same standardized process and team matching happens at the end. Moreover, the people interviewing you won’t necessarily be from your future team (and likely won’t be).\n\n### Step 1: Recruiter Call\n\nThis step doesn’t happen 100% of the time. It depends on whether Robinhood recruiters reached out to you or whether you applied. If you applied, you do the recruiter call, and it’s standard stuff. They’ll discuss the role, expectations, your previous work history and Robinhood itself.\n\nIt’s really important at this stage not to reveal your salary expectations, salary history, or where you are in the process with other companies. We wrote a [detailed post about salary negotiation that lays out exactly what to say when recruiters pressure you to name the first number](https://www.google.com/url?q=https://interviewing.io/blog/negotiate-salary-recruiter&sa=D&source=editors&ust=1687337715912604&usg=AOvVaw2DmZ7rz25IR1Q42FqB380A).\n\n### Step 2: Technical Phone Screen\n\nRobinhood's technical phone screen combines algorithms/data structures and system design, conducted through Karat. If you haven’t interviewed at a company that uses Karat before, it’s basically interviewer-as-a-service – companies pay them to conduct interviews on their behalf. This means that the person you’ll be talking to isn’t a Robinhood employee and won’t be able to answer questions about the team, roadmap, projects, etc. Their only objective is to screen you.\n\nThere are 30 minutes for each section, the coding portion will often feature one or two medium difficulty questions, whereas the system design will focus on general knowledge in a trivia-like format.\n\n### Step 3: Second Recruiter Call\n\nThe second recruiter call is meant primarily to prep you for the onsite, especially if you didn’t do the first call.\n\n### Step 4: Onsite\n\nThe onsite at Robinhood consists of 5 interview rounds with the following steps:\n\n* Coding (1 hour)\n* System design (2 hour)\n* Past project review (1 hour)\n* Hiring manager call (1 hour)\n\n#### Coding\n\nThis is an algorithms and data structures interview. Robinhood places an emphasis on correctness over scalability. These interviews typically happen in CoderPad.\n\n#### System Design\n\nTypically, the onsite interviews include two [system design interview](https://interviewing.io/guides/system-design-interview) rounds, each covering different aspects. One interview assesses your familiarity with a wide range of system design concepts, emphasizing high-level and well-rounded solutions. The other interview will delve into a specific facet of system design, challenging you to problem-solve in a more targeted area (e.g. front-end system design).\n\n#### Past Project Review\n\nThe past project round will evaluate your ability to communicate about technical topics. You’ll be asked to prepare something ahead of time based on a prompt. Be ready to discuss what you bring in depth. An example prompt is: Prepare a system diagram of a previous project.\n\n#### Hiring Manager Call\n\nRobinhood’s hiring manager call is a standard assessment of behavioral and cultural fit. The discussions are centered around role-related scenarios, with the aim of gauging problem solving and communication abilities.\n\nTypes of Interview Questions to Expect at Robinhood\n---------------------------------------------------\n\n### Coding\n\nRobinhood has a question bank with company-specific versions of common questions, but it’s not mandatory for interviewers to use, so there will often be a mix of those and other standard medium difficulty coding problems.\n\nTo figure out what specific types of questions to expect in Robinhood interviews, we did two things. First, we spoke to some current and former Robinhood interviewers in our community. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews. Based on all of the above, here are the types of questions you’re likely to encounter:\n\n[Binary Trees](/binary-trees-interview-questions)\n\n[Questions   \n& tips](/binary-trees-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=binary-trees)\n\n[Trees](/trees-interview-questions)\n\n[Questions   \n& tips](/trees-interview-questions)\n\n[Watch 7   \n interview replays](/mocks?technical=trees)\n\n[Linked Lists](/linked-lists-interview-questions)\n\n[Questions   \n& tips](/linked-lists-interview-questions)\n\n[Watch 12   \n interview replays](/mocks?technical=linked-lists)\n\n[Maps](/maps-interview-questions)\n\n[Questions   \n& tips](/maps-interview-questions)\n\n[Watch 2   \n interview replays](/mocks?technical=maps)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Graphs](/graphs-interview-questions)\n\n[Questions   \n& tips](/graphs-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=graphs)\n\n### System Design\n\nThese questions will usually revolve around Robinhood’s own product, or adjacent/related products.\n\nCommon [system design](https://interviewing.io/guides/system-design-interview) questions at Robinhood include:\n\n* Build Twitter\n* Design stock exchange\n* Design a new feature for Robinhood\n* Build Google Docs\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\nFAANG Interview in Design Robinhood\n\nAdvance this person to the next round?\n\n![Thumbs up](/static/images/recordings/thumbs_up.svg)\n\nTechnical Skills:\n\n3/4\n\nProblem Solving Ability:\n\n4/4\n\nCommunication Ability:\n\n3/4\n\n[More Details\n\n![New Tab](/static/images/NewWindow.svg)](/mocks/faang-system-design-design-robinhood)\n\nHow Robinhood Makes Hiring Decisions\n------------------------------------\n\nRobinhood uses a hiring committee to make decisions, except in cases where there is a low volume of candidates for a role. In the event there is a hiring committee, those who participated in the interview directly will come together to discuss and review performance, before coming to an agreement on hire or no hire.\n\nWant to know if you're ready to interview at Robinhood? Do anonymous interviews with interviewers from top companies, and see exactly where you stack up.\n\n[See available times](https://interviewing.io/signup)\n\n![](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcta.47351a0d.png&w=1200&q=75)\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/robinhood-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Datadog’s Interview Process & Questions",
      "content": "[Browse all interview replays](/mocks)\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\n![Video Preview](/static/images/cyber-corgi/artboard.svg)\n\nGet instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays.\n\n[Get started](/signup)\n\nDatadog’s Interview Process & Questions\n=======================================\n\n*The info below is based on conversations with Datadog engineers.*\n\nPublished:\n\nDatadog's Interview Process for Software Engineers: 3 Steps\n-----------------------------------------------------------\n\nMid to senior-level engineers interviewing at Datadog can expect the following process:\n\n* Recruiter call (30 minutes)\n* Technical phone screen (1 hour)\n* Onsite (4 hours)\n\nGeneral tips:\n\n* Practice algorithmic questions that lean practical in terms of the type of work Datadog does (but practicing LeetCode questions will help do well in these – many of their questions start as a LeetCode medium and then layer on complexity/get more practical)\n* Prepare to answer some technical questions about your past work during the behavioral portion of the onsite.\n\n![Datadog’s interview process: Recruiter call, Technical phone screen, Onsite](https://strapi-iio.s3.us-west-2.amazonaws.com/Datadog_Interview_Process_db7ec1616e.png)\n\nAt Datadog, the process is centralized, which means that you won’t do team matching until after the onsite phase. You’ll be interviewed by people from different teams during the interview loop.\n\nThe entire process takes about 6 weeks, and we’ve heard it can seem quite slow at times. You might have to push to speed things up.\n\n### Step 1: Recruiter Call\n\nDatadog’s recruiter call lasts 30 minutes, and it’s pretty standard fare – they’ll ask you about your previous experience, why you’re interested in Datadog, and what you’re looking for moving forward.\n\nIt’s really important, at this stage, to not reveal your salary expectations or where you are in the process with other companies. We’ve written a [detailed post about salary negotiation that lays out exactly what to say if recruiters pressure you to name the first number](https://interviewing.io/blog/negotiate-salary-recruiter).\n\n### Step 2: Technical Phone Screen\n\nDatadog’s technical phone screen lasts about an hour and is conducted in CoderPad. You will get two questions in this interview.\nWe will cover what we know of their question style in the section called [“Types of Interview Questions to Expect at Datadog” below](https://interviewing.io/datadog-interview-questions#question-types).\n\n### Step 3: Onsite\n\nOnsite interview loops can vary slightly depending on the role and seniority, but the below is generally what you’ll get.\n\n* **Coding** (1 hour). This interview will be conducted in CoderPad. For more detail about the kinds of questions to expect, see the [Coding section below](https://interviewing.io/datadog-interview-questions#question-coding).\n* **[Only for roles below Staff] Second coding** (1 hour). As above.\n* **System design** (1 hour). This interview will be conducted in your choice of tool (many candidates choose Excalidraw). For more detail about the kinds of questions to expect, see the [System Design section below](https://interviewing.io/datadog-interview-questions#question-design).\n* **Behavioral** (1 hour). This interview will be conducted by a hiring manager or director of engineering. It may feature some technical questions! For more info about what questions to expect, see the [Behavioral section below](https://interviewing.io/datadog-interview-questions#question-behavioral) .\n* **[For Staff level roles and above] Presentation** (1 hour). This will replace one of the coding rounds. You will be asked to present a project you’ve worked on in the past to a panel. You’ll be expected to justify the technical decisions involved and explain the business context or rationale for what you did. If you do this interview, you may have to pick teams you're interested in beforehand so people from those teams can attend.\n\nTypes of Interview Questions to Expect at Datadog\n-------------------------------------------------\n\n### Coding\n\nFrom the first technical phone screen through to the onsite coding rounds, you should expect algorithmic questions but likely NOT questions taken verbatim from LeetCode – Datadog has their own internal question bank. We’ve heard that the questions are a hybrid between practical and LeetCode-style. They might start with something similar to what you can find on LeetCode but then layer on additional complexity. Datadog themselves recommend practicing medium-level LeetCode questions. You might be asked questions like these:\n\n* Bucketing numbers given specific requirements\n* Given a root directory, find the total size of all the files across all sub-directories\n* Given an interface and a file class, build a buffered file writer\n\nBelow are the technical topics you’re likely to encounter in Datadog interviews. To compile this list, we did two things. First, we spoke to some current and former Datadog engineers. Then we cross-referenced all the anecdotes we heard with Glassdoor data AND our own data-set of mock interviews:\n\n[Binary Trees](/binary-trees-interview-questions)\n\n[Questions   \n& tips](/binary-trees-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=binary-trees)\n\n[Hash Maps](/hash-tables-interview-questions)\n\n[Questions   \n& tips](/hash-tables-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=hash-maps)\n\n[Matrices](/matrices-interview-questions)\n\n[Questions   \n& tips](/matrices-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=matrices)\n\n[Sets](/sets-interview-questions)\n\n[Questions   \n& tips](/sets-interview-questions)\n\n[Watch 4   \n interview replays](/mocks?technical=sets)\n\n[Binary Search](/binary-search-interview-questions)\n\n[Questions   \n& tips](/binary-search-interview-questions)\n\n[Watch 6   \n interview replays](/mocks?technical=binary-search)\n\n[Arrays](/arrays-interview-questions)\n\n[Questions   \n& tips](/arrays-interview-questions)\n\n[Watch 20   \n interview replays](/mocks?technical=arrays)\n\n[Strings](/strings-interview-questions)\n\n[Questions   \n& tips](/strings-interview-questions)\n\n[Watch 14   \n interview replays](/mocks?technical=strings)\n\n[Graphs](/graphs-interview-questions)\n\n[Questions   \n& tips](/graphs-interview-questions)\n\n[Watch 10   \n interview replays](/mocks?technical=graphs)\n\n### System Design\n\nWe’ve heard that this round is less broad than it can be at other companies. You won’t be asked to “Design Twitter”, for example. Instead, you might get a question such as, “Given a service that returns flight deals for the last 7 days, design a system that surfaces relevant flight data to a user and notifies the user when there’s a new flight that matches their criteria.”\n\nAs one of our users said:\n\n> *\"[I] didn’t have to go into detail in every area. They kept it very high level, and the interview felt fair.”*\n\nWhile it might be a fair interview, it’s also used for leveling so make sure to brush up on system design. We’ve heard of candidates being down-leveled for less-than-flawless performance here.\n\nCheck out [our guide to system design interviews](https://interviewing.io/guides/system-design-interview) to help you prepare.\n\n### Behavioral\n\nThis interview will be with someone in a leadership role at the company, possibly a director. It will contain some standard behavioral questions but also some technical questions about your past work.\n\nYou might be asked to show a simple design of something you built at a previous company. The interviewer will want to know why certain design choices were made so they can understand your impact on the project and your level of experience working within teams.\n\n### How Datadog Makes Hiring Decisions\n\nAfter a successful onsite, a Hiring Committee will decide if you should be hired.\n\nIf they want to make an offer, your profile will be shared with multiple hiring managers for team matching. You’ll get a chance to meet a few people from the team including the hiring manager. This is more about you figuring out a good fit than being a continuation of the interview process.\n\nSee something inaccurate? Please fill out [this form](https://iiosurveys.typeform.com/to/LXqccdMk), and tell us about it.\n\n![Interview Problems](/_next/image?url=%2Fstatic%2Fimages%2FHeroImage.png&w=3840&q=75)\n\nAbout interviewing.io\n---------------------\n\ninterviewing.io is a **[mock interview practice platform](/)**. We've hosted over 100K mock interviews, conducted by senior engineers from FAANG & other top companies. We've drawn on data from these interviews to bring you the best interview prep resource on the web.\n\n[Sign up for a mock interview](/signup)\n\n![Interview Problems](/static/images/cyber-corgi/iio-cyber-corgi-with-pattern.svg)",
      "content_type": "other",
      "source_url": "https://interviewing.io/datadog-interview-questions",
      "author": "",
      "user_id": ""
    },
    {
      "title": "A Senior Engineer’s Guide to Meta's Interview Process and Questions",
      "content": "We helped write the sequel to \"Cracking the Coding Interview\". [Read 9 chapters for free →](https://bctci.co/free-chapters)",
      "content_type": "other",
      "source_url": "https://interviewing.io/guides/hiring-process/meta-facebook",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Ultimate Guide to FAANG Interviews for Senior Engineers",
      "content": "We helped write the sequel to \"Cracking the Coding Interview\". [Read 9 chapters for free →](https://bctci.co/free-chapters)",
      "content_type": "other",
      "source_url": "https://interviewing.io/guides/hiring-process",
      "author": "",
      "user_id": ""
    },
    {
      "title": "A Senior Engineer’s Guide to Google’s Interview Process and Questions",
      "content": "We helped write the sequel to \"Cracking the Coding Interview\". [Read 9 chapters for free →](https://bctci.co/free-chapters)",
      "content_type": "other",
      "source_url": "https://interviewing.io/guides/hiring-process/google",
      "author": "",
      "user_id": ""
    },
    {
      "title": "A Senior Engineer's Guide to the Amazon Leadership Principles Interview",
      "content": "We helped write the sequel to \"Cracking the Coding Interview\". [Read 9 chapters for free →](https://bctci.co/free-chapters)",
      "content_type": "other",
      "source_url": "https://interviewing.io/guides/amazon-leadership-principles",
      "author": "",
      "user_id": ""
    },
    {
      "title": "A Senior Engineer’s Guide to Amazon's Interview Process and Questions",
      "content": "We helped write the sequel to \"Cracking the Coding Interview\". [Read 9 chapters for free →](https://bctci.co/free-chapters)",
      "content_type": "other",
      "source_url": "https://interviewing.io/guides/hiring-process/amazon",
      "author": "",
      "user_id": ""
    },
    {
      "title": "A Senior Engineer's Guide to the System Design Interview",
      "content": "We helped write the sequel to \"Cracking the Coding Interview\". [Read 9 chapters for free →](https://bctci.co/free-chapters)",
      "content_type": "other",
      "source_url": "https://interviewing.io/guides/system-design-interview",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Iterative Tree Traversals: A Practical Guide",
      "content": "Iterative Tree Traversals: A Practical Guide\n\nIntroduction\n\nI don't know how often tree traversals come up in actual software projects, but they are popular in coding interviews and competitive programming.\nIn this article, I share an approach for implementing tree traversal algorithms iteratively that I found to be simple to remember and implement, while being flexible enough to do anything that a recursive algorithm can (I also didn't like most suggestions I saw online). The main technique is given in section\"Iterative Postorder and Inorder Traversal\", but first I give some context. I also link to practice problems onleetcode.comfor the reader to play with. I provide some solutions, but I suggest trying the problems out first. The code snippets are in C++, but leetcode accepts most languages.\n\nWhat are Tree Traversals\n\nMathematically, trees are just connected acyclic graphs. However, in the context of tree traversals, we are usually working withrooted treesrepresented with a recursive structure such as the following (which is the default definition in Leetcode for binary trees). A leaf is a node with two null pointers as children:\n\nA tree traversal is an algorithm that visits every node in a tree in a specific order (and does some computation with them, depending on the problem). For binary trees specifically, there are three important orders:\n\nPreorder:root before children. As we will see, this is the simplest to implement.Inorder:left child, then root, then right child. This traversal is most often used onbinary search trees(BST). A BST is a rooted binary tree with the additional property that every node in the left subtree has a smaller value than the root, and every node in the right subtree has a larger value than the root. This traversal is called \"inorder\" because, when used on a BST, it will visit the nodes from smallest to largest.Postorder:children before root. It comes up in problems where we have to aggregate information about the entire subtree rooted at each node. Classic examples are computing the size, the height, or the sum of values of the tree.\n\nPreorder:root before children. As we will see, this is the simplest to implement.\n\nInorder:left child, then root, then right child. This traversal is most often used onbinary search trees(BST). A BST is a rooted binary tree with the additional property that every node in the left subtree has a smaller value than the root, and every node in the right subtree has a larger value than the root. This traversal is called \"inorder\" because, when used on a BST, it will visit the nodes from smallest to largest.\n\nPostorder:children before root. It comes up in problems where we have to aggregate information about the entire subtree rooted at each node. Classic examples are computing the size, the height, or the sum of values of the tree.\n\nBecause rooted trees are recursive data structures, algorithms on trees are most naturally expressed recursively. Here are the three traversals. I use the functionprocess(node)as a placeholder for whatever computation the problem calls for.\n\nSide-note: in C++, pointers are implicitly converted to booleans: a pointer evaluates to true if and only if it is not null. So, in the code above, \"if (!root)\" is equivalent to \"if (root == NULL)\".\n\nTraversal problems on leetcode\n\nhttps://leetcode.com/problems/binary-tree-preorder-traversal/https://leetcode.com/problems/binary-tree-inorder-traversal/https://leetcode.com/problems/binary-tree-postorder-traversal/\n\nhttps://leetcode.com/problems/binary-tree-preorder-traversal/\n\nhttps://leetcode.com/problems/binary-tree-inorder-traversal/\n\nhttps://leetcode.com/problems/binary-tree-postorder-traversal/\n\nWhy / When to Use an Iterative Traversal\n\nIf the recursive implementation is so simple, why bother with an iterative one? Of course, to avoid stack overflow. Most runtime engines/compilers set a limit on how many nested calls a program can make. For example, according tothis article:\n\nDefault stack size varies between 320k and 1024k depending on the version of Java and the system used. For a 64 bits Java 8 program with minimal stack usage, the maximum number of nested method calls is about 7000.\n\nIf the height of the tree is larger than this limit, the program will crash with astack overflow error. A recursive implementation is safe to use if:\n\nSomehow we know that the input trees will be small enough.The tree isbalanced, which means that, for each node, the left and right subtrees have roughly the same height. In a balanced tree, the height is guaranteed to belogarithmicon the number of nodes (indeed, that is why balanced BSTs guaranteeO(log n)search time), so any tree that fits in RAM (or even disk) will require a tiny number of recursive calls.\n\nSomehow we know that the input trees will be small enough.\n\nThe tree isbalanced, which means that, for each node, the left and right subtrees have roughly the same height. In a balanced tree, the height is guaranteed to belogarithmicon the number of nodes (indeed, that is why balanced BSTs guaranteeO(log n)search time), so any tree that fits in RAM (or even disk) will require a tiny number of recursive calls.\n\nHowever, if we are not in either of the cases above, an iterative solution is safer.\n\nRecursive and iterative traversals have the same runtime complexity, so this is not a concern when choosing either (all the problems shown in this article can be solved in linear time using either).\n\nThe main approach for converting recursive implementations to iterative ones is to \"simulate\" the call stack with an actual stack where we push and pop the nodes explicitly. This works great \"out-of-the-box\" with preorder traversal.\n\nIncidentally, when implementing tree traversals we need to make an implementation choice about how to handle NULL pointers. We can be eager and filter them out before adding them to the stack, or we can be lazy and detect them once we extract them from the stack. Both are fine—what matters is to be deliberate and consistent about which approach we are using. I prefer the latter as it yields slightly shorter code, so I will use it in all the following examples. For comparison, here is the iterative preorder traversal with both approaches:\n\nNote thatthe right child is pushed to the stack before the left one. This is because we want the left child to be above in the stack so that it is processed first.\n\nPreorder traversal practice problems\n\nhttps://leetcode.com/problems/invert-binary-tree/https://leetcode.com/problems/maximum-depth-of-binary-tree/\n\nhttps://leetcode.com/problems/invert-binary-tree/\n\nhttps://leetcode.com/problems/maximum-depth-of-binary-tree/\n\nThis problem asks to find the depth of a binary tree (follow the link for the description and examples). It requires passing information from each node to its children. We can do this by changing the stack tostack<pair<TreeNode*, int>>, so that we can pass anintto each child, as in the solution below:\n\nIn the code above, the{}notation is used to create pairs (e.g.,{root, 0}). If one is not familiar with pairs in C++, or is using a language without the equivalent, a simple alternative is to use two separate stacks, one for the nodes and one for the info.\n\nThe next two problems are similar:\n\nhttps://leetcode.com/problems/minimum-depth-of-binary-tree/https://leetcode.com/problems/path-sum/https://leetcode.com/problems/symmetric-tree/\n\nhttps://leetcode.com/problems/minimum-depth-of-binary-tree/\n\nhttps://leetcode.com/problems/path-sum/\n\nhttps://leetcode.com/problems/symmetric-tree/\n\nA solution for the last one, this time using a stack with a pair of nodes:\n\nIterative Postorder and Inorder Traversal\n\nWhile iterative preorder traversal is straightforward, with postorder and inorder we run into a complication: we cannot simply swap the order of the lines as with the recursive implementation. In other words, the following doesnotyield a postorder traversal:\n\nThe node is still processed before its children, which is not what we want.\n\nThe workaround, once again emulating the recursive implementation, is to visit each node twice.We consider postorder traversal first. In the first visit, we only push the children onto the stack. In the second visit, we do the actual processing.\nThe simplest way to do this is to enhance the stack with a\"visit number flag\". Implementation-wise, we change the stack tostack<pair<TreeNode*, int>>so that we can pass the flag along with each node. The iterative postorder looks like this:\n\nNote the order in which the nodes are added to the stack whenvisit == 0. The parent ends up under its children, with the left child on top. Since it is the first time that the children are added to the stack, their visit-number flag is 0. For the parent, it is 1.\nFor simplicity, I also follow the convention to always immediately call pop after extracting the top element from the stack.\n\nThe same approach also works for inorder traversal (that's the point). Here is a version where we visit each node three times: one to push the left child, one to process the node, and one to push the right child.\n\nIn fact, the second and third visits can be merged together: processing the node does not modify the stack, so the two visits are followed one after the other anyway. Here is my preferred version:\n\nFor completeness, here is the version found in most of my top Google hits (seethisfor a nice explanation):\n\nWhile it is shorter, it cannot be easily converted to postorder traversal, so it is not as flexible. Also, I find it easier to follow the execution flow with the visit-number flag.\n\nInorder traversal practice problems\n\nhttps://leetcode.com/problems/kth-smallest-element-in-a-bst/\n\nhttps://leetcode.com/problems/kth-smallest-element-in-a-bst/\n\nA solution (follow the link for the statement and examples):\n\nhttps://leetcode.com/problems/validate-binary-search-tree/\n\nhttps://leetcode.com/problems/validate-binary-search-tree/\n\nA solution:\n\nPostorder traversal practice problems\n\nhttps://leetcode.com/problems/balanced-binary-tree/\n\nhttps://leetcode.com/problems/balanced-binary-tree/\n\nThis problem asks to check if a binary tree is balanced. It requires passing information back from the children to the parent node in a postorder traversal. Passing information from the children to the parent is easy with recursion. It can be done both with return values or with parameters passed by reference. For this problem we need to pass two things: aboolindicating if the subtree is balanced, and anintindicating its height. I use a reference parameter for the latter (returning apair<bool,int>would be cleaner).\n\nPassing information from the children to the parent in an iterative implementation is more intricate. There are three general approaches:\n\nUse a hash table mapping each node to the information.\n\nUse a hash table mapping each node to the information.\n\nThis is the easiest way, but also the most expensive.\nWhile the asymptotic runtime is still linear, hash tables generally have significant constant factors.\n\nAdd a field to the definition of the node structure for the information needed.\n\nAdd a field to the definition of the node structure for the information needed.\n\nThen, we can read it from the parent node by traversing the children's pointers.\nIn Leetcode we cannot modify theTreeNodedata structure so, to illustrate this approach, I build a new tree first with a new struct:\n\nPass the information through an additional stack.\n\nPass the information through an additional stack.\n\nThis is the most efficient, but one must be careful to keep both stacks in synch. When processing a node, that node first pops the information from its children, and then pushes its own info for its parent. Here is a solution (with eager NULL-pointer detection):\n\nhttps://leetcode.com/problems/diameter-of-binary-tree/\n\nhttps://leetcode.com/problems/diameter-of-binary-tree/\n\nThis problem also requires passing information from the children to the parent in a postorder traversal. Here is a solution using the third approach again, but this time with lazy NULL-pointer detection. Note that we push a 0 to thedepthsstack when we extract a NULL pointer from the main stack, and during processing we always do two pops regardless of the number of non-NULL children:\n\nhttps://leetcode.com/problems/binary-tree-tilt/https://leetcode.com/problems/most-frequent-subtree-sum/https://leetcode.com/problems/maximum-product-of-splitted-binary-tree/\n\nhttps://leetcode.com/problems/binary-tree-tilt/\n\nhttps://leetcode.com/problems/most-frequent-subtree-sum/\n\nhttps://leetcode.com/problems/maximum-product-of-splitted-binary-tree/\n\nTraversals in n-ary Trees\n\nSo far, we have looked at binary trees. In an n-ary tree, each node has an arbitrary number of children.\n\nFor n-ary trees, preorder traversal is also straightforward, and inorder traversal is not defined.\n\nFor postorder traversal, we can use a visit-number flag again. Two visits suffice for each node: one to push all the children into the stack, and another to process the node itself. I do not include the code here because it is very similar to the binary tree case.\n\nConsider a more complicated setting where we need to compute something at the node after visiting each child. Let's call this \"interleaved traversal\". I useprocess(node, i)as placeholder for the computation done before visiting the i-th child. Here is the recursive implementation and the corresponding iterative one using visit-number flags.\n\nN-ary tree practice problems\n\nhttps://leetcode.com/problems/n-ary-tree-preorder-traversal/https://leetcode.com/problems/n-ary-tree-postorder-traversal/\n\nhttps://leetcode.com/problems/n-ary-tree-preorder-traversal/\n\nhttps://leetcode.com/problems/n-ary-tree-postorder-traversal/\n\nAn Alternative Way of Passing the Visit Flag\n\nThe common framework to all our solutions has been to pass a visit-number flag along with the nodes on the stack. User \"heiswyd\" on leetcode postedherean alternative way to pass the flag implicitly: initially, it pushes each node on the stack twice. Then, it can distinguish between the first visit and the second visit by checking whether the node that has just been extracted from the stack matches the node on top of the stack. This happens only when we extract the first of the two occurrences. Post-order traversal looks like this:\n\nIt is cool, but I prefer passing the flag explicitly for clarity.",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/iterativetreetraversal?category=dsa",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Reachability Problems and DFS",
      "content": "Reachability Problems and DFS\n\nIntroduction\n\nDepth-first search, or DFS, is a fundamental graph algorithm that can be used to solvereachabilityproblems. This post shows how to adapt the basic DFS template to solve several problems of this kind. Reachability problems are often easier in undirected graphs. Below, we specify if the algorithm works for undirected graphs, directed graphs, or both.\n\nPrerequisites\n\nWe assume that the reader is already familiar with the concept of DFS.Hereis an excellent video introducing DFS with step-by-step animations. We also assume that the reader is familiar with the adjacency list representation of a graph, and we use big-O notation in the analysis.\n\nCoding conventions\n\nThe algorithms below are in Python.ndenotes the number of nodes. Nodes are identified with integers in the range0..n-1. The graphGis a graph stored as an adjacency list:Gis a list ofnlists. For eachvbetween0andn-1,G[v]is the list of neighbors ofG.\n\nIf the graph is given as an edge list instead, we can initialize it as follows:\n\nIf the graph is given as an adjacency matrix, we can iterate through the rows of the adjacency matrix instead of through the adjacency lists. To iterate through the neighbors of a nodev, instead of\n\nwe do\n\nNote that using an adjacency matrix affects the runtime analysis of DFS:O(n²)instead ofO(m).\n\nWhich nodes can be reached from node s?\n\nThis is the simplest question that can be answered with DFS. The primary data structure in DFS is a list of booleans to keep track of already visited nodes (we call itvis). If we start a DFS search from a nodes, the reachable nodes will be the ones for whichvisis true.\n\nFor this,Gcan be directed or undirected. We make use of a nested function in Python so that we do not need to passGandvisas parameters (in Python nested functions have visibility over the variables in the scope where they are defined).\n\nDFS runs inO(m)time andO(n)space, wheremis the number of edges. This is because each edge is considered twice, once from each endpoint, if the endpoints end up being visited, or zero times if the endpoints are not visited.\n\nIterative version\n\nThe iterative version takesO(m)space instead ofO(n)because nodes can be inserted into the stack multiple times (up to one time for each incident edge). Alternatively, we can mark the nodes as visited when we add them to the stack instead of when we remove them. This change reduces the space usage to the usualO(n). However, with this change, the algorithm is no longer DFS. It still works for answering reachability questions because the set visited nodes is the same, but the order in which they are visited is no longer consistent with a depth-first search order (it is closer to a BFS (breath-first search) order, but also not exactly a BFS order).\n\nThe difference between marking nodes when they added vs removed from the stack is discussed in detailhere. Since the recursive version is shorter and optimal in terms of space, we favor it from now on. That said, it should be easy to adapt the iterative version above to the problems below.\n\nCan node s reach node t?\n\nWe use the same code from before, but we add early termination as soon as we seet. Now, the recursive function has a return value.\n\nAdding the early termination can make the DFS faster, but in the worst-case the time/space complexity is the same.\n\nPractice problems\n\nhttps://leetcode.com/problems/the-maze/\n\nhttps://leetcode.com/problems/the-maze/\n\nThe hardest part on this problem is constructing the graph in the first place.\n\nFind a path from s to t\n\nThe edges \"traversed\" in a DFS search form a tree called the \"DFS tree\". The DFS tree changes depending on where we start the search. The starting node is called the root. We can construct the DFS tree by keeping track of the predecessor of each node in the search (the root has no predecessor). If we construct the DFS tree rooted ats, we can follow the sequence of predecessors fromttosto find a path fromstotin reverse order.\n\nInstead of using the listvisto keep track of visited nodes, we know a node is unvisited if it has no predecessor yet. We indicate that a node has no predecessor with the special value-1.\n\nNote that DFS doesnotfind the shortest path formstot. For that, we can use BFS (breath-first search). It just returns any path without repeated nodes.\n\nIs the graph connected?\n\nFor undirected graphs, this is almost the same question as the first question (\"which nodes can be reached bys?\") because of the following property:\n\nAn undirected graph is connected if and only if every node can be reached froms, wheresis any of the nodes.\n\nThus, the code is exactly the same as for the first question, with two differences: 1) we choosesto be0(could be anything), and 2) we change the last line to check if every entry invisis true.\n\nFor directed graphs, we need to take into account the direction of the edges. A directed graph isstrongly connectedif every node can reach every other node. We can use the following property:\n\nA directed graph is strongly connected if and only ifscan reach every node and every node can reachs, wheresis any of the nodes.\n\nWe already know how to use DFS to check ifscan reach every node. To check if every node can reachs, we can do a DFS starting froms,but in the reverse graph of G. The reverse graph ofGis likeGbut reversing the directions of all the edges.\n\nThe runtime is stillO(m), but the space is nowO(m)because we need to create and store the reverse graph. There are alternative algorithms (like Tarjan's algorithm) which can do this inO(n)space.\n\nHow many connected components are there?\n\nWe can use the typical DFS to answer this question for undirected graphs. We use a common pattern in DFS algorithms: an outer loop through all the nodes where we launch a search for every yet-unvisited node.\n\nThe runtime is nowO(n+m)because, ifm < n, we still spendO(n)time iterating through the loop at the end.\n\nFor directed graphs, instead of connected components, we talk aboutstrongly connected components. A strongly connected component is a maximal subset of nodes where every node can reach every other node.\n\nIf we want to find the number of strongly connected components, we can use something likeTarjan's algorithm, a DFS-based algorithm that requires some additional data structures.\n\nPractice problems\n\nhttps://leetcode.com/problems/number-of-connected-components-in-an-undirected-graph/(Premium only)https://leetcode.com/problems/number-of-islands/https://leetcode.com/problems/friend-circles/\n\nhttps://leetcode.com/problems/number-of-connected-components-in-an-undirected-graph/(Premium only)\n\nhttps://leetcode.com/problems/number-of-islands/\n\nhttps://leetcode.com/problems/friend-circles/\n\nWhich nodes are in the same connected components?\n\nThis question is more general than the previous two. We label each nodevwith a numberCC[v]so that nodes with the same number belong to the same CC. Instead of having a listCCin addition tovis, we use the CC number-1to indicate unvisited nodes. This way, we do not needvis\n\nFor directed graphs, again we need Tarjan's algorithm or an equivalent algorithm.\n\nPractice problems\n\nhttps://leetcode.com/problems/max-area-of-island/https://leetcode.com/problems/sentence-similarity-ii/\n\nhttps://leetcode.com/problems/max-area-of-island/\n\nhttps://leetcode.com/problems/sentence-similarity-ii/\n\nIn the second problem, nodes are given by names, not indices, so they need to be converted.\n\nIs the graph acyclic?\n\nFor undirected graphs, this question is simple. First, we consider the problem in each CC independently. This is very common pattern in graph problems. We do this with an outer loop through all the nodes where we launch a search for every yet-unvisited node.\n\nDuring the DFS search in each CC, if we find an edge to an already visited node that is not the predecessor in the search (the node we just came from), there is a cycle. Such edges in a DFS search are calledback edges. We add one parameter to the recursive functionvisitto know the predecessor node.\n\nFor directed graphs, it is not as simple: the fact that a neighbornbris already visited during the DFS search does not mean thatnbrcan reach the current node. To check if a directed graph is acyclic, we can use the linear-timepeel-off algorithmfor finding a topological ordering. This algorithm detects if the graph is acyclic and finds a topological ordering if so, though we are only interested in the first part.\n\nPractice problems\n\nhttps://leetcode.com/problems/redundant-connection/\n\nhttps://leetcode.com/problems/redundant-connection/\n\nThis problem is easier to solve using union-find, but it can be done with DFS.\n\nIs the graph a tree?\n\nUsually, we ask this question for undirected graphs. We can use this characterization of trees:\n\nAn undirected graph is a tree if and only if it is connected and has exactlyn-1edges.\n\nWe already saw how to check if the graph is connected with DFS, and counting the number of edges is straightforward:\n\nPractice problems\n\nhttps://leetcode.com/problems/graph-valid-tree/\n\nhttps://leetcode.com/problems/graph-valid-tree/\n\nIs the graph bipartite?\n\nThis is exactly the same question as whether the graph can be two-colored, so see the next section.\n\nCan the graph be two-colored?\n\nTwo-coloring a graph means assigning colors to the nodes such that no two adjacent nodes have the same color, using only two colors. Usually, we consider coloring question for undirected graphs.\n\nWe consider whether each CC can be colored independently from the others. We can color each CC using DFS. We use values0and1for the colors. The color of the start node can be anything, so we set it to0. For the remaining nodes, the color has to be different from the parent, so we only have one option.\n\nInstead of having avisarray, we use the special color-1to denote unvisited nodes.\n\nWith 3 or more colors, the problem becomesa lot harder.\n\nPractice problems\n\nhttps://leetcode.com/problems/is-graph-bipartite/\n\nhttps://leetcode.com/problems/is-graph-bipartite/\n\nWhat is the distance from a node s to every other node in a tree?\n\nWe cannot use DFS to find the distance between nodes in a graph which can have cycles, because DFS is not guaranteed to follow the shortest path from the root to the other nodes. For that, BFS is more suitable (if the graph is unweighted). However, since trees are acyclic, there is a unique path between any two nodes, so DFS must use the unique path, which, by necessity, is the shortest path. Thus, we can use DFS to find distances in a tree.\n\nPractice problems\n\nhttps://leetcode.com/problems/time-needed-to-inform-all-employees/\n\nhttps://leetcode.com/problems/time-needed-to-inform-all-employees/\n\nFind a spanning tree\n\nA spanning tree of a connected, undirected graphGis a subgraph which has the same nodes asGthat is a tree.\nThe edges traversed by a DFS search on a connected graph form a spanning tree (sometimes called a DFS tree). Thus, we do DFS and add the traversed edges to the resulting tree.\n\nConclusions\n\nDFS has many uses. We showed how to make minor modifications to the DFS template to answer reachability and connectivity questions.\n\nAfter DFS, the next algorithm to learn would be BFS (breath-first search). Like DFS, it can answer reachability questions. On top of that, it can also answer questions about distance in undirected graphs.",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/reachability-problems-and-dfs?category=dsa",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Actually Implementing Dijkstra's Algorithm",
      "content": "Actually Implementing Dijkstra's Algorithm\n\nIntroduction\n\nDijkstra's algorithm for the shortest-path problem is one of the most important graph algorithms, so it is often covered in algorithm classes. However, going from the pseudocode to an actual implementation is made difficult by the fact that it relies on a priority queue with a \"decrease key\" operation. While most programming languages offer a priority queue data structure as part of their standard library, this operation is generally not supported (e.g., in C++, Java or Python). In this blog, we go over the different ways to implement Dijkstra's algorithm with and without this operation, and the implications of using each. All in all, we consider 5 versions of Dijkstra (names mostly made up by me):\n\nTextbook Dijkstra: the version commonly taught in textbooks where we assume that we have a priority queue with the \"decrease key\" operation. As we said, this often does not hold true in reality.Linear-search Dijkstra: the most naive implementation, but which is actually optimal for dense graphs.Lazy Dijkstra: practical version which does not use the \"decrease key\" operation at all, at the cost of using some extra space.BST Dijkstra: version which uses a self-balancing binary search tree to implement the priority queue functionality, including the \"decrease key\" operation.Theoretical Dijkstra: version that uses a Fibonacci heap for the priority queue in order to achieve the fastest possible runtime in terms of big-O notation. This is actually impractical due to the complexity and high constant factors of the Fibonacci heap.\n\nTextbook Dijkstra: the version commonly taught in textbooks where we assume that we have a priority queue with the \"decrease key\" operation. As we said, this often does not hold true in reality.\n\nLinear-search Dijkstra: the most naive implementation, but which is actually optimal for dense graphs.\n\nLazy Dijkstra: practical version which does not use the \"decrease key\" operation at all, at the cost of using some extra space.\n\nBST Dijkstra: version which uses a self-balancing binary search tree to implement the priority queue functionality, including the \"decrease key\" operation.\n\nTheoretical Dijkstra: version that uses a Fibonacci heap for the priority queue in order to achieve the fastest possible runtime in terms of big-O notation. This is actually impractical due to the complexity and high constant factors of the Fibonacci heap.\n\nRoughly, each of the 5 versions corresponds to a different data structure used to implement the priority queue. Throughout the post, letnbe the number of nodes andmthe number of edges. Here is summary of the resulting runtime and space complexities:\n\nTextbook Dijkstra: indexed binary heap. Runtime:O(m*log n); space:O(n).Linear-search Dijkstra: unordered array. Runtime:O(n²); space:O(n).Lazy Dijkstra: binary heap. Runtime:O(m*log n); space:O(m).BST Dijkstra: self-balancing BST. Runtime:O(m*log n); space:O(n).Theoretical Dijkstra: Fibonacci heap. Runtime:O(m + n*log n); space:O(n).\n\nTextbook Dijkstra: indexed binary heap. Runtime:O(m*log n); space:O(n).\n\nLinear-search Dijkstra: unordered array. Runtime:O(n²); space:O(n).\n\nLazy Dijkstra: binary heap. Runtime:O(m*log n); space:O(m).\n\nBST Dijkstra: self-balancing BST. Runtime:O(m*log n); space:O(n).\n\nTheoretical Dijkstra: Fibonacci heap. Runtime:O(m + n*log n); space:O(n).\n\nWe provide implementations in Python and C++. The initial sections are mostly background. If you are already familiar with Dijkstra's algorithm, you can skip to the code snippets.\n\nThe shortest-path problem\n\nThe input consists of a graphGand a special nodes. The edges ofGare directed and have non-negative weights. The edge weights represent the \"lengths\" of the edges. The goal is to find the distance fromsto every other node inG. The distance fromsto another node is the length of the shortest path fromsto that node, and the length of a path is the sum of the lengths of its edges. If a node is unreachable froms, then we say that the distance is infinite.\n\nMore precisely, this is known as the \"single-source shortest-path\" (SSSP) problem, because we find the distance from one node to every other node. Related problems include the \"all-pairs shortest paths\" problem and the single-source single-destination problem. Dijkstra's algorithm is a really efficient algorithm for the SSSP problem when the edges are non-negative. Dijkstra's algorithm does not work in the presence of negative edges (zero-weight edges are fine). IfGcontains negative edges, we should use the Bellman-Ford algorithm instead.\n\nThe constraint that the edges are directed is not important: ifGis undirected, we can simply replace every undirected edge{u,v}with a pair of directed edges(u,v)and(v,u)in opposite directions and with the weight of the original edge.\n\nTo simplify things, we make a couple of assumptions that do not make any actual difference:\n\nNodes not reachable bysplay no role in the algorithm, so we assume thatscan reach every node. This is so that, in the analysis, we can assume thatn=O(m).We assume that the distance fromsto every node is unique. This allows us to talk about \"the\" shortest path to a node, when in general there could be many.\n\nNodes not reachable bysplay no role in the algorithm, so we assume thatscan reach every node. This is so that, in the analysis, we can assume thatn=O(m).\n\nWe assume that the distance fromsto every node is unique. This allows us to talk about \"the\" shortest path to a node, when in general there could be many.\n\nThe graph's representation\n\nA graph is a mathematical concept. In the context of graph algorithms, we need to specify how the graph is represented as a data structure. For Dijkstra's algorithm, the most convenient representation is the adjacency list. The valuable thing about the adjacency list representation is that it allows us to iterate through the out-going edges of a node efficiently.\n\nIn the version of the adjacency list that we use, each node is identified with an index from0ton-1. The adjacency list contains one list for each node. For each nodeubetween0andn-1, the listG[u]contains one entry for each neighbor ofu. In a directed graph, if we have an edge(u,v)fromutov, we say thatvis a neighbor ofu, butuis not a neighbor ofv. Since the graph is weighted, the entry for each neighborvconsists of a pair of values,(v, l): the destination nodev, and the lengthlof the edge(u,v).\n\nDijkstra's algorithm idea\n\nOne of the data structures that we maintain is a listdistwheredist[u]is the best distance known foruso far. At the beginning,dist[s] = 0, and for every other nodedist[u] = infinity. These distances improve during the algorithm as we consider new paths. Our goal is to get to the point wheredistcontains the correct distance for every node.\n\nDuring the algorithm, thedistlist is only updated through an operation called \"relaxing\" an edge.\n\nIn words, relaxing an edge(u,v)means checking if going toufirst and then using the edge(u,v)is shorter than the best distance known forv. If it is shorter, then we updatedist[v]to the new, better value.\n\nDijkstra's algorithm is based on the following observations:\n\nifdist[u]is correctandthe shortest path fromstovends in the edge(u,v), then if we relax the edge(u,v), we will find the correct distance tov. If either of the conditions are not satisfied, relaxing(u,v)may improvedist[v], but it will not be the correct distance.To find the correct distance tov, we need to relax all the edges in the shortest path fromstov, in order. If we do it in order, each node in the path will have the correct distance when we relax the edge to the next node, satisfying the conditions.\n\nifdist[u]is correctandthe shortest path fromstovends in the edge(u,v), then if we relax the edge(u,v), we will find the correct distance tov. If either of the conditions are not satisfied, relaxing(u,v)may improvedist[v], but it will not be the correct distance.\n\nTo find the correct distance tov, we need to relax all the edges in the shortest path fromstov, in order. If we do it in order, each node in the path will have the correct distance when we relax the edge to the next node, satisfying the conditions.\n\nDijkstra's algorithm is efficient because every edge is relaxed only once (unlike other algorithms like Bellman-Ford, which relaxes the edges multiple times). To relax every edge only once, we must relax the out-going edges of each node only after we have found the correct distance for that node.\n\nAt the beginning, onlyshas the correct distance, so we relax its edges. This updates the entries indistfor its neighbors. The neighbor ofsthat is closest tos, say,x, has the correct distance at this point. This is because every other path fromstoxstarts with a longer edge, and, since the graph does not have negative-weight edges, additional edges can only increase the distance. Next, sincexhas the correct distance, we can relax its out-going edges. After that, the nodeywith the 3rd smallest distance indist(aftersandx) has the correct distance because the node beforeyin the shortest path fromstoymust be eithersorx. It cannot be any other node because simply reaching any node that is notsorxis already more expensive than the distance we have found fory. We continue relaxing the out-going edges of nodes, always taking the next node with the smallest found distance. By generalizing the argument above, when we relax the out-going edges of each node, that node already has the correct distance. We finish after we have gone through all the nodes. At that point,distcontains the correct distance for every node.\n\nIn order to implement Dijkstra's algorithm, we need to decide the data structures used to find the unvisited node with the smallest distance at each iteration.\n\nPriority queues\n\nPriority queues are data structures that are useful in many applications, including Dijkstra's algorithm.\n\nIn a normal queue, we can insert new elements and extract the oldest element. A priority queue is similar, but we can associate a priority with each element. Then, instead of extracting the oldest element, we extract the one with highest priority. Depending on the context, \"highest priority\" can mean the element with the smallest or largest priority value. In this context, we will consider that the highest priority is the element with the smallest priority value.\n\nA priority queue is anabstractdata structure. That means that it only specifies which operations it supports, but not how they are implemented. There actually exist many ways to implement a priority queue. To make matters more confusing, different priority queues implementations support different sets of operations. The only agreed part is that they must support two basic operations:\n\ninsert(e, k): insert elementewith priorityk.extract_min(): remove and return the element with the smallest priority value.\n\ninsert(e, k): insert elementewith priorityk.\n\nextract_min(): remove and return the element with the smallest priority value.\n\nFor Dijkstra's algorithm, we can use a priority queue to maintain the nodes, usingdist[u]as the priority for a nodeu. Then, at each iteration we can extract the unvisited node with the smallest distance. However, there is a problem: when we relax an edge, the valuedist[u]may decrease. Thus, we need the priority queue to support a third operation which is not commonly supported:\n\nchange_priority(e, k): set the priority ofetok(assuming thateis in the priority queue).\n\nchange_priority(e, k): set the priority ofetok(assuming thateis in the priority queue).\n\nA related operation is removing elements that are not the most prioritary:\n\nremove(e): removee(assuming thateis in the priority queue).\n\nremove(e): removee(assuming thateis in the priority queue).\n\nIf a priority queue implements remove, we can use it to obtain the same functionality aschange-priority(e, k): we can first callremove(e)and then reinsert the element with the new key by callinginsert(e, k).\n\nPseudocode with a priority queue\n\nAssuming that we have a priority queue data structure that supportsinsert,extract-min, andchange-priority, Dijkstra's pseudocode would be as follows.\n\nThe priority queue contains the unvisited nodes, prioritized by distance froms. At the beginning, the priority queue contains all the nodes, and they are removed as they are visited.\n\nA common variation is to add them to the priority queue when they are reached for the first time, instead of adding all the nodes at the beginning. The only change is how the priority queue is initialized and the if-else cases at the end:\n\nIt does not change the runtime or space complexity, but there is also no downside to deferring insertions to the PQ. On average, the PQ will contains fewer elements.\n\nAnalysis of Dijkstra's algorithm\n\nUsually, we analyze the algorithmsafterimplementing them. However, in order to choose the best data structure for the priority queue, we need to analyze how much we use each type of operation.\nThus, it is convenient to define the runtime in terms of the priority queue operations, without specifying yet how they are done. LetT_ins,T_min, andT_changebe the time perinsert,extract_min, andchange_priorityoperation, respectively, on a priority queue containingnelements.\n\nThe mainwhileloop hasniterations, and the total number of iterations of the innerforloop, across allniterations, ism. This is because each edge is relaxed once.\n\nThe runtime is dominated by the priority queue operations, so it isO(n*T_ins + n*T_min + m*T_change). These operations dominate the runtime because everything else combined (like updating thedistlist) takesO(n+m)time.\n\nLinear-search Dijkstra for dense graphs\n\nThe simplest way to simulate theextract_minfunctionality of a priority queue is to iterate through the entiredistlist to find the smallest value among the non-visited entries. If we do this, we don't need a priority queue. We call thislinear-search Dijkstra. We getT_ins = O(1),T_min = O(n), andT_change = O(1). Plugging those in, the total runtime of linear-search Dijkstra isO(n + n*n + m) = O(n²), where we simplify out themterm becausen² > min any graph. More precisely, a directed graph withnnodes has at mostn*(n-1)=O(n²)edges.\n\nA graph with \"close to\"n*(n-1)edges is called dense.Linear-search Dijkstra is actually optimal for dense graphs.This is because Dijkstra's algorithm must takeO(m)time just to relax all edges, so it cannot be faster thanO(m), and, in dense graphs that is already proportional toO(n²).\n\nHere is a Python implementation:\n\nAnd C++. We omit the includes and \"using namespace std;\".\n\nPriority queues for sparse graphs\n\nTheO(n²)time from the implementation above is slow if the graphGis sparse, meaning that the number of edges is small relative toO(n²). Recall that the time isO(n*T_ins + n*T_min + m*T_change). Ifmis more similar tonthan ton², then we would be happy to trade a slowerchange_prioritytime for a fasterextract_mintime.\n\nThe best possible answer in terms of big-O notation is to use a priority queue implementation based on a data structure known as aFibonacci Heap. A Fibonacci heap containing at mostnelements achieves the following times:\n\ninsert:O(log n)amortized time.extract_min:O(log n)amortized time.change_priority:O(1)amortized time.\n\ninsert:O(log n)amortized time.\n\nextract_min:O(log n)amortized time.\n\nchange_priority:O(1)amortized time.\n\nAmortized time means that it could take more time, but, if we average out the times for that operation across the execution of an algorithm, each one takes that time on average.\n\nUsing a Fibonacci heap, we get a total time ofO(n*log n + m)for Dijkstra's algorithm. This is really fast in terms of big-O notation, but Fibonacci heaps have larger constant factors than other data structures, making them slower in practice.\n\nThe most common way to implement a priority queue is with abinary heap. It is simple and fast in practice. Binary heaps supportinsertandextract_mininO(log n)like a Fibonacci heap. However, they do not support thechange_priorityoperation.\n\nIt is possible to modify a binary heap to to support thechange_priorityoperation inO(log n)time. The result is sometimes called an \"indexed priority queue\". Using an indexed priority queue, we would get a total runtime ofO(n*log n + m*log n) = O(m*log n). This is slightly worse than with a Fibonacci heap, and faster in practice.\n\nIn any case, the priority queues provided by languages like C++, Python, and Java, do not support thechange_priorityoperation. This creates a disconnect between the pseudocode taught in classrooms and the actual code that we can write.\n\nThe goal of this post is to illustrate the options to deal with this issue. There are 3:\n\nTextbook Dijkstra: find or implement our own indexed priority queue.Lazy Dijkstra: we implement Dijkstra without using thechange_priorityoperation at all.BST Dijkstra: we use a self-balancing binary search tree as the priority queue.\n\nTextbook Dijkstra: find or implement our own indexed priority queue.\n\nLazy Dijkstra: we implement Dijkstra without using thechange_priorityoperation at all.\n\nBST Dijkstra: we use a self-balancing binary search tree as the priority queue.\n\nWe will cover the latter two options. The first option is an interesting exercise in data structures (Iimplementedit once for a project), but it is more about the inner workings of binary heaps than it is about Dijkstra's algorithm.\n\nAll three options have a runtime ofO(m*log n). Note that for dense graphs, this becomesO(n² log n)time, so they are all worse than the naive linear-search Dijkstra. In terms of space, lazy Dijkstra is worse than the others, as it needsO(m)space, as opposed toO(n)for the other options.\n\nLazy Dijkstra\n\nWe implement Dijkstra using a priority queue that does not support the change-priority operation. We need the following change: when we find a shorter distance to a node that is already in the priority-queue, instead of using the \"change-priority\" operation, we simply use an \"insert\" operation and add a copy of the node in the priority queue with the new distance. Then, when we extract a node from the priority queue, we ignore it if it is not the first time we extract that node. We call this version of Dijkstra \"lazy Dijkstra\" because we \"postpone\" the removal of the pre-existing copy of the node.\n\nHere is a Python version. The logical structure of a binary heap is a binary tree, but, internallythe tree is represented as an arrayfor efficiency reasons. Python is a bit whack because, instead of having a priority queue module that encapsulates the implementation, we have theheapqmodule, which provides priority queue operations that can be used directly on a list representing a binary heap.heapqoffers functionsheappop(equivalent toextract_min) andheappush(equivalent toinsert). These functions receive a normal Python list as a parameter, and this list is assumed to represent a binary heap. In Python, if the priority queue contains tuples, then the first element in the tuple is the priority. Thus, in the implementation we insert tuples to the priority queue with the distance first and the node second.\n\nHere is a C++ version:\n\nAnalysis: since nodes can be added to the priority queue multiple times, in lazy Dijkstra the maximum number of elements in the priority queue increases fromO(n)toO(m). As a result, we doO(m)extract_minandinsertoperations. The total runtime isO(m*log m). This can be simplified toO(m*log n), becauselog m < log (n²) = 2 log n = O(log n). Thus, in terms of big-O notation,lazy Dijkstra is equally fast as textbook Dijkstra(Dijkstra with an indexed priority queue). The only thing that got worse is the space used by the priority queue.\n\nBST Dijkstra\n\nSelf-balancing binary search trees, like red-black trees or AVL trees, are a type of data structure that maintains a set of elements ordered according to values associated with the elements, known as the elements' keys. They support a few operations, all inO(log n)time. For our use case, we are interested in the following ones:\n\nInsert an element with a given key.Find the element with the smallest/largest key.Given a key, find if there is an element with that key, and optionally remove it.\n\nInsert an element with a given key.\n\nFind the element with the smallest/largest key.\n\nGiven a key, find if there is an element with that key, and optionally remove it.\n\nThese operations allow us to use a self-balancing BST to implement a priority queue. With the third operation, we can even implement thechange_priorityoperation, as we mentioned.\n\nPython does not actually have a self-balancing binary search tree module (why?!), so we cannot implement this version of Dijkstra either without finding or implementing our own self-balancing BST.\n\nHere is a C++ version. In C++, the set data structure is implemented as a self-balancing BST:\n\nAnalysis: in a sense, BST Dijkstra combines the best of both worlds: it has the same runtime and space complexity as textbook Dijkstra, without needing the extra space of Lazy Dijkstra, but it uses a much more ubiquitous data structure, a self-balancing BST. However, in practice, self-balancing BSTs are slower than binary heaps. This has to do with the fact that heaps can be implemented on top of an array, while BSTs use recursive tree data structures with child pointers. The array has much betterlocality of reference. For sparse graphs, I'd expect the performance of the different versions to be ordered as follows:\n\nTextbook Dijkstra > Lazy Dijkstra > BST Dijkstra > Theoretical Dijkstra > Linear-search Dijkstra\n\nPractice problems\n\nHere are some problems on leetcode:\n\nNetwork Delay TimeFind the City With the Smallest Number of Neighbors at a Threshold DistanceReachable Nodes In Subdivided GraphPath with Maximum Minimum Value(Premium only)\n\nNetwork Delay Time\n\nFind the City With the Smallest Number of Neighbors at a Threshold Distance\n\nReachable Nodes In Subdivided Graph\n\nPath with Maximum Minimum Value(Premium only)",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/implementing-dijkstra?category=dsa",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Get Binary Search Right Every Time, Explained Without Code",
      "content": "Get Binary Search Right Every Time, Explained Without Code\n\nOne of the things that makes binary search tricky to implement is that you usually need to tweak the pointer manipulation logic in subtle ways based on the specifics of the problem.\n\nE.g., an implementation that works for finding a target in a sorted array when the target is present, may not work if the target is missing. Or, it may not be clear how to tweak the code to find the last occurrence of the target instead of the first one. And of course, there are plenty of less conventional applications of binary search where the input is not an array, likecatching bike thieves.\n\nInBeyond Cracking the Coding Interview, we wanted to simplify this, so we went looking for a general binary search template. Going into it, I thought we might need at least two templates, but we ended up with just one, which we called the \"transition point recipe\", and which works for every problem we tried, including the 17 problems in the binary search chapter of the book. If you find one where it doesn't work, let me know!\n\nThe transition point problem\n\nHere is the thesis of the transition point recipe:\n\nEvery binary search problem can be reduced to the 'transition point problem'.\n\nIn the 'transition point problem', you are given an array with just two values, say1and2, where all the1s come before the2s, and you need to point where it changes.\n\nE.g., in the array[1, 1, 1, 1, 1, 2, 2, 2], the last1is at index4and the first2is at index5.\n\nKnowing how to solve this specific problem is key to our recipe. The specific binary search implementation is not important, but there is an invariant we can follow that makes it quite easy: ensure that the left pointer is always at a1and the right pointer is always at a2.\n\nWe give code in the book, but remembering exact code in an interview is error prone. Instead, the four bullet points below are all Ipersonallyremember, and I feel confident that I can derive the rest easily.\n\nStart by handling some edge cases:The array is emptyEvery value is1Every value is2Initialize two pointers,leftandright, to the first and last indices, respectively.For the main binary search loop, always maintain theinvariantthat the value atleftis1and the value atrightis2. Let this invariant guide your pointer manipulation logic, so that you don't need to memorize any code.Stop when theleftandrightpointers are next to each other (i.e.,left + 1 == right).\n\nStart by handling some edge cases:The array is emptyEvery value is1Every value is2\n\nThe array is emptyEvery value is1Every value is2\n\nThe array is empty\n\nEvery value is1\n\nEvery value is2\n\nInitialize two pointers,leftandright, to the first and last indices, respectively.\n\nFor the main binary search loop, always maintain theinvariantthat the value atleftis1and the value atrightis2. Let this invariant guide your pointer manipulation logic, so that you don't need to memorize any code.\n\nStop when theleftandrightpointers are next to each other (i.e.,left + 1 == right).\n\nCombining the invariant with the stopping condition, we get that, at the end,leftwill be at the last1andrightwill be at the first2.\n\nThese bullet points rely on two ideas to make binary search easier: (1) handling edge cases upfront, and (2) letting strong invariants guide the implementation. Notice how the invariant even guides the edge cases at the beginning, as they are the necessary ones to be able to initializeleftandrightin a way that satisfies it.\n\nThe reduction\n\nOk, so now, let's take for granted that we can solve the transition point problem. How does this help us solve other binary search problems?\n\nThe idea is to come up with a (problem-specific)predicate, like< target,>= target, orx % 2 == 0, which splits the search range into two regions, the \"before\" region and the \"after\" region.\n\nThis predicate is a function that takes an element of the search range and returns a boolean, and -- as you probably saw coming -- it is key that all the elements withtruevalues come before the elements withfalsevalues (or the other way around).\n\nThen, we can use the solution to the transition point problem to find the transition point between the 'before' and 'after' regions. The only difference is that, instead of checking boolean values directly, we check the result of the predicate.\n\nYou can even wrap the predicate in a function, which we calledis_before(x)in the book, which tells you whether a given element is in the 'before' region. Then, it's really obvious that we are just solving the transition point problem every time.\n\nThe only part that requires some thinking is choosing the right transition point. For example:\n\nif we want to find thefirstoccurrence oftargetin a sorted array, we can useis_before(x) = x < target, which means that, iftargetis present, the first occurrence is the first element in the 'after' region (so, we can check/return therightpointer at the end).if we want to find thelastoccurrence oftargetin a sorted array, we can useis_before(x) = x <= target, which means that, iftargetis present, the last occurrence is the last element in the 'before' region (so, we can check/return theleftpointer at the end).\n\nif we want to find thefirstoccurrence oftargetin a sorted array, we can useis_before(x) = x < target, which means that, iftargetis present, the first occurrence is the first element in the 'after' region (so, we can check/return therightpointer at the end).\n\nif we want to find thelastoccurrence oftargetin a sorted array, we can useis_before(x) = x <= target, which means that, iftargetis present, the last occurrence is the last element in the 'before' region (so, we can check/return theleftpointer at the end).\n\nAnd so on for other problems.\n\nPractice\n\nYou can try the transition-point recipe on all the problems from the binary search chapter of the book online atstart.interviewing.io/beyond-ctci/part-vii-catalog/binary-search, even if you don't have the book. There, you can also find all our solutions using the recipe, in Python, JS, Java, and C++.\n\nBy the way, the binary search chapter of the book is free -- it's inbctci.co/free-chapters.\n\nWant to leave a comment? You can post under thelinkedin postor theX post.",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/binary-search?category=dsa",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Lazy vs Eager Algorithms",
      "content": "Lazy vs Eager Algorithms\n\nWarning: I have not tested any code snippet below. Please let me know if you find a bug.\n\nIntroduction\n\nMost algorithms have multiple valid implementations. For instance, in a binay tree problem, you have multiple ways of handling NULL nodes. I'm currently writingBeyond Cracking the Coding Interview(beyondctci.com), which means that my co-authors and I need to take a stance on what version of each algorithm to use. Ideally, we want to show the simplest version of each algorithm:\n\nEasy to recall for interview,Easy to explain to interviewers,Easy to debug by hand,Short, so that it is quick to code.\n\nEasy to recall for interview,\n\nEasy to explain to interviewers,\n\nEasy to debug by hand,\n\nShort, so that it is quick to code.\n\nIn the book, we don't claim that the version we show is \"the best\" - we say to use the one that works best for you. But showing one in the book is an implicit endorsement.\n\nOne particular decision that comes up again and again with recursive algorithms is choosing between thelazyversion and theeagerversion of an algorithm.\n\nAneagerrecursive function expects 'valid' inputs and ensures to only call the recursive function with 'valid' inputs. We can also call it aclean(call)stackalgorithm.Alazyrecursive algorithm allows 'invalid' inputs, so it starts by validating the input. Then, it calls the recursive function without validating the inputs passed to it. We can also call it adirty stackalgorithm.\n\nAneagerrecursive function expects 'valid' inputs and ensures to only call the recursive function with 'valid' inputs. We can also call it aclean(call)stackalgorithm.\n\nAlazyrecursive algorithm allows 'invalid' inputs, so it starts by validating the input. Then, it calls the recursive function without validating the inputs passed to it. We can also call it adirty stackalgorithm.\n\nWhat 'valid' means depends on the algorithm--we'll see plenty of examples. We'll also translate the concept of eager vs lazy to iterative algorithms.\n\nLazy vs Eager Tree Traversals\n\nAneagertree traversal eagerly validates that the children are not NULL before passing them to the recursive function. Alazytree traversal doesn't, so it needs to check if the current node is NULL before accessing it.\n\nFor instance, here is eager vs lazy preorder traversal:\n\nBoth have the same runtime and space analysis. Even the constant factors probably don't change much, so it comes down to style preference. Which one do you prefer?\n\nLazy vs Eager graph DFS\n\nAneagergraph DFS eagerly checks that the neighbors are not already visited before passing them to the recursive function. Alazygraph DFS doesn't, so it needs to check if the current node is already visited.\n\nFor a graph DFS, we can also do a mix between lazy and eager: we can eagerly check if nodes are already visited, and lazily mark them as visited:\n\nAgain, they all have the same analysis. Which one do you prefer?\n\nLazy vs Eager grid algorithms\n\nConsider the same DFS algorithm but on a grid of 0's and 1's. The 0's are walkable cells, the 1's are obstacles, and\nwalkable cells next to each other are connected. This time, we need to check that the neighbors are not out of bounds, which we can do lazily or greedily.\n\nLazy vs Eager Memoization DP\n\nIn alazymemoization DP (Dynamic Programming) algorithm, we call the recursive function for a subproblem without checking first if we have already computed that subproblem. In aneageralgorithm, we only call the recursive function for subproblems that we still need to compute.\n\nFor memoization DP, I thinklazyis cleaner and more conventional.\n\nLazy vs Eager Iterative Tree traversals\n\nConsider a level-order traversal on a binary tree. A level-order traversal is an iterative algorithm that uses a queue data structure.\n\nAlazyversion puts children in the queue without checking if they are NULL first. We can call it adirty queuealgorithm.Aneagerversion checks for NULL nodes and avoids putting them in the queue. We can call it aclean queuealgorithm.\n\nAlazyversion puts children in the queue without checking if they are NULL first. We can call it adirty queuealgorithm.\n\nAneagerversion checks for NULL nodes and avoids putting them in the queue. We can call it aclean queuealgorithm.\n\nEager Graph BFS is better than lazy Graph BFS\n\nThis is the first exception where one is better than the other in terms of big O analysis. ThelazyBFS allows adding already-visited nodes to the queue, while theeagerone does not. We'll first look at the two versions, and then analyze them.\n\nIt may come as a surprise that these two arenotequivalent like all the other examples.\n\nLet's sayVis the number of nodes andEis the number of edges. To keep things simple, consider that the graph is connected, meaning thatEis at leastV-1and at mostO(V²).\n\nBoth versions takeO(E)time. The difference is in the space complexity: the eager version takesO(V)space because we never have the same node twice in the queue. The lazy version takesO(E)space because we allow the same nodes multiple times in the queue.\n\nTo see this, consider a complete graph:\n\nWhen we visit start, we add A, B, C, D, E to the queue. Now the queue is:[A, B, C, D, E]When we visit A, we add start, B, C, D, E to the queue. Now the queue is:[B, C, D, E, start, B, C, D, E]When we visit B, we add start, A, C, D, E to the queue. Now the queue is:[C, D, E, start, B, C, D, E, start, A, C, D, E]And so on.\n\nWhen we visit start, we add A, B, C, D, E to the queue. Now the queue is:[A, B, C, D, E]\n\nWhen we visit A, we add start, B, C, D, E to the queue. Now the queue is:[B, C, D, E, start, B, C, D, E]\n\nWhen we visit B, we add start, A, C, D, E to the queue. Now the queue is:[C, D, E, start, B, C, D, E, start, A, C, D, E]\n\nAnd so on.\n\nBy the time we finish popping the nodes added as neighbors of the start node, we've doneVqueue pops andV²queue appends, so the queue size isO(V²).\n\nSo, why didn't this happen for other lazy algorithms we have seen?\n\nFor tree traversals, each tree node has a single parent that it can be reached from, so we don't need to worry about the same node appearing twice in the call stack or in the level-order traversal queue.For graph DFS,every node in the call stackis marked visited, so if we callvisit()on a node that is already in the call stack, we'll immediately return as we'll see it is marked as visited.\n\nFor tree traversals, each tree node has a single parent that it can be reached from, so we don't need to worry about the same node appearing twice in the call stack or in the level-order traversal queue.\n\nFor graph DFS,every node in the call stackis marked visited, so if we callvisit()on a node that is already in the call stack, we'll immediately return as we'll see it is marked as visited.\n\nEager Dijkstra is better than Lazy Dijkstra, but harder to implement\n\nI wrote extensively about different Dijktsra implementations inthis Dijkstra blog post.\n\nDijkstra is similar to BFS, with the main difference that it uses a priority queue (PQ) instead of a queue to visit the nodes that are closer first (in terms of shortest paths).\n\nIn BFS, when a node is added to the queue, its distance from the starting node is already established and there is never a reason to add it again to the queue. In Dijkstra, when a node is added to the PQ, we might later find a shorter path while it is still in the PQ. When that happens, we can do two things:\n\nLazy Dijkstra: just add the node again with the new, improved distance. It will get popped before the previous occurrence because it has higher priority in the PQ. When a node with a \"stale\" distance gets popped off from the queue, we just ignore it.Eager Dijkstra(called textbook Dijkstra in the other blog post): instead of adding the node again, find the existing occurrence of it in the PQ, and update it with the new found distance. This guarantees that the same node never appears twice in the PQ.\n\nLazy Dijkstra: just add the node again with the new, improved distance. It will get popped before the previous occurrence because it has higher priority in the PQ. When a node with a \"stale\" distance gets popped off from the queue, we just ignore it.\n\nEager Dijkstra(called textbook Dijkstra in the other blog post): instead of adding the node again, find the existing occurrence of it in the PQ, and update it with the new found distance. This guarantees that the same node never appears twice in the PQ.\n\nBoth versions takeO(E*log V)time, but eager is more space efficient, analogously to eager BFS:O(V)for eager Dijkstra vsO(E)for lazy Dijkstra.\n\nHere is lazy Dijkstra:\n\nUnfortunately, eager Dijkstra is not so easy to implement in Python because we are missing thedecrease_key()operation in a heap (and Python does have a self-balancing BST data structure, which can also be used for eager Dijkstra). You can see a BST-based C++ implementation in my other blog post.\n\nThedijkstra_lazy()algorithm above is more or less standard and it has been known as \"lazy Dijkstra\" for a while. However, it is possible to make an even lazier version which has the same runtime and space analysis (but likely bigger constant factors). The idea is that instead of only adding to the PQ the neighbors for whom we find an improved distance, we can simply add all of them, and discard duplicates once we extract them from the PQ:\n\nSo, Lazy or Eager?\n\nWe could keep looking at lazy vs eager algorithms, but I'll stop here. In aggregate, these are the pros and cons that I see:\n\nPros of lazy algorithms\n\nLazy algorithms require less code.This is because you only need to validate the parameters of the recursive function once at the beginning, instead of validating what you pass to each recursive call. This is specially true in binary tree problems, where you usually have two recursive calls. It doesn't apply as much for graphs.Lazy algorithms require less indentation.For instance, in graph problems, we don't need to do checks inside the for loop over the neighbors.Lazy algorithms do not require special handling for the first recursive call.You don't need to worry about things like checking if the root is NULL or marking the start node as visited.Lazy recursive functions have simpler preconditions.You can just pass anything to them, and they work.\n\nLazy algorithms require less code.This is because you only need to validate the parameters of the recursive function once at the beginning, instead of validating what you pass to each recursive call. This is specially true in binary tree problems, where you usually have two recursive calls. It doesn't apply as much for graphs.\n\nLazy algorithms require less indentation.For instance, in graph problems, we don't need to do checks inside the for loop over the neighbors.\n\nLazy algorithms do not require special handling for the first recursive call.You don't need to worry about things like checking if the root is NULL or marking the start node as visited.\n\nLazy recursive functions have simpler preconditions.You can just pass anything to them, and they work.\n\nPros of eager algorithms\n\nFor a graph BFS, eager has a better space complexity.This is a case where eager is objectively better. (Eager Dijkstra is also better but it is not expected to be implemented in interviews. Your interviewer is probably expecting lazy Dijkstra.)Eager algorithms do fewer recursive calls or iterations.In a binary tree, the number of NULL nodes is always one more than the number of internal nodes. This means that a lazy traversal does twice as many recursive calls/iterations as the eager counterpart. This could make a big difference if you want to debug the code manually. For instance, in this picture, you can see that adding NULLs to the queue makes visualizing the steps more painful:\n\nFor a graph BFS, eager has a better space complexity.This is a case where eager is objectively better. (Eager Dijkstra is also better but it is not expected to be implemented in interviews. Your interviewer is probably expecting lazy Dijkstra.)\n\nEager algorithms do fewer recursive calls or iterations.In a binary tree, the number of NULL nodes is always one more than the number of internal nodes. This means that a lazy traversal does twice as many recursive calls/iterations as the eager counterpart. This could make a big difference if you want to debug the code manually. For instance, in this picture, you can see that adding NULLs to the queue makes visualizing the steps more painful:\n\nEager algorithm can 'feel safer'.A friend commented that, with a lazy algorithm, they feel like they are missing an edge case.\n\nEager algorithm can 'feel safer'.A friend commented that, with a lazy algorithm, they feel like they are missing an edge case.\n\nMy preference\n\nHere are my personal preferences for coding interviews (not those of the other authors of 'Beyond Cracking the Coding Interview'):\n\nStrong preferences:\n\nFor BFS, use eager. This one is clear cut.For memoization DP, use lazy. It is much cleaner to code.For Dijkstra, use lazy Dijkstra (not super lazy Dijkstra). It is what is realistic to do in an interview and probably what the interviewer expects.\n\nFor BFS, use eager. This one is clear cut.\n\nFor memoization DP, use lazy. It is much cleaner to code.\n\nFor Dijkstra, use lazy Dijkstra (not super lazy Dijkstra). It is what is realistic to do in an interview and probably what the interviewer expects.\n\nWeak preferences:\n\nFor binary tree traversals (iterative or recursive), use lazy. It is a bit cleaner.For graph DFS, use eager. It is a bit more standard, and aligned with a graph BFS.\n\nFor binary tree traversals (iterative or recursive), use lazy. It is a bit cleaner.\n\nFor graph DFS, use eager. It is a bit more standard, and aligned with a graph BFS.\n\nIn the book, we'll definitely mention that some algorithms can be implemented in a lazy or eager way (in way less detail than here), and that you should choose the one that feels easier to you. But, we still need to pick one to show in the problem solutions. One idea is trying to be consistent throughout (e.g., doing all tree and graph traversals in an eager way). If you have an opinion on which one is better, please reach out! I'd love to hear it.",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/lazy-vs-eager?category=dsa",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Problem Solving BCtCI Style",
      "content": "Problem Solving BCtCI Style\n\nHere's a thought: You don't want the first time you think about the question\"What should I do if I get stuck in a coding interview?\"to be when you are stuck in a coding interview.\n\nIn a way, getting stuck in a coding interview is an opportunity. The main goal of the interview is to see your problem-solving thought process, and being stuck is the ideal time to showcase it.\n\nBut you want to be prepared. It's valuable to have a plan for this exact scenario. We all dread blanking out in an interview, but having a plan makes it easy to simply focus on executing it. So, let's talk about what such a plan could look like in this blog post.\n\nInBeyond Cracking the Coding Interview, we go over all the steps in an interview, and our best tips to do well in each of them:\n\nIn this blog post, I'll zoom in on the problem-solving step, \"Design the Algorithm,\" and illustrate the thought process with a problem.\n\nAs you can see, we break it down into four steps:\n\nMinimally sketch the naive solutionto establish a baseline.Identify upper and lower boundsusing big O analysis to narrow down the range of possible solutions.Look for triggers (Keywords)that point to a specific approach.Employ boosters: problem-solving strategies that give you the \"boost\" you need when you are stuck.\n\nMinimally sketch the naive solutionto establish a baseline.\n\nIdentify upper and lower boundsusing big O analysis to narrow down the range of possible solutions.\n\nLook for triggers (Keywords)that point to a specific approach.\n\nEmploy boosters: problem-solving strategies that give you the \"boost\" you need when you are stuck.\n\nThese are not revolutionary ideas -- it's what good problem solvers do and think about instinctively. One of the main goals of the book, and of this blog post, is to spell out the thought process of people who are really good at this in a relatable way so that anyone can reproduce it.\n\nWe playfully call this theMIKE template(Minimally sketch brute force,Identify bounds,Keywords (triggers),Employ boosters) afterMike Mroczka, one of the authors of BCtCI.\n\nRather than expanding on these now, we'll see them in action with the following problem.\n\nProblem Statement\n\nThe problem is based onLeetCode 3458, which appeared in a recent contest. You can go and give it a try before reading on (it's labeled as medium, but I think it's on the harder end of medium). The thought process I'll walk through here is based on how I solved it during the contest.\n\nGiven a strings, a substring ofsisspecialif any character in it does not appear outside it.\n\nFor example, ifsis\"abcba\":\n\n\"bcb\"is a special substring because'b'and'c'do not appear insoutside\"bcb\".\"abc\"is not a special substring because'a'appears insoutside\"abc\".\n\n\"bcb\"is a special substring because'b'and'c'do not appear insoutside\"bcb\".\n\n\"abc\"is not a special substring because'a'appears insoutside\"abc\".\n\nGiven a stringsconsisting ofnlowercase English letters, determine the maximum number of disjointspecialsubstrings. Two substrings are disjoint if they do not overlap.\n\nConstraints:\n\n2 <= n <= 10^5sconsists only of lowercase English letters.\n\n2 <= n <= 10^5\n\nsconsists only of lowercase English letters.\n\nDigesting the problem\n\nFirst, we need to digest what the problem is asking. This problem follows a common pattern: it introduces a kind of esoteric definition, \"special substring\", and then asks us to do something with it.\n\nTo make sure we understand what a special substring is, it's good to look at a few examples, starting with the provided ones. For instance, in\"abcba\", do you understand why\"a\"is not special but\"c\"is?\n\nTake some time to come up with your own examples. Rushing to solving a problem before understanding it well is a common but often costly mistake.\n\nApproach\n\nSometimes, it helps to tackle just one part of the problem first, so we can start making progress.\n\nWe can think of an algorithm with 2 parts:\n\nPart A: Find all the special substrings.Part B: Find the most non-overlapping special substrings.\n\nPart A: Find all the special substrings.\n\nPart B: Find the most non-overlapping special substrings.\n\nLet's start with part A.\n\nPart A: Find all the special substrings\n\nWe'll walk through the MIKE template.\n\nM: Minimally sketch brute force\n\nThe key here is to not overthink it. We just want to get the ball rolling and have a baseline we can improve upon.\n\nSince we don't want to spend too much time in an interview, you could even just describe the idea in a sentence and move on. But we prefer to briefly sketch it in very high-level pseudocode. We call it 'intended English': it's written like English, but with indentation to show the code structure:\n\nInterviews often involve considering trade offs between algorithms, so it's a\ngood habit to give them names and list their time/space complexity.\n\nIn this case, the space complexity depends on how many special substrings we might find, which is not clear yet, so we'll leave it out for now.\n\nSketching the brute force solution helps us ensure we understand the problem\n(and if we are solving for the wrong thing, we give the interviewer a chance\nto let us know).\n\nI: Identify upper and lower bounds\n\nWe can use big O analysis to narrow down the range of possible solutions. An upper bound means \"we don't have to consider any solution that takes longer than this\", and a lower bound means the opposite: \"we don't have to consider any solution that takes less time than this\". In the book, we go over two ways of establishing an upper bound and two ways of establishing a lower bound:\n\nUpper bounds:\n\nBrute force upper bound: we just saw that we can find all special substrings inO(n^4)time, so we don't have to consider any solution that takes longer than that.TLE (Time Limit Exceeded) upper bound: here is where we use the problem constraints to establish an upper bound. The problem says thatn <= 10^5, which usually means thatO(n^2)solutions are too slow, butO(n log n)or faster solutions are fine.\n\nBrute force upper bound: we just saw that we can find all special substrings inO(n^4)time, so we don't have to consider any solution that takes longer than that.\n\nTLE (Time Limit Exceeded) upper bound: here is where we use the problem constraints to establish an upper bound. The problem says thatn <= 10^5, which usually means thatO(n^2)solutions are too slow, butO(n log n)or faster solutions are fine.\n\nLower bounds:\n\nOutput-size lower bound: thespacetaken by the output is a lower bound for the time complexity, because that's how long it takes just to write the output. In our case, the output of the overall problem is just a number, so this lower bound is trivial:O(1). Bounds are not always useful!Task-based lower bound: some problems involve an inherent task thatanysolution must fulfill. The runtime of this task is a lower bound. In this case, we know weat leastneed to read every letter in the input, so we have a lower bound ofO(n). In other words, we can rule out solutions that takeO(log n)orO(1)time.\n\nOutput-size lower bound: thespacetaken by the output is a lower bound for the time complexity, because that's how long it takes just to write the output. In our case, the output of the overall problem is just a number, so this lower bound is trivial:O(1). Bounds are not always useful!\n\nTask-based lower bound: some problems involve an inherent task thatanysolution must fulfill. The runtime of this task is a lower bound. In this case, we know weat leastneed to read every letter in the input, so we have a lower bound ofO(n). In other words, we can rule out solutions that takeO(log n)orO(1)time.\n\nCombining our findings, we can narrow down our search range toO(n log n)orO(n)algorithms (something likeO(n log^2 n)would also be fine, it's just less common).\n\nK: Keywords (triggers)\n\nThere are certain properties of problems that point to a specific approach. Here are some triggers we can identify for this problem:\n\nfinding substrings->sliding windowsO(n log n)possible target complexity->sorting or heaps\n\nfinding substrings->sliding windows\n\nO(n log n)possible target complexity->sorting or heaps\n\nUnfortunately, triggers are not a guarantee, and these triggers don't seem to help for this problem:\n\nIn sliding windows, once you move past a character, you don't later go back. So, in Example 1, it would be impossible to find both\"abcba\"and\"bcb\": if you find\"abcba\"first, therightpointer would have to go back to find\"bcb\". But if you find\"bcb\"first, theleftpointer would have to go back to find\"abcba\".Sorting doesn't seem like a good fit because the input order is important.\n\nIn sliding windows, once you move past a character, you don't later go back. So, in Example 1, it would be impossible to find both\"abcba\"and\"bcb\": if you find\"abcba\"first, therightpointer would have to go back to find\"bcb\". But if you find\"bcb\"first, theleftpointer would have to go back to find\"abcba\".\n\nSorting doesn't seem like a good fit because the input order is important.\n\nDo you think I missed any other triggers?\n\nE: Employ boosters\n\nSo, triggers didn't help, and brute force is still far from the target complexity. It's time to employ boosters.\n\nHere's an overview:\n\nThe boosters are roughly ordered, but we don't always have to use them in order. In fact, here's a plot twist: what we did at the beginning, splitting the problem into two parts, is the third booster:Decrease the Difficulty->Break Down the Problem.\n\nBooster 1: Brute force optimization\n\nThe first booster is straightforward: take the brute force pseudocode we already have and try to optimize it.\n\nIn the boosters diagram, we list three ways to go about it. One of them is theData structure pattern. Many bottlenecks come from having to do some calculation inside a loop. In those situations, ask yourself,\n\n\"Do I know of any data structure which makes this type of operation faster?\"\n\nFor this problem, we can use a hash set to optimize the innermost loop:\n\nIf you have working code or pseudocode but think of an optimization or better\napproach, do NOT edit your code. Copy-paste it and work on a separate copy.\nThis way, if you don't have time to finish or realize it's wrong, you'll still\nhave the previous working version.\n\nBooster 2: Hunting for properties\n\nWe got down toO(n^3)time, but we know we still need to bring this down to the target complexity.\n\nLet's say we don't know how to optimize the code further. Often, the breakthrough comes from uncovering some \"hidden\" observation orpropertynot explicitly mentioned in the statement. Our second booster is to go hunting for those.\n\nIn the book, we discuss a bunch of ways of doing this, but the most basic and effective one is to try to solve the problem manually with a non-trivial example. By non-trivial, we mean that is is not some weird edge case, which would not be helpful for figuring out a general algorithm.\n\nLet's actually do that: takes = \"mississippi\"andmanuallytry to find all the special substrings.\n\nDon't overthink it. Don't think about algorithms yet. Just write them down.\n\nDone? Ok,nowtry to reverse-engineer what shortcuts your brain took. This is one property you may have noticed:\n\nProperty 1\n\nProperty 1:a special substring must start at the first occurrence of a letter.\n\nYou may have noticed this property when your brain skipped over the second, third, or fourth'i's inmississippiand intuitively realized that there is no special substring starting at those. Writing down the propertyformalizesthis instinct and ropes in the interviewer.\n\nNow that we have a property, we have to find a way to use it.Property 1allows us to optimize the outer loop: it means we only have26 = O(1)possible starts to check (problems where the input consists of only lowercase letters often have optimizations like this).\n\nAs we iterate through the possible starts, we can track letters seen so far (e.g., in a hash set):\n\nWe like to write down the big O simplification (O(26 * n^2) = O(n^2)), so\nthe interviewer doesn't think we missed steps.\n\nWe haven't hit our target time complexity yet, so let's keep hunting for properties. Here is another one:\n\nProperty 2\n\nProperty 2:of all the special substrings that start at a given letter, we only care about the shortest one.\n\nOur ultimate goal is to find the most non-overlapping special substrings. If we can choose between two special substrings, one of which contains the other, it is always \"optimal\" or, at least, \"safe\" to pick the smaller one.\n\nFor instance, ifsis\"baa\", we have two choices for special substrings starting at'b':\"baa\"and\"b\". We should pick\"b\"so that the\"aa\"part can be in another disjoint special substring.\n\nAgain, when we find a property, we need to think of how to apply it.Property 2means that, for each starting pointi, we can grow a substring one letter at a time, and stop as we find the first special substring.\n\nLet's break this down a bit more: say you start at indexi.\n\nIf you find a lettercthat appears at some later point, we need to grow the substring up to that index.If you find a lettercthat appears beforei, we can stop the search. No substring starting atican be special.\n\nIf you find a lettercthat appears at some later point, we need to grow the substring up to that index.\n\nIf you find a lettercthat appears beforei, we can stop the search. No substring starting atican be special.\n\nFor example, imagineistarts at the first'b'in the following string:\n\nThat means we need to grow the substring at least up to the last'b'in the string:\n\nAs we grow the substring, we hit an'a', which appears beforei, and we realize that no substring starting atican be special.\n\nWe can now add this logic to our algorithm. We can start the algorithm by computing the first and last index of each letter (this is an example of thepreprocessing patternin the boosters diagram -- it's common for properties from Booster 2 to enable optimizations from Booster 1).\n\nThen, as we grow each substring, we keep track of the farthest index we need to reach. (This is actually a common pattern in sliding window algorithms, where we maintain information about the window as it 'slides', rather than computing it from scratch every time the window moves. So, the 'sliding windows' trigger wasn't completely off).\n\nWe got the time down toO(n). Since we hit the lower bound, we can be confident Part A is as good as it can be, and we can move on to Part B.\n\nPart B: Find the most non-overlapping special substrings\n\nLet's be honest: even if in the book we reallyemphasizedeveloping your problem-solving skills by using the MIKE template and the boosters, knowing a bunch of leetcode questions DOES give you an edge in coding interviews. So, I'll tell you how I actually solved this problem in the contest. I realized that Part B is just a variation of a classic greedy problem: most non-overlapping intervals. Indeed, a substring can be seen as an interval of the string.\n\nThe \"most non-overlapping intervals\" problem is in BCtCI, so I already knew that it can be solved with a greedy algorithm that sorts the intervals by their end time and then iterates through them, picking the ones that don't overlap with the previous one (hereis a similar problem on leetcode). This algorithm fits within our target time complexity, so I didn't have to think beyond that.\n\nIf I didn't already know the solution, I would have walked through the MIKE template again for Part B.\n\nFull implementation\n\nHere is a full implementation:\n\nYou may think that the bottleneck is the sorting, but it's not. Recall that there are only up to 26 special substrings (by Property 1). Sorting26intervals takesO(26 log 26) = O(1)time.\n\nConclusion\n\nI wanted to give an overview of all the high-level ideas for problem-solving in leetcode-style interviews. We could dive a lot deeper into any of those ideas, so this blog post may feel a bit rushed, but the meta-point is thatyou should have a plan for when you are stuck in an interview(and you should be following it during your practice sessions so it becomes second nature). It's not important that you use the MIKE template --yourplan should work foryou. But the ideas covered in this post should probably be part of it.\n\nIf you have any comments, let me know onlinkedinorX.",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/problem-solving-bctci-style?category=dsa",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Heapify Analysis Without Math",
      "content": "Heapify Analysis Without Math\n\nI'm writing about heaps for Beyond Cracking the Coding Interview (beyondctci.com), and the most technical part is the analysis ofheapify. It's easy to show that it takesO(n log n)time, wherenis the number of nodes in the heap, but it's not straightforward to show that this is not tight and the method actually takesO(n). time.\n\nEvery proof I have found online involves a summation over the levels of the heap that ends up looking something like the one inWikipedia heap page:\n\nwhich is more math than I want to put in this book (the bulk of the audience consists of people trying to land a SWE job, not math enthusiasts).\n\nBelow is the proof \"without complicated math\" I came up with that heapify takesO(n)time. If you are familiar with the classic proof, let me know if you find it easier - I might use it for the book. Also, please let me know if you've seen someone else proving it in a similar way.\n\nIf you already know what heapify is, you can jump directly to theProof.\n\nHeap Recap\n\nHeaps are binary trees with two special properties:\n\nThey arecompletebinary trees: all the levels except the last one have the maximum number of nodes; the last level may not be full, but all the nodes are aligned to the left. (In particular, this implies that heaps have logarithmic height, which is key to the big O analysis.)Theheap property:every node is smaller than its children (this is assuming a min-heap - it would be the opposite for a max-heap).\n\nThey arecompletebinary trees: all the levels except the last one have the maximum number of nodes; the last level may not be full, but all the nodes are aligned to the left. (In particular, this implies that heaps have logarithmic height, which is key to the big O analysis.)\n\nTheheap property:every node is smaller than its children (this is assuming a min-heap - it would be the opposite for a max-heap).\n\nI will focus on the heapify operation and its analysis, but if you want to learn heaps from scratch, the Algorithms with Attitude Youtube channel has agreat videoon it. He also covered theclassic linear-time prooffor heapify, if you want to compare it to mine.\n\nIn any case, I left a full Python heap implementation at the bottom of this post.\n\nWhat's Heapify?\n\nHeapify (invented byRobert W. Floyd) converts a binary tree which is already complete, but may not have the heap property, into a proper heap.\n\nHeapify uses the \"bubble-down\" procedure, which starts at a node that may not satisfy the heap property, and recursively swaps it with the smallest of its two children until the heap property is restored:\n\nHeapify works by \"bubbling down\" every non-leaf (internal) node, from bottom to top:\n\nThis figure shows the heapify steps for a min-heap. The first tree is the initial state, which doesn't yet have the min-heap property. Leaves are already at the bottom, so bubbling them down has no effect. The next 3 trees show the evolution after bubbling down the two nodes at depth 1 and then the node at depth 0.\n\nIn the array-based heap implementation,heapify()looks like this:\n\nThe reason why we start bubbling down from the middle of the heap is that, in a complete tree, at least half the nodes are leaves, and we don't need to bubble those down.\n\nHere, we won't prove that itworks, only that its analysis isO(n).\n\nProof\n\nI'll start with a definition and a fact we'll use later:\n\nAperfectbinary tree is a complete tree where the last level is full:\n\nFact 1: In a perfect tree, the number of leaves is 1 more than the number of internal nodes.\n\nFor instance:\n\nFact 1 is true because the number of nodes at each level is a power of 2, so:\n\nthe number of leaves is a power of 2, andthe number of internal nodes is the sum of all the previous powers of 2.\n\nthe number of leaves is a power of 2, and\n\nthe number of internal nodes is the sum of all the previous powers of 2.\n\nThe sum of the first few powers of 2 add up to one less than the next power of 2. You can see that if you line them up like this:\n\nIt's a bit likeZeno's paradox, where each power of 2 in the sum halves the remaining distance, but never quite gets to 64.\n\nWith that out of the way, back to heapify:\n\nIn the worst case, each node will get bubbled down all the way to a leaf. Thus, each node needs to move downO(log n)levels, so one might reasonably expect heapify to takeO(n log n)time. This is correct in the 'upper bound' sense, but not tight: the total time is actuallyO(n). The intuition for why that is the case is that most nodes are in the deeper levels of the tree, where they don't need to travel a lot to get to the bottom.\n\nWe'll actually prove astrongerclaim:\n\nMain Claim: If you heapify a perfect tree, the number of 'bubble-down' swaps is smaller thann, the number of nodes.\n\nWe'll assume the worst case, in which every node is bubbled down to a leaf position.If the claim is true and heapify does<nswaps, then it takesO(n)time, since most bubble-down iterations involve a swap.We make the claim about perfect trees (rather than complete trees in general) to keep things simple.\n\nWe'll assume the worst case, in which every node is bubbled down to a leaf position.\n\nIf the claim is true and heapify does<nswaps, then it takesO(n)time, since most bubble-down iterations involve a swap.\n\nWe make the claim about perfect trees (rather than complete trees in general) to keep things simple.\n\nThe proof goes like this:\n\nWhen the height is 1, the claim is true because the tree has 1 node and requires 0 swaps to heapify.Every time we add a new level,the number of nodes increases more than the number of swaps(by one, to be exact).\n\nWhen the height is 1, the claim is true because the tree has 1 node and requires 0 swaps to heapify.\n\nEvery time we add a new level,the number of nodes increases more than the number of swaps(by one, to be exact).\n\nIf we can prove (2), together with (1) it implies the Main Claim because, as we add levels, the number of swaps can never catch up to the number of nodes.\n\nTo see why (2) is true, imagine we add a new level to a perfect tree:\n\nEvery preexisting node needs to bubble down one additional level, while the new leaves do not need to do to be bubbled down. Thus, the number of swaps increases by the number of preexisting nodes.\nBut, another way of saying Fact 1 is that if you take a perfect tree and add a new level, the number of new/leaf nodes is 1 more than the number of preexisting/internal nodes.\n\nAnd that's the proof!\n\nAppendix: The Proof in Numbers\n\nWe can compute the actual number of swaps needed, in the worst case, for concrete heights:\n\nWe can see that the number of swaps never catches up with the root, per the Main Claim.\n\nWe can also show how the number of nodes grows vs the number of swaps:\n\nAppendix: Full heap implementation",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/heapify-analysis?category=dsa",
      "author": "",
      "user_id": ""
    },
    {
      "title": "In Defense of Coding Interviews",
      "content": "In Defense of Coding Interviews\n\nThere is already a lot of discourse about everything wrong with coding interviews. Indeed, one of the first chapters inBeyond Cracking the Coding InterviewisWhat's Broken About Coding Interviews?(it's one of the sneak peek free chapters inbctci.co/free-chapters).\n\nHere, I want tocollect all the arguments for the contrary view: that there are no clear better alternatives to coding interviews at Big Tech companies.\n\nDisclaimers:\n\nI am one of the authors ofBeyond Cracking the Coding Interview, a prep book for coding interviews. Thus, I am vested in coding interviews not going away.I love leetcoding and specialized in DS&A for my PhD, so I haven't personally experienced thedreadthat most people feel grinding it.I've been an interviewer at Google in the past, but I'm not currently working for Big Tech, and I don't have any inside knowledge. This is just my assessment.This post is only about Big Tech. I don't think coding interviews are a good idea for startups.This post contains \"Strong Opinions, Weakly Held\". I believe everything here, but I'm very receptive to pushback and opposing data.\n\nI am one of the authors ofBeyond Cracking the Coding Interview, a prep book for coding interviews. Thus, I am vested in coding interviews not going away.\n\nI love leetcoding and specialized in DS&A for my PhD, so I haven't personally experienced thedreadthat most people feel grinding it.\n\nI've been an interviewer at Google in the past, but I'm not currently working for Big Tech, and I don't have any inside knowledge. This is just my assessment.\n\nThis post is only about Big Tech. I don't think coding interviews are a good idea for startups.\n\nThis post contains \"Strong Opinions, Weakly Held\". I believe everything here, but I'm very receptive to pushback and opposing data.\n\nThe rationale for coding interviews\n\nI think Big Tech companies understand that being cracked at DS&A is not really necessary to be a good SWE. I don't think coding interviews are about that at all.\n\nImagine you are a Big Tech company, like Google. You receive a massive stream of applications, and you have to trim that down to a still large number of hires. Your hiring system needs to bescalable:\n\nyou need to quickly train many interviewersyou need a way to evaluate candidates that minimizes interviewer bias (notyourbias, or a specific person's bias, but all the biases of a large, heterogeneous group)\n\nyou need to quickly train many interviewers\n\nyou need a way to evaluate candidates that minimizes interviewer bias (notyourbias, or a specific person's bias, but all the biases of a large, heterogeneous group)\n\nSo, the first thing you do to scale--in true engineering fashion--is decoupling hiring and team matching. But that means you cannot hire for specific tech or domain experience: You don't know in what team candidates will end up, and your teams use a bunch of different languages and tech stacks (and a lot of it is internal anyway, so you definitely can't hire for that).\n\nSo, you need acompetence assessmentthat is independent of any particulars about the job, much like the role the SAT plays for college admissions. How do you do that?\n\nIf you are a Big Tech company, what you actually want is candidates who can take any complex software system (that's not part of the candidate's previous expertise) and answer hard questions about it, like what's the best way to add a feature, how to optimize it, or how it should be refactored. In other words, the competence you want to assess is general problem-solving skills, and that's what coding interviews are designed for: you are given a tough problem that you haveideallynever seen before (more on this later), and asked to showcase your thought process on how you approach it. When working as intended, I believe it gives moresignalabout your problem-solving skills and is easier to evaluate impartially than other popular interview formats, like talking about previous experience or take-home assignments. And there's an impartial way to evaluate them, by looking at the optimality of the solution.\n\nYes, there's a lot more to being a SWE than problem-solving skills--and that's why Google also does system design and behavioral interviews, but you still want to hire for this trait.\n\nThe two crucial flaws: memorization and cheating\n\nHopefully, the rationale above covered one of the most common criticisms of coding interviews: that they do not reflect the day-to-day work of an engineer. Instead, I want to focus on whatIthink are the two biggest issues with coding interviews:\n\nMemorizing an absurd amount of leetcode problems gives you an edge.This is the classic reason why peoplehatecoding interviews with a passion. It has led to an \"arms race\" where candidates have to memorize more and more problems to improve their odds, and interviewers keep asking about more niche topics. At the extreme, coding interviews end up feeling like a lottery, and candidates find prep a soul-sucking waste of time.Cheating has become easy with AI.This is a newer issue that's becoming more prevalent due to the fact that LLMs are pretty good at leetcoding. In real time, a cheater can feed the problem statement to an LLM (without obvious tales like \"select all\"), get a solution, and even a script for what to say.\n\nMemorizing an absurd amount of leetcode problems gives you an edge.This is the classic reason why peoplehatecoding interviews with a passion. It has led to an \"arms race\" where candidates have to memorize more and more problems to improve their odds, and interviewers keep asking about more niche topics. At the extreme, coding interviews end up feeling like a lottery, and candidates find prep a soul-sucking waste of time.\n\nMemorizing an absurd amount of leetcode problems gives you an edge.This is the classic reason why peoplehatecoding interviews with a passion. It has led to an \"arms race\" where candidates have to memorize more and more problems to improve their odds, and interviewers keep asking about more niche topics. At the extreme, coding interviews end up feeling like a lottery, and candidates find prep a soul-sucking waste of time.\n\nCheating has become easy with AI.This is a newer issue that's becoming more prevalent due to the fact that LLMs are pretty good at leetcoding. In real time, a cheater can feed the problem statement to an LLM (without obvious tales like \"select all\"), get a solution, and even a script for what to say.\n\nCheating has become easy with AI.This is a newer issue that's becoming more prevalent due to the fact that LLMs are pretty good at leetcoding. In real time, a cheater can feed the problem statement to an LLM (without obvious tales like \"select all\"), get a solution, and even a script for what to say.\n\nFrom the company's side, Issue (1) is not much of an issue. It definitely undermines the \"problem-solving\" part of the interview if a candidate is just recalling the question, but, statistically, if they do enough rounds, it's unlikely to happen every round. Some people (not me) also argue that the memorization is even good for the companies because it rewards hard work and dedication.\n\nFor what it's worth, one thing we hoped to change about the interview prep\ndiscourse with BCtCI is that candidates should focus on improving their\nproblem-solving skills rather than memorizing. See, for instance, how weteach binary searchor how weapproach hard\nproblems. But yes,\ngrinding is still necessary.\n\nIssue (1) also means that they'll lose a big chunk of candidates who are great SWEs but won't put up with grinding leetcode or that simply don't perform well under pressure (and, from personal experience, many great developers fall in this group). This sucks from the candidate's perspective, but if you are Google, you receive an overwhelming amount of applications from qualified candidates, so you are more OK with rejecting good candidates than accepting bad ones.\n\nIssue (2), on the other hand, has the potential to completely ruin coding interviews from the company's side. I'm seeing a quick rise of stories from frustrated interviewers who interviewed or even hired cheaters who could then not do the job (ExhibitA).\n\nI expect to see some kind of systematic response to this from Big Tech, but it's not clear what as of April 2025.This articleincludes some internal comments from Google execs:\n\n[Brian] Ong [Google’s vice president of recruiting] said candidates and Google employees have said they prefer virtual job interviews because scheduling a video call is easier than finding a time to meet in available conference rooms. The virtual interview process is about two weeks faster, he added.\n\nHe said interviewers are instructed to probe candidates on their answers as a way to decipher whether they actually know what they’re talking about.\n\n“We definitely have more work to do to integrate how AI is now more prevalent in the interview process,” said Ong. He said his recruiting organization is working with Google’s software engineer steering committee to figure out how the company can refine its interviewing process.\n\n“Given we all work hybrid, I think it’s worth thinking about some fraction of the interviews being in person,” Pichai responded. “I think it’ll help both the candidates understand Google’s culture and I think it’s good for both sides.”\n\nI thought going back to in-person interviews would be ano-brainerfor a company like Google, but my reading of these comments is that they don't seem too bothered for now. ~shrug~\n\nDisclaimer: I haven't worked for a Big Tech company since before AI cheating\nwent viral, so I don't have internal insight into what people in charge of\nhiring are actually thinking.\n\nTwo related arguments that I don't subscribe to are (1) that leetcode-style interviews are no longer relevant because AI can solve them, and (2) that LLMs should be allowed during coding interviews because they are allowed on the job. The fact that AI can solve coding questions doesn't change that it still gives you the important signal that you want from humans: algorithmic thinking and general problem-solving skills. We just need humans to not cheat.\n\nI'll share my thoughts on how to improve coding interviews to address these issues. First, let's see why I think the alternatives are not better.\n\nThe problems with the alternatives\n\nTake-home assignments\n\nTake-home assignments are even more subject to cheating, so that can't be the answer to cheating. Never mind LLMs, you don't even know who did the assignment. But take-home assignments have other flaws:\n\nThey create an asymmetry between company and candidate, where the company asks for a ton of work from the candidate without putting any effort in. \"Oh, we have way too many candidates we need to filter down to a shortlist? Send all of them a complex task to do over the weekend.\" I prefer a model where both company and candidate have to put in time. I'm more OK with take-home assignments as the final stage of the process.They favor people who are unemployed and/or have a lot of free time to polish the assignment.\n\nThey create an asymmetry between company and candidate, where the company asks for a ton of work from the candidate without putting any effort in. \"Oh, we have way too many candidates we need to filter down to a shortlist? Send all of them a complex task to do over the weekend.\" I prefer a model where both company and candidate have to put in time. I'm more OK with take-home assignments as the final stage of the process.\n\nThey favor people who are unemployed and/or have a lot of free time to polish the assignment.\n\nPrevious experience\n\nI find this too subjective to give signal about problem-solving skills, and it's more about being a good \"salesperson\". I also think it's more subject to bias:people with a similar background as yours are probably more likely to have similar interests, and thus you may find their side-projects more interesting.\n\nTrial periods\n\nThis makes sense to me in smaller companies, where you find a candidate with the perfect profile for the task at hand. It doesn't scale to Big Tech companies.\n\nOther alternatives\n\nIf there are other alternatives that fulfill the same purpose as coding interviews but don't suffer from the same issues, I'd love to hear about them.\n\nOne idea I liked is going through a code review during the interview, but it's not clear that (1) it offers as much signal about problem-solving skills, and (2) it is easy to evaluate impartially.\n\nHow to improve coding interviews\n\nRight now, FAANG interviewers focus too much on \"Did they solve the question or not?\" That's because they don't get much training on how to interview well (if at all), and it's the most straightforward way to pass on a hire/no hire recommendation to the hiring committee. This leads to many interviewers just pasting the prompt in and mostly sitting in silence. This is the ideal scenario for cheaters.\n\nThe obvious things\n\nThere are obvious ways to improve this situation:\n\nIn-person interviews. These have other benefits, like allowing the candidate to get a better sense of the company culture.Not using publicly available questions, and actively scanning for leaks.Cheating detection software (privacy is a concern here -- would it be too crazy for a company to ship a laptop to the candidate just for the interview?).Stop asking questions that require knowing some niche trick that a normal person wouldn't be able to figure out on the spot. Those reinforce a focus on memorization.\n\nIn-person interviews. These have other benefits, like allowing the candidate to get a better sense of the company culture.\n\nNot using publicly available questions, and actively scanning for leaks.\n\nCheating detection software (privacy is a concern here -- would it be too crazy for a company to ship a laptop to the candidate just for the interview?).\n\nStop asking questions that require knowing some niche trick that a normal person wouldn't be able to figure out on the spot. Those reinforce a focus on memorization.\n\nLow effort ways of countering cheating\n\nI also think that measures designed to throw LLMs off could be effective (at least in the short term) and require minimal effort, such as:\n\nStating the question, or part of it, instead of writing the whole thing downIncluding a 'decoy' question and telling the candidate, \"Ignore that line, it is part of our anti-cheating measures.\"\n\nStating the question, or part of it, instead of writing the whole thing down\n\nIncluding a 'decoy' question and telling the candidate, \"Ignore that line, it is part of our anti-cheating measures.\"\n\nSeeLinkedIn discussion.\n\nA fundamental tradeoff\n\nPerhaps the most effective way to counter both memorization and cheating is to make coding interviews more open ended and conversational. To use a chess analogy, a cheater may make a great move, but if you ask them to explain why they did it, they may not be able to.\n\nThe interviewer can use a coding question as a launching point, but then drill down on technical topics as they come up. So, e.g., if a candidate chooses to use a heap, the interviewer could go into:\n\nWhat made you think of using a heap? What properties are important for this problem?What are the tradeoffs of using a heap vs binary search trees?How would you go about implementing a heap that supports arbitrary priorities?Why isheapify faster than inserting one by one?\n\nWhat made you think of using a heap? What properties are important for this problem?\n\nWhat are the tradeoffs of using a heap vs binary search trees?\n\nHow would you go about implementing a heap that supports arbitrary priorities?\n\nWhy isheapify faster than inserting one by one?\n\nIf interviewers did that, it wouldn't even be necessary to ask tricky questions. They could evenaskFibonacci.\n\nThe problem is that, the more open ended the interview is, the more difficult\nit is to evaluate candidates systematically. To start, you'd need better\ninterviewers and better interviewer training. However, it seems to me that\nthere isa fundamental tradeoff between how objective the evaluation is and\nhowgameablethe interview is by memorizing or cheating.\n\nI don't have a good solution to this--I would love to hear yours.\n\nMore good things about coding interviews\n\nOnly one thing to study\n\nAn underrated upside of leetcode interviews is that you only need to study one thing for all the big companies. I feel like if every company asked different things, interview prep time would decrease for any specific company but increase overall.\n\nIn fact, a likely outcome of the push for fewer leetcode-style interviews is an even worse compromise: coding interviews won't completely go away, so you'll still need to grind leetcode, but you'll also have to prep a bunch of specialized stuff for each company on top of that.\n\nSeeLinkedIn discussion.\n\nThey are not based on pedigree\n\nCoding interviews act as a form of standardized testing, similar to the role of SAT for college admissions in the US. And, much like the SAT allows high-school students from all backgrounds to attend top colleges, coding interviews allow candidates from all backgrounds to get at the top companies. The leetcode grind is the same for everyone.\n\nIf we kill coding interviews without a good alternative, it seems inevitable that Big Tech companies will give more weight to resume and referrals. We all agree that's a bad thing.\n\nFinal thoughts\n\nThe best question we got in ourReddit AMAfor BCtCI was whetherwe'd use coding interviews ourselves if we were in charge of hiring. You can see Gayle's, Mike's (mikemroczka.com), and my answers. We all saidnoin its current form, but yes with caveats/improvements.\n\nMy favorite answer was Mike's. He's less of a proponent of leetcode-style interviews than I am, but I think he strikes a thoughtful balance between DS&A and practical stuff:\n\nBest question so far. Yes, I would ask DS&A questions still, but not exclusively and not difficult ones. Many startups shouldn't ask them though, because most people are bad at discerning what a reasonable question is.\n\nI would do 4-5 rounds of interviews because less than that is hard to be significant, but more than that and you're wasting too much of a candidate's time (Netflix has a whopping 8 rounds!!). For a senior engineer role, I'd do something like this.\n\nRound 1: An online DS&A assessment to filter out people that can't do the simple things (easy & very simple medium questions only, not hard)\n\nRound 2: Live interview of DS&A (simple medium, not hard. essentially just making sure you didn't cheat on the previous round by asking you to explain your answers and code something new from scratch)\n\nRound 3: System design (no need for perfect answers, but I'd ask an uncommon question to ensure it was something they hadn't memorized)\n\nRound 4: Behavioral, with a focus on cross-team impact. This would just be a simple pass/fail and just a vibe check. It might also be skipped if the prior two rounds had good signal for emotional intelligence\n\nRound 5: Remote logging into a server and working on an actual bug that was fixed in our codebase before. There would be no time limit, but time on the server would be logged to weed people out who needed days to complete a simple task.\n\nThis ends up testing a little bit of theory, practical knowledge, emotional intelligence, and the generalized SWE skillset.\n\nFull disclosure. This is my answer. Not the answer of every author. Again, I'd stress that the average startup wouldn't benefit from DS&A and shouldn't be asking them\n\nWant to leave a comment? You can post under thelinkedin postor theX post.",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/in-defense-of-coding-interviews?category=dsa",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Breaking Down Dynamic Programming",
      "content": "Breaking Down Dynamic Programming\n\nNote: the approach in this guide later became the foundation for the dynamic programming chapter inBeyond Cracking the Coding Interview.\n\nIntroduction\n\nWhen I was a TA for \"Algorithm Design and Analysis\", the students struggled with dynamic programming. To simplify/demystify it, I tried to break it down into a logical sequence of steps, each of which should not feel too intimidating on its own. This is explained in detail here. To complement the explanations, there are links to problems onleetcode.com, in case the reader wants to practice. The code snippets are in Python, but Leetcode accepts most popular languages.\n\nOverview: Recursive vs Iterative DP\n\nIn short, dynamic programming (DP) is a technique for problems that seem hard to solve as a whole, but become easy if we know the solution to smaller subproblems. More technically, we can use it in problems where the (value of the) solution can be expressed as an equation which is a function of the input, and is expressed in terms of itself with smaller inputs. This is called arecurrence equation. The classic example is the Fibonacci recurrence:Fib(n) = Fib(n-1) + Fib(n-2).\n\nhttps://leetcode.com/problems/fibonacci-number/\n\nhttps://leetcode.com/problems/fibonacci-number/\n\nA recurrence equation can be translated into code:\n\nHowever, the above function has an exponential runtime. A recursive function becomes exponential when it is possible to reach the same subcall through different execution paths. In the Fibonacci case, we have the following nested calls:Fib(n) -> Fib(n-1) -> Fib(n-2), andFib(n) -> Fib(n-2). SinceFib(n-2)is called twice all the work from this call is duplicated, which in turn means that subcalls made fromFib(n-2)will start to duplicate and grow exponentially.\n\nDynamic programming is simply a workaround to this duplication issue. Instead of recomputing the solutions of the subproblems, we store them and then we recall them as needed. This guarantees that each subproblem is computed only once.\n\nThere are two main approaches for DP.\n\nRecursive / Top-down DP\n\nWe start with the code which is a literal translation of the recurrence equation, but then we add a dictionary / hash table to store results.\n\nThere are three changes in the code above:\n\ndeclaring our dictionary for storing results,memooutside the recursive function (memo comes \"memorization\" or \"memoization\", a name used in the literature).before computing the result, we check if the solution has already been computed. This check can be done before or after the base case.before returning, we save the result in thememotable.\n\ndeclaring our dictionary for storing results,memooutside the recursive function (memo comes \"memorization\" or \"memoization\", a name used in the literature).\n\nbefore computing the result, we check if the solution has already been computed. This check can be done before or after the base case.\n\nbefore returning, we save the result in thememotable.\n\nUsing a memoization table in this way solves the inefficiency (we will go deeper into the analysis part later).\n\nIterative / Bottom-up DP\n\nInstead of starting from the largest input and recursively reaching smaller subproblems, we can directly compute the subproblems from smallest to largest. This way, we already have the solutions to the subproblems when we need them. For this approach, we change the dictionary for an array/vector, and we change recursive calls for a for loop.\n\nMost problems can be solved with both recursive and iterative DP. Here are some considerations for how to choose:\n\nRecursive DP matches the recurrence equation more directly, so it can be easier to implement.Both have the same runtime complexity, but the recursive version will generally have larger constant factors due to all the recursive function calling and due to using a hash table instead of an array.Iterative DP often allows for an optimization to reduce the space complexity (discussed later).\n\nRecursive DP matches the recurrence equation more directly, so it can be easier to implement.\n\nBoth have the same runtime complexity, but the recursive version will generally have larger constant factors due to all the recursive function calling and due to using a hash table instead of an array.\n\nIterative DP often allows for an optimization to reduce the space complexity (discussed later).\n\nRecursive DP in 5 Steps\n\nChoose what your subproblems are.Find the recurrence equation.Translate the recurrence equation into recursive code.Add memoization.(Optional) Reconstruct the solution.\n\nChoose what your subproblems are.\n\nFind the recurrence equation.\n\nTranslate the recurrence equation into recursive code.\n\nAdd memoization.\n\n(Optional) Reconstruct the solution.\n\nWe already saw steps 1–4 with the Fibonacci example. Now, we will walk through all the steps in more detail using a more complicated example, thelongest common subsequence problem:\n\nGiven two stringss1ands2, find the length of the longest string which is a subsequence of boths1ands2. A stringtis asubsequenceof a stringsif every char intappearsin orderins, but arenot necessarily contiguous. For example,abcis a subsequence ofaxbyz, butbais not (do not confuse subsequence with substring or subset).\n\nhttps://leetcode.com/problems/longest-common-subsequence/\n\nhttps://leetcode.com/problems/longest-common-subsequence/\n\nStep 1: choose our subproblems. This varies from problem to problem, but when the input to the problem is a string, a natural way to obtain smaller problems is to look at shorter strings. Here we can use as a subproblem aprefixofs1and a prefix ofs2.\n\nSome notation: letnbe the length ofs1andmthe length ofs2. LetLCS(i,j)be the solution for the LCS problem for the prefix ofs1of lengthn(s1[0..i-1]) and the prefix ofs2of lengthm(s2[0..j-1]). Then, our goal is to findLCS(n, m).\n\nStep 2: find the recurrence equation. Now we need to come up with an expression forLCS(i,j)as a function ofLCSwith smaller indices (as well as a base case). This is the hardest step of DP, and often it is here that we realize that we chose bad subproblems in Step 1. If that happens, hopefully we will discover some hint for what our subproblems should be.\n\nIn order to derive the recurrence equation for LCS, we need the following observation: if the two strings end with the same characterc, then, to maximize the length of the subsequence, it is \"safe\" to addcto the subsequence. In contrast, if both strings end with different characters, thenat leastone of them cannot appear in the subsequence. The complication is that we do not know which one. Thus, instead of guessing, we can simply consider both options.\n\nThis observation yields the recurrence equation (excluding base case):\n\nThis step is not intuitive at first, and requires practice. After having done a few problems, one starts to recognize the typical patterns in DP. For instance, usingmaxamong a set of options of which we do not know which one is the best is easily the most common pattern in DP.\n\nStep 3. Translate the recurrence equation into recursive code. This step is a very simple programming task. Pay attention to the base case.\n\nIf we draw the few first steps of the call graph, we will see that the same subproblem is reached twice. Thus, call graph blows up, leading to an exponential runtime.\n\nStep 4. Add memo table. This step should be automatic: one does not even need to understand the previous code in order to add the memo table.\n\nThe base case corresponds to when one of the strings is empty. The LCS of an empty string with another string is clearly an empty string.\n\nIncidentally, if we flip the check on the memo table, the code becomes a bit more streamlined (fewer lines + merging the two returns). I prefer this form (it does the same):\n\nWe have eliminated the exponential blowup. In general, DP algorithms can be analyzed as follows: # of distinct subproblems times time per subproblem excluding recursive calls. For LCS, we getO(nm)*O(1)=O(nm).\n\nStep 5. Reconstruct the solution.\n\nWe used DP to compute the length of the LCS. What if we want to find the LCS itself?\nA naive way to do it would be to store the entire result of each subproblem in the memoization table instead of just its length. While this works, it is clear that it will require a lot of memory to storeO(nm)strings of lengthO(min(n,m))each. We can do better.\n\nStep 5, \"Reconstruct the solution\", is how to reuse the table that we constructed in Step 4 to find the actual solution instead of just its length. I said that this step is optional because sometimes we just need thevalueof the solution, so there is no reconstruction needed.\n\nThe good news is that we do not need to modify the code that we already wrote in Step 4. The reconstruction is a separate step that comes after. In addition, the reconstruction step is very similar (follows the same set of cases) as the step of building the memo table. In short, we use the memo table as an \"oracle\" to guide us in our choices. Based on the values in the memo table, we know which option is better, so we know how to reconstruct the solution.\n\nIn the code above, first we runLCS(n,m)to fill the memo table. Then, we use it in the reconstruction. The conditionmemo[(i-1,j)] >= memo[(i,j-1)]tells us that we can obtain a longer or equal LCS by discarding a char froms1instead of froms2.\n\nNote that there is a single recursive call in the reconstruction function, so the complexity is justO(n+m).\n\nIterative DP in 6 Steps\n\nChoose what your subproblems are.Find the recurrence equation.Design the memo table.Fill the memo table.(Optional) Reconstruct the solution.(Optional) Space optimization.\n\nChoose what your subproblems are.\n\nFind the recurrence equation.\n\nDesign the memo table.\n\nFill the memo table.\n\n(Optional) Reconstruct the solution.\n\n(Optional) Space optimization.\n\nThe new/different steps are highlighted. Step 3. is to design the layout of the table/matrix where we are going to store the subproblem solutions. There is no coding in this step. By \"design\", I mean making the following choices:\n\nwhat are the dimensions of the table, and what does each index mean. Generally speaking, the table should have one dimension for each parameter of the recurrence equation. In the case of LCS, it will be a 2-dimensional table.where are the base cases.where is the cell with the final solution.what is the ``dependence relationship'' between cells (which cells do you need in order to compute each cell).which cells do not need to be filled (in the case of LCS, we need them all).\n\nwhat are the dimensions of the table, and what does each index mean. Generally speaking, the table should have one dimension for each parameter of the recurrence equation. In the case of LCS, it will be a 2-dimensional table.\n\nwhere are the base cases.\n\nwhere is the cell with the final solution.\n\nwhat is the ``dependence relationship'' between cells (which cells do you need in order to compute each cell).\n\nwhich cells do not need to be filled (in the case of LCS, we need them all).\n\nHere is how I would lay out the table for LCS (you can find a different layout in the problems below):\n\nNext (Step 4), we fill the memo table with a nested for loop. If the layout is good, this should be easy. Before the main loop, we fill the base case entries. Then, we must make sure to iterate through the table in an order that respects the dependencies between cells. In the case of LCS, we can iterate both by rows or by columns.\n\nWe obtain the following algorithm:\n\nIn the code above, the base case entries are filled implicitly when we initialize the table with zeros everywhere.\n\nIf we need to reconstruct the solution, we can do it in the same way as for the recursive DP. The only difference is that memo is a matrix instead of dictionary.\n\nSpace optimization\n\nClearly, the space complexity of iterative DP is the size of the DP table. Often, we can do better. The idea is to only store the already-computed table entries that we will use to compute future entries. For instance, in the case of Fibonacci, we do not need to create an entire array -- keeping the last two numbers suffice. In the case of a 2-dimensional DP table, if we are filling the DP table by rows and each cell only depends on the previous row, we only need to keep the last row (and similarly if we iterated by columns). Here is the final version for LCS where we improve the space complexity fromO(nm)toO(n+m):\n\nNote: this optimization is incompatible with reconstructing the solution, because that uses the entire table as an \"oracle\".\n\nDP Patterns\n\nHere are some typical patterns:\n\nFor Step 1. The subproblems.\n\nIf the input is a string or a list, the subproblems are usually prefixes or substrings/sublists, which can be specified as a pair of indices.If the input is a number, the subproblems are usually smaller numbers.Generally speaking, the number of subproblems will be linear or quadratic on the input size.\n\nIf the input is a string or a list, the subproblems are usually prefixes or substrings/sublists, which can be specified as a pair of indices.\n\nIf the input is a number, the subproblems are usually smaller numbers.\n\nGenerally speaking, the number of subproblems will be linear or quadratic on the input size.\n\nFor Step 2. The recurrence equation.\n\nOften, we usemaxorminto choose between options, or sum to aggregate subsolutions.The number of subproblems is most often constant, but sometimes it is linear on the subproblem size. In the latter case, we use an inner loop to aggregate/choose the best solution.Sometimes, the recurrence equation is not exactly for the original problem, but for a related but more constrained problem. See an example below, \"Longest Increasing Subsequence\".\n\nOften, we usemaxorminto choose between options, or sum to aggregate subsolutions.\n\nThe number of subproblems is most often constant, but sometimes it is linear on the subproblem size. In the latter case, we use an inner loop to aggregate/choose the best solution.\n\nSometimes, the recurrence equation is not exactly for the original problem, but for a related but more constrained problem. See an example below, \"Longest Increasing Subsequence\".\n\nPractice Problems\n\nHere are some practice problems showcasing the patterns mentioned above. Follow the Leetcode links for the statements and example inputs. I jump directly to the solutions. I'd recommend trying to solve the problems before checking them.\n\nhttps://leetcode.com/problems/palindromic-substrings/\n\nhttps://leetcode.com/problems/palindromic-substrings/\n\nHere, the goal is to count the number of substrings of a stringswhich are palindromic. There is a trivialO(n³)time solution without DP:\n\nWe can improve this toO(n²)with DP. The subproblems are all the substrings ofs. LetPal(i, j)be true iffs[i..j]is a palindrome. We have the following recurrence equation (excluding base cases):\n\nBased on this recurrence equation, we can design the following DP table:\n\nThis type of \"diagonal\" DP tables are very common when the subproblems are substrings/sublists. In this case, the base cases are substrings of length 1 or 2. The goal isPal(0,n-1).\n\nGiven the dependency, the table can be filled by rows (starting from the last row), by columns (starting each column from the bottom), or by diagonals (i.e., from shortest to longest substrings). In the code below, I illustrate how to fill the table by diagonals.\n\nhttps://leetcode.com/problems/minimum-path-sum/\n\nhttps://leetcode.com/problems/minimum-path-sum/\n\nHere, a subproblem can be a grid with reduced width and height. LetT[i][j]be the cheapest cost to reach cell(i,j). The goal is to findT[n-1][m-1], wherenandmare the dimensions of the grid. The base case is when eitheriorjare zero, in which case we do not have any choices for how to get there. In the general case, we have the recurrence equationT[i][j] = grid[i][j] + min(T[i-1][j], T[i][j-1]): to get to(i,j), we first need to get to either(i-1,j)or to(i,j-1). We useminto choose the best of the two. We convert this into an iterative solution:\n\nhttps://leetcode.com/problems/unique-paths-ii/\n\nhttps://leetcode.com/problems/unique-paths-ii/\n\nThis is similar to the previous problem, but we need to accumulate the solutions to the subproblems, instead of choosing between them. Problems aboutcountingsolutions can often be solved with DP.\n\nhttps://leetcode.com/problems/longest-increasing-subsequence/\n\nhttps://leetcode.com/problems/longest-increasing-subsequence/\n\nThis problem will illustrate a new trick: if you cannot find a recurrence equation for the original problem, try to find one for a more restricted version of the problem which nevertheless you enough information to compute the original problem.\n\nHere, the input is a listLof numbers, and we need to find the length of the longest increasing subsequence (a subsequence does not need to be contiguous). Again, the subproblems correspond to prefixes of the list.\nLetLIS(i)be the solution for the prefix of lengthi(L[0..i]). The goal is to findLIS(n-1), wherenis the length ofL.\nHowever, it is not easy to give a recurrence equation forLIS(i)as a function of smaller prefixes. In particular,the following is wrong(I will let the reader think why):\n\nThus, we actually give a recurrence equation for a slightly modified type of subproblems: letLIS2(i)be the length of the LISending at index i. This constraint makes it easier to give a recurrence equation:\n\nIn short, since we know that the LIS ends atL[i], we consider all candidate predecessors, which are the numbers smaller than it, and get the best one by usingmax. Crucially, this recurrence works forLIS2(i)but not forLIS(i).\nHere is a full solution:\n\nAt the end, we do not simply returnT[n-1]becauseTis the table forLCS2, notLCS. We returnmax(T)because the LCS must endsomewhere, soLCS(n-1) = LCS2(j)for somej < n.\n\nNote that the runtime isO(n²)even though the table has linear size. This is because we take linear time per subproblem.\n\nhttps://leetcode.com/problems/number-of-longest-increasing-subsequence/\n\nhttps://leetcode.com/problems/number-of-longest-increasing-subsequence/\n\nA harder version of the previous problem. A similar approach works. First solve the LIS problem as before, and then do a second pass to count the solutions.\n\nhttps://leetcode.com/problems/shortest-common-supersequence/\n\nhttps://leetcode.com/problems/shortest-common-supersequence/\n\nThis problem is similar to LCS, and it requires reconstruction.\n\nI should mention that noteveryproblem that can be solved with DP fits into the mold discussed above. Despite that, it should be a useful starting framework. Here are many more practice problems:\n\nhttps://leetcode.com/tag/dynamic-programming/\n\nhttps://leetcode.com/tag/dynamic-programming/",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/breaking-down-dynamic-programming?category=dsa",
      "author": "",
      "user_id": ""
    },
    {
      "title": "Sneak Peek: Beyond CTCI - First 7 Chapters",
      "content": "BEYOND\nCRACKING \nthe\nCODING INTERVIEW\nPass Tough CODING Interviews,\nget noticed, and Negotiate succesfully\nSNEAK PEEK\namazon.com/dp/195570600X\n0\nINTERVIEW CHECKLIST\nBEYOND CRACKING THE CODING INTERVIEW\n1\nSTUDY PLAN\nBEYOND CRACKING THE CODING INTERVIEW\nBOOSTERS\nCRACKING THE CODING INTERVIEW\n189 PROGRAMMING QUESTIONS AND SOLUTIONS\nCRACKING THE PM CAREER\nTHE SKILLS, FRAMEWORKS, AND PRACTICES TO BECOME A GREAT PRODUCT MANAGER\nCRACKING THE PM INTERVIEW\nHOW TO LAND A PRODUCT MANAGER JOB IN TECHNOLOGY\nCRACKING THE TECH CAREER\nINSIDER ADVICE ON LANDING A JOB AT GOOGLE, \nMICROSOFT, APPLE, OR ANY TOP TECH COMPANY\nBEYOND\nCRACKING\nthe\nCODING INTERVIEW\nGAYLE L. MCDOWELL\nMIKE MROCZKA\nALINE LERNER\nNIL MAMANO\nCareerCup, LLC\nPalo Alto, CA\nBEYOND CRACKING THE CODING INTERVIEW \nCopyright © 2025 by CareerCup. \nAll rights reserved. No part of this book may be reproduced in any form by any electronic or me-\nchanical means, including information storage and retrieval systems, without permission in writing \nfrom the author or publisher, except by a reviewer who may quote brief passages in a review.\nPublished by CareerCup, LLC, Palo Alto, CA. Compiled Jun 3, 2025.\nFor more information, or to enquire about bulk or university copies, contact \nsupport@careercup.com. \nPlease report bugs or issues at beyondctci.com.\n978-1955706001 (ISBN 13)\nTo my favorite coders, Davis and Tobin—\nGayle\nTo my dog, my wife, and our readers (and not necessarily in that order)—\nMike\nTo my two wonderful kids (or if I have more, then whichever two are the most wonderful)—\nAline\nAls meus pares— \nNil\nWHAT’S INSIDE\nI.  \n__init__()  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\nREADME . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\nHello World. Hello Reader. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  12\nCrash & Learn: Our Failed Interviews  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nII.  \nUgly Truths & Hidden Realities  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nCh 0.  \nWhy Job Searches Suck. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nCh 1.  \nA Brief History of Technical Interviews . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nCh 2.  \nWhat’s Broken About Coding Interviews . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nCh 3.  \nWhat Recruiters Won’t Tell You . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nCh 4.  \nWhat Interviewers Won’t Tell You. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\nCh 5.  \nMindset and the Numbers Game. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\nIII.  Job Searches, Start to Finish .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  42\nCh 6.  \nResumes .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  44\nCh 7.  \nGetting in the Door  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  53\nCh 8.  \nMechanics of the Interview Process .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  68\nCh 9.  \nManaging Your Job Search. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\nIV.  Offers & Negotiation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\nCh 10.  Components of the Offer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\nCh 11.  The What & Why of Negotiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .108\nCh 12.  Pre-Offer Negotiation Mistakes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .111\nCh 13.  Getting the Offer: Exactly What to Say . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .120\nCh 14.  How to Negotiate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .123\nV.  \nBehavioral Interviews .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  136\nCh 15.  When and How They Matter. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .138\nCh 16.  Content: What to Say  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .141\nCh 17.  Communication: How to Say It .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .154\nVI.  Principles of Coding Interviews . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\nTechnical README . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .168\nCh 18.  How to Practice. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .170\nCh 19.  How You Are Evaluated. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .180\nCh 20.  Anatomy of a Coding Interview  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .190\nCh 21.  Big O Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .206\nCh 22.  Boundary Thinking  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .231\nCh 23.  Trigger Thinking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .243\nCh 24.  Problem-Solving Boosters .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .249\nVII.  Catalog of Technical Topics.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  280\nCh 25.  Dynamic Arrays.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .282\nCh 26.  String Manipulation .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .288\nCh 27.  Two Pointers.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .294\nCh 28.  Grids & Matrices  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .312\nCh 29.  Binary Search .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .326\nCh 30.  Sets & Maps .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .345\nCh 31.  Sorting  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .361\nCh 32.  Stacks & Queues  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .379\nCh 33.  Recursion   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .392\nCh 34.  Linked Lists  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .412\nCh 35.  Trees.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .429\nCh 36.  Graphs  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .456\nCh 37.  Heaps .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .489\nCh 38.  Sliding Windows .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .509\nCh 39.  Backtracking   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .537\nCh 40.  Dynamic Programming.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .564\nCh 41.  Greedy Algorithms.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .584\nCh 42.  Topological Sort  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .598\nCh 43.  Prefix Sums   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .610\nVIII.  exit() .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  624\nAcknowledgments.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .626\nPost-Mortem Example Log   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .629\nReference Materials .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .630\nMy Notes & Reminders .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .639\nYou can access all of our online materials and \nbonus chapters here:\nTalk with the authors, get help if you're stuck, and \ngeek out with us on Discord. \nbctci.co\nbctci.co/discord\nGet $50 Off on Mock Interviews\nPractice anonymously on interviewing.io with FAANG interviewers: bctci.co/discount-X3A4\nI\nT E C H N I C A L  R E A D M E\nThis chart represents how we see the landscape of interview questions:\nFigure 1. Landscape of Interview Questions, for someone who prepares with BCtCI.\nOur goal with this book is twofold:\n1. Teach you the 80% most common topics and ideas used in interview problems. That's what Part VII: The \nCatalog of Technical Topics, is all about. The remaining 20% are niche topics.\nPRINCIPLES AND CATALOG ▸ TECHNICAL README \n169\n2. Teach you problem-solving strategies so you can ! gure out 80% of questions on your own, even if you \nhaven't seen the idea before. This is what the problem-solving boosters (Chapter 24: Problem-Solving \nBoosters, pg 249) are for. The remaining 20% of questions rely on tricks (ideas that are really hard to \ncome up with on your own if you haven't seen them before).1\nCombining these two, after going through the book, you should be able to tackle all but 20% * 20% = 4%\nof questions, which are those based on niche topics and requiring tricks. But if that happens, you've been \ntruly unlucky.\nTHE PRINCIPLES AND THE CATALOG\nBesides solving problems, we want to help you practice e\" ectively and know how to navigate an interview \nsetting. This is covered in Part VI: Principles of Coding Interviews (pg 166). It includes:\n•\nStudy Plan: A detailed study plan for how to practice using this book's materials.\n•\nUniversal Rubric: How you're evaluated by interviewers.\n•\nInterview Checklist: Breaking down each step you should take in a coding interview.\n•\nBig-O Analysis: In-depth coverage of the \"language\" of technical interviews.\n•\nProblem-Solving Strategies: Boundary thinking, trigger thinking, and problem-solving boosters.\nThe second part is a Catalog of data structures and algorithms topics. We've broken the technical topics into \ntiers, with Tier 1 being the highest priority.\n•\nTier 1: Essential topics from sets & maps to trees and graphs.\n•\nTier 2: Intermediate topics like heaps, sliding windows, and pre! x sums.\n•\nTier 3: Niche (online-only) topics that didn't warrant a spot in the physical book because they don’t \ncome up that often (this is where we enter the niche 20% territory). The online-only chapters can be \nfound at bctci.co/bonus.\nChapter 18: How to Practice (pg 170) should be your entry point to the rest of the book.\nTOPICS, RECIPES, AND REUSABLE IDEAS\nThere are three related concepts you'll ! nd as you peruse the Catalog: topics, reusable ideas, and recipes. \nHere's a quick de! nition to keep them straight:\n•\nTopic: A chapter from the Catalog, like Binary Search.\n•\nReusable Idea: A coding idea that can typically be used across problems (and even across topics). They \nare tactical tips worth remembering, such as \"pass indices, not strings in recursive code to avoid using \nextra space.\" You'll typically ! nd them next to the ! rst problem where they are used (look for the \n icon).\n•\nCoding Recipe: A pseudo-code template related to a speci! c topic that can be used as a building block \nto solve similar problems with small tweaks.\nQuestions, comments, or bugs? Report bugs at bctci.co/bugs or geek out with the authors on Discord: bctci.\nco/discord.\n1  Our mantra? If you encounter something once, it’s a trick; if you encounter it repeatedly, it’s a tool. \nC H A P T E R  2 9\n AI interviewer, replays, and more materials for this chapter at bctci.co/binary-search\nI\nC H A P T E R  2 9\nI\nC H A P T E R  2 9\nI\nB I N A R Y  S E A R C H\n▶\nPrerequisites: None\nWhen it comes to binary search, software engineers are split in half: One camp thinks it's too basic to be an \ninterview question, and the other dreads it because they always mess up the index manipulation.\nThe ! rst group overlooks the fact that binary search has many uses beyond the basic \"! nd a value in a sorted \narray.\" Far from it, binary search has many non-obvious applications, which we'll cover in this chapter. For \nthe second group, we'll provide a recipe focusing on simplicity and reusability across applications—even \nthe unconventional ones we just foreshadowed.\nBINARY SEARCH IS EASY TO MESS UP\nLet's start with something simple—the classic binary search setting—and then build up to harder problems.\nPROBLEM 29.1 \n SEARCH IN SORTED ARRAY\nGiven a sorted array of integers, arr, and a target value, target, return the target's index if it exists in the \narray or -1 if it doesn't.\n▶Example: arr = [-2, 0, 3, 4, 7, 9, 11], target = 3\nOutput: 2.\n▶Example: arr = [-2, 0, 3, 4, 7, 9, 11], target = 2\nOutput: -1.\nSOLUTION 29.1 \n SEARCH IN SORTED ARRAY\nWe assume most engineers are familiar with the basic premise of binary search: two pointers move inward \nfrom the ends of a sorted array, closing in on the target by checking if the midpoint is too small or too large.\nPop quiz! Here is an attempted solution, but it has a bug. Can you spot it?\n1 \ndef BUGGED_binary_search(arr, target): # DON'T USE IN INTERVIEWS!\n2 \n  l, r = 0, len(arr)\n3 \n  while l <= r:            \n4 \n    mid = (l + r) // 2\n5 \n    if mid == target:\n6 \n      return mid\n7 \n    if target > mid:\n8 \n      r = mid+1\n9 \n    else:\n10 \n      l = mid-1\n11 \n  return -1\nCHAPTER 29 ▸ BINARY SEARCH \n327\nCheck the solution in the footnote.1 Regardless of what you found, the point is that it is easy to miss errors \nin a binary search implementation.\nHere is one way to do it correctly:\n1  def binary_search(arr, target):\n2   n = len(arr)\n3   if n == 0:\n4     return -1\n5   l, r = 0, n - 1\n6   if arr[l] >= target or arr[r] < target:\n7     if arr[l] == target:\n8       return 0\n9     return -1\n10   while r - l > 1:\n11     mid = (l + r) // 2\n12     if arr[mid] < target:\n13       l = mid\n14     else:\n15       r = mid\n16   if arr[r] == target:\n17     return r\n18   return -1\nFigure 1. Binary search for target 3.\nWe'll punt on breaking down this solution until we talk about our transition-point recipe (pg 330).\nBINARY SEARCH HAS SURPRISING APPLICATIONS\nImagine that your bike had gotten stolen, and your only chance of getting it back hinged on your ability to \nexplain binary search to a police oﬃ  cer. That is the very situation Tom Whipple, a science journalist, found \nhimself in.2\nThe rack from which the bike went missing was right under a security camera, but the police told him they \ndidn't have the resources to watch many hours of footage just to identify when the bike was stolen. Tom \nexplained that this wouldn't be necessary: they could skip ahead to the middle of the video and check if the \nbike was still there. If it was, the bike was stolen during the latter half; if not, it was stolen earlier. This could \nbe repeated to quickly narrow down the time of the crime.\nIn the end, the thief was never caught—the footage was too grainy. Regardless, the story showcases an \nunconventional use of binary search. We can formalize it into an interview question:\nPROBLEM 29.2 \n CCTV FOOTAGE\nYou are given an API called is_stolen(t) which takes a timestamp as input and returns True if the bike \nis missing at that timestamp and False if it is still there. You're also given two timestamps, t1 and t2, \nrepresenting when you parked the bike and when you found it missing. Return the timestamp when the \nbike was ! rst missing, minimizing the number of API calls. Assume that 0 < t1 < t2, is_stolen(t1) is \nFalse, and is_stolen(t2) is True.\n1  We were not completely honest—there isn't just one bug; there are closer to six, depending on how you count them. \n(1) r is initialized out of bounds, (2 & 3) we check mid instead of arr[mid] (twice!), (4) we update r when we should \nbe updating l, (5) l should be set to mid+1, not mid-1, and (6) r should be set to mid-1, not mid+1.\n2  https://www.thetimes.com/article/i-have-owned-11-bikes-this-is-how-they-were-stolen-d3r553gx3\n328 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\nFigure 2.\nSOLUTION 29.2 \nCCTV FOOTAGE\nThis problem is quite di\" erent from Problem 29.1: Search In Sorted Array (pg 326): it doesn't have an array \ninput, and we don't have a target value. In fact, if we tried to use the same binary search code from earlier, \nwe'd have to change almost every line in the algorithm before it would work correctly. That many tweaks \nmake it easy to reintroduce bugs. Nonetheless, we can still use binary search because the range of possible \nanswers can be broken down into two regions: (1) before the bike was stolen and (2) after it was stolen. \nWe are searching for the transition point from 'before' to 'after’:\nFigure 3.\nInitially, we don't know where the transition point is, but we can binary search for it:\nFigure 4.\n1 \ndef is_before(val):\n2 \n  return not is_stolen(val)\n3 \n4 \ndef fi nd_bike(t1, t2):\n5 \n  l, r = t1, t2\n6 \n  while r - l > 1:\n7 \n    mid = (l + r) // 2\n8 \n    if is_before(mid):\n9 \n      l = mid\n10 \n    else:\nCHAPTER 29 ▸ BINARY SEARCH \n329\n11 \n      r = mid\n12 \n  return r\nAt the beginning, l is in the 'before' region, and r is in the 'after' region. From there, l never leaves the \n'before' region and r never leaves the 'after' region, but they end up next to each other: at the end, l is the \nlast 'before' and r is the ! rst 'after.'\nHere is the kicker: every binary search solution can be reframed as ﬁ nding a transition point. For \ninstance, Problem 29.1: Search In Sorted Array can be reframed as ! nding the \"transition point\" from elements \nsmaller than target to elements greater than or equal to target.\nIf we learn a recipe for ! nding transition points, we'll be able to use it for every binary search problem. We \ndon't need specialized recipes for various problem types.\n TRANSITION-POINT RECIPE\nIn an important interview—with tensions mounting and anxiety running high—you are not working at total \ncapacity. We joke that you are ~20% dumber than during practice. To counter this, it helps to have a recipe \nyou know well for tricky algorithms like binary search. A good recipe should be easy to remember, have \nstraightforward edge cases, and make it easy to avoid o\" -by-one errors.\n RECIPE 1. TRANSITION-POINT RECIPE\ntransition_point_recipe()\n  defi ne is_before(val) to return whether val is 'before'\n  initialize l and r to the fi rst and last values in the range\n  handle edge cases:\n    - the range is empty\n    - l is 'after'  (the whole range is 'after')\n    - r is 'before' (the whole range is 'before')\n  while l and r are not next to each other (r - l > 1)\n    mid = (l + r) / 2\n    if is_before(mid)\n      l = mid\n    else\n      r = mid\n  return l (the last 'before'), r (the fi rst 'after'), or something else,\n         depending on the problem\nThe point of the initialization and the initial edge cases is to get to a setting that looks like the ! rst row \nof Figure 4: l must be in the 'before' region, and r must be in the 'after' region. The three edge cases are \ndesigned to ensure this.\nOnce we get to that point, the main while loop is the same for every problem—no tweaking needed!\nThe loop has the following invariants, which are guarantees that make our lives easier:\n•\nFrom start to end, l is in the 'before' region, and r is in the 'after' region. They are never equal and never \ncross over.3\n•\nThe midpoint is always strictly between l and r (l < mid < r), which guarantees we always make \nprogress (we don't need to worry about in! nite loops).\n•\nWhen we exit the loop, l and r are always next to each other.\n3  If we were a little more willing to buck conventions, we'd rename from l and r to b and a, since they always map to \n'before' and 'after' values. However, you might get odd looks from an interviewer if you do this!\n330 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\nSomething that is typically tricky with binary search is the exit condition of the loop. Here, we keep going \nuntil l and r are next to each other (i.e., until the 'unknown' region in Figure 4 is empty), which happens \nwhen r - l is 1. That's why the condition says r - l > 1.4\nAnother tricky part is knowing what to return. With this recipe, we just need to reason about the transition \npoint: do we need the ! nal 'before' or the ! rst 'after'?\nWe recommend starting by de! ning the is_before() function. Keep in mind that, for binary search to work, \nwe must de! ne it in such a way that the search range is monotonic: all the 'before' elements must appear \nbefore all the 'after' elements. That's why binary search doesn't work on unsorted arrays.\n Revisiting Solution 29.1\nHere is how we applied the recipe in Solution 29.1: we de! ned the 'before' region as the elements < target, \nand the 'after' region as the elements ≥target.\nIn the initialization, we have the three edge cases from the recipe to ensure that l is 'before' and r is 'after':\n1 \nif n == 0:\n2 \n  return -1\n3 \nl, r = 0, n - 1\n4 \nif arr[l] >= target or arr[r] < target:\n5 \n  if arr[l] == target:\n6 \n    return 0\n7 \n  return -1\nThe while loop is just like the recipe, except that we didn't factor out is_before() into a helper function:\n1 \nwhile r - l > 1:\n2 \n  mid = (l + r) // 2\n3 \n  if arr[mid] < target:\n4 \n    l = mid\n5 \n  else:\n6 \n    r = mid\nFinally, when we ! nd the transition point, we consider what that means: l is at the largest value smaller than \nthe target, and r is at the smallest value greater than or equal to the target. So, if the target is in the array \nat all, it must be at index r.\n1 \nif arr[r] == target:\n2 \n  return r\n3 \nreturn -1\nWhat to do at the end depends on how we de! ne the 'before' region. We could have also de! ned 'before' \nas \"less than or equal to the target,\" in which case, at the end, we would have to check the element at l\ninstead of r.\nThis recipe is a bit like a one-size-! ts-all pair of socks. While more concise (but less reusable) implementations \nmay exist for some problems, there is value in needing only one easy-to-remember recipe.\nTRANSITION-POINT PROBLEM SET\nFor each of the following problems:\n•\nReframe it as ! nding a transition point by de! ning 'before' and 'after' regions.\n•\nFind the location of l and r after ! nding the transition point for the given example input.\n4  We could have also written this in other ways, like r > l + 1. One way to remember the formula for the number of \nelements between l and r, r-l-1, is that it looks like a sleepy cat.\nCHAPTER 29 ▸ BINARY SEARCH \n331\n•\nIdentify what to return after ! nding the transition point.\nYou don't need to code anything yet—focus on the transition logic.\nQUESTION 1 \nGIT COMMITS\nFind the ! rst commit that fails a test in a sequence of Git commits. We know the test was passing for every \ncommit until it started failing at some point.\n [\"pass\", \"pass\", \"pass\", \"pass\", \"fail\", \"fail\", \"fail\"]\nQUESTION 2 \nSQUARED TARGET\nGiven a sorted array of positive integers and a target value, ! nd the largest number in the array that can be \nsquared and still be less than or equal to the target, if any. Return the number (not its index).\n [2, 3, 4, 5, 6, 7, 8, 11, 20, 21, 23, 25, 25], target = 36\nQUESTION 3 \nFIRST NON-NEGATIVE\nReturn the index of the ! rst non-negative integer in a sorted array (duplicates allowed), if any.\n [-21, -15, -9, -5, -5, -1, -1, 0, 0, 4, 7, 12, 21]\nQUESTION 4 \nFIRST 'P' \nFind the ! rst word that begins with 'p' in an array of words in dictionary order, if any.\n [\"apple\", \"banana\", \"peach\", \"strawberry\"]\nQUESTION 5 \n NEAREST ELEMENT \nIn a sorted array of integers (duplicates allowed), ! nd the last occurrence of a given target value. If the target \ndoes not exist, return the index of the next closest value (it could be smaller or larger than the target).\n [1, 3, 5, 6, 7, 7, 8, 11, 13, 21], target = 7\nQUESTION 6 \nDECK CUT\nYou're given an array that contains each number from 1 to 52 once, representing a deck of playing cards. \nThe deck started in order, but it was then \"cut,\" meaning that a random number of cards was taken from the \ntop (the front of the array) and moved as a block to the bottom (the back of the array). Determine the index \nwhere you must \"cut\" the deck again to return to sorted order (that is, the index with the 52).\n [36, 37, 38, ..., 50, 51, 52, 1, 2, 3, ..., 33, 34, 35]\nPROBLEM SET SOLUTIONS\nIn the solutions below, we circled which of the two pointers we should return at the end.\nANSWER 1 \nGIT COMMITS\n332 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\nANSWER 2 \nSQUARED TARGET\nWe should return arr[l], since the last number in the 'before' region is the largest number that still works.\nANSWER 3 \nFIRST NON-NEGATIVE\nIncluding 0 in the 'before' region would be a mistake: if there are multiple zeros, l would point to the last \none, but the goal is to return the ! rst one.\nANSWER 4 \nFIRST 'P'\nThe ‘before’ region consists of words that start with a–o and the ‘after’ region consists of words that start with \np–z. If there are words that start with ‘p’, the ! rst one will be at index r.\nIncluding words that start with 'p' in the 'before' region would be incorrect: if we inserted another word \nstarting with 'p,' like \"pear,\" l would point to the last word starting with 'p' rather than the ! rst one.\nANSWER 5 \nNEAREST ELEMENT\nPost-processing requires a bit of thought. If the target is in the array, it will be at l. We can peek at arr[l]\nand return l if it is the target. Otherwise, we need to ! nd the closest value to it, which could be at l or r. We \nreturn either l or r, based on whether arr[l] or arr[r] is closer.\nANSWER 6 \nDECK CUT\nTrick question! This doesn't require binary search as the answer is always 52 - deck[0]. Still, we could ! nd \nthe transition point from 52 back to 1 with a binary search. The 'before' region would be numbers ≥deck[0]. \nThe l pointer would end up at the 52 and the r pointer at the 1. We would return l.\nVALIDATION & DRAWING ADVICE\nVisualizing the binary search in an interview is helpful both for you and your interviewer. Instead of trying to \nverbally explain what you're doing, show them in the shared editor. Our suggestions are similar to the Two \nPointers chapter (pg 296):\nCHAPTER 29 ▸ BINARY SEARCH \n333\n•\nWrite each pointer (l, r, and m for the midpoint) on its own line so you can move them independently \nwith ease.\n•\nWriting indices on the top of the array makes it faster to do midpoint calculations.\n•\nYou can also draw the transition point between 'before' and 'after.'\nInstead of\nTry\n[1, 2, 2, 3, 3, 4, 5, 8, 8]\n ^           ^           ^\nleft        mid        right\n 0 1 2 3 4 5 6 7 8\n[1 2 2 3 3 4|5 8 8]\n l\n                 r\n         m\nm = (0+8)/2\n So, which inputs should you validate and visualize? Consider the following edge cases, when applicable:\n•\nThe range is empty.\n•\nThe range only has 'before' elements.\n•\nThe range only has 'after' elements.\n•\nThe target is not in the array.\n•\nThe target is in the array multiple times.\nANALYSIS\nBut oﬃ  cer, it is O(log n)!\nIn the Big O analysis chapter , we de! ned log2(n) as roughly the number of times we need to halve a number \nto reach 1. Binary search halves the search range at each step, so binary search converges in O(log n)\niterations, where n is the size of the range (e.g., t2 - t1 in Problem 29.2: CCTV Footage (pg 327).5\nSome binary search implementations stop early when arr[mid] == target. For simplicity, our recipe \ndoesn't have that, which means that it takes O(log n) time even in the best case. That's ! ne—we mainly \ncare about the worst case.\nDon't forget to factor in the time it takes to compute is_before() in the runtime calculation if it is not \nconstant!\nFinally, the extra space is O(1). Binary search can also be implemented recursively, in which case the extra \nspace increases to O(log n) for the call stack.\nBINARY SEARCH PROBLEM SET\nTry these problems with AI Interviewer: bctci.co/binary-search-problem-set-1\nLet's tackle some problems that require creative approaches for using binary search. The transition-point \nrecipe should prove useful!\nPROBLEM 29.3 \n VALLEY BOTTOM\nA valley-shaped array is an array of integers such that:\n•\nit can be split into a non-empty pre! x and a non-empty suﬃ  x,\n•\nthe pre! x is sorted in decreasing order,\n5   See page 219 for why we \"drop\" the base of the logarithm in big O notation.\n334 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\n•\nthe suﬃ  x is sorted in increasing order,\n•\nall the elements are unique.\nGiven a valley-shaped array, arr, return the smallest value.\n▶Example: arr = [6, 5, 4, 7, 9]\nOutput:  4\n▶Example: arr = [5, 6, 7]\nOutput:  5. The prefix sorted in decreasing order is just [5].\n▶Example: arr = [7, 6, 5]\nOutput:  5. The suffix sorted in increasing order is just [5].\nPROBLEM 29.4 \n 2-ARRAY 2-SUM\nYou are given two non-empty arrays of integers, sorted_arr and unsorted_arr. The ! rst one is sorted, but \nthe second is not. The goal is to ! nd one element from each array with sum 0. If you can ! nd them, return an \narray with their indices, starting with the element in sorted_arr. Otherwise, return [-1, -1]. Use O(1)\nextra space and do not modify the input.\n▶Example:  \nsorted_arr = [-5, -4, -1, 4, 6, 6, 7]\n \nunsorted_arr = [-3, 7, 18, 4, 6]\nOutput:   \n[1, 3]. We can use -4 from the sorted array and 4 from the  \n \n \nunsorted array.\nPROBLEM 29.5 \n TARGET COUNT DIVISIBLE BY K\nGiven a sorted array of integers, arr, a target value, target, and a positive integer, k, return whether the \nnumber of occurrences of the target in the array is a multiple of k.\n▶Example:  \narr = [1, 2, 2, 2, 2, 2, 2, 3]\n \ntarget = 2, k = 3\nOutput:   \nTrue. 2 occurs 6 times, which is a multiple of 3.\n▶Example:  \narr = [1, 2, 2, 2, 2, 2, 2, 3]\n \ntarget = 2, k = 4\nOutput:   \nFalse. 2 occurs 6 times, which is not a multiple of 4.\n▶Example:  \narr = [1, 2, 2, 2, 2, 2, 2, 3]\n \ntarget = 4, k = 3\nOutput:   \nTrue. 4 occurs 0 times, and 0 is a multiple of any number.\nPROBLEM 29.6 \n RACE OVERTAKING\nYou are given two arrays of positive integers, p1 and p2, representing players in a racing game. The two arrays \nare sorted, non-empty, and have the same length, n. The i-th element of each array corresponds to where \nthat player was on the track at the i-th second of the race. We know that:\n1. player 1 started ahead (p1[0] > p2[0]),\n2. player 2 overtook player 1 once, and\n3. player 2 remained ahead until the end (p1[n - 1] < p2[n - 1]).\nAssume the arrays have no duplicates, and that p1[i] != p2[i] for any index.\nReturn the index at which player 2 overtook player 1.\n▶Example:  \np1 = [2, 4, 6, 8, 10], \n \np2 = [1, 3, 5, 9, 11]\nCHAPTER 29 ▸ BINARY SEARCH \n335\nOutput:   \n3\nPROBLEM 29.7 \n SEARCH IN SORTED GRID\nYou're given a 2D grid of integers, grid, where each row is sorted (without duplicates), and the last value in \neach row is smaller than the ! rst value in the following row. You are also given a target value, target. If the \ntarget is in the grid, return an array with its row and column indices. Otherwise, return [-1, -1].\n▶Example:  target = 4\ngrid = [[1, 2, 4, 5],\n        [6, 7, 8, 9]]\nOutput:   [0, 2]. The number 4 is found in row 0 column 2.\n▶Example:  target = 3\ngrid = [[1, 2, 4, 5],\n \n     [6, 7, 8, 9]]\nOutput:   [-1, -1]\nPROBLEM 29.8 \n SEARCH IN HUGE ARRAY\nWe are trying to search for a target integer, target, in a sorted array of positive integers (duplicates allowed) \nthat is too big to ! t into memory. We can only access the array through an API, fetch(i), which returns the \nvalue at index i if i is within bounds or -1 otherwise. Using as few calls to the API as possible, return the \nindex of the target, or -1 if it does not exist. If the target appears multiple times, return any of the indices. \nThere is no API to get the array's length.\nPROBLEM SET SOLUTIONS\nSOLUTION 29.3 \n VALLEY BOTTOM\nThis problem shows that binary search can be used even if the input array is not monotonically sorted.\nIntuitively, we want to de! ne the 'before' region as the descending pre! x and the 'after' region as the ascend-\ning suﬃ  x. The tricky part is that an array like [6, 5, 4, 7, 9] can be formed in two ways:\n•\nWith a descending pre! x [6, 5, 4] and an ascending suﬃ  x [7, 9].\n•\nWith a descending pre! x [6, 5] and an ascending suﬃ  x [4, 7, 9].\nWe need a clear rule for how to de! ne is_before(). For instance, if we want the 4 to be in the 'before' \nregion, we can say that a number is in the 'before' region if (a) it is the ! rst element, or (b) it's smaller than \nthe previous element.\nThis de! nition is workable, but according to it, an array like [7, 6, 5] only contains 'before' elements; we \nneed to check for that case during preprocessing.\nWith this de! nition, elements in the ‘after’ region are always greater than the previous element, so the small-\nest value in the entire array will be the last one in the ‘before’ region.\n1 \ndef valley_min_index(arr):\n2 \n  def is_before(i):\n3 \n    return i == 0 or arr[i] < arr[i-1]\n4 \n  l, r = 0, len(arr)-1\n5 \n  if is_before(r):\n6 \n    return arr[r]\n7 \n  while r - l > 1:\n8 \n    mid = (l + r) // 2\n336 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\n9 \n    if is_before(mid):\n10 \n      l = mid\n11 \n    else:\n12 \n      r = mid\n13 \n  return arr[l]\nInterestingly, in the variation of this problem where we allow duplicates in the input, binary search does not \nwork: if mid lands on a value that is the same as the previous one and the next one, we can't tell if we are in \nthe descending pre!x or the ascending suﬃx.6\nSOLUTION 29.4 \n 2-ARRAY 2-SUM\nThis problem, which is a variant of the classic 2-sum problem, shows binary search as a building block of a \nbroader algorithm.\nLet n1 be the length of the sorted array and n2 the length of the unsorted array. The \"only O(1) extra space\" \nconstraint means that we can't use a map-based solution, which would take O(n1) or O(n2) space.\nInstead, we can iterate through the numbers in the unsorted array and, for each one, binary search for its \ninverse in the sorted array. The total runtime will be O(n2 * log n1).\n1 \ndef two_array_two_sum(sorted_arr, unsorted_arr):\n2 \n  for i, val in enumerate(unsorted_arr):\n3 \n    idx = binary_search(sorted_arr, -val)\n4 \n    if idx != -1:\n5 \n      return [idx, i]\n6 \n  return [-1, -1]\nWe omit the binary search step because it is the same as Solution 1.\nSOLUTION 29.5 \n TARGET COUNT DIVISIBLE BY K\nThe key is to !nd the !rst and last occurrence of the target, first and last. If present, the number of occur-\nrences of the target is last - first + 1. We can check if this number is multiple of k.\nIf the target is in the array, we can !nd first and last with a binary search for each:\n \n•\none de!ning 'before' as '< target' and returning r,\n \n•\none de!ning 'before' as '< target + 1' and returning l.\nThe runtime is O(2*log n) = O(log n).\nSOLUTION 29.6 \n RACE OVERTAKING\nWe say an index is 'before' if player 2 has not overtaken player 1 yet. That is:\n1 \ndef is_before(i):\n2 \n  return p1[i] > p2[i]\nAccording to the statement, index 0 is 'before' and index n-1 is 'after,' so we don't need to worry about the \ninitial edge cases. We just need to !nd the transition point and return r.\nSOLUTION 29.7 \n SEARCH IN SORTED GRID\nWe could solve this problem in two steps:\n1. Binary search over the rows to !nd a single row that may contain the target.\n2. Binary search over the row.\n6  In fact, for this variant, we cannot do better than O(n) time. The array could consist of all 1's and a single 0, which could \nbe anywhere and can only be found with a linear scan.\nCHAPTER 29 ▸ BINARY SEARCH \n337\nWhile this works, a trick that makes the implementation easier is to imagine that we \"% atten\" the grid into \na single, long array with all the rows consecutively:\nFigure 5.\nThis would be a sorted array with R*C elements. We can binary search over this '% attened-grid' array without \nactually creating it. We'd start with l = 0 and r = R*C - 1. To de! ne the is_before() function, we must \nmap the '% attened-grid’ array index to the actual grid coordinates on the % y:\n1 \ndef is_before(grid, i, target):\n2 \n  num_cols = len(grid[0])\n3 \n  row, col = i // num_cols, i % num_cols\n4 \n  return grid[row][col] < target\nOnce we ! nd the transition point, we have to map r back to grid coordinates to check if the target is there \nand return them.\nREUSABLE IDEA: GRID FLATTENING\nIf we want to iterate or search through a grid with dimensions RxC as if it was a 'normal' array of length \nR*C, we can use the following mapping from grid coordinates to \"% attened-grid array\" coordinates:\n \n[r, c] → r * C + c\nand the reverse mapping to go from \"% attened-grid array\" coordinates to grid coordinates:\n \ni → [i // C, i % C]\nFor instance, cell [1, 2] in Figure 5 (the 9) becomes index 1 * 4 + 2 = 6, and, conversely, index 6\nbecomes cell [6 // 4, 6 % 4] = [1, 2].\nSOLUTION 29.8 \n SEARCH IN HUGE ARRAY\nLeveraging the break down the problem booster, we can break the problem into two. One problem is \nquickly ! nding the target in a huge array. Binary search is an obvious choice here, but it leads to the second \nproblem: our left pointer can start at zero, but where do we start our right pointer without knowing the \nlength of the array?\nA silly way to solve this would be to keep trying one index after another until the API eventually returns -1. \nInstead, we can double our index at each step. If the length is n, we'll reach it in approximately log2(n) steps. \nThe rest of the problem is a straightforward application of the transition-point recipe.\n1 \ndef fi nd_through_api(target):\n2 \n  def is_before(idx):\n3 \n    return fetch(idx) < target\n4 \n  l, r = 0, 1\n5 \n  # Step 1: Get the rightmost boundary\n6 \n  while fetch(r) != -1:\n7 \n    r *= 2\n8 \n  # Step 2: Binary search\n9 \n  # ...\n338 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\nThe total runtime is O(log n), where n is the size of the huge array.\n REUSABLE IDEA: EXPONENTIAL SEARCH\nWhenever we need to search for a value in a range, but the upper bound (or even lower bound) of the \nrange is unknown, we can ! nd it eﬃ  ciently with repeated doubling.\nThis is often useful in the guess-and-check technique (e.g., Problem 29.10: Water Re! lling, pg 340).\n GUESS-AND-CHECK TECHNIQUE\nSince I was unable to come up with any approach, \nI knew the solution was going to be Binary Search.\nAnonymous Leetcode User, 2023\nAs the stolen bike story illustrates, binary search is often used in problems where it is not an obvious choice. \nNow that we have a solid recipe for any binary search problem, we will discuss the guess-and-check tech-\nnique, which allows us to use binary search on many optimization problems.\nRecall that an optimization problem is one where you are asked to ! nd some minimum or maximum value, \nsubject to some constraint. For example, consider the following problem:\nPROBLEM 29.9 \n MIN-SUBARRAY-SUM SPLIT\nGiven a non-empty array with n positive integers, arr, and a number k with 1 ≤ k ≤ n,  the goal is to split \narr into k non-empty subarrays so that the largest sum across all subarrays is minimized. Return the largest \nsum across all k subarrays after making it as small as possible. Each subarray must contain at least one value.\n▶Example: arr = [10, 5, 8, 9, 11], k = 3\nOutput:  17. There are six ways of splitting the array into three subarrays. \nThe optimal split is: [10, 5], [8, 9], and [11]. The largest sum \namong the three subarrays is 17.\n▶Example: arr = [10, 10, 10, 10, 10], k = 2\nOutput:  30.\nSOLUTION 29.9 \n MIN-SUBARRAY-SUM SPLIT\nThis is an optimization problem because we have a goal and a constraint: we are trying to minimize the \nlargest subarray sum, subject to having at most k subarrays. Without the constraint, we would just put every \nelement in its own subarray.\nA naive solution that tries every way of splitting the array into k subarrays would take exponential time.7\nThere is a dynamic programming solution that takes O(n*k) time (pg 572).\nHere, we'll use a di\" erent approach. Given a value, max_sum, we can ask:\nIs there a way to split arr into k subarrays such that every subarray has sum at most max_sum?\n•\nFor max_sum < max(arr), the answer is \"no\" (some numbers are too big to be in a subarray, even by \nthemselves).\n7  You need to choose k out of n-1 possible splitting points, so there are (n-1 choose k) options, which is O((n-1)k) \n= O(nk) for any constant value of k. If k is n/2, the number becomes exponential on n (O(2n/√n) to be exact, but you \ndon't need to worry about where that formula comes from). Once k gets larger than n/2, the number of possibilities \nstarts decreasing (if we are picking more than half the points, we can think about picking the points not to split at, of \nwhich there are fewer than n/2).\nCHAPTER 29 ▸ BINARY SEARCH \n339\n \n•\nFor max_sum == sum(arr), the answer is \"yes\" (any split will do).\nWe can binary search for the transition point where the answer goes from \"no\" to \"yes\" with our transition \npoint recipe. The value x corresponding to the !rst \"yes\" is the value of the optimal solution.\nTo implement our is_before(max_sum) function, we need to be able to compute the answer to the ques-\ntion. Thankfully, it is much easier than the original problem: we can grow each subarray up until the point \nwhere its sum would exceed max_sum. At that point, we start a new subarray, and so on. If we need more \nthan k subarrays, the answer is \"no.\" Otherwise, the answer is \"yes.\"\n1 \n# \"Is it impossible to split arr into k subarrays, each with sum <= max_sum?\"\n2 \ndef is_before(arr, k, max_sum):\n3 \n  splits_required = get_splits_required(arr, max_sum)\n4 \n  return splits_required > k\n5 \n6 \n# Returns the minimum number of subarrays with a given maximum sum.\n7 \n# Assumes that max_sum >= max(arr).\n8 \ndef get_splits_required(arr, max_sum):\n9 \n  splits_required = 1\n10 \n  current_sum = 0\n11 \n  for num in arr:\n12 \n    if current_sum + num > max_sum:\n13 \n      splits_required += 1\n14 \n      current_sum = num  # Start a new subarray with the current number.\n15 \n    else:\n16 \n      current_sum += num\n17 \n  return splits_required\n18 \n19 \ndef min_subarray_sum_split(arr, k):\n20 \n  l, r = max(arr), sum(arr)  # Range for the maximum subarray sum.\n21 \n  if not is_before(arr, k, l):\n22 \n    return l\n23 \n  while r - l > 1:\n24 \n    mid = (l + r) // 2\n25 \n    if is_before(arr, k, mid):\n26 \n      l = mid\n27 \n    else:\n28 \n      r = mid\n29 \n  return r\nLet S be the sum of arr. Binary search will take O(log S) steps to converge, and each is_before() check \ntakes O(n) time. The total runtime is O(n log S). Depending on whether O(k) or O(log S) is larger, DP \nor binary search will be better. Neither dominates the other.\nTo recap, the guess-and-check technique involves narrowing in on the value of the optimal solution by \nguessing the midpoint and checking whether it's too high or too low. To start, we need lower and upper \nbounds for the value of the optimal solution (if the bounds are not obvious, exponential search can help).\nFor minimization problems (like Problem 29.9: Min-Subarray-Sum Split), there is often a transition point where \nsmaller values do not satisfy the constraint, but larger values do. Conversely, for maximization problems, \nthere is often a transition point where larger values do not satisfy the constraint, but smaller values do.\nWhen should I use the guess-and-check technique?\nWe can try it when we have an optimization problem and !nding the optimal value directly is \nchallenging. Ask yourself:\n340 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\n\"Is it easier to solve the yes/no version of the problem, where we just check if a given value (optimal or \nnot) satisﬁ es the constraint?\"\nThink of it like making a deal: You get to solve an easier problem (checking if a speci! c value \nsatis! es the constraint), but you pay a 'logarithmic tax' in the runtime (to binary searching for the \ntransition point).\nWe've seen the guess-and-check technique before, in the Boundary Thinking chapter with Problem 22.1: \nTunnel Depth (pg 232). It is easier to binary search for the ! rst depth where the tunnel doesn't reach than \nto try to compute the maximum depth directly.\nBOUNDARY THINKING IN ACTION8 \nINTERVIEW REPLAY\nView Online:\nbctci.co/binary-search-replay-1 @ 38:55 - end\nThe Question:\nReturn the maximum tunnel depth in a grid.\nWhat You'll See:\nThe candidate chose a graph traversal after seeing the grid, and the \ninterviewer and candidate discussed multiple solutions and how to avoid \ngetting \"tunnel\" vision.\nWho: \nInterviewer: Software Engineer at Google\nCandidate: 7 years exp.\nGUESS-AND-CHECK PROBLEM SET\nTry these problems with AI Interviewer: bctci.co/binary-search-problem-set-2\nPROBLEM 29.10 \n WATER REFILLING\nWe have an empty container with a capacity of a gallons of water and another container with a capacity of \nb gallons. Return how many times you can pour the second container full of water into the ! rst one without \nover% owing. Assume that a > b.\n•\nConstraint: You are not allowed to use the division operation, but you can use still divide by powers of \ntwo with the right-shift operator, >>. Recall that x >> 1 is the same as x // 2.\n▶Example: a = 18, b = 5\nOutput:  3. After pouring 5 gallons three times, the first container will be \nat 15, and 5 more gallons would make it overflow.\nPROBLEM 29.11 \nMIN PAGES PER DAY\nYou have upcoming interviews and have selected speci! c chapters from BCtCI to read beforehand. Given \nan array, page_counts, where each element represents a chapter’s page count, and the number of days, \ndays, until your interview, determine the minimum number of pages you must read daily to ! nish on time. \nAssume that:\n•\nYou must read all the pages of a chapter before moving on to another one.9\n8  I (Mike) am the interviewer in this particular interview. This question was the opener for the Boundary Thinking chapter \nand you can see a candidate make the same mistakes we discuss in that chapter and me walking through the boundary \nthinking mentality.\n9  Hypothetically! It's ! ne to jump around chapters when reading this actual book.\nCHAPTER 29 ▸ BINARY SEARCH \n341\n•\nIf you ! nish a chapter on a given day, you practice for the rest of the day and don't start the next chapter \nuntil the next day.\n•\nlen(page_counts) ≤ days.\n▶Example: page_counts = [20, 15, 17, 10], days = 14\nOutput:  5. We can read 5 pages daily and finish all chapters. At a maximum \nof 5 pages per day, we spend:\n   4 days on the first chapter.\n \n3 days on the second chapter.\n \n4 days on the third chapter (stopping when we finish early).\n \n2 days on the fourth chapter.\n \nIn total, we spent 13 days reading 5 pages a day, which is the  \n \nlowest amount we can read daily and still finish on time.\n▶Example: page_counts = [20, 15, 17, 10], days = 5\nOutput:  17\nPROBLEM 29.12 \nTIDE AERIAL VIEW\nYou are provided a series of aerial-view pictures of the same coastal region, taken a few minutes apart from \neach other around the time the tide rises. Each picture consists of an nxn binary grid, where 0 represents a \npart of the region above water, and 1 represents a part below water.\n•\nThe tide appears from the left side and rises toward the right, so, in each picture, for each row, all the \n1's will be before all the 0's.\n•\nOnce a region is under water, it stays under water.\n•\nAll pictures are di\" erent.\nDetermine which picture shows the most even balance between regions above and below water (i.e., where \nthe number of 1's most closely equals the number of 0's). In the event of a tie, return the earliest picture.\nFigure 6. Example input for Problem 9. The empty cells are 0's and the cells with water are 1's.\n▶Example: The pictures from Figure 6.\nOutput:  2. The pictures at index 2 and 3 are equally far from having 50%    \nwater. We break the tie by picking the earlier one, 2.\nPROBLEM SET SOLUTIONS\nSOLUTION 29.10  WATER REFILLING\nTry to solve this problem manually for a = 182983 and b = 90. Use a calculator if you want, just do not \nuse the division operation. Done? How did you do it? Try to reverse engineer your process and map it to an \nalgorithmic technique.\nWhat you deﬁ nitely didn't do is check sequential multiples of 90 until you reached 182983. Most people \nmake guesses in increasingly larger jumps until they ! nd a guess that is too large. If we double the guess at \n342 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\neach time, this is exponential search (pg 338). Then, they start searching between their closest guess below \nand above the answer, closing in on the number—something we can do with binary search. Despite the \nproblem not having clear triggers for binary search, it's a natural !t for this thought process.\n1 \ndef num_refills(a, b):\n2 \n  # \"Can we pour 'num_pours' times?\"\n3 \n  def is_before(num_pours):\n4 \n    return num_pours * b <= a\n5 \n6 \n  # Exponential search (repeated doubling until we find an upper bound).\n7 \n  k = 1\n8 \n  while is_before(k * 2):\n9 \n    k *= 2\n10 \n11 \n  # Binary search between k and k*2\n12 \n  l, r = k, k * 2\n13 \n  while r-l > 1:\n14 \n    gap = r - l\n15 \n    half_gap = gap >> 1  # Bit shift instead of division\n16 \n    mid = l + half_gap\n17 \n    if is_before(mid):\n18 \n      l = mid\n19 \n    else:\n20 \n      r = mid\n21 \n  return l\nSOLUTION 29.11 MIN PAGES PER DAY\nFor a given value daily_limit, we pose the question:\nCan I !nish all the chapters in time reading at most daily_limit pages a day?\nWe can guess and check for the answer to this question.\nSince we can only !nish one chapter per day, the maximum answer is the longest chapter (20 pages in the \nexample). The minimum is 1 page per day. We can binary search between these bounds and simulate reading \nthe guessed amount of pages per day. If the guess allows us to !nish within the given days, we try a smaller \nnumber. If it takes too many days, we try a larger number.\n1 \ndef days_to_finish(page_counts, daily_limit):\n2 \n  days = 0\n3 \n  for pages in page_counts:\n4 \n    days += math.ceil(pages / daily_limit)\n5 \n  return days\n6 \n7 \ndef is_before(page_counts, daily_limit, days):\n8 \n  return days_to_finish(page_counts, daily_limit) <= days\nSOLUTION 29.12 TIDE AERIAL VIEW\nWe can binary search for the transition point where it goes from majority above water to majority underwater. \nThe 'before' pictures are < 0.5 water, and the 'after' pictures are ≥ 0.5 water. The answer will be the last \n'before' or the !rst 'after.'\nHowever, if you constructed the is_before() function to just loop through the matrix, counting the number \nof cells underwater, you missed something! Besides doing a binary search across the range of pictures, we can \nCHAPTER 29 ▸ BINARY SEARCH \n343\nspeed up the count of underwater cells by also doing a binary search on each row: the rows are monotonic, \nwith all 1's followed by all 0's.\n1 def get_ones_in_row(row):\n2   if row[0] == 0:\n3     return 0\n4   if row[-1] == 1:\n5     return len(row)\n6 \n7   def is_before_row(idx):\n8     return row[idx] == 1\n9 \n10   l, r = 0, len(row)\n11   while r - l > 1:\n12     mid = (l + r) // 2\n13     if is_before_row(mid):\n14       l = mid\n15     else:\n16       r = mid\n17   return r\n1 def is_before(picture):\n2   water = 0\n3   for row in picture:\n4     water += get_ones_in_row(row)\n5   total = len(picture[0])**2\n6   return water/total < 0.5\nChecking the number of ones in a row takes O(log n) time. Checking the number of ones in an entire grid \ntakes O(n log n) time. The total time is O(n log n log k), where k is the number of pictures.\nBINARY SEARCH GONE WRONG \nINTERVIEW REPLAY\nView Online:\nbctci.co/binary-search-replay-2 @ 2:45 - 26:07:00\nThe Question:\nWrite an algorithm to compute the square root of a given non-negative \nnumber\nWhat You'll See:\nThe candidate struggled to implement a working version of binary search, \nand each change led to further problems with the algorithm.\nWho: \nInterviewer: Software Engineer at Meta\nCandidate: 7 years exp.\nCONCLUSIONS\nBinary Search triggers: The input is a sorted array/string. The brute force involves repeated linear \nscans. We are given an optimization problem that's hard to optimize directly. \nKeywords: sorted, threshold, range, boundary, ! nd, search, minimum/maximum, ! rst/last, small-\nest/largest.\nBinary search is often a step or a possible optimization in more complicated algorithms. Binary search is so \ncommon that it can (and will) be seen alongside almost every other Catalog topic, like Graphs (Problem 36.9: \nFirst Time All Connected, pg 468), Sliding Windows (Chapter 38: Longest Repeated Substring, pg 523), \nand Greedy Algorithms (Problem 41.6: Time Traveler Max Year, pg 593).\nThe key idea in this chapter is that we can reframe every binary search problem as ! nding a transition point. \nThis way, we only need one recipe for every scenario—the transition-point recipe (pg 329)—and we can \nfocus our energy on more complicated parts of the code.\n344 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\nAt this point, you should be ready to start adding binary search problems to your practice rotation. You can \n! nd the problems in this chapter and additional problems in the companion AI interviewer.\nONLINE RESOURCES\nOnline resources for this chapter include:\n•\nA chance to try each problem in this chapter in AI Interviewer\n•\nInterview replays that show speci! c mistakes people make with binary search \nproblems\n•\nFull code solutions for every problem in the chapter in multiple programming \nlanguages\nTry online at  bctci.co/binary-search.\nC H A P T E R  3 8\n AI interviewer, replays, and more materials for this chapter at bctci.co/sliding-windows\nI\nC H A P T E R  3 8\nI\nC H A P T E R  3 8\nI\nS L I D I N G  W I N D O W S\n▶\nPrerequisites: None\nIn this chapter, we will use the sliding window technique to tackle problems about ! nding or counting \nsubarrays.1\nWe will use the following setting for problems throughout this chapter: a bookstore is looking at the number \nof book sales. The sales for each day are stored in an array of non-negative integers called sales. We say \na good day is a day with at least 10 sales, while a bad day is a day with fewer than 10 sales. An interviewer \ncould ask questions such as the following:\n•\nFind the most sales in any 7-day period (Problem 1).\n•\nFind the most consecutive days with no bad days (Problem 5).\n•\nFind the longest period of time with at most 3 bad days (Problem 8).\n•\nFind the shortest period of time with more than 20 sales, if any (Problem 14).\n•\nCount the number of subarrays of sales with at most 10 bad days (Problem 18).\n•\nCount the number of subarrays of sales with exactly 10 bad days (Problem 19).\n•\nCount the number of subarrays of sales with at least 10 bad days (Problem 20).\nAll these questions receive an array as input, sales. The ! rst four ask us to ! nd a subarray, while the last three \nask us to count subarrays, making them ideal candidates for the sliding window technique. In this chapter, \nwe will cover variants of the sliding window technique to tackle each of the mentioned problems and more.\nThe basic idea of a sliding window is to consider a subarray (the \"window\"), marked by left (l) and right (r) \npointers. We move or \"slide\" the window to the right by increasing the l and r pointers, all while computing \nsome value about the current window.2 3\n1  Beyond DS&A, the term 'sliding window' is also used in network protocols like TCP (https://en.wikipedia.org/wiki/\nTransmission_Control_Protocol) and in machine learning architectures like convolutional neural networks (https://\nen.wikipedia.org/wiki/Convolutional_neural_network).\n2  A sliding window is a special case of the two-pointer technique. Like in the Two Pointers chapter, we use the terms \n\"pointer\" and \"index\" interchangeably.\n3  Sliding windows are usually not useful for problems about subsequences because they don't have a good way of \ndealing with \"skipping\" elements. Subsequence problems are more commonly tackled with other techniques that \nwe will see later, like dynamic programming or backtracking.\n510 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\nFigure 1. Window 1 is a subarray from l = 4 (included) to r = 10 (excluded). We can slide it and \nget Window 2 by increasing l and r.\nTHE ELEMENTS OF A SLIDING WINDOW PROBLEM\nProblems where sliding windows may be useful tend to involve the following:\n•\nYou have to ! nd a subarray of an input array.\n•\nThis subarray must satisfy some constraint, which separates the subarrays into valid and invalid. Examples \nof constraints:\n»\nThe length must be k (for some given value k).\n»\nThe sum must be at least / at most / exactly k.\n»\nIt must contain or not contain speci! c elements.\n»\nIt must not contain repeated elements.\n•\nThere is usually an objective that makes some subarrays \"better\" than others. For example:\n»\nMaximize/minimize the length of the window.\n»\nMaximize/minimize the sum of the elements in the window.\n»\nMaximize/minimize the number of distinct elements in the window.\n•\nLess commonly, if there is no objective, the goal may be to count the number of valid subarrays.\nFor instance, in the ! rst bookstore problem, the constraint is \"the length of the subarray must be 7,\" and \nthe objective is to maximize the sum. In the last one, the constraint is \"at least 10 bad days,\" and there is \nno objective since it is a counting problem. Can you identify the constraints and objectives for the other \nbookstore problems?\nBRUTE FORCE BASELINE\nMost sliding window problems can be solved with a brute force algorithm that checks every subarray one \nby one. If the subarray is valid, then we check if it's the best one so far.\nThe brute force solution is correct, but we'd ideally like a more optimized solution. Before diving into how\nto do this, it can be useful to consider what our upper bound, lower bound, and target runtimes might be \n(see the Boundary Thinking chapter).\n•\nUpper bound: O(n3) will be the most common brute force upper bound across sliding window prob-\nlems, where n is the length of the input array. There are O(n2) subarrays to search through. For each of \nthose, checking whether it is valid and the best so far in a naive way could take O(n) time.\n•\nLower bound: if we don't look at every element in the input, we won't even know what some substrings \nlook like, so O(n) is the natural lower bound.\n•\nTarget: the sliding window technique often allows us to reach a linear runtime, so we should aim for that.\nCHAPTER 38 ▸ SLIDING WINDOWS \n511\n HOW TO SLIDE A WINDOW\nTo make things easy to remember, we follow some conventions for initializing and updating all sliding \nwindows in this chapter:\n1. The window goes from the element at index l (inclusive) to the element at index r (exclusive). This \nmeans:\n»\nthe window is empty when l == r,\n»\nr points to the ! rst element after the window (if any), and\n»\nthe length of the window is r - l.\n2. We always initialize l and r to 0, meaning the window starts empty.\n3. We grow the window by incrementing r. We can only grow it when r < len(arr).\n4. We shrink the window by incrementing l. We can only shrink it when l < r.\n5. We always have 0 ≤ l ≤ r ≤ len(arr).\nConsistency enables us to predict what our possible o\" -by-one errors are likely to be. For instance, r - l\nalways means \"the size of the window,\" and l == r always means \"the window is empty,\" without worrying \nabout o\" -by-one errors.4\nANALYZING SLIDING WINDOWS\nEvery sliding window consists of a main loop, where, at each iteration, we either grow or shrink the window, \nor both. Since r never decreases and runs from 0 to n, our window can only grow n times. By this same token, \nthe window can only shrink (by increasing l) n times. This means that any properly implemented sliding \nwindow does at most O(2n) = O(n) iterations. To get the total runtime, we need to multiply the number \nof iterations, O(n), by the time per iteration.\nAs long as each iteration grows or shrinks the window (or both), a sliding window algorithm takes \nO(n*T) time, where n is the size of the array we are sliding over and T is the time per iteration.\nTypically, we will be unlikely to reduce the number of iterations—we must reach the end of the array—so we \nshould focus on reducing T: the time per iteration. We should try to get it down to constant time.\nIn terms of space analysis, remember that the window is not materialized, it is just identi! ed by the two \npointers. So, the space analysis will depend on what other information about the window we need to store.\nFIXED-LENGTH WINDOWS\nIn ! xed-length window problems, we have to ! nd a subarray under the constraint that it has a given length. \nSuch problems, which are fairly common, are on the easier side because there are not many subarrays to \nconsider: for a value k in the range 1 ≤ k ≤ n,  an array only has n-k+1 = O(n) subarrays of length k—a \nlot fewer than O(n2). Recall the ! rst opening problem:\nPROBLEM 38.1 \n MOST WEEKLY SALES\nGiven an array, sales, ! nd the most sales in any 7-day period.\n▶Example:  sales = [0, 3, 7, 12, 10, 5, 0, 1, 0, 15, 12, 11, 1]\n4  Our convention is that l is inclusive and r is exclusive, but this is merely our convention. If you prefer to consider r as \ninclusive, this is equally correct, but be sure to update the little details like the size of the window (which would now \nbe r - l + 1). Whatever you do, be explicit about your convention and make sure the little details match.\n512 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\nOutput:   44. The 7-day period with the most sales is [5, 0, 1, 0, 15, 12, 11]\n▶Example:  sales = [0, 3, 7, 12]\nOutput:   0. There is no 7-day period.\nSOLUTION 38.1 \n MOST WEEKLY SALES\nThe fact that we are only looking for windows of length 7 gives us a simple strategy for when to grow and \nshrink the window:\n1. Grow the window until it has length 7.\n2. Grow and shrink at the same time so that the length stays at 7.\nHere is a full solution:\n1 \ndef most_weekly_sales(sales):\n2 \n  l, r = 0, 0\n3 \n  window_sum = 0\n4 \n  cur_max = 0\n5 \n  while r < len(sales):\n6 \n    window_sum += sales[r]\n7 \n    r += 1\n8 \n    if r - l == 7:\n9 \n      cur_max = max(cur_max, window_sum)\n10 \n      window_sum -= sales[l]\n11 \n      l += 1\n12 \n  return cur_max\nFigure 2. The sliding window of most_weekly_sales().\nOn top of our window pointers, l and r, we have:\n•\nwindow_sum: the sum of elements in the window, which corresponds to the objective we have to \nmaximize. The key is to update it whenever the window grows or shrinks and not compute it from \nscratch at each iteration.\n•\ncur_max: where we keep the current maximum we have seen so far.\nEach iteration starts by growing the window, which involves two things: updating window_sum to re% ect \nthat sales[r] is now in the window, and increasing r. The order of these operations matters!\nAfter growing the window, we check if it is valid, meaning the window length (r - l) is 7. If it is valid, we \ncheck if it is the best one seen so far and update cur_max accordingly.\nCHAPTER 38 ▸ SLIDING WINDOWS \n513\nIf the window has a length of 7, we end the iteration by shrinking it so that when we grow it in the next itera-\ntion, it will have the right size again. Like growing, shrinking consists of two actions: updating window_sum\nand increasing l.\nThe algorithm ends when the window can no longer grow (r == len(sales)).\nWe can put these ideas together in a general recipe for ! xed-length window problems: \n RECIPE 1. FIXED-LENGTH WINDOW RECIPE.\nfi xed_length_window(arr, k):\n  initialize:\n  - l and r to 0 (empty window)\n  - data structures to track window info\n  - cur_best to 0\n  while we can grow the window (r < len(arr))\n    grow the window (update data structures and increase r)\n    if the window has the correct length (r - l == k)\n      update cur_best if needed\n      shrink the window (update data structures and increase l)\n  return cur_best\nBy \"data structures,\" we mean any information about the window that we need to maintain as we slide it in \norder to evaluate each window quickly. The data structures that we need change from problem to problem, \nand they could range from nothing at all to things like sets and maps. In the following problem set, you will \nhave to consider what information to store about the window and how to update it eﬃ  ciently.\n#\nMaintaining information about the window as it slides is a key idea in designing eﬃ  cient sliding windows.\nNESTED LOOPS ARE TOO SLOW FOR SLIDING WINDOW QUESTIONS \nINTERVIEW REPLAY\nView Online:\nbctci.co/sliding-windows-replay-1 @ 10:36 - 47:30\nThe Question:\nGiven an array of positive numbers and a positive number k, ! nd the maxi-\nmum sum of any contiguous subarray of size k.\nWhat You'll See:\nThe candidate struggled to identify the problem as a sliding window prob-\nlem and coded the brute force instead of an optimal answer.\nWho:\nInterviewer: Software Engineer at FAANG+\nCandidate: College student\nFIXED-LENGTH WINDOWS PROBLEM SET\nTry these problems with AI Interviewer: bctci.co/sliding-windows-problem-set-1\nWe will continue with the bookstore setting. In addition to the sales array, we have an array of strings, \nbest_seller, with the title of the most sold book for each day.\nConstraints:\nsales and best_seller have a length of at most 106.\nEach book title in best_seller has a length of at most 100.\n514 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\nPROBLEM 38.2 \n MOST SALES IN K DAYS\nGiven the array sales and a number k with 1 ≤ k ≤ len(sales), ! nd the most sales in any k-day period. \nReturn the ! rst day of that period (days start at 0). If there are multiple k-day periods with the most sales, \nreturn the ! rst day of the ! rst one.\n▶Example:  sales = [8, 1, 3, 7], k = 2\nOutput:  2. The subarray of length 2 with maximum sum is [3, 7], which starts \nat index 2.\nPROBLEM 38.3 \n UNIQUE BEST SELLER STREAK\nGiven the array best_seller and a number k with 1 ≤ k ≤ len(sales), return whether there is any \nk-day period where each day has a di\" erent best-selling title.\n▶Example:  best_seller = [\"book3\", \"book1\", \"book3\", \"book3\", \"book2\", \"book3\",\n               \"book4\", \"book3\"], k = 3\nOutput:   True. There is a 3-day period without a repeated value: [\"book2\", \n\"book3\", \"book4\"].\n▶Example:  best_seller = [\"book3\", \"book1\", \"book3\", \"book3\", \"book2\", \"book3\",\n\"book4\", \"book3\"], k = 4\nOutput:  False. There are no 4-day periods without a repeated value.\nPROBLEM 38.4 \n ENDURING BEST SELLER STREAK\nGiven the array best_seller and a number k with 1 ≤ k ≤ len(sales), return whether there is any \nk-day period where every day has the same best-selling title.\n▶Example: best_seller = [\"book3\", \"book1\", \"book3\", \"book3\", \"book2\"], k = 3\nOutput:  False.\n▶Example: best_seller = [\"book3\", \"book1\", \"book3\", \"book3\", \"book2\"], k = 2\nOutput:  True.\nPROBLEM SET SOLUTIONS\nSOLUTION 38.2 \n MOST SALES IN K DAYS\nWe can reuse our solution to the previous problem, tweaking it slightly: replacing 7 with k and tracking the \nposition of the best window in addition to its sum.\nSOLUTION 38.3 \n UNIQUE BEST SELLER STREAK\nIn this problem, we need to check each window of length k for duplicate titles. Checking for duplicates in \nan array can be done in linear time using a hash map, assuming we can hash each element in constant time \n(recall that 'checking for duplicates' is a trigger for hash sets and maps).\nAs mentioned, eﬃ  cient sliding window algorithms usually maintain information about the window. In this \ncase, the information we need is a frequency map (the reusable idea from pg 348): a hash map from the \ntitles in the window to the number of times that they appear in the window. So, for a window like [\"book3\", \n\"book1\", \"book3\"], the map would be {\"book3\": 2, \"book1\": 1}. We can update this map in O(1)\ntime whenever the window grows or shrinks.\nCHAPTER 38 ▸ SLIDING WINDOWS \n515\nWe remove book titles from the map if their count goes back down to 0. This way, the size of the map always \nrepresents the number of unique keys (book titles) in the window, and the window satis! es the constraint \nif the map size is k. The extra space of our solution is O(k).5\n1 \ndef has_unique_k_days(best_seller, k):\n2 \n  l, r = 0, 0\n3 \n  window_counts = {}\n4 \n  while r < len(best_seller):\n5 \n    if not best_seller[r] in window_counts:\n6 \n      window_counts[best_seller[r]] = 0\n7 \n    window_counts[best_seller[r]] += 1\n8 \n    r += 1\n9 \n    if r - l == k:\n10 \n      if len(window_counts) == k:\n11 \n        return True\n12 \n      window_counts[best_seller[l]] -= 1\n13 \n      if window_counts[best_seller[l]] == 0:\n14 \n        del window_counts[best_seller[l]]\n15 \n      l += 1\n16 \n  return False\n#\nFrequency maps are often useful in sliding window problems.\nSOLUTION 38.4 \n ENDURING BEST SELLER STREAK\nThis problem can be solved exactly the same way as the previous one, just by changing the window validity \ncondition from len(window_counts) == k to len(window_counts) == 1. However, this solution \nrequires O(k) extra space for the frequency map. Can you think of a constant-space solution? We'll see one \nin the next section about the next type of sliding windows: resetting windows.\nRESETTING WINDOWS\nWe call the next type of sliding window \"resetting windows.\" It is for problems where a bigger window is \nusually better, but a single element in the array can make the whole window invalid. Our approach will be \nsimple: grow the window if we can, and otherwise reset it to empty past the problematic element.\nRecall the second opening bookstore problem:\nPROBLEM 38.5 \n LONGEST GOOD DAY STREAK\nGiven an array, sales, ! nd the most consecutive days with no bad days (fewer than 10 sales).\n▶Example: sales = [0, 14, 7, 12, 10, 20]\nOutput:  3. The subarray [12, 10, 20] has no bad days.\nSOLUTION 38.5 \n LONGEST GOOD DAY STREAK\nThis is a resetting window problem because if we encounter a bad day, whatever window we have so far \nneeds to be discarded. This gives us a simple strategy for when to grow and shrink the window:\n1. If the next day is good, grow the window.\n2. If the next day is bad, skip it and reset the window.\n5  Don't forget that when storing strings in a map, the space complexity of the map is not just the number of strings, as \nwe also need to factor in the length of the strings. For this problem, we said that all the titles would have length at \nmost 100, so the space complexity is O(100 * k) = O(k).\n516 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\n1 def max_no_bad_days(sales):\n2   l, r = 0, 0\n3   cur_max = 0\n4   while r < len(sales):\n5     can_grow = sales[r] >= 10\n6     if can_grow:\n7       r += 1\n8       cur_max = max(cur_max, r - l)\n9     else:\n10       l = r+1\n11       r = r+1 \n12   return cur_max\nFigure 3. Illustration of the sliding window for \nmax_no_bad_days().\nUnlike in the ! xed-length window case, a resetting window stays valid throughout the algorithm. We also \nintroduced a can_grow variable to decide whether to grow or reset.6 When sales[r] is a bad day, we reset \nthe window by moving both l and r past the problematic element.\nOnce the window cannot grow anymore (r == len(sales)), we stop, as we surely won't ! nd a bigger \nwindow by shrinking it.\nWe can put these ideas together in a general recipe for resetting window problems. \n RECIPE 2. RESETTING WINDOW RECIPE.\nresetting_window(arr):\n  initialize:\n  - l and r to 0 (empty window)\n  - data structures to track window info\n  - cur_best to 0\n  while we can grow the window (r < len(arr))\n    if the window is still valid with one more element\n      grow the window (update data structures and increase r)\n      update cur_best if needed\n    else\n      reset window and data structures past the problematic element\n  return cur_best\nNow that we have seen two types of sliding windows, it is worth mentioning that problems can ! t the criteria \nfor more than one window type.\nRecall Problem 38.4: \"Given the array best_seller and a number k with 1 ≤ k ≤ len(sales), return \nwhether there is any k-day period where every day has the same best-selling title.\" We can solve it with a \n! xed-length window like we saw, or with a resetting window:\n6  You could skip declaring the variable can_grow and put the condition directly in the if statement, but the name \n\"can_grow\" makes it clear what the if/else cases correspond to, so it is extra easy for the interviewer to follow.\nCHAPTER 38 ▸ SLIDING WINDOWS \n517\nWe grow the window when it is (a) empty or (b) the next title is the same as every element in the window. \nWe reset the window when the next element is di\" erent from the ones in the window. In that case, rather \nthan skipping over the element that is di\" erent; we start growing a new window from that new element.\n1 \ndef has_enduring_best_seller_streak(best_seller, k):\n2 \n  l, r = 0, 0\n3 \n  cur_max = 0\n4 \n  while r < len(best_seller):\n5 \n    can_grow = l == r or best_seller[l] == best_seller[r]\n6 \n    if can_grow:\n7 \n      r += 1\n8 \n      if r - l == k:\n9 \n        return True\n10 \n    else:\n11 \n      l = r\n12 \n  return False\nThis solution improves the extra space to O(1).\nRESETTING WINDOWS PROBLEM SET\nTry these problems with AI Interviewer: bctci.co/sliding-windows-problem-set-2\nPROBLEM 38.6 \n MAX SUBARRAY SUM\nGiven a non-empty array arr of integers (which can be negative), ! nd the non-empty subarray with the \nmaximum sum and return its sum.\n▶Example:  arr = [1, 2, 3, -2, 1]\nOutput:   6. The subarray with the maximum sum is [1, 2, 3].\n▶Example:  arr = [1, 2, 3, -2, 7]\nOutput:  11. The subarray with the maximum sum is the whole array.\n▶Example:  arr = [1, 2, 3, -8, 7]\nOutput:   7. The subarray with the maximum sum is [7].\n▶Example:  arr = [-2, -3, -4]\nOutput:  -2. The subarray cannot be empty.\nPROBLEM 38.7 \n LONGEST ALTERNATING SEQUENCE\nGiven the array sales, ! nd the longest sequence of days alternating between good days (at least 10 sales) \nand bad days (fewer than 10 sales).\n▶Example:  sales = [8, 9, 20, 0, 9]\nOutput:  3. The only good day is day 2, so the subarray [9, 20, 0] alternates \nfrom bad to good to bad.\n▶Example:  arr = [0, 0, 0]\nOutput:  1. Every day is bad, so we cannot find any pair of consecutive days \nthat alternate.\n518 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\nPROBLEM SET SOLUTIONS\nSOLUTION 38.6 \n MAX SUBARRAY SUM\nThis is such a classic problem that the resetting window algorithm for it has its own name: Kadane's algorithm.\nLet's consider the logic for when to grow and shrink our window. If we encounter a positive number, we \nde! nitely want to grow the window, as it makes the window sum bigger. If we encounter a negative number, \nshould we keep it and keep growing (as in Example 2), or should we reset the window past it (as in Example \n3)? The answer depends on the sum of the window elements so far: \n•\nIf our current window plus the negative element is still positive, it is worth keeping the current window \neven with the negative element. \n•\nIf the negative element makes the window sum negative, it is not worth keeping; we should reset it.7\n1 \ndef max_subarray_sum(arr):\n2 \n  max_val = max(arr)\n3 \n  if max_val <= 0: # Edge case without positive values.\n4 \n    return max_val\n5 \n  l, r = 0, 0\n6 \n  window_sum = 0\n7 \n  cur_max = 0\n8 \n  while r < len(arr):\n9 \n    can_grow = window_sum + arr[r] >= 0\n10 \n    if can_grow:\n11 \n      window_sum += arr[r]\n12 \n      r += 1\n13 \n      cur_max = max(cur_max, window_sum)\n14 \n    else:\n15 \n      window_sum = 0\n16 \n      l = r+1\n17 \n      r = r+1\n18 \n  return cur_max\nSOLUTION 38.7 \n LONGEST ALTERNATING SEQUENCE\nThis is a resetting window problem because if we ! nd two consecutive days that are both good or both bad, \nthe whole window becomes invalid and we need to reset it (starting from the second of the two consecutive \nelements of the same type). We can grow the window when (a) it is empty or (b) the next element does not \nbreak the \"chain\" of alternating days ((sales[r - 1] < 10) != (sales[r] < 10)).\nMAXIMUM WINDOWS\nWe are going to tackle general maximization problems (maximum length, maximum sum, etc.) with what \nwe call maximum windows. Maximum windows grow when they can and shrink when they must. They are \nsimilar to resetting windows, but when we encounter an element that makes the window invalid, we don't \ndiscard the whole window and reset it. Instead, we shrink it element by element (by increasing l) until it \nbecomes valid again.\n7  In both cases, we need to increment r. You can see in our implementation that we could factor out that increment \noutside of the if/else cases and save a line of code. However, as we mentioned on page 297, when trying to come \nup with a valid solution, it is easier to think about each case independently ! rst—shared code between the cases can \nmake it harder to reason about the correctness of your code. If you have time, once you are sure your code is correct, \nyou can do a 'clean-up' pass to tidy up the code.\nCHAPTER 38 ▸ SLIDING WINDOWS \n519\nRecall the third opening bookstore problem:\nPROBLEM 38.8 \n MAXIMUM WITH AT MOST 3 BAD DAYS\nGiven an array sales, ! nd the most consecutive days with at most 3 bad days (fewer than 10 sales).\n▶Example: sales = [0, 14, 7, 9, 0, 20, 10, 0, 10]\nOutput:  6. There are two 6-day periods with at most 3 bad days, \n[14, 7, 9, 0, 20, 10] and [9, 0, 20, 10, 0, 10].\nSOLUTION 38.8 \n MAXIMUM WITH AT MOST 3 BAD DAYS\nWe can follow this strategy for when to grow and shrink the window:\n1. If the next day is good or the window contains fewer than 3 bad days, grow the window.\n2. Otherwise, shrink it.\nHere is a full solution:\n1 \ndef max_at_most_3_bad_days(sales):\n2 \n  l, r = 0, 0\n3 \n  window_bad_days = 0\n4 \n  cur_max = 0\n5 \n  while r < len(sales):\n6 \n    can_grow = sales[r] >= 10 or window_bad_days < 3\n7 \n    if can_grow:\n8 \n      if sales[r] < 10:\n9 \n        window_bad_days += 1\n10 \n      r += 1\n11 \n      cur_max = max(cur_max, r - l)\n12 \n    else:\n13 \n      if sales[l] < 10:\n14 \n        window_bad_days -= 1\n15 \n      l += 1\n16 \n  return cur_max\nFigure 4. Sliding window for max_at_most_3_bad_days().\n520 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\nMany elements of the solution should look similar to the resetting window recipe, like the can_grow variable. \nThe only new part is what happens when we cannot grow the window. We remove only the ! rst element in \nthe window (sales[l]).\nHere is a recipe for maximum windows. Note how, before shrinking the window, we need to check the case \nwhere the window is empty (l == r)—an empty window cannot be shrunk! For many problems, an empty \nwindow is always valid, so we can omit this check (for example, in this problem, an empty window is always \nvalid because it has 0 bad days).\n RECIPE 3. MAXIMUM WINDOW RECIPE.\nmaximum_window(arr):\n  initialize:\n  - l and r to 0 (empty window)\n  - data structures to track window info\n  - cur_best to 0\n  while we can grow the window (r < len(arr))\n    if the window would still be valid with one more element\n      grow the window (update data structures and increase r)\n      update cur_best if needed\n    else if the window is empty\n      advance both l and r\n    else\n      shrink the window (update data structures and increase l)\n  return cur_best\nMAXIMUM WINDOWS PROBLEM SET\nTry these problems with AI Interviewer: bctci.co/sliding-windows-problem-set-3\nFollow the maximum window recipe to tackle the following questions.\nPROBLEM 38.9 \n AD CAMPAIGN BOOST\nImagine that our little bookstore has an array, projected_sales, with the projected number of sales per \nday in the future. We are trying to pick k days for an advertising campaign, which we expect to boost the \nsales on those speci! c days by at least 20. If we pick the days for the advertising campaign correctly, what is \nthe maximum number of consecutive good days in a row we can get? (Recall that a good day is a day with \nat least 10 sales.)\n▶Example: projected_sales = [5, 0, 20, 0, 5], k = 2\nOutput:  3. The only good day is day 2. We can boost days 0 and 1, days 1 \nand 3, or days 3 and 4. For instance, if we boost days 0 and 1, the \nprojected sales become [25, 20, 20, 0, 5], with 3 consecutive good \ndays.\n▶Example: arr = [0, 10, 0, 10], k = 1\nOutput:  3. We can boost day 2; boosting day 0 is suboptimal.\nPROBLEM 38.10 \n AD CAMPAIGN WITH SMALL BOOSTS\nIn the previous problem, what would change if the boost from the advertising campaign was only 5 books \ninstead of 20? You cannot boost the same day more than once. What is the maximum number of consecutive \ngood days in a row we can get?\nCHAPTER 38 ▸ SLIDING WINDOWS \n521\n▶Example:  projected_sales = [8, 4, 8], k = 3\nOutput:  1. We can boost all 3 days, resulting in [13, 9, 13] projected \nsales. The max consecutive good days is 1.\n▶Example: projected_sales = [10, 5, 8], k = 1\nOutput:  2. We should boost day 1, resulting in [10, 10, 8] projected sales.\nPROBLEM 38.11 \n BOOSTING DAYS MULTIPLE TIMES\nIn Problem 38.9, what would change if the boost from the advertising campaign was only 1 book instead \nof 20, but you can boost the same day more than once? What is the maximum number of consecutive good\ndays in a row we can get?\n▶Example:  projected_sales = [5, 5, 15, 0, 10], k = 12\nOutput:  3. We can reach 3 consecutive good ways in two ways: boosting days 0 \nand 1, so both reach 10 sales, or boosting day 3.\n▶Example:  projected_sales = [5, 5, 15, 0, 10], k = 15\nOutput:  4. We can boost days 1 and 3.\nPROBLEM 38.12 \n LONGEST PERIOD AT-MOST K DISTINCT\nGiven an array of strings, best_seller, that lists the title of the most sold book for each day, and a number \nk ≥ 1, ! nd the maximum consecutive days with at most k distinct best-selling books.\n▶Example: projected_sales = [\"book1\", \"book1\", \"book2\", \"book1\", \"book3\", \n\"book1\"], k = 2\nOutput:  4. The subarray [\"book1\", \"book1\", \"book2\", \"book1\"] contains only 2 \ndistinct titles.\nConstraints: best_seller has a length of at most 106, and each book title in \nbest_seller has a length of at most 100.\nPROBLEM SET SOLUTIONS\nSOLUTION 38.9 \n AD CAMPAIGN BOOST\nThis problem introduces a new dimension: we need to make choices that \"modify\" the window that we are \nsliding over. This seems complicated at ! rst since there could be many choices.\nA naive solution would be to consider all possible sets of k days we could pick, but that would be very \nineﬃ  cient.8 The key for this type of problem is usually to use the REFRAME THE PROBLEM booster. We want to \n! nd a way to reframe it in a way that eliminates the choice aspect. For our problem, instead of choosing k days \nto turn from bad to good, we can look for the longest window with at most k bad days because we can pick \nthose bad days and turn them into good days.\nWith this reframing, the question becomes just like Problem 38.8, which we solved previously, but with a \ngeneric limit of k bad days instead of 3.\n#\nWhen a problem asks you to choose k elements to change (or % ip, remove, etc.), the problem can often \nbe reframed in terms of ! nding a window with at most k elements that need to be changed.\n8  If k is a constant, the number of subsets of size k, denoted (n choose k), is O(nk). The worst case is when k is n/2, \nas (n choose n/2) = O(2n/√n). Once k gets larger than n/2, the number of possibilities starts decreasing. For \ninstance, there are only n subsets of size n-1.\n522 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\nSOLUTION 38.10  AD CAMPAIGN WITH SMALL BOOSTS\nWe can REFRAME THE PROBLEM as: \"!nd the longest window with at most k values between 5 and 9 and 0 \nvalues less than 5.\" Then, it becomes a standard maximum window problem.\nSOLUTION 38.11  BOOSTING DAYS MULTIPLE TIMES\nFirst, we never want to boost a day beyond 10 projected sales, since we only care about the day being 'good.' \nThe 'cost' of turning a day with x sales into a good day is max(10-x, 0). Thus, we can REFRAME THE PROBLEM \nas: \"Find the longest window where the sum of max(10-x, 0) over each element x in the window is at \nmost k.\" Then, it becomes a standard maximum window problem.\n1 \ndef max_consecutive_with_k_boosts(projected_sales, k):\n2 \n  l, r = 0, 0\n3 \n  used_boosts = 0\n4 \n  cur_max = 0\n5 \n  while r < len(projected_sales):\n6 \n    can_grow = used_boosts + max(10 - projected_sales[r], 0) <= k\n7 \n    if can_grow:\n8 \n      used_boosts += max(10 - projected_sales[r], 0)\n9 \n      r += 1\n10 \n      cur_max = max(cur_max, r - l)\n11 \n    elif l == r:\n12 \n      r += 1\n13 \n      l += 1\n14 \n    else:\n15 \n      used_boosts -= max(10 - projected_sales[l], 0)\n16 \n      l += 1\n17 \n  return cur_max\nSOLUTION 38.12  LONGEST PERIOD AT-MOST K DISTINCT\nWe need to keep track of the number of distinct books in the window. Again, we can use a frequency map \nfrom book titles in the window to their number of occurrences. When we shrink the window, if a count goes \ndown to 0, we remove the corresponding key from the map. This way, the size of the map re%ects the number \nof distinct elements in the window, and the window is valid if the map's size is at most k.\n1 \ndef max_at_most_k_distinct(best_seller, k):\n2 \n  l, r = 0, 0\n3 \n  window_counts = {}\n4 \n  cur_max = 0\n5 \n  while r < len(best_seller):\n6 \n    can_grow = best_seller[r] in window_counts or len(window_counts) + 1 <= k\n7 \n    if can_grow:\n8 \n      if not best_seller[r] in window_counts:\n9 \n        window_counts[best_seller[r]] = 0\n10 \n      window_counts[best_seller[r]] += 1\n11 \n      r += 1\n12 \n      cur_max = max(cur_max, r - l)\n13 \n    else:\n14 \n      window_counts[best_seller[l]] -= 1\n15 \n      if window_counts[best_seller[l]] == 0:\n16 \n        del window_counts[best_seller[l]]\n17 \n      l += 1\n18 \n  return cur_max\nCHAPTER 38 ▸ SLIDING WINDOWS \n523\nLIMITATIONS OF MAXIMUM WINDOWS\nAll the problems we solved with a maximum window (as well as a resetting window) have this property: \nGrowing an invalid window never makes it valid.\n We call this the maximum window property, and it is critical—without it, the maximum window recipe may \nnot work. The intuition is that, without it, we may have to grow through invalid solutions in order to get to \nthe optimal one, making it hard to know when to grow or shrink.\nFor example, consider a simpli! ed version of Problem 43.7: Longest Subarray With Sum K (pg 618): Given \nan array of integers, which may be negative, return if any subarray adds up to 0.\nThis problem looks like a maximum window problem because it asks for the longest subarray satisfying a \nconstraint, but we can't follow the typical maximum window policy of \"grow when you can, shrink when \nyou must.\"\nFor instance, if the input starts with [1, 4, -2, -2, 5, ...] and we (somehow) grow the window up to \n[1, 4, -2, -2], should we shrink it in order to ! nd a valid window of length 3, or keep growing because \nthere may be a longer solution with the initial 1, like [1, 4, -2, -2, 5, -6]? It is impossible to say.\nFor problems without the maximum window property, it is better to ditch the sliding window approach \nentirely and think of di\" erent approaches. A linear-time algorithm may also be less realistic (although it is \npossible for this particular problem using pre! x sums, pg 618).\n#\nSliding windows often don't work with negative values.9\nFinally, there are also problems that have the maximum window property, meaning that the maximum \nwindow recipe gives the optimal answer, but it is just really hard to implement eﬃ  ciently. Here is an example \nof a classic problem:\nPROBLEM 38.13 \n LONGEST REPEATED SUBSTRING\nGiven a string, s, return the longest substring that appears more than once in s (overlapping is allowed) or \nthe empty string if there is none.\n▶Example:  s = \"murmur\"\nOutput:  \"mur\"\n▶Example:  s = \"murmurmur\"\nOutput:  \"murmur\"\n▶Example:  s = \"aaaa\"\nOutput:  \"aaa\"\nSOLUTION 38.13  LONGEST REPEATED SUBSTRING\nThis can be seen as a maximum window problem because we are looking for the longest window with some \nproperty. Further, it has the maximum window property: if a substring is not repeated, it won't suddenly \nbecome repeated if we make it longer.\nThe challenge for this problem is assessing whether a substring is valid or not is not easy because it depends \non what is outside the window instead of what is inside of it.\nFor problems like this, where the maximum window recipe works but is hard to implement eﬃ  ciently, we \nrecommend trying something other than sliding windows.\n9  Kadane's algorithm (Solution 38.6: Max Subarray Sum (pg 518) for the maximum subarray sum problem is an excep-\ntion.\n524 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\nFor this particular problem, we can try the guess-and-check technique we learned in the Binary Search \nchapter (pg 338). That is, we can try to binary search over the length of the optimal window. This approach \nstarts by asking: \"If I somehow knew the length of the optimal window, would that make the problem \neasier?\" The answer is often yes because then we can use the ! xed-length window recipe, which is the most \nstraightforward one. In this case, the ! xed-length window version of the problem is \"For a given k, is there a \nsubstring of length k that appears more than once?\" If we can solve this eﬃ  ciently, we can then binary search \nfor the transition point in the range of values of k where the answer goes from \"yes\" to \"no\". The ! xed-length \nwindow version can be solved in O(n) time using a rolling hash (bctci.co/set-and-map-implementations, \nRolling Hash Algorithm section), leading to O(n log n) total time.\n#\nWhen a problem has the maximum window property, but you cannot ! nd an eﬃ  cient way to check \nif the window is valid or evaluate the window, consider using the guess-and-check technique. For an \n'extra' factor of O(log n) in the runtime, it turns the problem into a potentially easier ! xed-length \nwindow problem.\nMINIMUM WINDOWS\nMinimum window problems are the opposite of maximum window problems. We try to ! nd a window as \nshort as possible, but the constraint restricts how small valid windows can be. We'll use 'minimum windows', \nwhich grow when they must and shrink when they can.\nRecall the fourth opening bookstore problem:\nPROBLEM 38.14 \n SHORTEST PERIOD WITH OVER 20 SALES\nGiven an array, sales, return the length of the shortest period of time with over 20 sales, or -1 if there isn't \nany.\n▶Example:  sales = [5, 10, 15, 5, 10]\nOutput:  2. The subarray [10, 15] has over 20 sales.\n▶Example:  sales = [5, 10, 4, 5, 10]\nOutput:  4. [5, 10, 4, 5] and [10, 4, 5, 10] have over 20 sales.\n▶Example:  sales = [5, 5, 5, 5]\nOutput:  -1. There is no subarray with more than 20 sales.\nSOLUTION 38.14  SHORTEST PERIOD WITH OVER 20 SALES\nThis is a minimum window problem because we are trying to ! nd a window as short as possible. \nFor minimum window problems, the empty window is invalid (in this problem, because it has fewer than 20\nsales). We need to grow it until it becomes valid, similar to how we did for ! xed-length window problems. \nWe can follow this strategy for when to grow and shrink the window:\n1. If the window has 20 sales or fewer: grow it.\n2. Otherwise, shrink it to look for a shorter one with over 20 sales.\nCHAPTER 38 ▸ SLIDING WINDOWS \n525\n1 def shortest_over_20_sales(sales):\n2   l, r = 0, 0\n3   window_sum = 0\n4   cur_min = math.inf\n5   while True:\n6     must_grow = window_sum <= 20\n7     if must_grow:\n8       if r == len(sales):\n9         break\n10       window_sum += sales[r]\n11       r += 1\n12     else:\n13       cur_min = min(cur_min, r - l)\n14       window_sum -= sales[l]\n15       l += 1\n16   if cur_min == math.inf:\n17     return -1\n18   return cur_min\nFigure 5. Illustration of the sliding window of short-\nest_over_20_sales().\nUnlike the other recipes, we initialize the result (cur_min) to in! nity because we update it by taking the \nminimum. At the end, we need to check if it is still in! nity, which means that we didn't ! nd any valid windows. \nIt is a common mistake to forget this ! nal check!\nIn the main loop, we start each iteration by declaring a variable must_grow (instead of can_grow for maxi-\nmum windows) which indicates if the current window is invalid. If we must grow, there is one edge case to \nconsider: if r == len(sales), we ran out of elements to grow, so we break out of the loop. We have this \nedge case for minimum windows but not maximum windows because the while-loop condition is di\" erent: \nwe don't stop as soon as r gets to the end because it might still be possible to make the window smaller \nand get a better answer.\nIf must_grow is false, then we have a valid window, so we update the current minimum ﬁ rst and then shrink \nthe window to see if we can make it even smaller.\nWe can put these ideas together in a general recipe for minimum window problems. \n526 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\n RECIPE 4. MINIMUM WINDOW RECIPE.\nminimum_window(arr):\n  initialize:\n  - l and r to 0 (empty window)\n  - data structures to track window info\n  - cur_best to infi nity\n  while true\n    if the window must grow to become valid\n      if the window cannot grow (r == len(arr))\n        break\n      grow the window (update data structures and increase r)\n    else\n      update cur_best if needed\n      shrink the window (update data structures and increase l)\n  return cur_best\n Recall that maximum windows only work for problems that have what we call the maximum window \nproperty? For the minimum window recipe to work, we need the opposite property: Shrinking an invalid \nwindow never makes it valid. We call this the minimum window property. It means that if the current \nwindow is invalid, we de! nitely need to grow it.\nSUCCESSFULLY SOLVES MINIMUM SLIDING WINDOW PROBLEM \nINTERVIEW REPLAY\nView Online:\nbctci.co/sliding-windows-replay-2 @ 4:15 - 43:23\nThe Question:\nFind the smallest substring in s containing all characters of t (including \nduplicates).\nWhat You'll See:\nThe candidate successfully applied a minimum sliding window solution.\nWho:\nInterviewer: Sta\"  Software Engineer at Meta\nCandidate: 1 year exp.\nOutcome:\nThe candidate got the job at Amazon!\nMINIMUM WINDOWS PROBLEM SET\nTry these problems with AI Interviewer: bctci.co/sliding-windows-problem-set-4\nPROBLEM 38.15 \n SHORTEST WITH ALL LETTERS\nGiven a string, s1, and a shorter but non-empty string, s2, return the length of the shortest substring of \ns1 that has every letter in s2 (as many times as they appear in s2). If there is no such substring, return -1.\n▶Example:  s1 = \"helloworld\", s2 = \"well\"\nOutput:  5. The substring \"ellow\" in s1 has all the letters in s2.\n▶Example:  s1 = \"helloworld\", s2 = \"weelll\"\nOutput:  -1. s1 does not have 2 e's.\nPROBLEM 38.16 \n SMALLEST RANGE WITH K ELEMENTS\nGiven an array of integers, arr, and an integer k with 1 ≤ k ≤ len(arr), return a pair of values, [low, \nhigh], with low ≤ high, representing the smallest range such that there are at least k elements in arr\nwith values at least low and at most high. If there are multiple valid answers, return any of them.\nCHAPTER 38 ▸ SLIDING WINDOWS \n527\n▶Example:  arr = [1, 2, 5, 7, 8], k = 3\nOutput:  [5, 8]. The range has 3 elements in arr (5, 7, and 8) and it is \nsmaller than other ranges with 3 elements, such as [1, 5].\n▶Example:  arr = [5, 5, 2, 2, 8, 8], k = 3\nOutput:  [2, 5]. The range has 4 elements in arr (5, 5, 2, and 2) and there \nis no smaller range with at least 3 elements. [5, 8] is also a valid \nanswer.\n▶Example:  arr = [0], k = 1\nOutput:  [0, 0].\nPROBLEM 38.17 \n STRONG START AND ENDING\nWe have an array, projected_sales, with the number of book sales we expect each day of the fall season. \nWe would like to start and close the season strong. We want to have as many consecutive good days as \npossible starting from day 0 and as many consecutive good days as possible ending on the last day (a good \nday is a day with at least 10 sales). We can pick k days to boost with advertising, which we expect to boost \nthe sales on those speci! c days by at least 20. What's the maximum number of combined initial good days \nand ! nal good days we can have?\n▶Example:  projected_sales = [10, 0, 0, 0, 10, 0, 0, 10], k = 2\nOutput:  5. We should boost days 5 and 6 so that the projected sales after \nboosting are [10, 0, 0, 0, 10, 20, 20, 10]. This way, we have 1 \ninitial and 4 final good days.\n▶Example:  arr = [0, 10, 0, 10], k = 1\nOutput:  3. We can boost either day 0 or day 2.\nPROBLEM SET SOLUTIONS\nSOLUTION 38.15  SHORTEST WITH ALL LETTERS\nThis is a minimum window problem since we have to minimize the window length. A window is valid if the \ncount for each letter is at least as big as the count in s2.\nThe key information we need to maintain about the window is a frequency map counting how many times \neach letter from s2 is missing. We can also keep a separate count of the number of distinct letters that are \nmissing. When this counter is at 0, the window is valid.\n1 \ndef shortest_with_all_letters(s1, s2):\n2 \n  l, r = 0, 0\n3 \n  missing = {}\n4 \n  for c in s2:\n5 \n    if not c in missing:\n6 \n      missing[c] = 0\n7 \n    missing[c] += 1\n8 \n  distinct_missing = len(missing)\n9 \n  cur_min = math.inf\n10 \n  while True:\n11 \n    must_grow = distinct_missing > 0\n12 \n    if must_grow:\n13 \n      if r == len(s1):\n14 \n        break\n15 \n      if s1[r] in missing:\n16 \n        missing[s1[r]] -= 1\n528 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\n17 \n        if missing[s1[r]] == 0:\n18 \n          distinct_missing -= 1\n19 \n      r += 1\n20 \n    else:\n21 \n      cur_min = min(cur_min, r - l)\n22 \n      if s1[l] in missing:\n23 \n        missing[s1[l]] += 1\n24 \n        if missing[s1[l]] == 1:\n25 \n          distinct_missing += 1\n26 \n      l += 1\n27 \n  return cur_min if cur_min != math.inf else -1\nSOLUTION 38.16  SMALLEST RANGE WITH K ELEMENTS\nThis is an interesting problem because we are not looking for a subarray of the input but rather a window \non the values in the array. Nonetheless, we can still use a minimum window to !nd this range.\nFirst, we sort the input since we do not care about the original order, and it will help us !nd values in arr \nthat are close together. After sorting, the problem can be REFRAMED as follows:\nFind the window containing at least k elements, minimizing the di\"erence between its maximum and \nminimum. Now that we have a constraint and an objective for the window, this is a standard minimum \nwindow problem.\nThanks to sorting, the minimum and maximum in the window are easy to calculate as they are the !rst and \nlast elements. Sorting is the runtime bottleneck in this solution, so it takes O(n log n) time instead of \nO(n) as usual.\n1 \ndef smallest_range_with_k_elements(arr, k):\n2 \n  arr.sort()\n3 \n  l, r = 0, 0\n4 \n  best_low, best_high = 0, math.inf\n5 \n  while True:\n6 \n    must_grow = (r - l) < k\n7 \n    if must_grow:\n8 \n      if r == len(arr):\n9 \n        break\n10 \n      r += 1\n11 \n    else:\n12 \n      if arr[r - 1] - arr[l] < best_high - best_low:\n13 \n        best_low, best_high = arr[l], arr[r - 1]\n14 \n      l += 1\n15 \n  return [best_low, best_high]\nSOLUTION 38.17  STRONG START AND ENDING\nIt is not obvious at all how a problem about maximizing pre!x and suﬃx lengths is related to minimum \nwindows, but we will show a clever trick to REFRAME THE PROBLEM into a minimum window problem.\nWhat is between a pre!x and a suﬃx? A window! Finding a pre!x and a suﬃx is the twin problem of !nding \nthe subarray between them. The next question is: \"If we look for a subarray instead of for a pre!x and a suﬃx, \nwhat property should the subarray have?\"\nLet's say projected_sales has B bad days in total. We know we can %ip k of them into good days, so we \nwill end up with B - k bad days (if B - k is 0 or negative, we can convert all bad days into good days, so \nCHAPTER 38 ▸ SLIDING WINDOWS \n529\nthe answer is the length of the input array). If we can ! nd the smallest window with B - k bad days, then \nwe can % ip every bad day outside the window and maximize the pre! x and suﬃ  x without bad days.\nWith this reframing, the problem becomes a standard minimum window problem.\nEXTRA CREDIT: COUNTING PROBLEMS\nBy this point, we have seen many sliding window problems in which the constraint that the window must \nsatisfy is of the form \"at most/at least/exactly k of something.\" In this section, we talk about how to count \nthe number of subarrays under a constraint like that, including the ! nal three opening bookstore problems.\nAT-MOST-K COUNTING\nPROBLEM 38.18 \n COUNT SUBARRAYS WITH AT MOST K BAD DAYS\nGiven an array, sales, count the number of subarrays with at most k bad days (days with fewer than 10 sales).\n▶Example:  sales = [0, 20, 5], k = 1 \nOutput:  5. [20] has 0 bad days, and [0], [0, 20], [20, 5], and [5] have 1 \nbad day each.\nSOLUTION 38.18  COUNT SUBARRAYS WITH AT MOST K BAD DAYS\nWe can leverage an interesting property about the maximum window recipe: if a problem has the maximum \nwindow property (pg 523), whenever we grow the window by adding an element arr[r - 1], the new \nwindow is the longest valid window that ends at arr[r - 1]. This means that the valid subarrays ending \nat arr[r - 1] are those starting at arr[l], arr[l + 1], and so on, up to arr[r - 1] itself. Thus, there \nare r - l valid subarrays ending arr[r - 1].\nAs we saw in the Problem-Solving Boosters chapter, we can often leverage properties into algorithmic ideas. \nIn this case, we can follow the maximum window recipe—as if we were trying to ! nd the longest window \nwith at most k bad days—and whenever we grow the window by adding an element arr[r - 1], we add \nr - l to a running count of valid subarrays.\nHere is a full solution based on this idea:\n530 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\n1 def count_at_most_k_bad_days(sales, k):\n2   l, r = 0, 0\n3   window_bad_days = 0\n4   count = 0\n5   while r < len(sales):\n1     can_grow = sales[r] >= 10 or\n2                window_bad_days < k\n3     if can_grow:\n4       if sales[r] < 10:\n5         window_bad_days += 1\n6       r += 1\n7       count += r - l\n8     else:\n9       if sales[l] < 10:\n10         window_bad_days -= 1\n11       l += 1\n12   return count\nFigure 6. Sliding window for count_at_most_k_bad_days(sales, 2). Every time we grow the \nwindow, we show all the valid subarrays that we add to the running count (16 in total).\nNote that the code is exactly the same as max_at_most_3_bad_days() from the previous section except \nfor computing count instead of cur_max (and k instead of 3).\n#\n If a problem has the maximum window property, the maximum window recipe also works for At-Most-K \ncounting problems. Whenever we add an element to the window, we add to the running count all the \nvalid subarrays ending at that element.\nFortunately, At-Most-K counting problems are a bit of a 'freebie' because we can reuse the maximum window \nrecipe. In the next section, we'll see a trick that makes Exactly-K counting problems equally easy!\nAs mentioned, this only works if the problem has the maximum window property. For instance, if we allow \nnegative numbers in the array in this problem, we cannot use the algorithm anymore.\nEXACTLY-K COUNTING\nPROBLEM 38.19 \n COUNT SUBARRAYS WITH EXACTLY K BAD DAYS\nGiven an array, sales, count the number of subarrays with exactly k bad days (days with fewer than 10 sales).\n▶Example:  sales = [0, 20, 5], k = 1 \nOutput:  4. The subarrays [0], [0, 20], [20, 5], and [5] have 1 bad day each.\nSOLUTION 38.19  COUNT SUBARRAYS WITH EXACTLY K BAD DAYS\nThis time, we will start with the solution and then break it down:\n1 \ndef count_exactly_k_bad_days(sales, k):\n2 \n  if k == 0:\n3 \n    return count_at_most_k_bad_days(sales, 0)\n4 \n  return count_at_most_k_bad_days(sales, k) -\nCHAPTER 38 ▸ SLIDING WINDOWS \n531\n5 \n           count_at_most_k_bad_days(sales, k-1)\nWe re-used the At-Most-K counting function count_at_most_k_bad_days() from the previous section.\nFor k == 0, 'at most 0' and 'exactly 0' are the same. For k == 1, the code works because the number of \nsubarrays with exactly 1 bad day is equal to the number of subarrays with at most 1 bad day (meaning 0\nor 1 bad days) minus the number of subarrays with at most 0 bad days. This applies to any higher k as well!\nFor example, Figure 7 shows an example sales array and all 16 subarrays with at most 2 bad days. Of those, \n10 have at most 1 bad day, meaning there are 16 - 10 = 6 subarrays with exactly 2 bad days. By the same \nlogic, there are 10 - 2 = 8 subarrays with exactly 1 bad day.\nFigure 7. The set of all subarrays with at most 2 bad days, which contains the set of subarrays with \nat most 1 bad day, which contains the set of subarrays with no bad days.\nSince we need to make two calls to the At-Most-K solution, the runtime is twice as long, which is still O(n).\nExactly-K counting problems can also be solved in linear time with pre! x sums: see Problem 38.6: Max \nSubarray Sum (pg 517). The pre! x-sums approach works even if the array has negative numbers.\nAT-LEAST-K COUNTING\nPROBLEM 38.20 \nCOUNT SUBARRAYS WITH AT LEAST K BAD DAYS\nGiven an array, sales, count the number of subarrays with at least k bad days (days with fewer than 10 sales).\n▶Example:  sales = [0, 20, 5], k = 1 \nOutput:  5. The subarrays [0], [0, 20], [20, 5], and [5] have 1 bad day each, \nand the subarray [0, 20, 5] has 2.\nSOLUTION 38.20  COUNT SUBARRAYS WITH AT LEAST K BAD DAYS\nWe will see how to reuse the At-Most-K counting solution once again. First, we need to know the total \nnumber of subarrays.\n What's the total number of subarrays?\nAn array of length n has (n+1)*n/2 non-empty subarrays.\nEach subarray is de! ned by a pair of indices, [start, end]. There are n2 [i, j] pairs where i\nand j are valid indices (0 ≤ i, j < n). Of those, there are n pairs where i == j, which de! ne \nsingle-element subarrays. Of the remaining n2 - n pairs, half have i < j and half have i > j. \n532 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\nThe latter half does not identify valid subarrays, so we don't count them. In total, we have n + \n(n2 - n)/2 = (n+1)*n/2 subarrays.\nThe subarrays with at least k bad days are those with k bad days, those with k+1 bad days, those with k+2\nbad days, and so on. Therefore, to count the subarrays with at least k bad days, we can start with the count \nof all subarrays (of which there are n*(n+1)/2) and subtract the number of subarrays with at most k-1 bad \ndays. Something we already saw how to compute!\n1 \ndef count_at_least_k_bad_days(sales, k):\n2 \n  n = len(sales)\n3 \n  total_subarrays = n*(n+1)//2\n4 \n  if k == 0:\n5 \n    return total_subarrays \n6 \n  return total_subarrays - count_at_most_k_bad_days(sales, k-1)\n#\nIf the At-Most-K version of a counting problem has the maximum window property, it can be reused to \nsolve the At-Least-K version.\n REUSABLE IDEA: TRANSFORM EXACTLY-K/AT-LEAST-K COUNTING TO AT-MOST-K COUNTING\nIf the At-Most-K version of a counting problem has the maximum window property, it can be reused to \nsolve the Exactly-K and At-Least-K versions.\n1. 'Exactly k' is equivalent to 'at most k' minus 'at most k - 1'.\n2. 'At Least k' is equivalent to 'total count' (n*(n+1)/2) minus 'at most k - 1'.\nThe At-Most-K version can be solved by tweaking the maximum window template, as in Solution 18.\nThere are counting problems that do not ! t into any of the At-Most-K / Exactly-K / At-Least-K categories. We \nshould tackle such problems on a case-by-case basis. Here is an example:\nPROBLEM 38.21 \n COUNT SUBARRAYS WITH GOOD START AND ENDING\nGiven an array, sales, return the number of subarrays that start and end on a good day (a day with at least \n10 sales).\nSOLUTION 38.21  COUNT SUBARRAYS WITH GOOD START AND ENDING\nFor this particular problem, the key property to leverage is that each distinct pair of good days in sales\ncontributes 1 to the ! nal count. Thus, the answer is g*(g + 1)/2, where g is the number of good days in \nsales.\nCOUNTING PROBLEM SET\nTry these problems with AI Interviewer: bctci.co/sliding-windows-problem-set-5\nPROBLEM 38.22 \n COUNT SUBARRAYS WITH DROPS\nGiven an array of integers, arr, and an integer k, count how many subarrays have (1) at most k drops, (2) \nexactly k drops, and (3) at least k drops. A drop is a sequence of two consecutive numbers where the ! rst is \nlarger than the second.\n▶Example: arr = [1, 2, 3], k = 1\nOutput: (1) 6. The array has no drops, so every subarray has 0 drops.\nCHAPTER 38 ▸ SLIDING WINDOWS \n533\n(2) 0. The array has no drops.\n(3) 0. The array has no drops.\n▶Example: arr = [3, 2, 1], k = 1\nOutput: (1) 5. [3, 2] and [2, 1] have 1 drop and [3], [2], and [1] have 0 \ndrops.\n(2) 2. [3, 2] and [2, 1] have exactly 1 drop.\n(3) 3. [3, 2] and [2, 1] have 1 drop and [3, 2, 1] has 2 drops.\nPROBLEM 38.23 \n COUNT SUBARRAYS WITH BAD DAYS IN RANGE\nGiven the array sales and two numbers k1 and k2 with 0 ≤ k1 ≤ k2, count the number of subarrays with \nat least k1 bad days and at most k2 bad days (days with fewer than 10 sales).\n▶Example:  sales = [0, 20, 5], k1 = 2, k2 = 2 \nOutput:  1. [0, 20, 5] has 2 bad days.\n▶Example: sales = [0, 20, 5], k1 = 1, k2 = 2 \nOutput:  5. [0, 20, 5] has 2 bad days, and [0], [0, 20], [20, 5], and [5] \nhave 1 bad day.\nPROBLEM 38.24 \n COUNT SUBARRAYS WITH ALL REMAINDERS\nGiven an array of positive integers, arr, return the number of subarrays that have at least one of each of \nthe following:\n•\na multiple of 3,\n•\na number with remainder 1 when divided by 3, and\n•\na number with remainder 2 when divided by 3.\n▶Example:  arr = [9, 8, 7]\nOutput:  1. [9, 8, 7] counts because 9 % 3 is 0, 7 % 3 is 1, and 8 % 3 is 2. \n▶Example:  arr = [1, 2, 3, 4, 5]\nOutput:  6. The subarrays are [1, 2, 3], [2, 3, 4], [3, 4, 5], [1, 2, 3, 4], \n[2, 3, 4, 5], and [1, 2, 3, 4, 5].\n▶Example:  arr = [1, 3, 4, 6, 7, 9]\nOutput:  0. There are no numbers with remainder 2 when divided by 3.\nPROBLEM 38.25 \n COUNT GOOD SUBARRAYS WITH AT LEAST K SALES\nGiven an array, sales, and a positive integer k, return the number of subarrays with no bad days and at least \nk total sales (bad days are days with fewer than 10 sales).\n▶Example: arr = [15, 20, 5, 30, 25], k = 50\nOutput:  1. The subarrays with no bad days are [15], [15, 20], [20], [30], \n[30, 25], and [25]. Of those, only [30, 25] has at least 50 sales.\nPROBLEM SET SOLUTIONS\nSOLUTION 38.22  COUNT SUBARRAYS WITH DROPS\nThis is a direct application of the techniques we discussed.\n1 \ndef count_at_most_k_drops(arr, k):\n2 \n  l, r = 0, 0\n534 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\n3 \n  window_drops = 0\n4 \n  count = 0\n5 \n  while r < len(arr):\n6 \n    can_grow = r == 0 or arr[r] >= arr[r-1] or window_drops < k\n7 \n    if can_grow:\n8 \n      if r > 0 and arr[r] < arr[r-1]:\n9 \n        window_drops += 1\n10 \n      r += 1\n11 \n      count += r - l\n12 \n    else:\n13 \n      if arr[l] > arr[l+1]:\n14 \n        window_drops -= 1\n15 \n      l += 1\n16 \n  return count\n17 \n18 \ndef count_exactly_k_drops(arr, k):\n19 \n  if k == 0:\n20 \n    return count_at_most_k_drops(arr, 0)\n21 \n  return count_at_most_k_drops(arr, k) - count_at_most_k_drops(arr, k-1)\n22 \n23 \ndef count_at_least_k_drops(arr, k):\n24 \n  n = len(arr)\n25 \n  total_count = n*(n+1)//2\n26 \n  if k == 0:\n27 \n    return total_count\n28 \n  return total_count - count_at_most_k_drops(arr, k-1)\nSOLUTION 38.23  COUNT SUBARRAYS WITH BAD DAYS IN RANGE\nThe same idea for Exactly-K counting problems also applies to a range:10\n1 \ndef count_bad_days_range(sales, k1, k2):\n2 \n  if k1 == 0:\n3 \n    return count_at_most_k_bad_days(sales, k2)\n4 \n  return count_at_most_k_bad_days(sales, k2) - \n5 \n           count_at_most_k_bad_days(sales, k1-1)\nSOLUTION 38.24  COUNT SUBARRAYS WITH ALL REMAINDERS\nEvery number has a remainder of 0, 1, or 2 when divided by 3. The count of subarrays that have a number \nfrom each group is equal to the total number of subarrays (n*(n+1)/2) minus the subarrays that have \nnumbers from at most 2 of those groups. For the latter, we use the at-most-k counting technique.\n1 \ndef count_all_3_groups(arr):\n2 \n  n = len(arr)\n3 \n  total_count = n * (n + 1) // 2\n4 \n  return total_count - count_at_most_2_groups(arr)\n5 \n6 \ndef count_at_most_2_groups(arr):\n7 \n  l, r = 0, 0\n8 \n  window_counts = {}\n9 \n  count = 0\n10 \n  while r < len(arr):\n11 \n    can_grow = arr[r] % 3 in window_counts or len(window_counts) < 2\n10  This is the same logic behind how we use pre!x sums to answer range sum queries (pg 612).\nCHAPTER 38 ▸ SLIDING WINDOWS \n535\n12 \n    if can_grow:\n13 \n      if not arr[r] % 3 in window_counts:\n14 \n        window_counts[arr[r] % 3] = 0\n15 \n      window_counts[arr[r] % 3] += 1\n16 \n      r += 1\n17 \n      count += r - l\n18 \n    else:\n19 \n      window_counts[arr[l] % 3] -= 1\n20 \n      if window_counts[arr[l] % 3] == 0:\n21 \n        del window_counts[arr[l] % 3]\n22 \n      l += 1\n23 \n  return count\nWe could have made the code a bit more eﬃcient by replacing the dictionary with an array of length 3, since \nthe set of keys that we are using is {0, 1, 2}.\nSOLUTION 38.25  COUNT GOOD SUBARRAYS WITH AT LEAST K SALES\nWe can use the 'break down the problem' booster:\n1. First, we can use a resetting window to !nd all maximal subarrays without any bad days. Then, we can \nfocus on each subarray we found without worrying about bad days.\n2. For each subarray, sub, from Step 1, we need to count the number of subarrays of sub with at least k \ntotal sales. To do this, we can use the trick for At-Least-K counting: counting the total number of subar-\nrays in sub and subtracting the number of subarrays in sub with at most k-1 total sales.\nKEY TAKEAWAYS\nIf you want to try using a sliding window, your !rst goal should be identifying the constraint and the objective. \nBased on those, you can choose the most appropriate window type: see the !xed-length window recipe (pg \n513), the resetting window recipe (pg 516), the maximum window recipe (pg 520), and the minimum \nwindow recipe (pg 526).\nOnce you choose a recipe, the next question is:\n \n▶\nWhat information do I need about the window to check its validity and evaluation eﬃciently?\nBased on the answer, we will pick which window data structures to maintain as we slide the window (and \nremember, if you can't !nd appropriate window data structures, you can always try the binary search guess-\nand-check technique (pg 338).\nFinally, we implement the sliding window. We recommend following some conventions such as those \ndescribed on page 511. Here are some edge cases to keep an eye for:\n \n•\nMake sure to consider the case where no valid window is found, especially for minimization problems.\n \n•\nMake sure not to shrink an empty window or grow a window past the end of the array.\nEven better, update your own personal Bug List (pg 172) with the edge cases that you tend to forget about.\nCounting problems are less common, but we can use the trick to adapt the maximum window recipe to \nAt-Most-K counting problems (pg 530). For Exactly-K/At-Least-K counting problems, see the reusable idea \nfor transforming them to At-Most-K problems (pg 532).\nFor maximization problems without the maximum window property (pg 523) or minimization problems \nwithout the minimum window property (pg 526), sliding windows may be the wrong technique! Consider \nother techniques commonly used on subarray problems, like two pointers, pre!x sums, and dynamic \nprogramming.\n536 \nBEYOND CRACKING THE CODING INTERVIEW ▸ CATALOG OF TECHNICAL TOPICS\nView online materials for Beyond Cracking the Coding Interview at bctci.co\nSliding window triggers: The input type is just an array of numbers or a string, and maybe a \nnumber. The lower bound is linear. \nKeywords: subarray, substring, length, contiguous, consecutive, range, longest, or shortest.\nAt this point, you should be ready to start adding sliding window problems to your practice rotation. You can \n! nd the problems in this chapter and additional problems in the companion AI interviewer.\nONLINE RESOURCES\nOnline resources for this chapter include:\n•\nA chance to try each problem in this chapter in AI Interviewer\n•\nInterview replays that show speci! c mistakes people make with  sliding windows \nproblems\n•\nFull code solutions for every problem in the chapter in multiple programming \nlanguages\nTry online at  bctci.co/sliding-windows.",
      "content_type": "book",
      "source_url": "",
      "author": "Nil Mamano",
      "user_id": ""
    }
  ]
}